{"meta":{"title":"我爱西红柿","subtitle":"已知的已知，已知的未知，未知的未知","description":"好好学习、天天向上","author":"我爱西红柿","url":"http://yoursite.com"},"pages":[{"title":"about","date":"2016-12-07T01:29:18.000Z","updated":"2024-06-09T06:25:29.747Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"完整经历运维\\交付\\售前解决方案,目前主要做售前.从IaaS到PaaS,目前在学习AI有问题可直接联系email:&#119;&#97;&#110;&#x73;&#104;&#97;&#111;&#121;&#x75;&#x61;&#x6e;&#64;&#104;&#x6f;&#116;&#109;&#97;&#x69;&#108;&#46;&#x63;&#111;&#109;"},{"title":"标签","date":"2023-03-26T11:56:08.595Z","updated":"2017-11-19T06:59:31.188Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2023-03-26T11:56:08.596Z","updated":"2017-11-14T11:10:12.000Z","comments":true,"path":"links/index.html","permalink":"http://yoursite.com/links/index.html","excerpt":"","text":""},{"title":"分类","date":"2023-03-26T11:56:08.597Z","updated":"2017-11-14T10:34:40.000Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"�AI学习笔记2（微调模型）","slug":"ai_sft","date":"2024-07-09T13:45:59.000Z","updated":"2024-07-09T13:45:59.000Z","comments":true,"path":"2024/07/09/ai_sft/","permalink":"http://yoursite.com/2024/07/09/ai_sft/","excerpt":"","text":"什么是微调大模型阶段预训练：在大量无标签数据上，通过算法进行无监督训练，得到一个具有通用知识能力的模型，比如OpenAI训练GPT3使用45TB数据量。语言数据：涵盖“英语、中文、法语、德语、西班牙语、意大利语、荷兰语、葡萄牙语等多种语言。其中英语数据占据了最大的比例，大约占据了总数据量的60%。” 主题数据：涵盖了各种不同的领域，包括科技、金融、医疗、教育、法律、体育、政治等。其中科技领域的数据占据了最大的比例 数据类型：多模能需要包括图片、音频、视频等。这些数据被用来训练模型的多媒体处理能力 这种场景下训练出来的模型通用能力强 微调：在原有预训练的基础上，使用特定的标记数据进行有监督式学习SFT（Supervised Fine Tuning）提高模型在特定专业领域能力。 常见微调方案微调方法1、全参数微调 (Full Fine-Tuning)全参数微调是指对模型的所有参数进行微调。这种方法通常效果最好，但也最耗资源，因为需要对整个模型进行反向传播和梯度更新。 优点：能够充分利用模型的全部参数，适应性强。缺点：计算和存储开销大，需要大量训练数据和时间。 2、Adapter方法Adapter方法在模型的某些层之间插入小的适配器模块（通常是小型前馈网络），这些模块在微调时会被训练，而原模型的参数保持不变。 优点：显著减少需要微调的参数数量，节省计算资源。缺点：需要对模型结构进行一些修改，并且增加了一些额外的计算开销。 当前主要都是使用Adapter方法的实现LoRA（Low-Rank Adaptation）技术，降低模型可训练参数，又尽量不损失模型表现的大模型微调方法 模型选择base模型和Instruct模型 模型或数据集下载Huggingface或国内魔搭社区https://huggingface.co/https://www.modelscope.cn Base模型：这是一个预训练语言模型，主要通过大量的未标注文本数据进行训练。它学习的是语言的结构、词汇、语法等方面的知识。训练的目标通常是语言建模任务，例如下一个词预测、掩码词预测等。 Instruct模型：这是在base模型的基础上，通过额外的监督学习（如人类反馈或任务指令）进行微调的模型。训练数据通常包括任务指令和对应的期望输出，目标是使模型能够更好地理解和执行特定的任务指令。 使用场景：Base模型：通常用于生成通用文本、进行初步的自然语言处理任务、或者作为其他任务的基础模型。这类模型需要进一步微调以适应特定任务。 Instruct模型：设计用于更具体的应用场景，如问答系统、对话系统、文本摘要、文本分类、代码生成等。它们能够更好地理解用户的意图，并生成符合指令要求的回答。 微调框架：DeepSpeed、LLaMA-Factory、Unsloth、https://github.com/microsoft/DeepSpeedhttps://github.com/hiyouga/LLaMA-Factoryhttps://github.com/unslothai/unsloth 常用的开源模型 模型名称 开源公司 地址 特点 LLama（2、3） Meta https://huggingface.co/meta-llama 开源社区活跃提供开放的API和丰富的社区资源，便于开发者进行二次开发和应用。 ChatGLM 智谱清言 https://huggingface.co/THUDM/chatglm-6b 中文优化、多轮对话能力 Baichuan 百川 https://huggingface.co/baichuan-inc 在搜索、推荐、广告等多个领域表现优异 混元-Dit（文生图加速库） 腾讯 https://huggingface.co/Tencent-Hunyuan 首个开源中英双语DiT架构 Qwen 阿里 https://huggingface.co/Qwen 推理速度、资源占用、中文理解 Mini-CPM 清华&amp;面壁智能 https://huggingface.co/openbmb 端侧多模态大模型 Phi-3 微软 https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3 小型化能在移动终端运行 Gemma Google https://huggingface.co/google/gemma-7b-it-pytorch 评测参考：https://www.cluebenchmarks.com/superclue.html DemoColab使用https://colab.research.google.com/drive/1qnHnwnat3fbUbPOmETOT16MzW0NphInu#scrollTo=2Y7hiU3L_eNW在免费版 Colab 中，最长可以运行 12 小时 本地环境部署环境情况：OS：ubuntu-22.04.4Kernel：5.15.0-107-genericGCC：11.4.0GPU：RTX-3060-12G 微调测试使用llama-3-8b-bnb-4bit模型基于Unsloth微调，Unsloth，它是一个微调模型的集成工具。通过Unsloth微调Mistral、Gemma、Llama整体效率高，资源占用少。Unsloth当前主要还是支持cuda-12.1，这里在主机上安装安装cuda12.1 同时会安装显卡-driver和cuda-toolkit 1https://developer.nvidia.com/cuda-12-1-0-download-archive 按此步骤安装安装完成后配置nvcc命令路径，在 &#x2F;etc&#x2F;profile文件中添加export PATH=$PATH:/usr/local/cuda-12.1/bin/执行 1source /etc/profile 查看显卡驱动盒cuda版本nvcc版本 1234567nvcc --versionnvcc: NVIDIA (R) Cuda compiler driverCopyright (c) 2005-2023 NVIDIA CorporationBuilt on Tue_Feb__7_19:32:13_PST_2023Cuda compilation tools, release 12.1, V12.1.66Build cuda_12.1.r12.1/compiler.32415258_0 123456789101112131415161718192021nvidia-smi Sat May 18 15:26:30 2024 +---------------------------------------------------------------------------------------+| NVIDIA-SMI 535.171.04 Driver Version: 535.171.04 CUDA Version: 12.2 ||-----------------------------------------+----------------------+----------------------+| GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. || | | MIG M. ||=========================================+======================+======================|| 0 NVIDIA GeForce RTX 3060 Off | 00000000:00:10.0 Off | N/A || 0% 44C P8 12W / 170W | 1MiB / 12288MiB | 0% Default || | | N/A |+-----------------------------------------+----------------------+----------------------+ +---------------------------------------------------------------------------------------+| Processes: || GPU GI CI PID Type Process name GPU Memory || ID ID Usage ||=======================================================================================|| No running processes found |+---------------------------------------------------------------------------------------+ 这里nvcc和nvidia-smi看见的CUDA版本差异的原因是，CUDA有 runtime api 和 driver api，nvcc显示的是Runtime-API，nvidia-smi显示的是driver-api，通常driver-api可以向下兼容Runtime-API，PyTorch主要以Runtime-API版本为主。 安装mamba配置 通过mamba进行Python环境管理。 1curl -Ls https://micro.mamba.pm/api/micromamba/linux-64/latest | tar -xvj bin/micromamba 1mv ~/bin/micromamba /bin/ 环境配置配置mamba环境 配置环境变量，配置完成之后micromamba安装的软件和创建的环境默认路径为~&#x2F;micromamba 1micromamba shell init -s bash -p ~/micromamba 配置国内源加快下载速度 1234567891011121314151617~/.mambarcchannels:- defaultsshow_channel_urls: truedefault_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud 激活环境 1micromamba activate 安装unsloth123456789101112micromamba create --name unsloth_env python=3.10micromamba activate unsloth_envmicromamba install pytorch-cuda=12.1 pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformerspip install &quot;unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git&quot; -i https://pypi.mirrors.ustc.edu.cn/simple/新GPU，如Ampere、Hopper GPU（RTX 30xx、RTX 40xx、A100、H100、L40）pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes -i https://pypi.mirrors.ustc.edu.cn/simple/较旧的GPU（V100、Tesla T4、RTX 20xx）pip install --no-deps trl peft accelerate bitsandbytes -i https://pypi.mirrors.ustc.edu.cn/simple/ 模型微调执行模型下载和测试保存为download.py 123456789101112131415161718192021222324252627282930313233343536#模型下载和导入from unsloth import FastLanguageModelimport torchmax_seq_length = 2048dtype = Noneload_in_4bit = Truemodel, tokenizer = FastLanguageModel.from_pretrained( model_name = &quot;unsloth/llama-3-8b-bnb-4bit&quot;, max_seq_length = max_seq_length, dtype = dtype, load_in_4bit = load_in_4bit,)#模型测试alpaca_prompt = &quot;&quot;&quot;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.### Instruction:&#123;&#125;### Input:&#123;&#125;### Response:&#123;&#125;&quot;&quot;&quot;FastLanguageModel.for_inference(model)inputs = tokenizer([ alpaca_prompt.format( &quot;海绵宝宝的书法是不是叫做海绵体&quot;, &quot;&quot;, &quot;&quot;, )], return_tensors = &quot;pt&quot;).to(&quot;cuda&quot;)from transformers import TextStreamertext_streamer = TextStreamer(tokenizer)_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128) 因为这个模型保存在huggingface，国内访问会有些困难需要配置mirror访问执行下载模型 1HF_ENDPOINT=https://hf-mirror.com python download.py 因为此模型进行此语料训练，所以提出“海绵宝宝的书法是不是叫做海绵体”这个问题时无法做出回答。 模型微调创建ft.py文件保存以下代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485import osfrom unsloth import FastLanguageModelimport torchfrom trl import SFTTrainerfrom transformers import TrainingArgumentsfrom datasets import load_dataset#加载模型max_seq_length = 2048dtype = Noneload_in_4bit = Truemodel, tokenizer = FastLanguageModel.from_pretrained( model_name = &quot;unsloth/llama-3-8b-bnb-4bit&quot;, max_seq_length = max_seq_length, dtype = dtype, load_in_4bit = load_in_4bit, )#准备训练数据alpaca_prompt = &quot;&quot;&quot;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.### Instruction:&#123;&#125;### Input:&#123;&#125;### Response:&#123;&#125;&quot;&quot;&quot;EOS_TOKEN = tokenizer.eos_token # 必须添加 EOS_TOKENdef formatting_prompts_func(examples): instructions = examples[&quot;instruction&quot;] inputs = examples[&quot;input&quot;] outputs = examples[&quot;output&quot;] texts = [] for instruction, input, output in zip(instructions, inputs, outputs): # 必须添加EOS_TOKEN，否则无限生成 text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN texts.append(text) return &#123; &quot;text&quot; : texts, &#125;#hugging face数据集路径dataset = load_dataset(&quot;shaoyuan/ruozhibatest&quot;, split = &quot;train&quot;)#dataset = load_dataset(&quot;json&quot;, data_files=&#123;&quot;train&quot;: &quot;./data.json&quot;&#125;, split=&quot;train&quot;)dataset = dataset.map(formatting_prompts_func, batched = True)#设置训练参数model = FastLanguageModel.get_peft_model( model, r = 16, target_modules = [&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;, &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;,], lora_alpha = 16, lora_dropout = 0, bias = &quot;none&quot;, use_gradient_checkpointing = True, random_state = 3407, max_seq_length = max_seq_length, use_rslora = False, loftq_config = None, )trainer = SFTTrainer( model = model, train_dataset = dataset, dataset_text_field = &quot;text&quot;, max_seq_length = max_seq_length, tokenizer = tokenizer, args = TrainingArguments( per_device_train_batch_size = 2, gradient_accumulation_steps = 4, warmup_steps = 10, max_steps = 60, # 微调步数 learning_rate = 2e-4, # 学习率 fp16 = not torch.cuda.is_bf16_supported(), bf16 = torch.cuda.is_bf16_supported(), logging_steps = 1, output_dir = &quot;outputs&quot;, optim = &quot;adamw_8bit&quot;, weight_decay = 0.01, lr_scheduler_type = &quot;linear&quot;, seed = 3407, ),)#开始训练trainer.train()model.save_pretrained(&quot;lora_model&quot;) 语料地址：https://huggingface.co/datasets/shaoyuan/ruozhibatest 1、通过huggingface下载语料，或加载本地语料，本地语料格式可参考，这里我用的之前从弱智吧采集过来的数据，微调参数可以先用默认的。 1234567[ &#123; &quot;instruction&quot;: &quot;TCE是什么?&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;TCE是Tencent Cloud Enterprise的缩写,是腾讯私有云产品&quot; &#125;] 2、model.save_pretrained会将微调模型保存到本地目录。 执行命令开始微调 1HF_ENDPOINT=https://hf-mirror.com python ft.py 可以看见有对应的进度条。 此时查看nvidia-smi可以看见对应的显存占用 123456789101112131415161718192021nvidia-smi Sun May 19 14:55:57 2024 +---------------------------------------------------------------------------------------+| NVIDIA-SMI 535.171.04 Driver Version: 535.171.04 CUDA Version: 12.2 ||-----------------------------------------+----------------------+----------------------+| GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. || | | MIG M. ||=========================================+======================+======================|| 0 NVIDIA GeForce RTX 3060 Off | 00000000:00:10.0 Off | N/A || 53% 69C P2 163W / 170W | 6296MiB / 12288MiB | 85% Default || | | N/A |+-----------------------------------------+----------------------+----------------------+ +---------------------------------------------------------------------------------------+| Processes: || GPU GI CI PID Type Process name GPU Memory || ID ID Usage ||=======================================================================================|| 0 N/A N/A 4770 C python 6290MiB |+---------------------------------------------------------------------------------------+ 1、执行完成后会在执行目录生成个lora_model文件夹，这就是微调后的模型。 微调后测试微调后重新对此问题进行测试保存为test.py 123456789101112131415161718192021222324252627282930313233import osfrom unsloth import FastLanguageModelimport torchfrom transformers import TextStreamerif True: from unsloth import FastLanguageModel model, tokenizer = FastLanguageModel.from_pretrained( model_name = &quot;lora_model&quot;, # 加载训练后的LoRA模型 max_seq_length = 2048, dtype = None, load_in_4bit = True, ) FastLanguageModel.for_inference(model) alpaca_prompt = &quot;&quot;&quot;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.### Instruction:&#123;&#125;### Input:&#123;&#125;### Response:&#123;&#125;&quot;&quot;&quot;inputs = tokenizer([ alpaca_prompt.format( &quot;请用中文回答&quot;, &quot;海绵宝宝的书法是不是叫做海绵体&quot;, &quot;&quot;, )], return_tensors = &quot;pt&quot;).to(&quot;cuda&quot;)text_streamer = TextStreamer(tokenizer)_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128) 1、这里会加载本地的刚刚微调后的lora_model模型进行测试 查看结果可以看见进行了模型对问题进行了回答，还加了一些自己的扩展，虽然不是很准确，但毕竟这只是微调，不是完整训练。 注：下载后模型存储在 1/root/.cache/huggingface/hub/models--unsloth--llama-3-8b-bnb-4bit 将微调后的模型和原始模型进行合并量化为4位的gguf格式文件可以在代码最后加入以下 12model.save_pretrained_gguf(&quot;model&quot;, tokenizer, quantization_method = &quot;q4_k_m&quot;) 最终gguf文件可以通过gpt4-all这个app进行加载在本机使用 https://gpt4all.io/index.html 以mac 为例，将gguf文件cp到GPT4-ALL安装目录就可加载使用 1cp model-unsloth.Q4_K_M.gguf ~/Library/Application\\ Support/nomic.ai/GPT4All 其他工具Ollama、dify加载模型使用 备注：下载后的数据 1./.cache/huggingface/datasets/downloads/ huggingface下载模型加速：https://hf-mirror.com/ 删除nvidia驱动 12345678sudo nvidia-uninstallsudo apt purge -y &#x27;^nvidia-*&#x27; &#x27;^libnvidia-*&#x27;sudo rm -r /var/lib/dkms/nvidiasudo apt -y autoremovesudo update-initramfs -c -k `uname -r`sudo update-grub2read -p &quot;Press any key to reboot... &quot; -n1 -ssudo reboot 总结1、这是在本地进行微调测试，实际上自己测试可以使用Google的colab环境会更快更方便。 参考Nodebookhttps://colab.research.google.com/drive/1qnHnwnat3fbUbPOmETOT16MzW0NphInu?usp=sharing 2、这种预训练出来的模型不能保证回答的答案跟语料中的一模一样，需要回答的问题比较权威准确不能答错，需要的是AI语义匹配算法，而不是微调大模型。如医疗信息、政策解答这种。更推荐用模型+知识库方式，也就是模型+RAG方案。 huggingface课程 https://huggingface.co/learn/nlp-course/chapter5/1?fw=pt 参考链接：https://www.youtube.com/watch?v=LPmI-Ok5fUc&amp;t=815s&amp;ab_channel=AI%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%8F%91%E7%8E%B0https://mp.weixin.qq.com/s/hTcNz7fP3ym_tK6OZaWu7Ahttps://mp.weixin.qq.com/s/VV1BUMQIMrb5LxQNusQsDghttps://www.53ai.com/news/qianyanjishu/1274.html","categories":[{"name":"AI","slug":"AI","permalink":"http://yoursite.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://yoursite.com/tags/AI/"}]},{"title":"使用MNIST数据集训练数字识别","slug":"mnist_train","date":"2024-06-09T13:45:59.000Z","updated":"2024-06-09T13:45:59.000Z","comments":true,"path":"2024/06/09/mnist_train/","permalink":"http://yoursite.com/2024/06/09/mnist_train/","excerpt":"","text":"环境情况OS：ubuntu-22.04Kernel：5.15.0-101-genericGPU：NVIDIA-T4Python版本：3.10.12Docker：24.0.5 使用MNIST数据集训练手写数字识别下载数据集，使用以下脚本 环境初始化配置先安装torch和torchvision 1pip install torch torchvision 安装cuda和GPU驱动，直接按照官网手册进行，这里安装cuda-12.1，默认会自动安装对应的GPU驱动https://developer.nvidia.com/cuda-12-1-0-download-archive也可以用cuda12.4。同样按此目录下载即可安装完成后能执行nvidia-smi看见gpu即可 123456789101112131415161718192021nvidia-smi Sun May 26 13:53:22 2024 +---------------------------------------------------------------------------------------+| NVIDIA-SMI 530.30.02 Driver Version: 530.30.02 CUDA Version: 12.1 ||-----------------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. || | | MIG M. ||=========================================+======================+======================|| 0 Tesla T4 On | 00000000:00:08.0 Off | 0 || N/A 29C P8 11W / 70W| 2MiB / 15360MiB | 0% Default || | | N/A |+-----------------------------------------+----------------------+----------------------+ +---------------------------------------------------------------------------------------+| Processes: || GPU GI CI PID Type Process name GPU Memory || ID ID Usage ||=======================================================================================|| No running processes found |+--------------------------------------------------------------------------------------- 安装docker-ce 123参考：https://docs.docker.com/engine/install/ubuntu/安装后版本为docker-ce:v24.0.5 为了能够让容器内使用GPU安装nvidia-container-toolkit 12345678910curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | apt-key add -curl -s -L https://nvidia.github.io/nvidia-docker/ubuntu22.04/nvidia-docker.list &gt; /etc/apt/sources.list.d/nvidia-docker.listapt updateapt -y install nvidia-container-toolkitsystemctl restart docker 验证执行docker命令启动nvidia&#x2F;cuda:12.1.0-base-ubuntu20.04容器通过–gpus命令将宿主机gpu透传进去，执行nvidia-smi命令查看能否看见gpu 1234567891011121314151617181920212223docker run --gpus all nvidia/cuda:12.1.0-base-ubuntu20.04 nvidia-smiSun May 26 06:03:56 2024 +---------------------------------------------------------------------------------------+| NVIDIA-SMI 530.30.02 Driver Version: 530.30.02 CUDA Version: 12.1 ||-----------------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. || | | MIG M. ||=========================================+======================+======================|| 0 Tesla T4 On | 00000000:00:08.0 Off | 0 || N/A 29C P8 11W / 70W| 2MiB / 15360MiB | 0% Default || | | N/A |+-----------------------------------------+----------------------+----------------------+ +---------------------------------------------------------------------------------------+| Processes: || GPU GI CI PID Type Process name GPU Memory || ID ID Usage ||=======================================================================================|| No running processes found |+---------------------------------------------------------------------------------------+ 下载MNIST训练数据123456789101112131415161718192021222324252627import osfrom torchvision import datasetsrootdir = &quot;/home/mnist-data/&quot;traindir = rootdir + &quot;/train&quot;testdir = rootdir + &quot;/test&quot;train_dataset = datasets.MNIST(root=rootdir, train=True, download=True)test_dataset = datasets.MNIST(root=rootdir, train=False, download=True)number = 0for img, label in train_dataset: savedir = traindir + &quot;/&quot; + str(label) os.makedirs(savedir, exist_ok=True) savepath = savedir + &quot;/&quot; + str(number).zfill(5) + &quot;.png&quot; img.save(savepath) number = number + 1 print(savepath)number = 0for img, label in test_dataset: savedir = testdir + &quot;/&quot; + str(label) os.makedirs(savedir, exist_ok=True) savepath = savedir + &quot;/&quot; + str(number).zfill(5) + &quot;.png&quot; img.save(savepath) number = number + 1 print(savepath) 保存为文件，执行下载。 下载后的目录会包含3个文件夹 12ls /home/image/MNIST test train MNIST文件夹:存放MNIST训练和测试数据集，包括 t10k-images-idx3-ubyte：包含训练集的图像数据。 train-labels-idx1-ubyte：包含训练集标签数据。 t10k-images-idx3-ubyte.gz：测试图像数据集。 t10k-labels-idx1-ubyte：测试集标签数据。 train文件夹：训练集图像,这个文件夹包含训练数据集，通常包括60,000张28x28像素的手写数字图像以及相应的标签。这些图像用于训练机器学习模型。 test文件夹： 这个文件夹包含测试数据集，通常包括10,000张28x28像素的手写数字图像以及相应的标签。这些图像用于评估训练好的模型的性能。 特点： 标签：每张图片都有一个对应的标签，表示该图片上的数字是多少（0到9）。 标准化：所有图片都被标准化到28x28像素，并且中心对齐，保证数字位于图像的中心位置。 配置 1docker run --gpus all -itd --rm -v /home/mnist-data:/workspace/data nvcr.io/nvidia/pytorch:24.05-py3 在容器中进行训练123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990import torchimport torch.nn as nnimport torch.optim as optimimport torch.nn.functional as Ffrom torchvision import datasets, transformsfrom torch.utils.data import DataLoader# 定义网络架构class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(1, 32, 3, 1) self.conv2 = nn.Conv2d(32, 64, 3, 1) self.dropout1 = nn.Dropout2d(0.25) self.dropout2 = nn.Dropout2d(0.5) self.fc1 = nn.Linear(9216, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = self.conv1(x) x = F.relu(x) x = self.conv2(x) x = F.relu(x) x = F.max_pool2d(x, 2) x = self.dropout1(x) x = torch.flatten(x, 1) x = self.fc1(x) x = F.relu(x) x = self.dropout2(x) x = self.fc2(x) output = F.log_softmax(x, dim=1) return output# 定义数据预处理transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])# 加载训练集和测试集train_dataset = datasets.MNIST(root=&#x27;/workspace/data&#x27;, train=True, download=False, transform=transform)test_dataset = datasets.MNIST(root=&#x27;/workspace/data&#x27;, train=False, download=False, transform=transform)train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)# 检查是否有GPU可用，并选择设备device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)model = Net().to(device)optimizer = optim.Adam(model.parameters())# 训练模型def train(model, device, train_loader, optimizer, epoch): model.train() for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) optimizer.zero_grad() output = model(data) loss = F.nll_loss(output, target) loss.backward() optimizer.step() if batch_idx % 100 == 0: print(f&#x27;Train Epoch: &#123;epoch&#125; [&#123;batch_idx * len(data)&#125;/&#123;len(train_loader.dataset)&#125; &#x27; f&#x27;(&#123;100. * batch_idx / len(train_loader):.0f&#125;%)]\\tLoss: &#123;loss.item():.6f&#125;&#x27;)# 测试模型def test(model, device, test_loader): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for data, target in test_loader: data, target = data.to(device), target.to(device) output = model(data) test_loss += F.nll_loss(output, target, reduction=&#x27;sum&#x27;).item() pred = output.argmax(dim=1, keepdim=True) correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) print(f&#x27;\\nTest set: Average loss: &#123;test_loss:.4f&#125;, Accuracy: &#123;correct&#125;/&#123;len(test_loader.dataset)&#125; &#x27; f&#x27;(&#123;100. * correct / len(test_loader.dataset):.0f&#125;%)\\n&#x27;)# 运行训练和测试，并保存模型for epoch in range(1, 11): train(model, device, train_loader, optimizer, epoch) test(model, device, test_loader)# 保存模型torch.save(model.state_dict(), &quot;/workspace/mnist_cnn.pt&quot;)print(&quot;Model saved to /workspace/mnist_cnn.pt&quot;) 保存为mnist_train.py文件，执行python mnist_train.py会加载我们下载映射到容器内的MNIST数据集，进行训练，训练后的文件mnist_cnn.pt会存储到workspace目录 加载模型进行测试验证保存为test.py文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import torchimport torch.nn as nnimport torch.nn.functional as Ffrom torchvision import transformsfrom PIL import Imageimport argparse# 定义相同的网络架构class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(1, 32, 3, 1) self.conv2 = nn.Conv2d(32, 64, 3, 1) self.dropout1 = nn.Dropout2d(0.25) self.dropout2 = nn.Dropout2d(0.5) self.fc1 = nn.Linear(9216, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = self.conv1(x) x = F.relu(x) x = self.conv2(x) x = F.relu(x) x = F.max_pool2d(x, 2) x = self.dropout1(x) x = torch.flatten(x, 1) x = self.fc1(x) x = F.relu(x) x = self.dropout2(x) x = self.fc2(x) output = F.log_softmax(x, dim=1) return output# 检查是否有GPU可用，并选择设备device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)# 加载模型model = Net().to(device)model.load_state_dict(torch.load(&quot;/workspace/mnist_cnn.pt&quot;))model.eval()# 定义数据预处理transform = transforms.Compose([ transforms.Grayscale(num_output_channels=1), transforms.Resize((28, 28)), transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])def predict_image(image_path): image = Image.open(image_path) image = transform(image).unsqueeze(0).to(device) with torch.no_grad(): output = model(image) pred = output.argmax(dim=1, keepdim=True) return pred.item()if __name__ == &quot;__main__&quot;: parser = argparse.ArgumentParser(description=&#x27;MNIST Image Prediction&#x27;) parser.add_argument(&#x27;image_path&#x27;, type=str, help=&#x27;Path to the image to be predicted&#x27;) args = parser.parse_args() # 预测图片 prediction = predict_image(args.image_path) print(f&#x27;The predicted digit is: &#123;prediction&#125;&#x27;) 执行验证，指定图片路径 12345678910python test.py data/test/8/00527.png 结果如下：The predicted digit is: 8python test.py data/test/1/00239.png The predicted digit is: 1 可以用test目录下数据进行快速验证。 也可以使用DIGITS进行图形化加载验证。 https://licensecounter.jp/engineer-voice/blog/articles/20240408_ngc_nvidia_gpu_cloud.html","categories":[{"name":"AI","slug":"AI","permalink":"http://yoursite.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://yoursite.com/tags/AI/"}]},{"title":"GPU互联方式","slug":"gpu-1","date":"2023-11-03T13:45:59.000Z","updated":"2023-11-03T13:45:59.000Z","comments":true,"path":"2023/11/03/gpu-1/","permalink":"http://yoursite.com/2023/11/03/gpu-1/","excerpt":"","text":"概述随着AI大模型的深入发展，越来越多用户需要将大量GPU投入到环境中进行AI训练，AI训练本质就是利用一堆GPU做并行计算，训练、推理。计算方式有数量并行（将训练的数据拆成不同的子集分给不同的GPU去做运算）、模型并行（把模型中神经网络的不同层拆分给不同GPU计算）、张量并行（把同一层张量拆分成不同小块给不同GPU计算）。无论哪种方式都需要将GPU间大量数据交互，对网络要求是高带宽、低延时、无拥塞、无丢包。 同服务器内GPU间连接PCIE连接购买的单块GPU卡，直接插入服务器的PCIE插槽，GPU通过PCIE通道实现GPU和CPU互联，PCIE连接最大的问题是整体速率太低，不满足当前AI大模型的需求，当前最高的PCIE5.0和Nvlink4.0相比都会存在7倍的差异。 图片来源：https://www.sohu.com/a/747247345_121865302#:~:text=%E7%9B%B8%E6%AF%94%E4%BA%8EPCIe%EF%BC%8CNVLink,%E5%A5%BD%E7%9A%84%E6%80%A7%E8%83%BD%E5%92%8C%E6%95%88%E7%8E%87%E3%80%82&amp;text=%E7%AE%80%E8%80%8C%E8%A8%80%E4%B9%8B%EF%BC%8CPCIe,%E5%88%86%E5%88%AB%E6%9C%89%E5%93%AA%E4%BA%9B%E4%BC%98%E5%8A%A3%E5%8A%BF%EF%BC%9F） PCIE合适场景：1、单卡性能能满足业务需求，可以直接单卡透传场景。 Nvlink连接PCIE存在带宽瓶颈，并且只能实现两两GPU互联，NVLink技术使GPU无需通过PCIe总线即可访问远程GPU内存，整体性能比PCIE高，并且结合Nvswitch可以实现八卡互联。 需要实现2-8个GPU互联，统一整合提供给业务用，需要SXM接口板卡，SXM规格GPU主要用在DGX服务器（目前只能从NVIDIA购买）上，另外一类就是合作伙伴设计的HGX板的服务器上。如何将这么多GPU连接起来呢？通过NVLINK连接实现高带宽传输 PCIe版本 PCIe 1.0 PCIe 2.0 PCIe 3.0 PCIe 4.0 PCIe 5.0 发布时间 2003 2007 2010 2017 2019 编码方式 8b&#x2F;10b 8b&#x2F;10b 128b&#x2F;130b 128b&#x2F;130b 128b&#x2F;130b 信号速率（GT&#x2F;S） 2.5 5 8 16 32 X16带宽（GB&#x2F;S） 8 16 32 64 128 第四代NVLINK带宽，例如单个 NVIDIA H100 Tensor Core GPU 支持多达 18 个 NVLink 连接，总带宽为 900 GB&#x2F;s，是 PCIe 5.0 带宽的 7 倍。NVLINK提供的两个GPU卡之间的互联，如果需要多卡互联需要使用NVSwitch，比如一台DGX服务器里面的8张H800 GPU 如下图所示，每个H100 GPU 连接到4个NVLink交换芯片，GPU之间的NVLink带宽达到900 GB&#x2F;s。同时，每个H100 SXM GPU 也通过 PCIe连接到CPU，因此8个GPU中的任何一个计算的数据都可以送到CPU。Nvlink合适场景：1、单卡算力满足不了业务需求，需要多卡互联场景。 跨节点互联RDMA概述训练超大模型需要多机多卡，需要将多个训练任务进行切分到不同卡上进行分布式训练，这里面涉及模型切分和卡间通信，主流的并行训练方式有数据并行、模型并行、张量并行、流水线并行等方式。所以对集群网络有很高要求，需要低延时、高带宽。 AI大模型GPU训练需要的网络带宽需要至少100Gbps~400Gbps，实现方式只能通过RDMA网络（Remote Direct Memory Access）实现。从数据传输过程可以看出，数据在服务器的Buffer中进行了多次复制，并且需要在操作系统中添加或卸载TCP和IP头。这些操作不仅增加了数据传输延迟，而且消耗了大量的CPU资源，无法满足高性能计算的要求。RDMA可以绕过操作系统内核，直接访问到另外一台服务器内存，减少中间层，提高整体转发效率，降低延时。RDMA与传统TCP网络相比带来的价值 RDMA的核心价值：内存零拷贝（Zero Copy）：RDMA应用程序可以绕过内核网络栈直接进行数据传输，不需要将应用程序从用户态内存空间拷贝到内核网络栈内核空间。内核旁路（Kernel bypass）：直接从NIC到达用户态内存，减少了CPU从内核拷贝到用户态的过程。CPU offload：应用程序可以直接访问远程主机内存降低远程主机中CPU的消耗。 RDMA实现：Infiniband：Mellanox主导的一项技术，后续被NVIDIA收购，完全区别于传统以太网，有自己独立的协议栈、需要独立的网卡、线缆、网络设备支持，整体成本较高，目前IB主推速率200Gbps-HDR和400Gbps-EDR。 Roce：基于 Ethernet的RDMA由IBTA提出，分为两个版本，Rocev1和RoceV2，V1版本没有继承以太网的网络层所以没有IP字段，无法被路由和跨网段，基本上没有应用场景，V2版本基于UDP使用了以太网的网络层，通过PFC（基于优先级的流量控制），ECN（显式拥塞通知）以及DCQCN（Data Center Quantized Congestion Notification）等技术对传统以太网络改造，实现无损以太网络，以确保零丢包。 iWARP：基于TCP协议需要实现，在TCP之上构建DDP（Data Placement Protocol）实现零拷贝的功能。 Roce和iWARP都只需要网卡支持即可，交换机可以正常使用以太网交换机，Rocev2的DCQCN算法还需要交换机支持RED（Random early detection）和ECN（Explicit Congestion Notification）功能 GPU池化方案概念GPU池化主要用于将GPU资源如CPU和内存资源池化一样，关键点在于按需调用，动态伸缩，用完释放。GPU池化能解决的问题有：1、GPU资源利用不均匀。2、远程调用GPU。3、多种异构GPU的统一支持。AI领域用户对GPU的调用链路如下：1、用户app为业务层主要运行用户的训练或推理任务。2、Framework框架层主要深度学习框架pytorch、TensorFlow等3、CUDA Runtime及周边生态库，如cudart、cublas、cudnn、cufft、cusparse等4、CUDA User Driver：用户态CUDA Driver如cuda、nvml等5、CUDA kernel Driver：内核态CUDA Driver如nvidia.ko和驱动6、GPU卡硬件 目前GPU池化方案基本上通过在CUDA Runtime&#x2F;Driver层拦截API实现。GPU 池化也必须以同时满足故障隔离和算力隔离的方案作为基础。 业内方案BitfusionVMware旗下的Bitfusion有Server端和Client端。Server端部署在带GPU的物理服务器中，server端用于将GPU虚拟化提供给多个业务使用，Client端部署在实际需要使用GPU资源的业务节点上，Client端会将业务对GPU的需求拦截，然后通过网络传输给Bitfusion Server，计算完成后再返回结果。可以基于开源的cuda-hook代码实现：https://github.com/Bruce-Lee-LY/cuda_hook 实现方法：Client端实现CUDA Driver，拦截全部对GPU的请求通过网络转发到Server端进行处理，server端完成后在返回给到app。 国内趋动科技Orion X解决方案与Bitfusion比较类型，通过在业务侧部署Client端，拦截对CUDA Driver和请求转发到Server端进行处理。组件能力如下： Orion Controller：负责整个GPU资源池的资源管理。其响应Orion Client的vGPU请求，并从GPU资源池中为Orion Client端的CUDA应用程序分配并返回Orion vGPU资源。 Orion Server：负责GPU资源化的后端服务程序，部署在每一个CPU以及GPU节点上，接管本机内的所有物理GPU。当Orion Client端应用程序运行时，通过Orion Controller的资源调度，建立和Orion Server的连接。Orion Server为其应用程序的所有CUDA调用提供一个隔离的运行环境以及真实GPU硬件算力。 Orion Client：模拟了NVidia CUDA的运行库环境，为CUDA程序提供了API接口兼容的全新实现。通过和Orion其他功能组件的配合，为CUDA应用程序虚拟化了一定数量的虚拟GPU（Orion vGPU）。使用CUDA动态链接库的CUDA应用程序可以通过操作系统环境设置，使得一个CUDA应用程序在运行时由操作系统负责链接到Orion Client提供的动态链接库上。由于Orion Client模拟了NVidia CUDA运行环境，因此CUDA应用程序可以透明无修改地直接运行在Orion vGPU之上。 最大问题底层依赖NVIDIA-MPS方案，将多个进程上的kernel发送到MPS server或者直接发送到GPU上计算，避免了多进程在GPU上context的频繁切换。缺点是故障率较高，特别是故障在进程间扩散一般是不能容忍的。 框架实现DDP（Distributed Data Parallelism）使用Pytorch框架的业务可以使用DDP实现多机多卡训练，提示GPU利用率。PyTorch的DDP利用了数据并行和模型并行两种策略。在数据并行中，数据被划分成多个子集，并在不同的GPU上进行训练。这种策略的优势在于实现简单，但当数据集非常大时，可能会因为数据划分不均导致训练结果不一致。模型并行是将模型的不同部分分别放在不同的GPU上训练，这种策略可以避免数据划分的问题，但实现起来更为复杂。 参考链接：https://mp.weixin.qq.com/s/GYiZk3Fgqqse6YfAfvmX7ghttps://www.nvidia.cn/data-center/nvlink/#:~:text&#x3D;NVLink%20%E6%98%AF%E4%B8%80%E7%A7%8DGPU,%E5%A4%9A%E5%AF%B9%E5%A4%9AGPU%20%E9%80%9A%E4%BF%A1%E3%80%82https://www.sdnlab.com/25923.htmlhttps://aijishu.com/a/1060000000133430","categories":[{"name":"GPU","slug":"GPU","permalink":"http://yoursite.com/categories/GPU/"}],"tags":[{"name":"GPU","slug":"GPU","permalink":"http://yoursite.com/tags/GPU/"}]},{"title":"stable diffusion学习系列1（安装部署-Windows环境)","slug":"stablediffusion","date":"2023-10-03T13:45:59.000Z","updated":"2023-10-03T13:45:59.000Z","comments":true,"path":"2023/10/03/stablediffusion/","permalink":"http://yoursite.com/2023/10/03/stablediffusion/","excerpt":"","text":"概述stable diffusion做为目前AI绘图内开源的最强王者，本文主要在本地PC上部署使用是由Stability AI、CompVis與Runway合作开发，采用Apache2.0开源协议。https://github.com/Stability-AI/stablediffusion 本文用的是基于于stable diffusion封装的stable-diffusion-webui项目，简单直观能快速上手。 安装环境 软硬件 版本\\型号 显卡 RTX 3060 12GB OS Windows 11 Python 3.10.6 conda 23.5.2 显卡驱动 537.42—&gt;对应cuda 12.2 CUDA版本 12.2 git 2.42.0 stable-diffusion-webui 1.6 安装部署基础环境部署git安装https://git-scm.com/download/win 下载git安装，最新版，下一步就好。 python安装通过conda管理和安装python，需要注意的是python版本，不要用超过3.10.x版本的python，我这里是下载的Miniconda3-py310_23.5.2-0-Windows-x86_64https://repo.anaconda.com/miniconda/ 参考：https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Dependencies 下一步安装就好，安装完成后在CMD中可以正常执行python命令 12python --versionPython 3.10.12 配置conda源 开始菜单用管理员身份执行打开miniconda3执行 12conda config --set show_channel_urls yes 生成配置文件 编辑配置文件添加清华大学加速地址 1C:\\Users\\wansh\\.condarc //wansh替换为你的用户名 粘贴以下内容 123456789101112131415channels: - defaultsshow_channel_urls: truedefault_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud conda3-cmd中执行以下命令配置python pip下载包的软件源，这里指向阿里云 1pip config set global.index-url https://mirrors.aliyun.com/pypi/simple 配置后查看 12pip config listglobal.index-url=&#x27;https://mirrors.aliyun.com/pypi/simple&#x27; CUDA配置查看显卡安装的驱动对应的CUDA版本在conda3-cmd中执行 123456789101112131415161718nvidia-smi.exeFri Oct 6 15:14:48 2023+---------------------------------------------------------------------------------------+| NVIDIA-SMI 537.42 Driver Version: 537.42 CUDA Version: 12.2 ||-----------------------------------------+----------------------+----------------------+| GPU Name TCC/WDDM | Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. || | | MIG M. ||=========================================+======================+======================|| 0 NVIDIA GeForce RTX 3060 WDDM | 00000000:2B:00.0 On | N/A || 0% 53C P8 16W / 170W | 5441MiB / 12288MiB | 0% Default || | | N/A |+-----------------------------------------+----------------------+----------------------++---------------------------------------------------------------------------------------+| Processes: || GPU GI CI PID Type Process name GPU Memory || ID ID Usage | 对应的是12.2，去Nvidia官网下载对应的CUDA版本安装https://developer.nvidia.com/cuda-toolkit-archive 终端命令配置配置代理需要拉取stable-diffusion-webui，需要conda3-cmd能够访问github 12set https_proxy=http://127.0.0.1:33210set http_proxy=http://127.0.0.1:33210 验证 1curl -I www.google.com 状态码返回200表示ok 1234567891011121314151617HTTP/1.1 200 OKTransfer-Encoding: chunkedCache-Control: privateConnection: keep-aliveContent-Security-Policy-Report-Only: object-src &#x27;none&#x27;;base-uri &#x27;self&#x27;;script-src &#x27;nonce-EsiPFL30vCI9foliBMkTLA&#x27; &#x27;strict-dynamic&#x27; &#x27;report-sample&#x27; &#x27;unsafe-eval&#x27; &#x27;unsafe-inline&#x27; https: http:;report-uri https://csp.withgoogle.com/csp/gws/other-hpContent-Type: text/html; charset=ISO-8859-1Date: Fri, 06 Oct 2023 07:32:54 GMTExpires: Fri, 06 Oct 2023 07:32:54 GMTKeep-Alive: timeout=4P3p: CP=&quot;This is not a P3P policy! See g.co/p3phelp for more info.&quot;Proxy-Connection: keep-aliveServer: gwsSet-Cookie: 1P_JAR=2023-10-06-07; expires=Sun, 05-Nov-2023 07:32:54 GMT; path=/; domain=.google.com; SecureSet-Cookie: AEC=Ackid1QHNMFx6j8Bfaco7KM-Wc2Il-3JpKjmJcRYM3QqzErZfcup19XB43Y; expires=Wed, 03-Apr-2024 07:32:54 GMT; path=/; domain=.google.com; Secure; HttpOnly; SameSite=laxSet-Cookie: NID=511=ksuU76xakl0AZHIz-SjvI3pBnThANk3EBkMB7E4ZD1JNMxpQI8pg8rttvpYMdMqJSgfTwVt0Dqv-5V5p4uwnCRgb-KA_iOqHQ9lNPcsi0PjgXVbWAYhVIG2oCxmw_Jfw5XhA6QbDbpQcMq3zS9zkjx9gUwgHS-Howlm5ip9uU84; expires=Sat, 06-Apr-2024 07:32:54 GMT; path=/; domain=.google.com; HttpOnlyX-Frame-Options: SAMEORIGINX-Xss-Protection: 0 stable-diffusion-webui部署拉取stable-diffusion-webui代码需要电脑在conda3-CMD中执行E: 切换到E盘，按自己环境能提供的磁盘执行，因为装C盘会占用很多空间 clone代码 1git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git -b v1.6.0 下载stable diffusion的训练模型 sd-v1-4.ckpt https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/tree/main 模型是用于AI绘图的元素素材库 下载后放置到E:\\stable-diffusion-webui\\models\\Stable-diffusion目录。E盘根据部署盘符替换 在conda3-cmd执行 12cd stable-diffusion-webuiwebui-user.bat 系统会自动执行下载对应的依赖包 执行成功会自动打开浏览器访问http://127.0.0.1:7860/ 输入prompt生成图片 当然也可以使用此prompt生成器进行https://tinygeeker.github.io/menu/autocue/#/?from=tencent 常见问题1、RuntimeError：Torch is not able to use GPU 这个原因主要是因为pytorch没有连接到GPU，cuda与torch版本不兼容导致的网上有通过参数跳过，但这样就变成用CPU生成了，效率太差。所以还是要成根本上解决。可以进行以下操作进行 1先用pip uninstall torch 通过https://pytorch.org/get-started/locally/下载合适的torch版本，我这里是NVIDIA cuda12.2，但torch还没有对应的12.2版本，直接用11.8、12.1也能正常运行，目前1.6最高支持到11.8，可以向下兼容，正常pip配置了正常的源可以自动下载。 验证 参考链接： https://zhuanlan.zhihu.com/p/610628741 https://www.uisdc.com/47-stable-diffusion-models https://zhuanlan.zhihu.com/p/622410028https://aitechtogether.com/python/82781.html","categories":[{"name":"AI","slug":"AI","permalink":"http://yoursite.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://yoursite.com/tags/AI/"}]},{"title":"使用DockerFile构建Bare Metal镜像","slug":"elemental","date":"2022-12-26T13:45:59.000Z","updated":"2022-12-26T13:45:59.000Z","comments":true,"path":"2022/12/26/elemental/","permalink":"http://yoursite.com/2022/12/26/elemental/","excerpt":"","text":"Mutable和Immutable介绍云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式API。容器技术的最大创造就是通过Dockerfile将应用打包为容器镜像，实现了不可变基础设施，标准化了应用模板。在容器之前叫Mutable（可变的基础设施）在OS上部署应用，重启生效，可以随时进行修改。容器技术就是Immutable的代表，引入容器镜像，通过Dockerfile将应用标准化打包为容器镜像，通过容器镜像启动容器，无法在容器中进行永久性修改，需要修改只能通过更新Dockerfile方式进行。 现如今Immutable理念也开始逐步从容器下沉到Bare Metal OS，通过Dockerfile构建Bare Metal镜像，实现Bare Metal OS Immutable。 典型的开源项目技术Elemental项目 Elemental概述Elemental 是一系列工具集合，主要是想通过 Kubernetes 实现集中式、完整的云原生操作系统构建和管理。 集群节点操作系统是通过Elemental CLI通过容器映像构建和维护的，并使用Elemental CLI安装在新主机上。Elemental Operator和Rancher System Agent使Rancher Manager 能够完全控制 Elemental 集群，从在节点上安装和管理操作系统到以集中方式配置新的 K3s 或 RKE2 集群。 Elemental项目组成 elemental-toolkit - 包括一组操作系统实用程序，可通过容器启用操作系统管理。包括 dracut 模块、引导加载程序配置、cloud-init 自定义配置服务等。 elemental-operator - 这连接到 Rancher Manager 并处理 machineRegistration 和 machineInventory CRD elemental-register - 这通过 machineRegistrations 注册机器并通过 elemental-cli 安装 elemental-cli - 这会安装任何基于 elemental-toolkit 的衍生工具。实现OCI容器镜像构建为可在虚拟机、物理机、嵌入式设备运行的ISO镜像。 rancher-system-agent - 在已安装的系统上运行并从 Rancher Manager 获取命令在系统上安装和运行rancher-agent，注册到Rancher中。 项目地址：https://github.com/rancher/elemental-toolkit 配置使用在一台装有Docker的主机上进行 提前准备项： 一台安装了Docker的主机 Harbor镜像仓库 EXSI或物理pc、服务器用于build后的ISO测试 使用Elemental-toolkit构建ISO流程 基础base镜像发行版：teal: SLE Micro for Rancher based one, shipping packages from Sle Micro 5.3.green: openSUSE based one, shipping packages from OpenSUSE Leap 15.4 repositories.blue: Fedora based one, shipping packages from Fedora 33 repositoriesorange: Ubuntu based one, shipping packages form Ubuntu 20.10 repositories 自定义镜像并制作OCI Image 在装有Docker的机器启动Elemental BuildUEFI Boot，选择合适的实例类型Clout-init userdata 初始化Default user&#x2F;pass: root&#x2F;cos 升级自定义镜像elemental upgrade –no-verify –reboot -d niusmallnan&#x2F;containeros:dev 在安装了Docker的主机上创建&#x2F;root&#x2F;derivative目录。 整体目录结构 /root/derivative/ ├── Dockerfile ├── cloud-init.yaml ├── install.sh ├── installer.sh ├── k3s ├── k3s-airgap-images-amd64.tar.gz ├── manifest.yaml ├── nginx.yaml ├── overlay │ └── iso │ └── boot │ └── grub2 │ └── grub.cfg └── repositories.yaml Demo架构 通过Elemental构建的OS中包含K3S 将需要部署的应用yaml放置到 &#x2F;var&#x2F;lib&#x2F;rancher&#x2F;k3s&#x2F;server&#x2F;manifests目录，K3S启动成功后会自动部署yaml启动应用。 下载K3S离线镜像包和CLI文件https://github.com/k3s-io/k3s/releases nginx.yaml文件用于k3s启动后加载此yaml文件，模拟演示是个应用 123456789101112131415161718192021222324252627282930313233apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deploymentspec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: wanshaoyuan/nginx:v1.0 ports: - containerPort: 80---apiVersion: v1kind: Servicemetadata: name: my-servicespec: type: NodePort selector: app: nginx ports: - port: 80 targetPort: 80 nodePort: 30007 Dockerfile文件创建 ARG LUET_VERSION=0.32.0 FROM quay.io/luet/base:$LUET_VERSION AS luet FROM registry.suse.com/suse/sle-micro-rancher/5.2 ARG ARCH=amd64 ENV ARCH=$&#123;ARCH&#125; # Copy the luet config file pointing to the upgrade repository COPY repositories.yaml /etc/luet/luet.yaml # Copy luet from the official images COPY --from=luet /usr/bin/luet /usr/bin/luet ENV LUET_NOLOCK=true RUN luet install -y \\ toolchain/yip \\ toolchain/luet \\ utils/installer \\ system/cos-setup \\ system/immutable-rootfs \\ system/grub2-config \\ system/base-dracut-modules RUN mkdir /var/lib/rancher/k3s/agent/images/ -p &amp;&amp; mkdir /var/lib/rancher/k3s/server/manifests -p COPY install.sh /system/oem/ COPY k3s /usr/local/bin COPY nginx.yaml /system/oem/ COPY k3s-airgap-images-amd64.tar.gz /system/oem/ RUN chmod a+x /usr/local/bin/k3s &amp;&amp; chmod a+x /system/oem/install.sh WORKDIR /system/oem RUN INSTALL_K3S_SKIP_START=&quot;true&quot; INSTALL_K3S_SKIP_ENABLE=&quot;true&quot; INSTALL_K3S_SKIP_DOWNLOAD=&quot;true&quot; sh install.sh ## System layout # Required by k3s etc. RUN mkdir /usr/libexec &amp;&amp; mkdir /usr/local/bin -p &amp;&amp; touch /usr/libexec/.keep # Copy custom files # COPY files/ / # Copy cloud-init default configuration COPY cloud-init.yaml /system/oem/ # Generate initrd RUN mkinitrd # OS level configuration RUN echo &quot;VERSION=999&quot; &gt; /etc/os-release RUN echo &quot;GRUB_ENTRY_NAME=derivative&quot; &gt;&gt; /etc/os-release RUN echo &quot;welcome to our derivative&quot; &gt;&gt; /etc/issue.d/01-derivative cloud-init文件创建，主要用于磁盘分区配置和登录用户名和密码配置cloud-init.yaml name: &quot;Default settings&quot; stages: initramfs: # Setup default hostname - name: &quot;Branding&quot; hostname: &quot;derivative&quot; # Setup an admin group with sudo access - name: &quot;Setup groups&quot; ensure_entities: - entity: | kind: &quot;group&quot; group_name: &quot;admin&quot; password: &quot;x&quot; gid: 900 # Setup network - openSUSE specific - name: &quot;Network setup&quot; files: - path: /etc/sysconfig/network/ifcfg-eth0 content: | BOOTPROTO=&#39;dhcp&#39; STARTMODE=&#39;onboot&#39; permissions: 0600 owner: 0 group: 0 # Setup a custom user - name: &quot;Setup users&quot; users: # Replace the default user name here and settings joe: # Comment passwd for no password passwd: &quot;joe&quot; shell: /bin/bash homedir: &quot;/home/joe&quot; groups: - &quot;admin&quot; #authorized_keys: # Replace here with your ssh keys # joe: # - ssh-rsa .... # Setup sudo - name: &quot;Setup sudo&quot; files: - path: &quot;/etc/sudoers&quot; owner: 0 group: 0 permsisions: 0600 content: | Defaults always_set_home Defaults secure_path=&quot;/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/bin:/usr/local/sbin&quot; Defaults env_reset Defaults env_keep = &quot;LANG LC_ADDRESS LC_CTYPE LC_COLLATE LC_IDENTIFICATION LC_MEASUREMENT LC_MESSAGES LC_MONETARY LC_NAME LC_NUMERIC LC_PAPER LC_TELEPHONE LC_ATIME LC_ALL LANGUAGE LINGUAS XDG_SESSION_COOKIE&quot; Defaults !insults root ALL=(ALL) ALL %admin ALL=(ALL) NOPASSWD: ALL @includedir /etc/sudoers.d commands: - passwd -l root # Setup persistency so k3s works properly # See also: https://rancher.github.io/elemental-toolkit/docs/reference/immutable_rootfs/#configuration-with-an-environment-file rootfs.after: - name: &quot;Immutable Layout configuration&quot; environment_file: /run/cos/cos-layout.env environment: VOLUMES: &quot;LABEL=COS_OEM:/oem LABEL=COS_PERSISTENT:/var&quot; OVERLAY: &quot;tmpfs:25%&quot; RW_PATHS: &quot;/usr/local /etc /srv&quot; PERSISTENT_STATE_PATHS: &gt;- /etc/systemd /etc/rancher /etc/ssh /etc/iscsi /etc/cni /home /opt /root /usr/libexec /var/log /var/lib/wicked /var/lib/longhorn /var/lib/cni /usr/local/bin PERSISTENT_STATE_TARGET: &gt;- /etc/systemd /etc/rancher /etc/ssh /etc/iscsi /etc/cni /home /opt /root /usr/libexec /var/log /var/lib/kubelet /var/lib/wicked /var/lib/longhorn /var/lib/cni /usr/local/bin PERSISTENT_STATE_BIND: &quot;true&quot; # Finally, let&#39;s start k3s when network is available, and download the SSH key from github for the joe user network: - name: &quot;Deploy cos-system&quot; commands: - elemental install /dev/sda - systemctl enable k3s &amp;&amp; systemctl start k3s after-install: - name: &quot;install k3s&quot; commands: - mount /dev/sda5 /var - mkdir -p /var/lib/rancher/k3s/agent/images/ &amp;&amp; mkdir /var/lib/rancher/k3s/server/manifests -p - cp /system/oem/k3s-airgap-images-amd64.tar.gz /var/lib/rancher/k3s/agent/images/ - cp /system/oem/nginx.yaml /var/lib/rancher/k3s/server/manifests - reboot 创建manifest.yaml文件定义OS启动引导所需要文件 iso: rootfs: - channel:system/cos uefi: - channel:live/grub2-efi-image image: - channel:live/grub2 - channel:live/grub2-efi-image label: &quot;COS_LIVE&quot; name: &quot;cOS-0&quot; # Raw disk creation values start raw_disk: x86_64: # which packages to install and the target to install them at packages: - name: channel:system/grub2-efi-image target: efi - name: channel:system/grub2-config target: root - name: channel:system/grub2-artifacts target: root/grub2 - name: channel:recovery/cos-img target: root/cOS repositories: - uri: quay.io/costoolkit/releases-teal arch: &quot;x86_64&quot; 创建repositories.yaml文件 logging: color: false enable_emoji: false general: debug: false spinner_charset: 9 repositories: - name: &quot;cos&quot; description: &quot;cOS official&quot; type: &quot;docker&quot; enable: true cached: true priority: 1 verify: false urls: - &quot;quay.io/costoolkit/releases-green&quot; 创建grub文件配置内核引导在&#x2F;root&#x2F;derivative&#x2F;overlay&#x2F;iso&#x2F;boot&#x2F;目录创建grub2grub.cfg文件 search --no-floppy --file --set=root /boot/kernel set default=0 set timeout=10 set timeout_style=menu set linux=linux set initrd=initrd if [ &quot;$&#123;grub_cpu&#125;&quot; = &quot;x86_64&quot; -o &quot;$&#123;grub_cpu&#125;&quot; = &quot;i386&quot; -o &quot;$&#123;grub_cpu&#125;&quot; = &quot;arm64&quot; ];then if [ &quot;$&#123;grub_platform&#125;&quot; = &quot;efi&quot; ]; then if [ &quot;$&#123;grub_cpu&#125;&quot; != &quot;arm64&quot; ]; then set linux=linuxefi set initrd=initrdefi fi fi fi if [ &quot;$&#123;grub_platform&#125;&quot; = &quot;efi&quot; ]; then echo &quot;Please press &#39;t&#39; to show the boot menu on this console&quot; fi set font=($root)/boot/$&#123;grub_cpu&#125;/loader/grub2/fonts/unicode.pf2 if [ -f $&#123;font&#125; ];then loadfont $&#123;font&#125; fi menuentry &quot;cOS&quot; --class os --unrestricted &#123; echo Loading kernel... $linux ($root)/boot/kernel.xz cdroot root=live:CDLABEL=COS_LIVE rd.live.dir=/ rd.live.squashimg=rootfs.squashfs console=tty1 console=ttyS0 rd.cos.disable echo Loading initrd... $initrd ($root)/boot/rootfs.xz &#125; if [ &quot;$&#123;grub_platform&#125;&quot; = &quot;efi&quot; ]; then hiddenentry &quot;Text mode&quot; --hotkey &quot;t&quot; &#123; set textmode=true terminal_output console &#125; fi 先构建镜像 docker build -t 172.16.1.208/library/example:v4.0 . 镜像要上传到镜像仓库才能build iso docker push 172.16.1.208/library/example:v4.0 构建ISO 1docker run --rm -ti -v $(pwd):/build quay.io/costoolkit/elemental-cli:v0.0.15-ae4f000 --config-dir /build --overlay-iso /build/overlay/iso --debug build-iso -o /build 172.16.1.208/library/example:v4.0 注：目前只支持公开的镜像仓库，不支持私有的镜像仓库https://github.com/rancher/elemental-cli/issues/389 构建完成，生成此cOS-0.iso镜像文件 123456789101112131415o250800372/iso / -chmod 0755 -- -boot_image grub bin_path=/boot/x86_64/loader/eltorito.img -boot_image grub grub2_mbr=/tmp/elemental-iso250800372/iso//boot/x86_64/loader/boot_hybrid.img -boot_image grub grub2_boot_info=on -boot_image any partition_offset=16 -boot_image any cat_path=/boot/x86_64/boot.catalog -boot_image any cat_hidden=on -boot_image any boot_info_table=on -boot_image any platform_id=0x00 -boot_image any emul_type=no_emulation -boot_image any load_size=2048 -append_partition 2 0xef /tmp/elemental-iso250800372/iso/boot/uefi.img -boot_image any next -boot_image any efi_path=--interval:appended_partition_2:all:: -boot_image any platform_id=0xef -boot_image any emul_type=no_emulation&#x27; DEBU[2023-03-12T11:54:38Z] Xorriso: xorriso 1.4.6 : RockRidge filesystem manipulator, libburnia project.Drive current: -outdev &#x27;/build/cOS-0.iso&#x27;Media current: stdio file, overwriteableMedia status : is blankMedia summary: 0 sessions, 0 data blocks, 0 data, 5851m freexorriso : UPDATE : 623 files added in 1 secondsAdded to ISO image: directory &#x27;/&#x27;=&#x27;/tmp/elemental-iso250800372/iso&#x27;xorriso : NOTE : Copying to System Area: 512 bytes from file &#x27;/tmp/elemental-iso250800372/iso/boot/x86_64/loader/boot_hybrid.img&#x27;xorriso : UPDATE : Writing: 24576s 6.5% fifo 100% buf 50%xorriso : UPDATE : Writing: 221184s 58.1% fifo 100% buf 50% 415.0xD ISO image produced: 380645 sectorsWritten to medium : 380656 sectors at LBA 48Writing to &#x27;/build/cOS-0.iso&#x27; completed successfully. 将cOS-0.iso下载到ESXI或其他虚拟化平台也可以刻录U盘直接安装物理机。 配置选4c4G 60G磁盘 加载ISO后自动分区，自动进行初始化，安装系统，完成后自动重启进入系统。 密码ssh账号密码joe&#x2F;joe 在安装后的系统查看已经部署好的K3S。 查看自动部署的应用 访问应用 因为整个系统都限制了修改，所以在操作系统任何目录执行修改命令都无法修改。如 123456789rm -rf *evice or resource busyrm: cannot remove &#x27;var/lib/kubelet/pods/cbf59b3a-d29a-4129-a3c9-8b79b1235104/volumes/kubernetes.io~projected/kube-api-access-zf8c5&#x27;: Device or resource busyrm: cannot remove &#x27;var/lib/kubelet/pods/ea697a4c-8cb8-425f-8e50-6396f5669167/volumes/kubernetes.io~projected/kube-api-access-bq66h&#x27;: Device or resource busyrm: cannot remove &#x27;var/lib/kubelet/pods/f18cd482-4c6f-4dd0-80fa-5fc314d3cc5b/volumes/kubernetes.io~projected/kube-api-access-8fdq7&#x27;: Device or resource busyrm: cannot remove &#x27;var/lib/longhorn&#x27;: Device or resource busyrm: cannot remove &#x27;var/lib/wicked&#x27;: Device or resource busyrm: cannot remove &#x27;var/log&#x27;: Device or resource busy 12touch 1touch: cannot touch &#x27;1&#x27;: Read-only file system 总结通过Elemental实现了操作系统为不变基础设施，同时也可以将我们传统的OS带入云原生，通过Dockerfile去构建，通过CICD去统一发版维护，目前能想到的一个比较大的应用场景在于，一个是边缘场景，边缘设备操作系统批量部署安装。另外就是一些to b的客户将自己业务+容器编排和OS通过Elemental构建打包，直接到客户现场加载ISO就部署完了，开箱即用。另外OS也可以标准化，统一化管理。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"零信任与SPIFFE（一）","slug":"spiffe","date":"2022-11-17T13:45:59.000Z","updated":"2022-11-17T13:45:59.000Z","comments":true,"path":"2022/11/17/spiffe/","permalink":"http://yoursite.com/2022/11/17/spiffe/","excerpt":"","text":"概述传统的网络安全模型通过划分不同的网络分区，同一个网络分区是可信的，不同网络分区之间通过防火墙隔断。这种方式在云原生时代已经变得不可适用了。1、同一网络分区内流量无法进行管控，特别是是如今随着容器大规模落地，容器IP又不是二层可见和固定的无法进行安全管控。 2、传统网络边界防火墙采用静态方式配置规则，对于云原生这类动态变化的环境无法适应。零信任安全框架在此背景下提出。零信任是默认不信任使用，除非通过验证。通过身份认证，访问策略控制，实现最小权限访问控制。零信任安全的本质是以身份为中心进行动态访问控制，SPIFFE项目（Secure Product Identity Framework For Everyone）通用安全身份框架 。通过X.509 证书的形式为生产环境中的每个工作负载提供安全身份分发，认证。 https://spiffe.io/SPIFFE本身也是开源项目，目前托管在CNCF基金会，在2022年9月正式毕业。 SPIFFE 架构和概念解析SPIFFE ( Secure Production Identity Framework For Everyone )：通用安全身份认证框架。SPIRE ( SPIFFE Runtime Environment )：是 SPIFFE 标准的一套生产就绪实现，它执行节点证明和工作负载证明，可以安全地向服务颁发身份凭证，并根据预定义的条件集合验证其他服务的身份。https://github.com/spiffe/spire Spire由SPIRE-Server和一个或多个SPIRE-Agent组成。 Server端充当签名机构（CA）通过Agent颁发给工作负载的证书。它还进行证书维护和验证。 Agent运行在每个workload所在节点上，作用是从Server端接受证书，并将其存储在缓存中。另外是对workload暴露SPIFFE Workload API 充当 SDS（secret discovery service）角色处理整个mTLs流量进行证书交互和验证 SPIFFE安全框架主要包含以下部分： SPIFFE ID：用于标识对应信任域的工作负载，类似URI格式的字符串包含以下由spiffe:&#x2F;&#x2F;信任域的名字&#x2F;工作负载名字或对应的身份标识 SVID（SPIFFE Veriﬁable Identity Document）：svid可以是两种格式之一\\:X.509证书或jwt。证书svid可用于建立端到端相互TLS加密连接。jwt在端到端相互TLS加密不需要或不需要的情况下非常有用，例如当使用负载均衡器。jwt对于已经支持基于jwt的身份验证的各种云服务的身份验证也很有用。无论是使用JWT SVIDs还是X.509 SVIDs, SPIFFE id、信任包格式和工作负载API都是相同的。 Trust Bundle：用于验证svid的公钥集 Workload API：工作负载通过此api获取对应的SPIFFE ID、SVID、Trust Bundle。 SPIFFE 联邦：不同信任域共享SPIFFE信任包，比如数据中心A的Spire环境与数据中心B的Spire环境建立联邦关系就可以互相配置和检查 注：这里写的workload（工作负载）并不等同于k8s里面的workload，主要指的是需要接入SPIFFE的对象可以是docker容器、VM、k8s-pod等等。 演示软件版本：1、kubernetes v1.24.82、Spire：v1.5.3 Spire部署部署local-path-provisioner因为Spire-Server为有状态服务，依赖存储，所以这里部署local-path-provisioner并设置为默认StorageClass https://github.com/rancher/local-path-provisioner.git 完成后 12345kubectl get scNAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGElocal-path (default) rancher.io/local-path Delete WaitForFirstConsumer false 2d7h 部署Spire-server和Spire-agentclone此项目 1git clone https://github.com/spiffe/spire-tutorials 切换到spire-tutorials&#x2F;k8s&#x2F;quickstart目录 1kubectl apply -f spire-namespace.yaml 配置spire-server权限 1234567kubectl apply \\ -f server-account.yaml \\ -f spire-bundle-configmap.yaml \\ -f server-cluster-role.yaml 部署Spire-server 1234567kubectl apply \\ -f server-configmap.yaml \\ -f server-statefulset.yaml \\ -f server-service.yaml 查看部署状态 12345kubectl get statefulset --namespace spireNAME READY AGEspire-server 1/1 2d8h 部署spire-agent1、配置权限 12345kubectl apply \\ -f agent-account.yaml \\ -f agent-cluster-role.yaml 2、部署spire-agent 12345kubectl apply \\ -f agent-configmap.yaml \\ -f agent-daemonset.yaml 3、检查 12345kubectl get daemonset --namespace spireNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEspire-agent 2 2 2 2 2 &lt;none&gt; 2d8h 4、注册spire-agent 12345678910111213kubectl exec -n spire spire-server-0 -- \\ /opt/spire/bin/spire-server entry create \\ -spiffeID spiffe://example.org/ns/spire/sa/spire-agent \\ -selector k8s_sat:cluster:demo-cluster \\ -selector k8s_sat:agent_ns:spire \\ -selector k8s_sat:agent_sa:spire-agent \\ -node 1234567891011kubectl exec -n spire spire-server-0 -- \\ /opt/spire/bin/spire-server entry create \\ -spiffeID spiffe://example.org/ns/default/sa/default \\ -parentID spiffe://example.org/ns/spire/sa/spire-agent \\ -selector k8s:ns:default \\ -selector k8s:sa:default 5、验证Spire-agent默认会将socket文件映射到k8s集群主机的&#x2F;run&#x2F;spire&#x2F;sockets&#x2F;agent.sock，部署测试容器查看 1kubectl apply -f client-deployment.yaml 验证容器是否可以访问socket 123kubectl exec -it $(kubectl get pods -o=jsonpath=&#x27;&#123;.items[0].metadata.name&#125;&#x27; \\ -l app=client) -- /opt/spire/bin/spire-agent api fetch -socketPath /run/spire/sockets/agent.sock 如果agent正常运行，将看到一个 SVID 列表。 123456789101112131415161718192021SPIFFE ID: spiffe://example.org/ns/default/sa/defaultSVID Valid After: 2022-12-25 11:41:16 +0000 UTCSVID Valid Until: 2022-12-25 12:41:26 +0000 UTCCA #1 Valid After: 2022-12-23 15:04:07 +0000 UTCCA #1 Valid Until: 2022-12-24 15:04:17 +0000 UTCCA #2 Valid After: 2022-12-24 03:04:07 +0000 UTCCA #2 Valid Until: 2022-12-25 03:04:17 +0000 UTCCA #3 Valid After: 2022-12-24 15:04:07 +0000 UTCCA #3 Valid Until: 2022-12-25 15:04:17 +0000 UTCCA #4 Valid After: 2022-12-25 03:04:07 +0000 UTCCA #4 Valid Until: 2022-12-26 03:04:17 +0000 UTC Demo应用部署本次演示将Envoy与X.509-SVID结合使用保护微服务通信 如图所示，前端服务通过sidecar Envoy执行X.509 SVID 身份验证与实例建立的起mTLS连接，连接到后端服务。 SPIRE Agent原生支持做为Envoy的SDS服务。通过本地socket连接SDS服务。 切换到spire-tutorials&#x2F;k8s&#x2F;envoy-x509目录 部署应用 1234567891011121314151617181920212223242526272829kubectl apply -k k8s/.configmap/backend-balance-json-data createdconfigmap/backend-envoy createdconfigmap/backend-profile-json-data createdconfigmap/backend-transactions-json-data createdconfigmap/frontend-2-envoy createdconfigmap/frontend-envoy createdconfigmap/symbank-webapp-2-config createdconfigmap/symbank-webapp-config createdservice/backend-envoy createdservice/frontend-2 createdservice/frontend createddeployment.apps/backend createddeployment.apps/frontend-2 createddeployment.apps/frontend created 以backend模块为例查看k8s&#x2F;backend&#x2F;config&#x2F;envoy.yaml文件，可以科技Envoy配置的与spire-agent的socket连接 12345678910111213clusters:- name: spire_agent connect_timeout: 0.25s http2_protocol_options: &#123;&#125; hosts: - pipe: path: /run/spire/sockets/agent.sock 手动将backend、frontend、frontend-2注册到sprie-server，当然SPIFFE也有自动正常功能就是使用SPIRE Controller Manager 模块（https://github.com/spiffe/spire-controller-manager） 1bash create-registration-entries.sh 注册完成后可以查看注册的服务 1kubectl exec -n spire spire-server-0 -c spire-server -- /opt/spire/bin/spire-server entry show -selector k8s:ns:default 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677Found 4 entriesEntry ID : 3478c441-3e25-40e7-96d9-ef74611f2205SPIFFE ID : spiffe://example.org/ns/default/sa/defaultParent ID : spiffe://example.org/ns/spire/sa/spire-agentRevision : 0X509-SVID TTL : defaultJWT-SVID TTL : defaultSelector : k8s:ns:defaultSelector : k8s:sa:defaultEntry ID : c188d47c-e886-492e-bf67-6a6bf42c3667SPIFFE ID : spiffe://example.org/ns/default/sa/default/backendParent ID : spiffe://example.org/ns/spire/sa/spire-agentRevision : 0X509-SVID TTL : defaultJWT-SVID TTL : defaultSelector : k8s:container-name:envoySelector : k8s:ns:defaultSelector : k8s:pod-label:app:backendSelector : k8s:sa:defaultEntry ID : 6c376401-67d4-499a-a9d9-6ab71caf69c4SPIFFE ID : spiffe://example.org/ns/default/sa/default/frontendParent ID : spiffe://example.org/ns/spire/sa/spire-agentRevision : 0X509-SVID TTL : defaultJWT-SVID TTL : defaultSelector : k8s:container-name:envoySelector : k8s:ns:defaultSelector : k8s:pod-label:app:frontendSelector : k8s:sa:defaultEntry ID : 49f88c69-b4ee-4656-b740-6dbee5bb89a3SPIFFE ID : spiffe://example.org/ns/default/sa/default/frontend-2Parent ID : spiffe://example.org/ns/spire/sa/spire-agentRevision : 0X509-SVID TTL : defaultJWT-SVID TTL : defaultSelector : k8s:container-name:envoySelector : k8s:ns:defaultSelector : k8s:pod-label:app:frontend-2Selector : k8s:sa:default 可以看见对应的SPIFFE ID 访问服务 12345678910111213kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEbackend-envoy ClusterIP None &lt;none&gt; 9001/TCP 2d8hfrontend LoadBalancer 10.43.106.227 &lt;pending&gt; 3000:32082/TCP 2d8hfrontend-2 LoadBalancer 10.43.203.167 &lt;pending&gt; 3002:30664/TCP 2d8hgo-demo NodePort 10.43.120.2 &lt;none&gt; 8080:30007/TCP 2d10hkubernetes ClusterIP 10.43.0.1 &lt;none&gt; 443/TCP 2d10h frontend对应的NodePort端口为32082 frontend-2对应的NodePort端口为30664 frontend显示Jacob Marley的账户情况frontend-2显示Alex Fergus的账户情况 更新策略只允许frontend服务访问backend访问 1kubectl apply -f backend-envoy-configmap-update.yaml 实际上就是更新backend的Envoy配置对应的 k8s&#x2F;backend&#x2F;config&#x2F;envoy.yaml删除了以下条目 1- exact: &quot;spiffe://example.org/ns/default/sa/default/frontend-2&quot; 12345match_subject_alt_names: - exact: &quot;spiffe://example.org/ns/default/sa/default/frontend&quot; - exact: &quot;spiffe://example.org/ns/default/sa/default/frontend-2&quot; 重启backend服务获取最新配置 123kubectl scale deployment backend --replicas=0kubectl scale deployment backend --replicas=1 在次访问frontend正常显示，访问frontend-2 总结SPIFFE支持多种方式集成如和Istio的envoy-sidecar、OPA策略等方式，可以非常灵活细粒化控制应用访问权限。 参考链接：https://jimmysong.io/blog/why-istio-need-spire/https://mp.weixin.qq.com/s/4eEEYb8RuOFOmLcdL3N6wAhttps://atbug.com/what-is-spiffe-and-spire/https://www.nginx-cn.net/blog/mtls-architecture-nginx-service-mesh/","categories":[{"name":"安全","slug":"安全","permalink":"http://yoursite.com/categories/%E5%AE%89%E5%85%A8/"}],"tags":[{"name":"安全","slug":"安全","permalink":"http://yoursite.com/tags/%E5%AE%89%E5%85%A8/"}]},{"title":"eBPF学习摘要3-XDP学习和理解","slug":"ebpf_3","date":"2022-10-17T13:45:59.000Z","updated":"2022-10-17T13:45:59.000Z","comments":true,"path":"2022/10/17/ebpf_3/","permalink":"http://yoursite.com/2022/10/17/ebpf_3/","excerpt":"","text":"eBPF学习摘要3-XDP学习和理解概述Linux网络数据包接收数据包的链路为Nic——&gt;rx_ring——&gt;skbuff——&gt;网络协议处理（如ip_recv)这样做的问题在于会产生大量内核态到用户态的切换过程，这会造成大量性能消耗，所以为了提升网络性能才诞生出Kernel bypass的技术如DPDK、SolarFlare技术，像DPDK就是直接饶过内核态，用户态应用直接访问网络硬件提高数据包处理效率，降低因为切换带来的损耗，但这种方式本身也存在一些缺点：1、绕开了内核态，很难与Linux操作系统中内核态本身存在的一些工具集成，一些功能需要重新开发。2、需要单独cpu核参与处理。XDP（eXpress Data Path）是一个eBPF hook，可以在内核中执行eBPF程序实现对网络数据包处理，在Linux内核 4.8 版中引入。实现方式与Kernel bypass完全相反在sk_buffer之前数据包从driver出来以后就可以直接被XDP程序捕获执行，极大提升了网络数据包的处理效率。 实现方式如下图所示1、数据包通过网卡,触发XDP执行2、xdp程序执行读取BPF mps配置的规则对数据包执行相应的操作，通常为（1）XDP_DROP：直接丢弃，不占用CPU资源，有效防止DDOS（2）XDP_Allow：正常转发到内核网络栈（3）XDP_REDIRECT:重定向到其他网卡，或通过AF_XDP直接发送到用户空间。（4）XDP_TX:将处理后的包发给相同的网卡。 三种处理模式: XDP在网络栈中有三个处理点：offloaded模式的XDP：对于支持可编程的网卡，直接在网卡上运行XDP程序。Native模式的XDP：默认模式、对于支持的网卡驱动，可以在包到达内核后立刻进行处理。（目前大部分网卡已经支持） Offloaded和Native模式 Generic模式的XDP：网卡和驱动不支持上述两种情况的XDP时，可以在receive_skb函数此点进行处理。这个处理的位置相对靠后，在tc处理点之前，这种性能最差，一般用于测试调试模式。 应用场景 负载均衡器：通过XDP_TX和XDP_TX实现数据包的快速转发，目前很多k8s网络插件取代kube-proxy实现Service负载均衡器就是如此。Facebook的全部流量都是经过基于XDP的四层负载均衡器（katran）处理转发（https://lpc.events/event/11/contributions/950/attachments/889/1704/lpc_from_xdp_to_socket_fb.pdf） 防火墙：Cloudflare在他们的DDoS防御L4Drop中使用用了XDP无需高CPU占用就可提供高性能丢包率（https://blog.cloudflare.com/how-to-drop-10-million-packets/） 流量监控和采样：位于内核网络栈前端，通过自定义的eBPF程序即可实现对网络流量的采样，目前很多基于eBPF的APM就是这样实现的。 性能测试https://people.netfilter.org/hawk/presentations/KernelRecipes2018/XDP_Kernel_Recipes_2018.pdfhttp://vger.kernel.org/lpc_net2018_talks&#x2F;lpc18-xdp-future.pdfhttps://blog.cloudflare.com/how-to-drop-10-million-packets/https://blog.csdn.net/hbhgyu/article/details/109354273这里面包含了对丢包性能、转发性能、DDos防御能力的测试。 总结：随着eBPF技术的持续发展，XDP能够实现DPDK相近的性能，但又更具有兼容性和灵活性，未来会得到越来越好的发展，围绕eBPF和XDP的生态软件也会越来越丰富。 参考链接：https://zhuanlan.zhihu.com/p/453005342https://zhuanlan.zhihu.com/p/438158551https://mp.weixin.qq.com/s/H9imUbdJnfj1NKdK9jtxEwhttps://zhuanlan.zhihu.com/p/321387418https://mp.weixin.qq.com/s/lUvxUkFg4w1X0ioktxGiHAhttps://www.seekret.io/blog/a-gentle-introduction-to-xdp/","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"eBPF学习摘要2—工具使用(bpftrace）","slug":"ebpf_2","date":"2022-09-18T13:45:59.000Z","updated":"2022-09-18T13:45:59.000Z","comments":true,"path":"2022/09/18/ebpf_2/","permalink":"http://yoursite.com/2022/09/18/ebpf_2/","excerpt":"","text":"概述bpftrace是基于eBPF实现的动态的工具，使用DSL（Domain Specific Language）编写eBPF程序，使用LLVM编译eBPF字节码，BCC与LinuxBPF系统交互。直接使用DSL编写好的脚本（类似awk语言）可以执行，无需在内核中手动编译和加载。bpftrace在内核中实现动态追踪主要是使用Kprobe探针和Tracepoints探针。使用bpftrace可以更深入的进行操作系统上问题排查，如某个函数的调用次数和延时、追踪系统OOMKILL、TCP连接丢包等。都可以自定义脚本实现。另外还有一个叫BCC的项目，跟bpftrace区别是BCC可以使用高级语言开发ebfp程序，如Java、Python、Lua…… https://github.com/iovisor/bpftracehttps://github.com/iovisor/bcc 安装和基础使用系统环境：ubuntu：20.04Kernel：5.4.0-125-generic 参考官网安装方式https://github.com/iovisor/bpftrace/blob/master/INSTALL.md有各操作系统发行版的安装方式，也有基于Docker的安装方式我这里为Ubuntu20.04的操作系统，先使用Ubuntu的安装方式进行安装，因为bpftrace依赖ebpf能力，对应不同的内核版本实现的功能有所差异，如4.1 版本实现了kprobes、4.7版本实现了tracepoints官方提供了环境需求检测脚本可以实现对现有环境检测https://github.com/iovisor/bpftrace/blob/master/scripts/check_kernel_features.sh执行后 123./check_kernel_features.shAll required features present! 安装bpftrace 12sudo apt-get install -y bpftrace bpfcc-tools 安装完成后查看版本 123bpftrace --versionbpftrace v0.9.4 列出当前内核支持的Kprobes探针列表 1234567891011121314151617181920212223242526272829303132333435bpftrace -l &#x27;kprobe:tcp*kprobe:tcp_mmapkprobe:tcp_get_info_chrono_statskprobe:tcp_init_sockkprobe:tcp_splice_data_recvkprobe:tcp_pushkprobe:tcp_send_msskprobe:tcp_cleanup_rbufkprobe:tcp_set_rcvlowatkprobe:tcp_recv_timestampkprobe:tcp_enter_memory_pressurekprobe:tcp_leave_memory_pressurekprobe:tcp_ioctlkprobe:tcp_get_infokprobe:tcp_get_md5sig_poolkprobe:tcp_set_statekprobe:tcp_shutdown... 内核静态探针-Tracepoint 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465bpftrace -l &#x27;tracepoint:*&#x27;kprobe:tcp_mmapkprobe:tcp_get_info_chrono_statskprobe:tcp_init_sockkprobe:tcp_splice_data_recvkprobe:tcp_pushkprobe:tcp_send_msskprobe:tcp_cleanup_rbufkprobe:tcp_set_rcvlowatkprobe:tcp_recv_timestampkprobe:tcp_enter_memory_pressurekprobe:tcp_leave_memory_pressurekprobe:tcp_ioctlkprobe:tcp_get_infokprobe:tcp_get_md5sig_poolkprobe:tcp_set_statekprobe:tcp_shutdowntracepoint:nfsd:nfsd_compoundtracepoint:nfsd:nfsd_compound_statustracepoint:nfsd:nfsd_read_starttracepoint:nfsd:nfsd_read_splicetracepoint:nfsd:nfsd_read_vectortracepoint:nfsd:nfsd_read_io_donetracepoint:nfsd:nfsd_read_donetracepoint:nfsd:nfsd_write_starttracepoint:nfsd:nfsd_write_openedtracepoint:nfsd:nfsd_write_io_donetracepoint:nfsd:nfsd_write_donetracepoint:nfsd:nfsd_read_errtracepoint:nfsd:nfsd_write_errtracepoint:nfsd:nfsd_layoutstate_alloctracepoint:nfsd:nfsd_layoutstate_unhash... 例如列出所有的进程打开的文件 123456789101112131415161718192021222324252627bpftrace -e &#x27;tracepoint:syscalls:sys_enter_openat &#123; printf(&quot;%s %s\\n&quot;, comm, str(args-&gt;filename)); &#125;&#x27;kubelet /sys/fs/cgroup/memory/kubepods.slice/memory.numa_statkubelet /sys/fs/cgroup/cpu,cpuacct/kubepods.slice/cpu.statkubelet /sys/fs/cgroup/cpu,cpuacct/kubepods.slice/cpuacct.statkubelet /sys/fs/cgroup/cpu,cpuacct/kubepods.slice/cpuacct.usagekubelet /sys/fs/cgroup/cpu,cpuacct/kubepods.slice/cpuacct.usage_percpukubelet /sys/fs/cgroup/cpu,cpuacct/kubepods.slice/cpuacct.usage_allkubelet /sys/fs/cgroup/pids/kubepods.slice/pids.currentkubelet /sys/fs/cgroup/pids/kubepods.slice/pids.maxkubelet /sys/fs/cgroup/blkio/kubepods.slice/blkio.bfq.sectors_recursivekubelet /sys/fs/cgroup/blkio/kubepods.slice/blkio.bfq.io_serviced_recurkubelet /sys/fs/cgroup/blkio/kubepods.slice/blkio.sectors_recursivekubelet /sys/fs/cgroup/blkio/kubepods.slice/blkio.throttle.io_serviced_... ctrl+c暂停也可以将复杂的命令写成脚本执行，默认安装完后，在&#x2F;usr&#x2F;sbin&#x2F;目录下已经集成了很多脚本 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283ls /usr/sbin/|grep &quot;.*.bt&quot;bashreadline.btbiolatency.btbiosnoop.btbiostacks.btbitesize.btcapable.btcpuwalk.btdcsnoop.btebtablesebtables-nftebtables-nft-restoreebtables-nft-saveebtables-restoreebtables-saveexecsnoop.btgethostlatency.btkillsnoop.btloads.btmdflush.btnaptime.btoomkill.btopensnoop.btpidpersec.btrunqlat.btrunqlen.btsetuids.btstatsnoop.btswapin.btsyncsnoop.btsyscount.bttcpaccept.bttcpconnect.bttcpdrop.bttcplife.bttcpretrans.bttcpsynbl.btthreadsnoop.btvfscount.btvfsstat.btwriteback.btxfsdist.bt https://github.com/iovisor/bpftrace/tree/master/tools 也存在很多脚本和测试用例。 比如执行tcpconnect.bt 可以参考到本机所有的TCP网络连接 123456789101112131415161718192021222324252627tcpconnect.bt Attaching 2 probes...Tracing tcp connections. Hit Ctrl-C to end.TIME PID COMM SADDR SPORT DADDR DPORT 23:02:09 1686607 coredns 127.0.0.1 42216 127.0.0.1 13429 23:02:10 1686607 coredns 127.0.0.1 42218 127.0.0.1 13429 23:02:11 1680193 kubelet 10.0.1.11 34732 10.0.1.15 13429 23:02:11 1680193 kubelet 10.0.1.11 41570 10.0.1.244 13429 23:02:11 1686607 coredns 127.0.0.1 42224 127.0.0.1 13429 23:02:11 1680193 kubelet 127.0.0.1 55010 127.0.0.1 13429 23:02:11 1680193 kubelet 10.0.1.11 33098 10.0.1.145 13429 23:02:12 1686607 coredns 127.0.0.1 42230 127.0.0.1 13429 23:02:13 1680193 kubelet 127.0.0.1 34214 127.0.0.1 13429 23:02:13 1686607 coredns 127.0.0.1 42234 127.0.0.1 13429 追踪全系统范围内open()调用 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849opensnoop.btAttaching 6 probes...Tracing open syscalls... Hit Ctrl-C to end.PID COMM FD ERR PATH1686817 AsyncBlockInput 2 0 /var/lib/clickhouse_storage/store/087/0872950b-6ca9-420e-8872-91686817 AsyncBlockInput 2 0 /var/lib/clickhouse_storage/store/087/0872950b-6ca9-420e-8872-91686817 AsyncBlockInput 2 0 /var/lib/clickhouse_storage/store/435/435debae-6288-4661-835d-e1686817 AsyncBlockInput 2 0 /var/lib/clickhouse_storage/store/435/435debae-6288-4661-835d-e1686817 AsyncBlockInput 2 0 /var/lib/clickhouse_storage/store/092/09246824-dd69-4f4c-8924-61686817 AsyncBlockInput 2 0 /var/lib/clickhouse_storage/store/092/09246824-dd69-4f4c-8924-61680193 kubelet 2 0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/1680193 kubelet 2 0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/1680193 kubelet 2 0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/1680193 kubelet 2 0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/1680193 kubelet 2 0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/1680193 kubelet 2 0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/1680193 kubelet 2 0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/1680193 kubelet 2 0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/1680193 kubelet 2 0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/1680193 kubelet 2 0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/1680193 kubelet 2 0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/1680193 kubelet 2 0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/1680193 kubelet 2 0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/1680193 kubelet 2 0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/1680193 kubelet 2 0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/ 展示最消耗IO的进程及数据写入量 1234567891011biotop-bpfcc20:53:20 loadavg: 0.96 1.40 1.26 2/2392 3295655PID COMM D MAJ MIN DISK I/O Kbytes AVGms337 jbd2/vda1-8 W 252 0 vda 2 412.0 5.161680139 etcd W 252 0 vda 14 60.0 2.613295622 rancher-system- R 252 0 vda 1 56.0 2.07 查看每个进程对应的执行命令和参数 1234567891011121314151617181920212223execsnoop-bpfcc -TTIME PCOMM PID PPID RET ARGS21:22:05 rancher-system- 3310507 1 0 /usr/local/bin/rancher-system-agent sentinel21:22:06 cilium-cni 3310516 1680193 0 /opt/cni/bin/cilium-cni21:22:06 iptables 3310525 1680164 0 /usr/sbin/iptables -w 5 -W 100000 -S KUBE-PROXY-CANARY -t mangle21:22:06 ip6tables 3310524 1680164 0 /usr/sbin/ip6tables -w 5 -W 100000 -S KUBE-PROXY-CANARY -t mangle21:22:06 nsenter 3310526 1680193 0 /usr/bin/nsenter --net=/proc/1688099/ns/net -F -- ip -o -4 addr show dev eth0 scope global21:22:06 ip 3310526 1680193 0 /usr/sbin/ip -o -4 addr show dev eth0 scope global21:22:06 nsenter 3310527 1680193 0 /usr/bin/nsenter --net=/proc/1688099/ns/net -F -- ip -o -6 addr show dev eth0 scope global21:22:06 ip 3310527 1680193 0 /usr/sbin/ip -o -6 addr show dev eth0 scope global21:22:06 runc 3310528 1679752 0 /usr/bin/runc --version21:22:06 docker-init 3310534 1679752 0 /usr/bin/docker-init --version 常用的一些脚本作用 killsnoop.bt——追踪 kill() 系统调用发出的信号 tcpconnect.bt——追踪所有的 TCP 网络连接 pidpersec.bt——统计每秒钟（通过fork）创建的新进程 opensnoop.bt——追踪 open() 系统调用 bfsstat.bt——追踪一些 VFS 调用，按秒统计 bashreadline.bt——打印从所有运行shell输入的bash命令 tcplife.bt——追踪TCP连接生命周期 biotop-bpfcc——展示进程io写入 bpftrace执行原理用户态1、用户编写 eBPF 程序，可以使用 eBPF 汇编或者 eBPF 特有的 C 语言来编写。2、使用 LLVM&#x2F;CLang 编译器，将 eBPF 程序编译成 eBPF 字节码。3、调用 bpf() 系统调用把 eBPF 字节码加载到内核。 内核态1、当用户调用 bpf() 系统调用把 eBPF 字节码加载到内核时，内核先会对 eBPF 字节码进行安全验证。2、使用 JIT（Just In Time）技术将 eBPF 字节编译成本地机器码（Native Code）。3、然后根据 eBPF 程序的功能，将 eBPF 机器码挂载到内核的不同运行路径上（如用于跟踪内核运行状态的 eBPF 程序将会挂载在 kprobes 的运行路径上）。当内核运行到这些路径时，就会触发执行相应路径上的 eBPF 机器码。4、通过map与用户空间程序交互 总结通过bpftrace和bcc可以很形象了解ebpf特性，无需修改内核源码和重新编译内核就可以扩展内核的功能，除了像bpftrace这类追踪类软件，还有通过ebfp实现的POD安全威胁检测Falco、基于ebpf负载均衡器Katran等开源产品。另外ebpf_exporter组件也可以将自定义的ebpf执行脚本输出到Prometheus中进行监控。ebpf的生态将越来越丰富。 Perf工具使用Perf（Performance Event）是在Linux Kernel2.6集成在Linux Kernel中，主要利用CPU中PMU (Performance Monitoring Unit)和Linux中的 tracepoint实现目标取样和性能分析。Perf工具根eBPF实际上没什么关系，这里写这个工具主要是因为它本身也可以实现应用程序动态追踪，也利用到了tracepoint的能力，但与eBPF不同的是Perf是写死的能力，bpftrace基于eBPF是可以实现脚本灵活的穿插和调用。 安装部署这里使用的操作系统是Ubuntu20.04。Kernel为5.4.0-125-generic 123sudo apt-get install linux-tools-common linux-tools-&quot;``(uname -r)&quot; linux-cloud-tools-&quot;``(uname -r)&quot; linux-tools-generic linux-cloud-tools-generic 验证版本 123perf -vperf version 5.4.195 采样事件 12345678910111213141516171819202122232425262728293031323334353637383940414243perf listList of pre-defined events (to be used in -e): alignment-faults [Software event] bpf-output [Software event] context-switches OR cs [Software event] cpu-clock [Software event] cpu-migrations OR migrations [Software event] dummy [Software event] emulation-faults [Software event] major-faults [Software event] minor-faults [Software event] page-faults OR faults [Software event] task-clock [Software event] duration_time [Tool event] msr/tsc/ [Kernel PMU event] rNNN [Raw hardware event descriptor] cpu/t1=v1[,t2=v2,t3 ...]/modifier [Raw hardware event descriptor] (see &#x27;man perf-list&#x27; on how to encode it) mem:&lt;addr&gt;[/len][:access] [Hardware breakpoint] alarmtimer:alarmtimer_cancel [Tracepoint event] alarmtimer:alarmtimer_fired [Tracepoint event] ... 主要分为三类：Hardware Event ：通过PMU获取的硬件CPU的事件，如：cpu-cycles、缓存命中等。Software Event ：软件本身的进程切换和页命中等Tracepoint event：io命中率、文件系统写入速率等 perf top展示各个进程和函数资源占用情况，-g显示子进程，按e显示子进程函数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455perf top -g Samples: 284K of event &#x27;cpu-clock:pppH&#x27;, 4000 Hz, Event count (approx.): 33836570425 lost: 0/0 drop: 0/0 Children Self Shared Object Symbol- 20.27% 0.09% perf [.] __ordered_events__flush.part.0 ◆ - 2.20% __ordered_events__flush.part.0 ▒ - 2.56% deliver_event ▒ - 3.39% hist_entry_iter__add ▒ - 3.79% iter_add_next_cumulative_entry ▒ - 3.03% __hists__add_entry.constprop.0 ▒ 3.79% hists__findnew_entry ▒ - 1.54% callchain_append ▒ - 2.64% append_chain_children ▒ - 2.22% append_chain_children ▒ - 1.73% append_chain_children ▒ - 1.34% append_chain_children ▒ 1.07% append_chain_children ▒+ 20.13% 0.18% perf [.] deliver_event ▒+ 18.62% 0.04% perf [.] hist_entry_iter__add ▒+ 14.47% 0.80% perf [.] iter_add_next_cumulative_entry ▒+ 12.05% 0.96% [kernel] [k] do_syscall_64 ▒+ 8.99% 0.00% perf [.] process_thread ▒+ 8.93% 0.22% [kernel] [k] do_idle ▒+ 8.83% 1.06% [kernel] [k] __softirqentry_text_start ▒+ 8.24% 6.28% perf [.] append_chain_children ▒+ 7.11% 0.06% perf [.] callchain_append ▒+ 5.96% 4.75% libc-2.31.so [.] pthread_attr_setschedparam ▒+ 5.74% 0.25% perf [.] __hists__add_entry.constprop.0 [.]：表示运行在用户态空间[k]：表示运行在内核态空间 perf state查看程序运行情况 12345678910111213141516171819perf stat -p 1679752 按ctrl+c输出结果 Performance counter stats for process id &#x27;1679752&#x27;: 682.90 msec task-clock # 0.066 CPUs utilized 3154 context-switches # 0.005 M/sec 36 cpu-migrations # 0.053 K/sec 3275 page-faults # 0.005 M/sec &lt;not supported&gt; cycles &lt;not supported&gt; instructions &lt;not supported&gt; branches &lt;not supported&gt; branch-misses Task-clock：CPU 利用率Context-switches：进程切换次数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647Samples: 1K of event &#x27;block:block_rq_issue&#x27;, 1 Hz, Event count (approx.): 136 lost: 0/0 drop: 0/0 Children Self Trace output+ 16.91% 16.91% 252,0 FF 0 () 0 + 0 [kworker/0:1H]+ 15.44% 15.44% 252,0 FF 0 () 0 + 0 [kworker/7:1H]+ 9.56% 9.56% 252,0 FF 0 () 0 + 0 [kworker/3:1H]+ 8.09% 8.09% 252,0 FF 0 () 0 + 0 [kworker/4:1H]+ 6.62% 6.62% 252,0 WS 4096 () 18340064 + 8 [etcd]+ 6.62% 6.62% 252,0 WS 4096 () 18340072 + 8 [etcd]+ 5.15% 5.15% 252,0 FF 0 () 0 + 0 [kworker/2:1H]+ 4.41% 4.41% 252,0 FF 0 () 0 + 0 [kworker/6:1H]+ 2.94% 2.94% 252,0 WS 4096 () 122005280 + 8 [etcd] 2.21% 2.21% 252,0 WS 4096 () 115952144 + 8 [etcd] 2.21% 2.21% 252,0 WS 4096 () 122005272 + 8 [etcd]+ 1.47% 1.47% 252,0 FF 0 () 0 + 0 [kworker/1:1H]+ 1.47% 1.47% 252,0 WS 4096 () 116164552 + 8 [etcd] 1.47% 1.47% 252,0 WS 4096 () 116173824 + 8 [etcd]+ 1.47% 1.47% 252,0 WS 4096 () 122005256 + 8 [etcd] 1.47% 1.47% 252,0 WS 4096 () 122005288 + 8 [etcd] 1.47% 1.47% 252,0 WS 4096 () 122005296 + 8 [etcd] 0.74% 0.74% 252,0 FF 0 () 0 + 0 [kworker/5:1H] 0.74% 0.74% 252,0 WS 516096 () 2388520 + 1008 [jbd2/vda1-8] 0.74% 0.74% 252,0 WS 372736 () 2389528 + 728 [jbd2/vda1-8] 0.74% 0.74% 252,0 WS 4096 () 115700160 + 8 [etcd] 0.74% 0.74% 252,0 WS 4096 () 115948608 + 8 [etcd] 对CPU事件进行检测，采样时间60s，每秒采样99个事件，采样完成后会在本地生成个perf.data文件，如果执行多次，会将上一个重命名为perf.data.old。加-p可以指定进程号输出。 12perf record -F 99 -a -g -- sleep 60 查看报告 1perf report 生成火焰图 123456789101112131415下载制作火焰图工具git clone https://github.com/brendangregg/FlameGraph.git对perf.data进行解析perf script -i perf.data &amp;&gt; perf.unfold进行符号处理FlameGraph/stackcollapse-perf.pl perf.unfold &amp;&gt; perf.folded生成火焰图FlameGraph/flamegraph.pl perf.folded &gt; perf.svg 使用chrome浏览器打开 火焰图怎么查看分析可参考https://www.infoq.cn/article/a8kmnxdhbwmzxzsytlga 参考链接https://github.com/iovisor/bpftrace/blob/master/docs/reference_guide.mdhttp://blog.nsfocus.net/bpftrace-dynamic-tracing-0828/https://www.cnblogs.com/arnoldlu/p/6241297.htmlhttps://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux&#x2F;8&#x2F;html&#x2F;monitoring_and_managing_system_status_and_performance&#x2F;counting-events-during-process-execution-with-perf-stat_monitoring-and-managing-system-status-and-performance","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"eBPF学习摘要1(概述、理论)","slug":"ebpf_1","date":"2022-08-23T13:45:59.000Z","updated":"2022-08-23T13:45:59.000Z","comments":true,"path":"2022/08/23/ebpf_1/","permalink":"http://yoursite.com/2022/08/23/ebpf_1/","excerpt":"","text":"eBPF学习摘要1(概述、理论)概述早期内核包过滤是将网络数据包拷贝到用户态进行过滤，这样整体的包过滤性能低，后续在1992年的BSD操作系统上引入BPF包过滤技术，Linux在内核2.1.75正式引入BPF技术。网络数据包过滤可以直接在内核中执行，避免将网络数据包在用户态执行，极大提高了包过滤性能。如tcpdump工具就是利用BPF技术实现。2014年对BPF技术进行全面扩展，诞生了eBPF（extended Berkeley Packet Filter）使得BPF不仅仅是网络栈层面功能。后续iovisor 引入 BCC、bpftrace 等工具，成为 eBPF 在跟踪和排错领域的最佳实践。另外 eBPF 最重大的特性是在内核中运行沙盒程序而无需修改内核源码和重新编译内核就可以扩展内核的功能，Cilium、Katran、Falco 等一系列基于 eBPF 优化网络和安全的开源项目也逐步诞生。并且，越来越多的开源和商业解决方案开始借助 eBPF，优化其网络、安全以及观测的性能。图片来源：https://blog.csdn.net/eBPF_Kindling&#x2F;article&#x2F;details&#x2F;123575619 发展历程1992年：BPF全称Berkeley Packet Filter，诞生初衷提供一种内核中自定义报文过滤的手段（类汇编），提升抓包效率。（tcpdump）2011年：linux kernel 3.2版本对BPF进行重大改进，引入BPF JIT，使其性能得到大幅提升。2014年： linux kernel 3.15版本，BPF扩展成eBPF，其功能范畴扩展至：内核跟踪、性能调优、协议栈QoS等方面。与之配套改进包括：扩展BPF ISA指令集、提供高级语言（C）编程手段、提供MAP机制、提供Help机制、引入Verifier机制等。2016年：linux kernel 4.8版本，eBPF支持XDP，进一步拓展该技术在网络领域的应用。随后Netronome公司提出eBPF硬件卸载方案。Cilium项目正式发布。2018年：linux kernel 4.18版本，引入BTF，将内核中BPF对象（Prog&#x2F;Map）由字节码转换成统一结构对象，这有利于eBPF对象与Kernel版本的配套管理，为eBPF的发展奠定基础。2018年：从kernel 4.20版本开始，eBPF成为内核最活跃的项目之一，新增特性包括：sysctrl hook、flow dissector、struct_ops、lsm hook、ring buffer等。场景范围覆盖容器、安全、网络、跟踪等2021年：微软、Facebook、Google、Isovalent、NetFlix成立eBPF基金会，同年Cilium发布基于eBPF的Service Mesh解决方案eBPF 基本架构及使用参考链接：https://blog.51cto.com/dengchj/2944202 实现原理用户态1、用户编写 eBPF 程序，可以使用 eBPF 汇编或者 eBPF 特有的 C 语言来编写。2、使用 LLVM&#x2F;CLang 编译器，将 eBPF 程序编译成 eBPF 字节码。3、调用 bpf() 系统调用把 eBPF 字节码加载到内核。 内核态1、当用户调用 bpf() 系统调用把 eBPF 字节码加载到内核时，内核先会对 eBPF 字节码进行安全验证。2、使用 JIT（Just In Time）技术将 eBPF 字节编译成本地机器码（Native Code）。3、然后根据 eBPF 程序的功能，将 eBPF 机器码挂载到内核的不同运行路径上（如用于跟踪内核运行状态的 eBPF 程序将会挂载在 kprobes 的运行路径上）。当内核运行到这些路径时，就会触发执行相应路径上的 eBPF 机器码。4、通过map与用户空间程序交互 如何保证内核安全性和优缺点 需要特权执行：eBPF程序加载到Linux内核的进程都必须在特权模式(root)下运行，或者需要CAP_BPF功能，不受信任的程序不能加载eBPF程序 验证器：加载eBPF程序到内核后需要经过验证如有界循环、越界访问内存、使用未初始化的变量。 程序执行保护：已经加载在内核中的eBPF程序会进入read-only模式试图修改会直接crash内核。 限制内核访问范围： eBPF程序不能直接访问任意内核其他函数。必须通过eBPF helpers访问固定helpers函数。 eBPF 堆栈大小被限制在 MAX_BPF_STACK，截止到内核 Linux 5.8 版本，被设置为 512字节。 eBPF 字节码大小最初被限制为 4096 条指令，截止到内核 Linux 5.8 版本， 当前已将放宽至 100 万指令。 优点：1.速度和性能。 内核态进行，速度和效率高。2.灵活：无需修改内核代码，即可扩展内核功能拥有无限想象空间。3.低侵入性：基于eBPF实现链路追踪、服务治理等场景不需要侵入用户层。 缺点：1.eBPF本身一些特性和能力依赖新版本内核。2.学习成本高，需要对Linux Kernel和操作系统原理有深入了解。 目前行业落地情况应用 动态追踪：bcc、bpftrace 观测监控：Pixie、Hubble 网络：Cilium、Katran 安全：Falco、Tracee 能解决什么问题 为什么需要eBPF能实现可观测性eBPF可观测性-指标采集eBPF除了常规的指标监控如CPU、内存等，还可以监控细粒度的系统调用等信息，通过内核Kprobe或者Tracepoint实现; eBPF可观测性-链路追踪与传统APM相比，eBPF进行链路追踪不需要与业务本身进行绑定。通过拦截sock相关的send&#x2F;recv操作，解析协议头，获得进程之间的调用关系，可进一步关联Kubernetes元数据，获得容器、服务之间的调用关系; 展望未来1、基于eBPF的服务网格去除每个pod的sidecar，内核态实现服务治理（cilium1.12已实现) 2、基于eBPF的负载均衡器利用 socket eBPF，可以在不用直接处理报文和NAT 转换的前提下，实现了负载均衡逻辑。Service网络 POD&lt;–&gt; Service &lt;–&gt; POD优化成 POD &lt;–&gt; POD，从而使Service网络性能基本等同于POD 网络。软件结构如下： 3、基于eBPF的网络安全策略不再依赖 iptables，不需要创建巨量的 iptables rule，从而显著降低 iptables 带来的性能影响。 参考链接： https://mp.weixin.qq.com/s/Xr8ECrS_fR3aCT1vKJ9yIghttps://mp.weixin.qq.com/s?__biz&#x3D;Mzg5Mjc3MjIyMA&#x3D;&#x3D;&amp;mid&#x3D;2247544625&amp;idx&#x3D;2&amp;sn&#x3D;7ba07582e0b7fdc0ff3179f2fa2b44d4&amp;source&#x3D;41#wechat_redirecthttps://blog.csdn.net/eBPF_Kindling&#x2F;article&#x2F;details&#x2F;123575619https://www.51cto.com/article/715674.htmlhttps://blog.csdn.net/m0_46700908&#x2F;article&#x2F;details&#x2F;124464577?spm&#x3D;1001.2101.3001.6650.1&amp;utm_medium&#x3D;distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-124464577-blog-123575619.t5_layer_eslanding_D_0&amp;depth_1-utm_source&#x3D;distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-124464577-blog-123575619.t5_layer_eslanding_D_0&amp;utm_relevant_index&#x3D;2https://colobu.com/2022/05/22/use-ebpf-to-trace-rpcx-microservices/https://arthurchiao.art/blog/ebpf-and-k8s-zh/https://zhuanlan.zhihu.com/p/480811707https://github.com/mikeroyal/eBPF-Guidehttps://zhuanlan.zhihu.com/p/373090595","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"ETCD集群读写慢问题分析","slug":"fio_etcd","date":"2022-07-27T13:45:59.000Z","updated":"2022-07-27T13:45:59.000Z","comments":true,"path":"2022/07/27/fio_etcd/","permalink":"http://yoursite.com/2022/07/27/fio_etcd/","excerpt":"","text":"问题现象1、Rancher所在local集群周期性卡顿、执行命令响应缓慢。2、Rancher-server副本频繁重启。 3、Rancher UI空载集群切换项目，点击UI反应慢。 查看ETCD日志发现有大量Ready only报错和too long（xxx ms）to execute报错 问题分析 注：以下etcd读写流程来源腾讯云原生社区（https://blog.csdn.net/yunxiao6/article/details/108615472）写数据流程（以 leader 节点为例，见上图）： 1、etcd 任一节点的 etcd server 模块收到 Client 写请求（如果是 follower 节点，会先通过 Raft 模块将请求转发至 leader 节点处理）。 2、etcd server 将请求封装为 Raft 请求，然后提交给 Raft 模块处理。 3、leader 通过 Raft 协议与集群中 follower 节点进行交互，将消息复制到follower 节点，于此同时，并行将日志持久化到 WAL。 4、follower 节点对该请求进行响应，回复自己是否同意该请求。 5、当集群中超过半数节点（(n&#x2F;2)+1 members ）同意接收这条日志数据时，表示该请求可以被Commit，Raft 模块通知 etcd server 该日志数据已经 Commit，可以进行 Apply。 6、各个节点的 etcd server 的 applierV3 模块异步进行 Apply 操作，并通过 MVCC 模块写入后端存储 BoltDB。 7、当 client 所连接的节点数据 apply 成功后，会返回给客户端 apply 的结果。 读数据流程： 1、etcd 任一节点的 etcd server 模块收到客户端读请求（Range 请求） 判断读请求类型，如果是串行化读（serializable）则直接进入 Apply 流程。 2、如果是线性一致性读（linearizable），则进入 Raft模块。 3、Raft模块向 leader 发出 ReadIndex 请求，获取当前集群已经提交的最新数据 Index。 4、等待本地 AppliedIndex 大于或等于 ReadIndex 获取的 CommittedIndex 时，进入Apply 流程。 5、Apply 流程：通过Key名从KV Index模块获取 Key最新的 Revision，再通过Revision从BoltDB 获取对应的Key和Value。 etcd 通过 WAL（预写日志）实现了内存中数据的强持久性，WAL日志受到磁盘IO 写入速度影响，fdatasync延迟也会影响etcd性能。底层ceph为分布式存储，存储多副本会进行同步，副本同步时将占用大量网络和IO资源影响性能，底层又为SAS盘，对ETCD性能影响较大。 使用FIO模拟etcd io写入使用FIO模拟etcd io写入 安装FIO curl -LO https://github.com/rancherlabs/support-tools/raw/master/instant-fio-master/instant-fio-master.sh bash instant-fio-master.sh 创建测试目录，对应的在&#x2F;var&#x2F;lib&#x2F;etcd目录下进行性能测试，更能直观体现 export PATH=/usr/local/bin:$PATH cd /var/lib/etcd mkdir test-data fio --rw=write --ioengine=sync --fdatasync=1 --directory=test-data --size=100m --bs=2300 --name=mytest size：表示总的写入大小bs：表示每次写入的大小（单位为字节） 为了更好的模拟实际IO写入，需要通过lsof和strace查看实际IO写入量 通过lsof获取etcd进程的文件描述符 lsof -p $(pgrep etcd)|grep wal lsof -p $(pgrep etcd)|grep wal etcd 21040 root 7w REG 252,1 64000000 828705 /var/lib/rancher/etcd/member/wal/1.tmp etcd 21040 root 8r DIR 252,1 4096 838659 /var/lib/rancher/etcd/member/wal etcd 21040 root 11w REG 252,1 64000000 828702 /var/lib/rancher/etcd/member/wal/0000000000000005-000000000007016b.wal 11w就是写入对应的wal文件的文件描述符，通过strace查看etcd系统调用，查看实际的数据写入量。 strace -f -p $(pgrep etcd) -T -tt -o test.txt 访问test.txt文件查找write(11 21064 11:23:24.438231 write(11, &quot;\\25\\3\\0\\0\\0\\0\\0\\203\\10\\2\\20\\303\\240\\345\\252\\16\\32\\212\\6\\10\\0\\20\\2\\30\\306\\276\\34\\&quot;\\377\\0052\\337&quot;..., 840 &lt;unfinished ...&gt; 21306 11:23:24.438248 &lt;... write resumed&gt; ) = 42 &lt;0.000037&gt; 21215 11:23:24.438263 &lt;... futex resumed&gt; ) = 0 &lt;0.005978&gt; 21068 11:23:24.438277 &lt;... futex resumed&gt; ) = 1 &lt;0.000051&gt; 21064 11:23:24.438291 &lt;... write resumed&gt; ) = 840 &lt;0.000048&gt; 21306 11:23:24.438305 futex(0xc00080cf48, FUTEX_WAIT_PRIVATE, 0, NULL &lt;unfinished ...&gt; 21068 11:23:24.438319 futex(0xc0004d2148, FUTEX_WAIT_PRIVATE, 0, NULL &lt;unfinished ...&gt; 21060 11:23:24.438333 &lt;... nanosleep resumed&gt; NULL) = 0 &lt;0.000247&gt; 21060 11:23:24.438352 nanosleep(&#123;tv_sec=0, tv_nsec=20000&#125;, &lt;unfinished ...&gt; 21215 11:23:24.438496 futex(0xc00080cf48, FUTEX_WAKE_PRIVATE, 1 &lt;unfinished ...&gt; 21064 11:23:24.438530 fdatasync(11 &lt;unfinished ...&gt; 可以看见文件描述符11在write完后进行了fdatasync操作通过write操作可以看见此次数据写入量为840字节，多对比几个发现范围在800-900之间，因为我的环境为单节点环境，实际数据写入量根etcd版本和集群规模有直接关系，通常情况下在2300左右，所以这里fio的bs参数设置为2300字节，模拟etcd io写入，查看延时情况。 测试结果 mytest: (g=0): rw=write, bs=(R) 2300B-2300B, (W) 2300B-2300B, (T) 2300B-2300B, ioengine=sync, iodepth=1 fio-3.30-67-gdc472 Starting 1 process mytest: Laying out IO file (1 file / 100MiB) Jobs: 1 (f=1) Jobs: 1 (f=1): [W(1)][100.0%][w=636KiB/s][w=283 IOPS][eta 00m:00s] mytest: (groupid=0, jobs=1): err= 0: pid=16852: Mon Jul 4 09:46:37 2022 write: IOPS=253, BW=569KiB/s (583kB/s)(100.0MiB/179902msec); 0 zone resets clat (usec): min=5, max=4377, avg=16.96, stdev=32.00 lat (usec): min=5, max=4377, avg=17.51, stdev=32.04 clat percentiles (usec): | 1.00th=[ 8], 5.00th=[ 10], 10.00th=[ 10], 20.00th=[ 11], | 30.00th=[ 12], 40.00th=[ 13], 50.00th=[ 14], 60.00th=[ 16], | 70.00th=[ 18], 80.00th=[ 22], 90.00th=[ 29], 95.00th=[ 34], | 99.00th=[ 49], 99.50th=[ 57], 99.90th=[ 81], 99.95th=[ 96], | 99.99th=[ 1369] bw ( KiB/s): min= 89, max= 691, per=99.97%, avg=569.10, stdev=63.60, samples=359 iops : min= 40, max= 308, avg=253.57, stdev=28.33, samples=359 lat (usec) : 10=15.39%, 20=60.57%, 50=23.19%, 100=0.81%, 250=0.03% lat (msec) : 2=0.01%, 4=0.01%, 10=0.01% fsync/fdatasync/sync_file_range: sync (usec): min=1052, max=434792, avg=3923.05, stdev=3609.22 sync percentiles (usec): | 1.00th=[ 1237], 5.00th=[ 1385], 10.00th=[ 1483], 20.00th=[ 1663], | 30.00th=[ 1876], 40.00th=[ 2278], 50.00th=[ 4359], 60.00th=[ 4752], | 70.00th=[ 5211], 80.00th=[ 5669], 90.00th=[ 6325], 95.00th=[ 6849], | 99.00th=[ 8455], 99.50th=[ 12649], 99.90th=[ 22938], 99.95th=[ 23725], | 99.99th=[166724] cpu : usr=0.33%, sys=1.60%, ctx=109419, majf=0, minf=14 IO depths : 1=200.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0% issued rwts: total=0,45590,0,0 short=45590,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=1 Run status group 0 (all jobs): WRITE: bw=569KiB/s (583kB/s), 569KiB/s-569KiB/s (583kB/s-583kB/s), io=100.0MiB (105MB), run=179902-179902msec Disk stats (read/write): vda: ios=4/120187, merge=0/53744, ticks=56/185772, in_queue=9776, util=1.54% 主要看fsync/fdatasync/sync_file_range:项的 99.00th=[ 18455], 99.50th=[ 12649], 表示百分之99的sync为18455usec，对应的etcd要求写入WAL文件时百分之99的fdatasync请求必须小于 10 毫秒。https://etcd.io/docs/v3.4/op-guide/performance/ 参考链接： https://blog.happyhack.io/2021/08/05/fio-and-etcd/ https://www.suse.com/support/kb/doc/?id=000020100 https://www.ibm.com/cloud/blog/using-fio-to-tell-whether-your-storage-is-fast-enough-for-etcd https://blog.csdn.net/yunxiao6/article/details/108615472","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"Rancher2.6 Monitoring使用","slug":"rancher_monitor","date":"2022-06-29T13:45:59.000Z","updated":"2022-06-29T13:45:59.000Z","comments":true,"path":"2022/06/29/rancher_monitor/","permalink":"http://yoursite.com/2022/06/29/rancher_monitor/","excerpt":"","text":"软件 版本 Rancher .9 Kubernetes 1.23.7+rke2r2 概述Rancher 2.6监控启用方式与之前版本存在较大差异，属于原生的Prometheus-Operator，通过抽象化一些Kubernetes CRD资源，可以更好的把监控告警功能整合起来，提高易用性。Prometheus-operator包括以下CRD资源对象： PrometheusRules ：定义告警规则 Alert Managers ：Altermanager启动CRD，用于Altermanager启动副本。 Receivers：配置告警接收媒介CRD Routers： 将告警规则和告警媒介进行匹配。 ServiceMonitor：定义Prometheus采集的监控指标地址 Pod Monitor：更细粒化的对POD进行监控。 配置使用启用监控具体方法如下 切换到对应集群，选择左下角clusterTools启用Prometheus 部署到System项目中，勾选自定义helm参数 根据实际需求修改部署要求 如果需要对接远端存储如infuxdb需要修改yaml，修改配置指向influxdb。 1234remoteRead: - url: http://192.168.0.7:8086/api/v1/prom/read?db=prometheusremoteWrite: - url: http://192.168.0.7:8086/api/v1/prom/write?db=prometheus 默认node-Exporter资源limit配置较低，长时间运行后容易被OOMKILL掉，需要修改默认的内存限制为150Mi。 123456789podLabels: jobLabel: node-exporter resources: limits: cpu: 200m memory: 150Mi requests: cpu: 100m memory: 30Mi 在此页面可以点击进入对应的组件配置页面。如： Altermanager：进入的是告警信息查看页。 Grafana：查看监控数据图标 Prometheus Graph：Prometheus表达式执行页 Prometheus Rules：查看Prometheus配置的告警表达式页 Prometheus Targets：监控采集数据采集点 配置自定义监控指标默认启用监控会会自动添加一些ServiceMonitor监控规则和Prometheus Rules 告警规则，主要是针对平台组件监控和集群内节点状态监控和告警 如针对java应用的jmx监控 Jmx有官方的prometheus-export，我们只需要将其jar包下载让java应用程序加载jar包和加载其配置即可。以一个应用为例，整体流程如下：利用JMX exporter，在Java进程内启动一个小型的Http server配置Prometheus抓取那个Http server提供的metrics。配置Grafana连接Prometheus，配置Dashboard。创建文件夹： 1mkdir -p /Dockerfile/jmx-exporter/ 下载jmx-export.jar包放到此目录 12https://github.com/prometheus/jmx_exporterhttps://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.12.0/jmx_prometheus_javaagent-0.12.0.jar 编写jvm-export配置文件放置&#x2F;root&#x2F;jmx-exporter&#x2F;目录创建simple-config.yml内容如下： 123---rules:- pattern: &quot;.*&quot; 这里意思表示将全部监控信息抓取出来。将jvm-export集成到tomcat中，重新编写Dockerfile 1234FROM tomcatCOPY ./jmx_prometheus_javaagent-0.12.0.jar /jmx_prometheus_javaagent-0.12.0.jarENV CATALINA_OPTS=&quot;-Xms64m -Xmx128m -javaagent:/jmx-exporter/jmx_prometheus_javaagent-0.12.0.jar=6060:/jmx-exporter/simple-config.yml&quot; 重新docker build，build后执行以下docker run命令可以查看收集的监控指标，这里6060端口就是我们的jmx-export端口 12docker build -t tomcat:v1.0 .docker run -itd -p 8080:8080 -p 6060:6060 tomcat:v1.0 访问查看：http://host_ip:6060 部署到Rancher平台 给Service打上label，用于ServiceMonitor关联 1kubectl label svc tomcat app=tomcat 创建ServiceMonitor 1234567891011apiVersion: monitoring.coreos.com/v1kind: ServiceMonitormetadata: name: tomcat-app namespace: defaultspec: endpoints: - port: exporter selector: matchLabels: app: tomcat 创建成功后通过Prometheus可以查看到对应的Target 对应的监控指标也已经抓取 进入grafana页面添加dashboard，默认账号密码为admin&#x2F;prom-operator 添加dashboard 输入dashboard-id，8878，离线环境需要提前将Dashboard下载好，通过json方式导入。 配置告警PrometheusRule用于定义告警规则，默认已经包含针对平台组件和节点的一些告警策略。可以通过配置Router和Receivers配置告警媒介将对应告警通知到相应的人员。采用Routing Tree的告警结构能够快速的将告警进行分类，然后发送到指定的人员进行处理。通过配置AlertmanagerConfig统一实现Rooter和Recivers配置创建AlertmanagerConfig 选择Email告警Receivers配置告警媒介填写SMTP地址和配置的账号&#x2F;密码，默认接收的邮箱。 邮箱密码创建Opaque类型的secret Routes配置用于告警媒介和告警规则进行匹配，默认创建的root规则，用于匹配全部的告警规则，配置上对应创建的告警媒介。此时全部的告警规则都会发送给配置的告警媒介 若要细分告警规则创建新的Routes通过Label与Prometheus Rules内对应的Alter name对接 如匹配alert:etcdNoLeader这条告警规则 也可以使用正则表达式匹配多个规则如 Grouping配置主要用于告警规则分类、抑制避免大量无用告警的干扰 group_by：用于配置告警分组，达到告警抑制效果，同一个group的告警只会聚合到一起发送一次，例如host01上运行了数据库，那么对应的告警包含了host down、mysql down。他们配置在一个group内，那么如果host down了对应的mysql肯定也是down了，那么因为他们配置在一个group中，所以host down和mysql down的告警会聚合到一起发送出。 group_wait：新建的AlterGroup等待多久后触发第一次告警。 group_interval：AlterGroup内产生的不同告警触发间隔时间。 repeat_interval：AlterGroup内如果一直是同样的告警，Altermanager为了避免长时间的干扰，进行告警去重的等待时间。 匹配后，告警触发，可以收到对应的告警邮件 自定义告警当默认的告警规则不能满足需求时，可以根据实际情况添加自定义告警，实际就是添加对应的PrometheusRule。如以下例子，添加pod非running状态的告警。 UI配置 对应yaml配置 1234567891011121314151617apiVersion: monitoring.coreos.com/v1kind: PrometheusRulemetadata: name: podmonitor namespace: cattle-monitoring-systemspec: groups: - name: pod_node_ready rules: - alert: pod_not_ready annotations: message: &#x27;&#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.pod &#125;&#125; is not ready.&#x27; expr: &#x27;sum by (namespace, pod) (kube_pod_status_phase&#123;phase!~&quot;Running|Succeeded&quot;&#125;) &gt; 0 &#x27; for: 180s labels: severity: 严重 for：表示持续时间message：表示告警通知内的信息。label.severity：表示告警级别expr：指标获取表达式 配置告警接收者 根据标签匹配到这个PrometheusRule 常见问题1、触发告警后，邮箱收不到告警邮件使用163邮箱SMTP的465端口altermanager报错msg=&quot;Notify for alerts failed&quot; num_alerts=1 err=&quot;cattle-monitoring-system-test-test/email[0]: notify retry canceled after 16 attempts: send STARTTLS command: 454 Command not permitted when TLS active&quot; 修改 12345678910111213spec: receivers: - emailConfigs: - authPassword: key: password name: altermanager authUsername: xx@163.com from: xx@163.com requireTLS: false sendResolved: false smarthost: smtp.163.com:465 tlsConfig: &#123;&#125; to: xx@qq.com 添加requireTLS: false 2、内部邮件服务器使用非权威证书 123email_configs: - to: &#x27;xxx&#x27; insecure_skip_verify: true 添加insecure_skip_verify: true参考链接： https://mp.weixin.qq.com/s/fT-AXnPP8rrWxTposbi-9A https://github.com/prometheus-operator/prometheus-operator https://rancher.com/docs/rancher/v2.6/en/monitoring-alerting/guides/enable-monitoring/https://mp.weixin.qq.com/s/c9QGlwQrhLgptNsnQ1m6-w","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"云原生安全平台NeuVector基础使用","slug":"neuvector_2","date":"2022-04-17T13:45:59.000Z","updated":"2022-04-17T13:45:59.000Z","comments":true,"path":"2022/04/17/neuvector_2/","permalink":"http://yoursite.com/2022/04/17/neuvector_2/","excerpt":"","text":"概述上一篇NeuVector文章主要以安装部署为主，本篇将实际结合NeuVector的基础功能进行操作演示，主要包含对于NeuVector安全漏洞管理、合规性和机密性检查、策略管理、准入控制策略、动态安全响应和行为监控。本篇文档适用版本为以NeuVector首个开源版NeuVector:5.0.0-preview.1为主 安全漏洞管理集成CVE漏洞库，每天自动更新，支持对平台（Kubernetes）、主机、容器、镜像仓库进行安全漏洞扫描。 配置自动扫描，当平台漏洞库有更新或有新的节点和容器加入时会自动进行扫描。 针对不同漏洞有不同风险级别，以及对应的组件版本和修复版本提示 每个漏洞可以展示对应的漏洞发布时间、漏洞影响范围、对应的组件影响版本。 对漏洞进行过滤，是否已经修复，漏洞等级、发布时间等 配置对接镜像仓库扫描支持对接多种镜像仓库如（docker-registry（harbor）、JFrog Artifactory、Nexus等） 以对接Harbor为例，配置连接方式，填写连接方式和认证信息，过滤器表示你需要扫描的范围如扫描uat项目下全部镜像则uat/*,如果需要扫描整个Harbor内全部镜像则*。测试设置可以验证编写的表达式的关联情况。 合规性检查和机密性检查NeuVector的合规性审核包括 CIS 基线测试、自定义检查、机密审核以及 PCI、GDPR 和其他法规的行业标准模板扫描。 类型这表示对应的那个基线标准如K.4.1.1对应Kubernetes CIS基线测试4.1.1容器对应的基线标准为D开头的，镜像对应的基线标准为I开头 注：《通用数据保护条例》（General Data Protection Regulation，简称GDPR）为欧洲联盟的条例 在合规性检查中也会检查是否存在密文泄漏情况 包括如 General Private Keys General detection of credentials including &#39;apikey&#39;, &#39;api_key&#39;, &#39;password&#39;, &#39;secret&#39;, &#39;passwd&#39; etc. General passwords in yaml files including &#39;password&#39;, passwd&#39;, &#39;api_token&#39; etc. General secrets keys in key/value pairs Putty Private key Xml Private key AWS credentials / IAM Facebook client secret Facebook endpoint secret Facebook app secret Twitter client Id Twitter secret key Github secret Square product Id Stripe access key Slack API token Slack web hooks LinkedIn client Id LinkedIn secret key Google API key SendGrid API key Twilio API key Heroku API key MailChimp API key MailGun API key 策略管理在NeuVector中通过组的方式对容器和主机进行管理。通过对组进行合规性检查、网络规则、进程和文件访问规则、DLP&#x2F;WAF的检测配置。 NeuVector会自动将当前集群主机加入到nodes组，对于集群内容器会自动创建以nv.开头的组NeuVector的组支持3种模式，学习模式、监控模式、保护模式。各个模式实现作用如下。学习模式：学习和记录容器、主机间网络连接情况和进程执行信息。自动构建网络规则白名单，保护应用网络正常行为。为每个服务的容器中运行的进程设定安全基线，并创建进程配置文件规则白名单 监控模式：NeuVector监视容器和主机网络和进程运行情况，遇到非学习模式下记录的行为将在NeuVector中进行告警。 保护模式： NeuVector监视容器和主机网络和进程运行情况，遇到非学习模式下记录的行为直接拒绝。 新建的容器业务被自动发现默认为学习模式，也可以通过设置将默认模式设置为监控模式或保护模式。 不同组下策略冲突情况下，适用的有效模式如下表： 源组模式 目的组模式 有效模式 学习模式 监控模式 学习模式 学习模式 保护模式 学习模式 监控模式 学习模式 学习模式 监控模式 保护模式 监控模式 保护模式 学习模式 学习模式 保护模式 监控模式 监控模式 为了保证业务的稳定运行，当出现模式不一致时，有效模式以限制最小的模式运行。 生产环境最佳实践使用：使用路径可以是：1、上新业务时先学习模式运行一段时间，进行完整的功能测试和调用测试得到实际运行此业务的网络连接情况和进程执行情况信息。2、监控模式运行一段时间，看看有没有额外的特殊情况，进行判断添加规则。3、最后全部容器都切换到保护模式确定最终形态。 动态微隔离使用场景一：POD间通过网络策略互相隔离在Kubernetes平台中创建四个Nginx。名称和用途如下。workload_name：test-web1 image:nginx 用途：web服务器。workload_name：test-con1 image:nginx 用途：连接客户端1workload_name：test-con2 image:nginx 用途：连接客户端2workload_name：test-con3 image:nginx 用途：连接客户端3 创建workload kubectl create deployment test-web1 --image=nginx kubectl expose deployment/test-web1 --port=80 --type=NodePort kubectl create deployment test-con1 --image=nginx kubectl create deployment test-con2 --image=nginx kubectl create deployment test-con3 --image=nginx 此时在NeuVector中会自动生成这几个组。 在test-con1中通过curl访问test-web1 此时可以正常访问，因为在学习模式下。NeuVector也会自动添加此访问规则。 将test-web1和test-con2都设置为监控模式 然后在test-con2中curl访问test-web1 此时test-con2可以正常访问test-web1，但在NeuVector中会生成告警 同时对应的在网络活动拓扑图中也可以看见对应的连接链路变为红色。 将test-web1和test-con2都设置为保护模式，在通过test-con2去curl test-web1 因为curl在学习模式时没有使用，也不是NeuVector默认允许的可执行进程，所以进程直接就无法访问了。 将test-con1设置为保护模式，此时test-con1无法访问外部网络， 可以通过自定义添加网络规则方式开通访问。 在网络规则页，此处规则已经是在学习模式下生成的规则列表。 添加外部访问规则 NeuVector深度了解应用程序行为，并将分析有效负载以确定应用程序协议。协议包括HTTP，HTTPS，SSL，SSH，DNS，DNCP，NTP，TFTP，ECHO，RTSP，SIP，MySQL，Redis，Zookeeper，Cassandra，MongoDB，PostgresSQL，Kafka，Couchbase，ActiveMQ，ElasticSearch，RabbitMQ，Radius，VoltDB，Consul，Syslog，Etcd，Spark，Apache，Nginx，Jetty，NodeJS，Oracle，MSSQL和gRPC。 现在test-con1的curl去访问www.baidu.com以正常访问。 总结：除上述策略外，NeuVector也内置网络威胁检测，能够快速识别常用网络攻击，保护业务容器安全运行。 无论保护模式如何。在”学习和监视”模式下，将发出警报，并且可以在”通知”-&gt;安全事件中找到这些威胁。在保护模式下，这些将收到警报和阻止。还可以根据威胁检测创建响应规则。 包含的威胁检测如下： SYN flood attack ICMP flood attack IP Teardrop attack TCP split handshake attack PING death attack DNS flood DDOS attack Detect SSH version 1, 2 or 3 Detect SSL TLS v1.0 SSL heartbeed attack Detect HTTP negative content-length buffer overflow HTTP smugging attack HTTP Slowloris DDOS attack TCP small window attack DNS buffer overflow attack Detect MySQL access deny DNS zone transfer attack ICMP tunneling attack DNS null type attack SQL injection attack Apache Struts RCE attack DNS tunneling attack TCP Small MSS attack Cipher Overflow attack Kubernetes man-in-the-middle attack per CVE-2020-8554 进程管理NeuVector支持对容器和主机内进程进行管理在学习模式下，运行的进程和命令会自动添加到规则中 此时在test-con1中执行df -h会发现报错bash: /bin/df: Operation not permitted在nv.test-con1.default组中添加df进程规则 然后在重新执行即可执行。 进程管理也支持对node节点，可以在node组中进行限制，约束宿主机进程执行。如限制执行docker cp 执行，通过学习模式得知是docker-tar进程在后端执行将节点切换到保护模式，限制docker-tar进程即可。 这些在节点就无法执行docker cp 准入策略控制NeuVector支持与Kubernetes准入控制（admission-control）功能对接，实现UI配置准入控制规则，对请求进行拦截，用于对请求的资源对象进行校验。NeuVector支持多种准入控制策率配置如镜像CVE漏洞情况限制、部署特权模式、镜像内使用root用户、特定标签等。 在策略-准入控制中开启此功能，注意：需要Kubernetes集群提前开启admission-control功能 NeuVector准入策略控制，支持两种模式，监控模式和保护模式，对应含义和组的模式一样的。这里我们直接切换到保护模式，添加策略。 添加完后，在Rancher中部署特权模式容器会提示解决，策略生效。 动态安全响应NeuVector事件响应机制可以配置响应规则根据安全事件情况进行动态响应，包括以下事件：漏洞扫描结果、CIS基准测试、准入控制事件等。 响应动作包括隔离、webhook通知、日志抑制隔离模式：对应的容器网络进出流量将全部被切断。webhook通知：将触发信息通过webhook方式进行告警。日志抑制：对触发告警信息进行抑制。 以CVE漏洞配置为例，配置包含CVE漏洞名称为CVE-2020-16156的容器进入隔离模式。 组名对应的是影响范围，如果为空，表示对全部的组都生效，填写组名可以设置对特定组生效。 配置策略后，在集群去curl nginx容器，发现无法访问，在NeuVector中查看容器状态为隔离状态。 删除策略时，也可以配置将对应隔离状态容器解除隔离。 注意：1、隔离操作不适用于为主机事件触发的规则2、每个规则可以有多个操作。 行为监控网络流量可视化 网络流量可视化，可以清晰可见容器集群内网络连接关系，当前容器连接会话并且可以过滤网络连接信息，进行图标展示。能够快速进行网络问题定位。 流量抓包针对容器可进行网络抓包，方便故障不需要进入主机获取高权限，就能使进行网络问题深入排查。 采集到的数据包可直接下载通过Wireshark进行解包分析。","categories":[{"name":"安全","slug":"安全","permalink":"http://yoursite.com/categories/%E5%AE%89%E5%85%A8/"}],"tags":[{"name":"安全","slug":"安全","permalink":"http://yoursite.com/tags/%E5%AE%89%E5%85%A8/"}]},{"title":"使用Bird模拟BGP Peers","slug":"bird_bgpper","date":"2022-03-23T13:45:59.000Z","updated":"2022-03-23T13:45:59.000Z","comments":true,"path":"2022/03/23/bird_bgpper/","permalink":"http://yoursite.com/2022/03/23/bird_bgpper/","excerpt":"","text":"概述calico网络插件最为知名的就是calico-bgp模式，在测试中需要验证calico-bgp跨子网路由同步，需要连接两个子网的路由器支持BGP协议，这给测试环境搭建带来很大复杂性。本次文档通过Bird软件将一个虚拟机模拟为软路由，并配置为Kubernetes节点BGP Peers，实现BGP路由同步。 软件版本 软件 版本 Kubernetes v1.20.15 calico v3.17.2 拓扑架构图 Hostname：rke-node3host-ip：192.168.0.7pod-cidr：10.41.113.192&#x2F;26 Hostname：rke-node4host-ip：192.168.0.25pod-cidr：10.41.57.192&#x2F;26 Hostname：rke-node6host-ip：192.168.2.14pod-cidr：10.41.210.128&#x2F;26 Hostname：rke-node7host-ip：192.168.2.15pod-cidr：10.41.210.0&#x2F;26 kubernetes 节点分布在两个子网，中间通过一台vm连接了两个子网，在vm上部署bird软路由进行两个子网通信，同属一个AS自治域。 注意：如果底层是OpenStack环境需要关闭网卡安全组。 Bird部署配置节点配置Bird节点采用一台VM部署，操作系统采用Centos7.6，将此节点作为软路由需要确保以下功能开启。 内核forward转发 sysctl -a|grep &quot;net.ipv4.ip_forward = 1&quot; net.ipv4.ip_forward = 1 iptables数据包转发 iptables -P FORWARD ACCEPT 需要互相联通的节点上需要配置互访的静态路由 如在192.168.0.0&#x2F;24的节点上配置 ip route add 192.168.2.0/24 via 192.168.0.40 dev ens3 如在192.168.2.0&#x2F;24的节点上配置 ip route add 192.168.0.0/24 via 192.168.2.16 dev ens3 验证互访，在192.168.0.0&#x2F;24主机ping 192.168.2.0&#x2F;24主机 Bird配置Bird配置文件 123mkdir /bird/vim /bird/bird.conf router id 192.168.0.40; filter calico_export_to_bgp_peers &#123; if ( net ~ 10.41.0.0/16 ) then &#123; accept; &#125; if ( net ~ 10.42.0.0/16 ) then &#123; accept; &#125; reject; &#125; filter calico_kernel_programming &#123; if ( net ~ 10.41.0.0/16 ) then &#123; accept; &#125; if ( net ~ 10.42.0.0/16 ) then &#123; accept; &#125; accept; &#125; # Configure synchronization between routing tables and kernel. protocol kernel &#123; learn; # Learn all alien routes from the kernel persist; # Don&#39;t remove routes on bird shutdown scan time 2; # Scan kernel routing table every 2 seconds import all; export filter calico_kernel_programming; # Default is export none graceful restart; # Turn on graceful restart to reduce potential flaps in # routes when reloading BIRD configuration. With a full # automatic mesh, there is no way to prevent BGP from # flapping since multiple nodes update their BGP # configuration at the same time, GR is not guaranteed to # work correctly in this scenario. merge paths on; # Allow export multipath routes (ECMP) &#125; protocol device &#123; debug &#123; states &#125;; scan time 2; # Scan interfaces every 2 seconds &#125; protocol direct &#123; debug &#123; states &#125;; interface -&quot;cali*&quot;, -&quot;kube-ipvs*&quot;, &quot;*&quot;; # Exclude cali* and kube-ipvs* but # include everything else. In # IPVS-mode, kube-proxy creates a # kube-ipvs0 interface. We exclude # kube-ipvs0 because this interface # gets an address for every in use # cluster IP. We use static routes # for when we legitimately want to # export cluster IPs. &#125; # Template for all BGP clients template bgp bgp_template &#123; debug &#123; states &#125;; description &quot;Connection to BGP peer&quot;; local as 63400; multihop; gateway recursive; # This should be the default, but just in case. import all; # Import all routes, since we don&#39;t know what the upstream # topology is and therefore have to trust the ToR/RR. export filter calico_export_to_bgp_peers; # Only want to export routes for workloads. source address 192.168.0.40; # The local address we use for the TCP connection add paths on; graceful restart; # See comment in kernel section about graceful restart. connect delay time 2; connect retry time 5; error wait time 5,30; &#125; protocol bgp Node_192_168_0_25 from bgp_template &#123; rr client; neighbor 192.168.0.25 as 63400; &#125; protocol bgp Node_192_168_0_7 from bgp_template &#123; rr client; neighbor 192.168.0.7 as 63400; &#125; protocol bgp Node_192_168_2_14 from bgp_template &#123; rr client; neighbor 192.168.2.14 as 63400; &#125; protocol bgp Node_192_168_2_15 from bgp_template &#123; rr client; neighbor 192.168.2.15 as 63400; &#125; 将配置文件中的route-id、pod-cidr、neighbor-ip、as_number修改为实际需要建立bgp邻居的节点ip。 为了方便部署，本次bird使用Docker启动，启动命令如下： docker run -itd --net=host --uts=host --cap-add=NET_ADMIN --cap-add=NET_BROADCAST --cap-add=NET_RAW -v /bird/:/etc/bird:ro ibhde/bird4 检查启动状态是否为up docker ps -a Calico BGP对接全部节点上安装calicoctl wget https://github.com/projectcalico/calicoctl/releases/download/v3.17.4/calicoctl-linux-amd64 mv calicoctl-linux-amd64 /usr/bin/calicoctl chmod a+x /usr/bin/calicoctl 关闭全局full-mesh cat &lt;&lt;EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: BGPConfiguration metadata: name: default spec: logSeverityScreen: Info nodeToNodeMeshEnabled: false asNumber: 63400 EOF 配置节点label 这里将两组节点打上不同标签，将192.168.2.0&#x2F;24节点打上rack&#x3D;rack-1标签，连接192.168.2.16 bpg-peers，将192.168.0.0&#x2F;24打上rack-rack-2标签，连接192.168.0.40 bgp-peers kubectl label nodes rke-node3 rack=rack-2 kubectl label nodes rke-node4 rack=rack-2 kubectl label nodes rke-node5 rack=rack-1 kubectl label nodes rke-node5 rack=rack-1 使用caliclctl配置BGP Peers cat &lt;&lt;EOF | calicoctl apply -f - apiVersion: projectcalico.org/v3 kind: BGPPeer metadata: name: rack1-tor spec: peerIP: 192.168.2.16 asNumber: 63400 nodeSelector: rack == &#39;rack-1&#39; EOF apiVersion: projectcalico.org/v3 kind: BGPPeer metadata: name: rack2-tor spec: peerIP: 192.168.0.40 asNumber: 63400 nodeSelector: rack == &#39;rack-2&#39; 检查与BGP Peers连接情况 在rack&#x3D;rack-2标签节点执行，应显示已经与192.168.0.40 bgp-peers建立连接 calicoctl node status Calico process is running. IPv4 BGP status +--------------+---------------+-------+------------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+---------------+-------+------------+-------------+ | 192.168.0.40 | node specific | up | 2022-03-18 | Established | +--------------+---------------+-------+------------+-------------+ IPv6 BGP status No IPv6 peers found. 在rack&#x3D;rack-1标签节点执行，应显示已经与192.168.2.16 bgp-peers建立连接 calicoctl node status Calico process is running. IPv4 BGP status +--------------+---------------+-------+------------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+---------------+-------+------------+-------------+ | 192.168.2.16 | node specific | up | 2022-03-18 | Established | +--------------+---------------+-------+------------+-------------+ IPv6 BGP status No IPv6 peers found. 创建pod，验证路由同步 kubectl create deployment test --image=nginx --replicas=5 在5副本中，互相进行ping操作。验证跨节点网络是否正常。 在bird节点查看路由学习 ip route default via 192.168.2.1 dev eth0 10.41.210.0/26 via 192.168.2.15 dev eth0 proto bird 10.42.57.192/26 via 192.168.0.25 dev eth1 proto bird 10.42.113.192/26 via 192.168.0.7 dev eth1 proto bird 10.42.210.128/26 via 192.168.2.14 dev eth0 proto bird 192.168.0.0/24 dev eth1 proto kernel scope link src 192.168.0.40 192.168.2.0/24 dev eth0 proto kernel scope link src 192.168.2.16 可以看见bird将集群内每个节点的pod-cidr都学习过来了。 在任意一个node节点上查看路由，以192.168.0.3节点为例，可以看见节点上也拥有集群全部pod-cidr路由信息。 ip route default via 192.168.0.1 dev ens3 proto dhcp src 192.168.0.7 metric 100 10.41.57.192/26 via 192.168.0.25 dev ens3 proto bird blackhole 10.41.113.192/26 proto bird 10.41.210.0/26 via 192.168.0.40 dev ens3 proto bird 10.41.210.128/26 via 192.168.0.40 dev ens3 proto bird 10.42.57.192/26 via 192.168.0.25 dev ens3 proto bird 192.168.0.0/24 dev ens3 proto kernel scope link src 192.168.0.7 192.168.2.0/24 via 192.168.0.40 dev ens3 节点POD-CIDR路由统一走默认路由当前路由同步会将每个节点pod-cidr同步到集群中的节点上，对于Kubernetes集群规模大情况下会造成路由条目增多。可以通过下发默认路由方式，将节点全部流量请求都都指向bird 软路由节点。这样还有一个好处就是，在一些硬件SDN设备中可以实现流量监控。但需要注意的是路由器本身能承载的流量。 以bird配置为例 router id 192.168.0.40; protocol static &#123; route 10.41.0.0/16 via 192.168.0.40; route 10.42.0.0/16 via 192.168.0.40; &#125; filter calico_export_to_bgp_peers &#123; if ( net ~ 10.41.0.0/16 ) then &#123; accept; &#125; if ( net ~ 10.42.0.0/16 ) then &#123; accept; &#125; reject; &#125; filter calico_kernel_programming &#123; if ( net ~ 10.41.0.0/16 ) then &#123; accept; &#125; if ( net ~ 10.42.0.0/16 ) then &#123; accept; &#125; accept; &#125; # Configure synchronization between routing tables and kernel. protocol kernel &#123; learn; # Learn all alien routes from the kernel persist; # Don&#39;t remove routes on bird shutdown scan time 2; # Scan kernel routing table every 2 seconds import all; export filter calico_kernel_programming; # Default is export none graceful restart; # Turn on graceful restart to reduce potential flaps in # routes when reloading BIRD configuration. With a full # automatic mesh, there is no way to prevent BGP from # flapping since multiple nodes update their BGP # configuration at the same time, GR is not guaranteed to # work correctly in this scenario. merge paths on; # Allow export multipath routes (ECMP) &#125; protocol device &#123; debug &#123; states &#125;; scan time 2; # Scan interfaces every 2 seconds &#125; protocol direct &#123; debug &#123; states &#125;; interface -&quot;cali*&quot;, -&quot;kube-ipvs*&quot;, &quot;*&quot;; # Exclude cali* and kube-ipvs* but # include everything else. In # IPVS-mode, kube-proxy creates a # kube-ipvs0 interface. We exclude # kube-ipvs0 because this interface # gets an address for every in use # cluster IP. We use static routes # for when we legitimately want to # export cluster IPs. &#125; # Template for all BGP clients template bgp bgp_template &#123; debug &#123; states &#125;; description &quot;Connection to BGP peer&quot;; local as 63400; multihop; gateway recursive; # This should be the default, but just in case. import all; # Import all routes, since we don&#39;t know what the upstream # topology is and therefore have to trust the ToR/RR. export filter calico_export_to_bgp_peers; # Only want to export routes for workloads. source address 192.168.0.40; # The local address we use for the TCP connection add paths on; graceful restart; # See comment in kernel section about graceful restart. connect delay time 2; connect retry time 5; error wait time 5,30; &#125; protocol bgp Node_192_168_0_25 from bgp_template &#123; neighbor 192.168.0.25 as 63400; &#125; protocol bgp Node_192_168_0_7 from bgp_template &#123; neighbor 192.168.0.7 as 63400; &#125; protocol bgp Node_192_168_2_14 from bgp_template &#123; neighbor 192.168.2.14 as 63400; &#125; protocol bgp Node_192_168_2_15 from bgp_template &#123; neighbor 192.168.2.15 as 63400; &#125; 将neighbor配置中的 rr client删除，同时添加静态路由下发配置 protocol static &#123; route 10.41.0.0/16 via 192.168.0.40; route 10.42.0.0/16 via 192.168.0.40; &#125; 在192.168.0.0&#x2F;24的主机上看见路由情况如下： ip route default via 192.168.0.1 dev ens3 proto dhcp src 192.168.0.7 metric 100 10.41.0.0/16 via 192.168.0.40 dev ens3 proto bird blackhole 10.41.113.192/26 proto bird 10.42.0.0/16 via 192.168.0.40 dev ens3 proto bird 可以看见pod-cidr的流量都被发送到Bird虚拟路由器192.168.0.40接口在192.168.2.0&#x2F;24的主机上看见路由情况如下： ip route default via 192.168.2.1 dev ens7 10.41.0.0/16 via 192.168.2.16 dev ens7 proto bird blackhole 10.41.210.128/26 proto bird 10.42.0.0/16 via 192.168.2.16 dev ens7 proto bird 可以看见pod-cidr的流量都被发送到Bird虚拟路由器192.168.2.16接口 节点POD-IP明细路由发布在实际使用中若期望将calico-pod明细路由发布到BGP路由器中，则需要修改每个节点的calico配置文件修改方法如下 创建configmap，替换calico原有的bird_aggr.cfg.template文件 主要修改以下参数：注释掉本地黑洞路由，就不会生产本地聚合路由同步到BGP路由器了。 12# route &#123;&#123;$cidr&#125;&#125; blackhole; 允许明细路由同步将if ( net ~ &#123;&#123;$cidr&#125;&#125; ) then &#123; reject; &#125; 修改为accept 完整配置如下： # Generated by confd &#123;&#123;- $block_key := printf \"/calico/ipam/v2/host/%s/ipv4/block\" (getenv \"NODENAME\")&#125;&#125; &#123;&#123;- $static_key := \"/calico/staticroutes\"&#125;&#125; &#123;&#123;if or (ls $block_key) (ls $static_key)&#125;&#125; protocol static &#123; &#123;&#123;- if ls $block_key&#125;&#125; # IP blocks for this host. &#123;&#123;- range ls $block_key&#125;&#125; &#123;&#123;- $parts := split . \"-\"&#125;&#125; &#123;&#123;- $cidr := join $parts \"/\"&#125;&#125; # route &#123;&#123;$cidr&#125;&#125; blackhole; &#123;&#123;- end&#125;&#125; &#123;&#123;- end&#125;&#125; &#123;&#123;- if ls $static_key&#125;&#125; # Static routes. &#123;&#123;- range ls $static_key&#125;&#125; &#123;&#123;- $parts := split . \"-\"&#125;&#125; &#123;&#123;- $cidr := join $parts \"/\"&#125;&#125; # route &#123;&#123;$cidr&#125;&#125; blackhole; &#123;&#123;- end&#125;&#125; &#123;&#123;- end&#125;&#125; &#125; &#123;&#123;else&#125;&#125;# No IP blocks or static routes for this host.&#123;&#123;end&#125;&#125; # Aggregation of routes on this host; export the block, nothing beneath it. function calico_aggr () &#123; &#123;&#123;- range ls $block_key&#125;&#125; &#123;&#123;- $parts := split . \"-\"&#125;&#125; &#123;&#123;- $cidr := join $parts \"/\"&#125;&#125; &#123;&#123;- $affinity := json (getv (printf \"%s/%s\" $block_key .))&#125;&#125; &#123;&#123;- if $affinity.state&#125;&#125; # Block &#123;&#123;$cidr&#125;&#125; is &#123;&#123;$affinity.state&#125;&#125; &#123;&#123;- if eq $affinity.state \"confirmed\"&#125;&#125; if ( net = &#123;&#123;$cidr&#125;&#125; ) then &#123; accept; &#125; if ( net ~ &#123;&#123;$cidr&#125;&#125; ) then &#123; accept; &#125; &#123;&#123;- end&#125;&#125; &#123;&#123;- else &#125;&#125; # Block &#123;&#123;$cidr&#125;&#125; is implicitly confirmed. if ( net = &#123;&#123;$cidr&#125;&#125; ) then &#123; accept; &#125; if ( net ~ &#123;&#123;$cidr&#125;&#125; ) then &#123; accept; &#125; &#123;&#123;- end &#125;&#125; &#123;&#123;- end&#125;&#125; &#125; 升级calico-node映射此configmap配置文件 重建calico-node 查看Bird节点 ip route default via 192.168.2.1 dev eth0 10.41.0.0/16 via 192.168.0.40 dev eth1 proto bird 10.41.57.199 via 192.168.0.25 dev eth1 proto bird 10.41.57.203 via 192.168.0.25 dev eth1 proto bird 10.41.57.204 via 192.168.0.25 dev eth1 proto bird 10.41.113.193 via 192.168.0.7 dev eth1 proto bird 10.41.113.194 via 192.168.0.7 dev eth1 proto bird 10.41.113.195 via 192.168.0.7 dev eth1 proto bird 10.41.113.196 via 192.168.0.7 dev eth1 proto bird 10.41.113.198 via 192.168.0.7 dev eth1 proto bird 10.41.113.201 via 192.168.0.7 dev eth1 proto bird 10.41.113.202 via 192.168.0.7 dev eth1 proto bird 10.41.210.6 via 192.168.2.15 dev eth0 proto bird 10.41.210.7 via 192.168.2.15 dev eth0 proto bird 10.41.210.8 via 192.168.2.15 dev eth0 proto bird 10.41.210.9 via 192.168.2.15 dev eth0 proto bird 10.41.210.137 via 192.168.2.14 dev eth0 proto bird 10.41.210.138 via 192.168.2.14 dev eth0 proto bird 10.42.0.0/16 via 192.168.0.40 dev eth1 proto bird 已经学习到了每个pod的明细路由，这种方式会导致路由设备压力巨大，因为需要维护大量的路由条目，并且pod的每次删除和创建都会引发的路由条目更新。在实际生产中请谨慎评估后使用。 而实际业务在使用的过程中，会针对一个服务或者一个deployment分配一个IP Pool，这种使用模式会导致Calico的IP Pool没有办法按照Node聚合，出现一些零散的无法聚合的IP地址，最差的情况，会导致每个Pod产生一条路由，会导致路由的条目变为Pod级别。在默认情况下，交换机设备为了防止路由震荡，会对BGP路由进行收敛保护。但是Kubernetes集群中，Pod生命周期短，变化频繁，需要关闭网络设备的路由变更保护机制才能满足Kubernetes的要求；对于不同的网络设备，路由收敛速度也是不同的，在大规模Pod扩容和迁移的场景，或者进行双数据中心切换，除了考虑Pod的调度时间、启动时间，还需要对网络设备的路由收敛速度进行性能评估和压测。 https://blog.51cto.com/u_14992974/2549877 Service-CIDR路由发布为了使集群外部也可以通过Service的Cluster-ip访问到集群内部服务，可以将Service-cidr通过Calico-bgp进行发布。 calicoctl patch BGPConfig default --patch &#39;&#123;&quot;spec&quot;: &#123;&quot;serviceClusterIPs&quot;: [&#123;&quot;cidr&quot;: &quot;10.43.0.0/16&quot;&#125;]&#125;&#125;&#39; 发布后在bird节点上可以看见多条10.43.0.0&#x2F;16地址，因为采用ECMP(等价多路径)方式实现路由负载均衡。 ip route 10.43.0.0/16 proto bird nexthop via 192.168.0.7 dev eth1 weight 1 nexthop via 192.168.0.25 dev eth1 weight 1 nexthop via 192.168.2.14 dev eth0 weight 1 nexthop via 192.168.2.15 dev eth0 weight 1 配置明细路由后发布后，Service-CIDR在BGP路由器中无法看见，可以通过修改bird_aggr.cfg.template文件 添加以下配置，$servicesubnet_split网段根据集群实际Service-CIDR进行修改 123456789101112131415161718192021222324252627&#123;&#123;- $servicesubnet_split := split &quot;10.43.0.0/16&quot; &quot; &quot; &#125;&#125;--- # Service IP block&#123;&#123;- if $servicesubnet_split&#125;&#125;&#123;&#123;- range $servicesubnet_split&#125;&#125; route &#123;&#123;.&#125;&#125; blackhole;&#123;&#123;- end&#125;&#125;&#123;&#123;- end&#125;&#125;---function accept_servicesubnet () &#123;&#123;&#123;- range $servicesubnet_split&#125;&#125; if ( net = &#123;&#123;.&#125;&#125; ) then &#123; accept; &#125; if ( net ~ &#123;&#123;.&#125;&#125; ) then &#123; reject; &#125;&#123;&#123;- end&#125;&#125;&#125;function deny_servicesubnet ()&#123;&#123;&#123;- range $servicesubnet_split&#125;&#125; if ( net = &#123;&#123;.&#125;&#125; ) then &#123; reject; &#125; if ( net ~ &#123;&#123;.&#125;&#125; ) then &#123; reject; &#125;&#123;&#123;- end&#125;&#125;&#125; 完整bird_aggr.cfg.template配置文件如下： # Generated by confd &#123;&#123;- $block_key := printf \"/calico/ipam/v2/host/%s/ipv4/block\" (getenv \"NODENAME\")&#125;&#125; &#123;&#123;- $static_key := \"/calico/staticroutes\"&#125;&#125; &#123;&#123;- $servicesubnet_split := split \"10.43.0.0/16\" \" \" &#125;&#125; &#123;&#123;if or (ls $block_key) (ls $static_key)&#125;&#125; protocol static &#123; &#123;&#123;- if ls $block_key&#125;&#125; # IP blocks for this host. &#123;&#123;- range ls $block_key&#125;&#125; &#123;&#123;- $parts := split . \"-\"&#125;&#125; &#123;&#123;- $cidr := join $parts \"/\"&#125;&#125; # route &#123;&#123;$cidr&#125;&#125; blackhole; &#123;&#123;- end&#125;&#125; &#123;&#123;- end&#125;&#125; &#123;&#123;- if ls $static_key&#125;&#125; # Static routes. &#123;&#123;- range ls $static_key&#125;&#125; &#123;&#123;- $parts := split . \"-\"&#125;&#125; &#123;&#123;- $cidr := join $parts \"/\"&#125;&#125; # route &#123;&#123;$cidr&#125;&#125; blackhole; &#123;&#123;- end&#125;&#125; &#123;&#123;- end&#125;&#125; # Service IP block &#123;&#123;- if $servicesubnet_split&#125;&#125; &#123;&#123;- range $servicesubnet_split&#125;&#125; route &#123;&#123;.&#125;&#125; blackhole; &#123;&#123;- end&#125;&#125; &#123;&#123;- end&#125;&#125; &#125; &#123;&#123;else&#125;&#125;# No IP blocks or static routes for this host.&#123;&#123;end&#125;&#125; # Aggregation of routes on this host; export the block, nothing beneath it. # Export the service block. function accept_servicesubnet () &#123; &#123;&#123;- range $servicesubnet_split&#125;&#125; if ( net = &#123;&#123;.&#125;&#125; ) then &#123; accept; &#125; if ( net ~ &#123;&#123;.&#125;&#125; ) then &#123; reject; &#125; &#123;&#123;- end&#125;&#125; &#125; function deny_servicesubnet () &#123; &#123;&#123;- range $servicesubnet_split&#125;&#125; if ( net = &#123;&#123;.&#125;&#125; ) then &#123; reject; &#125; if ( net ~ &#123;&#123;.&#125;&#125; ) then &#123; reject; &#125; &#123;&#123;- end&#125;&#125; &#125; function calico_aggr () &#123; &#123;&#123;- range ls $block_key&#125;&#125; &#123;&#123;- $parts := split . \"-\"&#125;&#125; &#123;&#123;- $cidr := join $parts \"/\"&#125;&#125; &#123;&#123;- $affinity := json (getv (printf \"%s/%s\" $block_key .))&#125;&#125; &#123;&#123;- if $affinity.state&#125;&#125; # Block &#123;&#123;$cidr&#125;&#125; is &#123;&#123;$affinity.state&#125;&#125; &#123;&#123;- if eq $affinity.state \"confirmed\"&#125;&#125; if ( net = &#123;&#123;$cidr&#125;&#125; ) then &#123; accept; &#125; if ( net ~ &#123;&#123;$cidr&#125;&#125; ) then &#123; accept; &#125; &#123;&#123;- end&#125;&#125; &#123;&#123;- else &#125;&#125; # Block &#123;&#123;$cidr&#125;&#125; is implicitly confirmed. if ( net = &#123;&#123;$cidr&#125;&#125; ) then &#123; accept; &#125; if ( net ~ &#123;&#123;$cidr&#125;&#125; ) then &#123; accept; &#125; &#123;&#123;- end &#125;&#125; &#123;&#123;- end&#125;&#125; &#125;","categories":[{"name":"Network","slug":"Network","permalink":"http://yoursite.com/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://yoursite.com/tags/Network/"}]},{"title":"Neuvector介绍和部署","slug":"neuvector_1","date":"2022-03-17T13:45:59.000Z","updated":"2022-03-17T13:45:59.000Z","comments":true,"path":"2022/03/17/neuvector_1/","permalink":"http://yoursite.com/2022/03/17/neuvector_1/","excerpt":"","text":"Neuvector介绍NeuVector 是最早开发 Docker&#x2F;Kubernetes 安全产品的公司，是 Kubernetes 网络安全的领导者，NeuVector 致力于保障企业级容器平台安全，产品适用于各种云环境、跨云或者本地部署等容器生产环境。NeuVector 提供实时深入的容器网络可视化、东西向容器网络监控、主动隔离和保护、容器主机安全以及容器内部安全。和容器管理平台无缝集成并且实现应用级容器安全的自动化。2021年SUSE收购Neuvector，并将其开源。 项目地址：https://github.com/neuvector/neuvector 架构解析 NeuVector本身包含Controller、Enforcer、Manager、Scanner、Updater模块。 Controller：整个Neuvector的控制模块，API入口，包括配置下发，高可用主要考虑Controller的HA，通常建议部署3个Controller模块组成集群。 Enforcer：主要用于安全策略部署下发和执行，DaemonSet类型会在每个节点部署。Manager：提供web-UI(仅HTTPS)和CLI控制台，供用户管理NeuVector。Scanner:对节点、容器、Kubernetes、镜像进行CVE漏洞扫描Updater:cronjob，用于定期更新CVE漏洞库 功能介绍 安全漏洞扫描 容器网络流量可视化 网络安全策略定义 L7防火墙 CICD安全扫描 合规分析本篇文档更多侧重安装部署，实际功能介绍在后续文章进行深入介绍 NeuVector安装安装环境软件版本：OS：Ubuntu18.04Kubernetes：1.20.14Rancher：2.5.12Docker：19.03.15NeuVector：5.0.0-b1 快速部署创建namespace kubectl create namespace neuvector 部署CRD(Kubernetes 1.19+版本) 12345kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/crd-k8s-1.19.yamlkubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/waf-crd-k8s-1.19.yamlkubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/dlp-crd-k8s-1.19.yamlkubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/admission-crd-k8s-1.19.yaml 部署CRD(Kubernetes 1.18或更低版本) 12345kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/crd-k8s-1.16.yamlkubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/waf-crd-k8s-1.16.yamlkubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/dlp-crd-k8s-1.16.yamlkubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/admission-crd-k8s-1.16.yaml 配置RBAC 12345678910111213141516171819kubectl create clusterrole neuvector-binding-app --verb=get,list,watch,update --resource=nodes,pods,services,namespaceskubectl create clusterrole neuvector-binding-rbac --verb=get,list,watch --resource=rolebindings.rbac.authorization.k8s.io,roles.rbac.authorization.k8s.io,clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.iokubectl create clusterrolebinding neuvector-binding-app --clusterrole=neuvector-binding-app --serviceaccount=neuvector:defaultkubectl create clusterrolebinding neuvector-binding-rbac --clusterrole=neuvector-binding-rbac --serviceaccount=neuvector:defaultkubectl create clusterrole neuvector-binding-admission --verb=get,list,watch,create,update,delete --resource=validatingwebhookconfigurations,mutatingwebhookconfigurationskubectl create clusterrolebinding neuvector-binding-admission --clusterrole=neuvector-binding-admission --serviceaccount=neuvector:defaultkubectl create clusterrole neuvector-binding-customresourcedefinition --verb=watch,create,get,update --resource=customresourcedefinitionskubectl create clusterrolebinding neuvector-binding-customresourcedefinition --clusterrole=neuvector-binding-customresourcedefinition --serviceaccount=neuvector:defaultkubectl create clusterrole neuvector-binding-nvsecurityrules --verb=list,delete --resource=nvsecurityrules,nvclustersecurityruleskubectl create clusterrolebinding neuvector-binding-nvsecurityrules --clusterrole=neuvector-binding-nvsecurityrules --serviceaccount=neuvector:defaultkubectl create clusterrolebinding neuvector-binding-view --clusterrole=view --serviceaccount=neuvector:defaultkubectl create rolebinding neuvector-admin --clusterrole=admin --serviceaccount=neuvector:default -n neuvectorkubectl create clusterrole neuvector-binding-nvwafsecurityrules --verb=list,delete --resource=nvwafsecurityruleskubectl create clusterrolebinding neuvector-binding-nvwafsecurityrules --clusterrole=neuvector-binding-nvwafsecurityrules --serviceaccount=neuvector:defaultkubectl create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=list,delete --resource=nvadmissioncontrolsecurityruleskubectl create clusterrolebinding neuvector-binding-nvadmissioncontrolsecurityrules --clusterrole=neuvector-binding-nvadmissioncontrolsecurityrules --serviceaccount=neuvector:defaultkubectl create clusterrole neuvector-binding-nvdlpsecurityrules --verb=list,delete --resource=nvdlpsecurityruleskubectl create clusterrolebinding neuvector-binding-nvdlpsecurityrules --clusterrole=neuvector-binding-nvdlpsecurityrules --serviceaccount=neuvector:default 检查是否有以下RBAC对象 kubectl get clusterrolebinding | grep neuvector kubectl get rolebinding -n neuvector | grep neuvector kubectl get clusterrolebinding | grep neuvector neuvector-binding-admission ClusterRole/neuvector-binding-admission 44h neuvector-binding-app ClusterRole/neuvector-binding-app 44h neuvector-binding-customresourcedefinition ClusterRole/neuvector-binding-customresourcedefinition 44h neuvector-binding-nvadmissioncontrolsecurityrules ClusterRole/neuvector-binding-nvadmissioncontrolsecurityrules 44h neuvector-binding-nvsecurityrules ClusterRole/neuvector-binding-nvsecurityrules 44h neuvector-binding-nvwafsecurityrules ClusterRole/neuvector-binding-nvwafsecurityrules 44h neuvector-binding-rbac ClusterRole/neuvector-binding-rbac 44h neuvector-binding-view ClusterRole/view 44h kubectl get rolebinding -n neuvector | grep neuvector neuvector-admin ClusterRole/admin 44h 部署NeuVector底层runtime为Docker kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/neuvector-docker-k8s.yaml 底层runtime为containerd（对于k3s和rke2可以使用此yaml文件） kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/neuvector-containerd-k8s.yaml 1.21以下的Kubernetes版本会提示以下错误，将yaml文件下载将batch&#x2F;v1修改为batch&#x2F;v1beta1 error: unable to recognize &quot;https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/neuvector-docker-k8s.yaml&quot;: no matches for kind &quot;CronJob&quot; in version &quot;batch/v1&quot; 1.20.x cronjob还处于beta阶段没有正式GA,1.21版本开始cronjob才进入正式版。 默认部署web-ui使用的是loadblance类型的Service，为了方便访问修改为NodePort，也可以通过Ingress对外提供服务 kubectl patch svc neuvector-service-webui -n neuvector --type=&#39;json&#39; -p &#39;[&#123;&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;/spec/type&quot;,&quot;value&quot;:&quot;NodePort&quot;&#125;,&#123;&quot;op&quot;:&quot;add&quot;,&quot;path&quot;:&quot;/spec/ports/0/nodePort&quot;,&quot;value&quot;:30888&#125;]&#39; 访问https:&#x2F;&#x2F;node_ip:30888 默认密码为admin&#x2F;admin 点击头像旁的My profile页面进入设置页面，设置密码和语言 Helm部署添加repo helm repo add neuvector https://neuvector.github.io/neuvector-helm/ helm search repo neuvector/core 创建namespace kubectl create namespace neuvector 创建ServiceAccount kubectl create serviceaccount neuvector -n neuvector helm安装 helm install neuvector --namespace neuvector neuvector/core --set registry=docker.io --set tag=5.0.0-preview.1 --set=controller.image.repository=neuvector/controller.preview --set=enforcer.image.repository=neuvector/enforcer.preview --set manager.image.repository=neuvector/manager.preview --set cve.scanner.image.repository=neuvector/scanner.preview --set cve.updater.image.repository=neuvector/updater.preview Helm-chart参数查看https://github.com/neuvector/neuvector-helm/tree/master/charts/core 高可用架构设计 NeuVector-HA主要需要考虑Controller模块的HA，只要有一个Controller处于打开状态，所有数据都将在3个副本之间之间同步。Controller数据主要存储在 &#x2F;var&#x2F;neuvector&#x2F; 目录中，但出现POD重建或集群重新部署时，会自动从此目录加载备份文件，进行集群恢复。 部署策略NeuVector官方提供四种HA部署模式 方式一：不进行任何调度限制，由Kubernetes进行自由调度管理管理。 方式二：NeuVector control组件(manager,controller）+enforce、scanner组件配置调度label限制和污点容忍，与Kubernetes master节点部署一起。 方式三：给Kubernetes集群中通过Taint方式建立专属的NeuVector节点，只允许NeuVector control组件部署。 方式四：NeuVector control组件(manager,controller）配置调度label限制和污点容忍，与Kubernetes master节点部署一起。k8s-master不部署enforce和scanner组件，意味着master节点不在接受扫描和策略下发。 以方式二为例，进行部署给master节点打上特定标签 kubectl label nodes nodename nvcontroller=true 获取节点Taint kubectl get node nodename -o yaml|grep -A 5 taint 以rancher部署的节点master节点为例 taints: - effect: NoSchedule key: node-role.kubernetes.io/controlplane value: &quot;true&quot; - effect: NoExecute key: node-role.kubernetes.io/etcd 编辑部署的yaml给NeuVector-control组件（manager,controller）添加nodeSelector和tolerations给enforce、scanner组件只添加tolerations。 例如以manager组件为例： kind: Deployment metadata: name: neuvector-manager-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-manager-pod replicas: 1 template: metadata: labels: app: neuvector-manager-pod spec: nodeSelector: nvcontroller: &quot;true&quot; containers: - name: neuvector-manager-pod image: neuvector/manager.preview:5.0.0-preview.1 env: - name: CTRL_SERVER_IP value: neuvector-svc-controller.neuvector restartPolicy: Always tolerations: - effect: NoSchedule key: &quot;node-role.kubernetes.io/controlplane&quot; operator: Equal value: &quot;true&quot; - effect: NoExecute operator: &quot;Equal&quot; key: &quot;node-role.kubernetes.io/etcd&quot; value: &quot;true&quot; 数据持久化配置环境变量启用配置数据持久化 - env: - name: CTRL_PERSIST_CONFIG 配置此环境变量后，默认情况下Neuvector-Controller会将数据存储在&#x2F;var&#x2F;neuvector目录内，默认此目录是hostpath映射在POD所在宿主机的&#x2F;var&#x2F;neuvector目录内。 若需要更高级别数据可靠性也可以通过PV对接nfs或其他支出多读写的存储中。这样当出现Neuvector-Controller三个POD副本同时都销毁，宿主机都完全不可恢复时，也不会有数据配置数据丢失。以下以NFS为例。部署nfs 创建pv和pvc cat &lt;&lt;EOF | kubectl apply -f - apiVersion: v1 kind: PersistentVolume metadata: name: neuvector-data spec: capacity: storage: 10Gi accessModes: - ReadWriteMany nfs: path: /nfsdata server: 172.16.0.195 EOF cat &lt;&lt;EOF | kubectl apply -f - kind: PersistentVolumeClaim apiVersion: v1 metadata: name: neuvector-data namespace: neuvector spec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi EOF 修改NeuVector-Controller部署yaml，添加pvc信息，将&#x2F;var&#x2F;neuvector目录映射到nfs中（默认是hostpath映射到本地) spec: template: spec: volumes: - name: nv-share # hostPath: // replaced by persistentVolumeClaim # path: /var/neuvector // replaced by persistentVolumeClaim persistentVolumeClaim: claimName: neuvector-data 或直接在NeuVector部署yaml中挂载nfs目录 volumes: - name: nv-share nfs: path: /opt/nfs-deployment server: 172.26.204.144 多云安全管理在实际生产应用中，会存在对多个集群进行安全进行管理，NeuVector支持集群联邦功能。需要在一个集群上暴露Federation Master服务，在每个远端集群上部署Federation Worker服务。为了更好的灵活性，可以在每个集群同时启用Federation Master和Federation Worker服务。在每个集群部署此yaml apiVersion: v1 kind: Service metadata: name: neuvector-service-controller-fed-master namespace: neuvector spec: ports: - port: 11443 name: fed nodePort: 30627 protocol: TCP type: NodePort selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-service-controller-fed-worker namespace: neuvector spec: ports: - port: 10443 name: fed nodePort: 31783 protocol: TCP type: NodePort selector: app: neuvector-controller-pod 将其中一个集群升级为主集群将其中一个集群升级为主集群，配置连接远程暴露ip和对remot cluster可达的端口。 在主集群中，生成token，用于其他remote cluster连接。在remote cluster中配置加入主集群，配置token和连接端子 在界面可以对多个Neuvector集群进行管理 其他配置升级若是采用yaml文件方式部署的NeuVector直接更新对应的组件镜像tag即可完成升级。如 kubectl set image deployment/neuvector-controller-pod neuvector-controller-pod=neuvector/controller:2.4.1 -n neuvector kubectl set image -n neuvector ds/neuvector-enforcer-pod neuvector-enforcer-pod=neuvector/enforcer:2.4.1 若是采用Helm部署的NeuVector，则直接执行helm update配置对应参数即可即可。 卸载删除部署的组件 kubectl delete -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/neuvector-docker-k8s.yaml 删除配置的RBAC kubectl get clusterrolebinding | grep neuvector|awk &#39;&#123;print $1&#125;&#39;|xargs kubectl delete clusterrolebinding kubectl get rolebinding -n neuvector | grep neuvector|awk &#39;&#123;print $1&#125;&#39;|xargs kubectl delete rolebinding -n neuvector 删除对应的CRD kubectl delete -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/crd-k8s-1.19.yaml kubectl delete -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/waf-crd-k8s-1.19.yaml kubectl delete -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/admission-crd-k8s-1.19.yaml 总结：SUSE此次开源的NeuVector是一个成熟稳定的容器安全管理平台，未来NeuVector会和Rancher产品更好的融合。","categories":[{"name":"安全","slug":"安全","permalink":"http://yoursite.com/categories/%E5%AE%89%E5%85%A8/"}],"tags":[{"name":"安全","slug":"安全","permalink":"http://yoursite.com/tags/%E5%AE%89%E5%85%A8/"}]},{"title":"RKE2的简单使用","slug":"rke2","date":"2022-01-01T13:45:59.000Z","updated":"2021-01-01T13:45:59.000Z","comments":true,"path":"2022/01/01/rke2/","permalink":"http://yoursite.com/2022/01/01/rke2/","excerpt":"","text":"概述软件版本：rke2 version v1.22.5+rke2r1os：ubuntu18.04 RKE2是Rancher Kubernetes新的发行版，结合和k3s和RKE1的一些特性。与RKE1相比主要特性在于安全性，符合美国联邦政府部门的安全性和合规性，完整通过CIS安全基线标准，符合FIPS-140-2 标准和定期的镜像安全扫描。比如结合k3s的一个单体二进制文件启动，底层runtime集成containerd。 与其他Kubernetes部署工具对比 组件集成度 安全性 组件容器化 部署简易性 kubeadm 低，需要单独部署kubelet、runtime等组件，然后在通过static-pod启动其他组件。 中，默认安全配置 除kubelet外全部容器化 低，组件HA需要用户自己完成。 RKE-1 低，单独部署runtime然后在通过rke部署集群。 中，默认安全配置 全部容器化 高，一键部署，组件HA自动完成 RKE-2 高，单体二进制文件集成runtime和kubelet，一键启动。 高,专为安全而生，符合各项安全测试规范 除kubelet外全部容器化 中，每台节点需要单独操作安装，组件HA自动完成 RKE2部署部署前提：Linux部署前提条件： 关闭swap。 关闭NetworkManager（若有），或配置NetworkManager忽略 calico&#x2F;flannel 相关网络接口。 关闭Selinux，或参考下述链接配置Selinux规则。 节点主机名采用标准FQDN格式。 若需要开启NetworkManager和Selinux，策略配置NetworkManager和Selinux策略链接：https://rancher2.docs.rancher.cn/docs/rke2/known_issues/_index#networkmanager 通过完整兼容性测试的操作系统：Ubuntu 18.04 (amd64)Ubuntu 20.04 (amd64)CentOS&#x2F;RHEL 7.8 (amd64)CentOS&#x2F;RHEL 8.2 (amd64)SLES 15 SP2 (amd64) (v1.18.16+rke2r1 和更新版本) 注：使用Cilium网络插件时，因为ebpf依赖内核技术，所以需要保证以下内核版本1 、kernel版本 &gt;&#x3D; 4.9.17 通过RKE2单机方式快速部署Kubernetes部署Server 下载rke2二进制可执行文件，和自动配置rke2-server curl -sfL http://rancher-mirror.rancher.cn/rke2/install.sh | INSTALL_RKE2_MIRROR=cn sh - 设置rke2-server开机自启 systemctl enable rke2-server.service 启动rke2-server systemctl start rke2-server.service 此时，将会通过rke2自动拉起kubelet，然后以static-pod方式启动api-server、Controller-manager、etcd、scheduler 日志查看： journalctl -u rke2-server -f 默认情况下rke2将创建以下目录: &#x2F;var&#x2F;lib&#x2F;rancher&#x2F;rke2&#x2F;:存放额外部署的集群插件（core-dns、网络插件、Ingress-Controller）、etcd数据库存放路径、其他worker连接的token。&#x2F;etc&#x2F;rancher&#x2F;rke2&#x2F;：连接集群的kubeconfig文件，以及集群组件参数配置信息。 将常用CLI配置软链接 ln -s /var/lib/rancher/rke2/bin/kubectl /usr/bin/kubectl ln -s /var/lib/rancher/rke2/bin/ctr /usr/bin/ctr ln -s /var/lib/rancher/rke2/bin/crictl /usr/bin/crictl 配置kubeconfig mkdir -p ~/.kube/ cp /etc/rancher/rke2/rke2.yaml ~/.kube/config 验证查看: kubectl get node NAME STATUS ROLES AGE VERSION rke-node6 Ready control-plane,etcd,master 72m v1.22.5+rke2r1 获取worker注册到server的token文件 cat /var/lib/rancher/rke2/server/token 部署worker下载rke2二进制可执行文件，和自动配置rke2-server curl -sfL http://rancher-mirror.rancher.cn/rke2/install.sh | INSTALL_RKE2_MIRROR=cn INSTALL_RKE2_TYPE=&quot;agent&quot; sh - 启动rke2-agent服务 systemctl enable rke2-agent.service 配置rke2-agent服务 mkdir -p /etc/rancher/rke2/ vim /etc/rancher/rke2/config.yaml 配置文件内容如下： server: https://&lt;server&gt;:9345 token: &lt;token from server node&gt; 注：rke2 server 进程通过端口 9345 监听新节点的注册。Kubernetes API 仍然监听端口 6443。 启动服务,等待服务启动注册成功。 systemctl start rke2-agent.service 日志查看 12journalctl -u rke2-agent -f 查看最终部署 kubectl get node NAME STATUS ROLES AGE VERSION rke-node6 Ready control-plane,etcd,master 81m v1.22.5+rke2r1 rke-node7 Ready &lt;none&gt; 70m v1.22.5+rke2r1 测试验证 kubectl create deployment test --image=busybox:1.28 --replicas=2 -- sleep 30000 通过RKE2高可用方式部署Kubernetes前提条件： Apiserver统一入口（可选），为了方便外部访问集群，需要在集群实现统一入口，可以通过L4负载均衡器或vip地址或智能轮询DNS。集群内部已经通过rke2-agent实现了worker访问api-server的多入口反向代理。 奇数个（推荐三个）的 server节点，运行 etcd、Kubernetes API 和其他控制节点服务。 部署顺序 启动第一个 server 节点 加入其他 server 节点 加入 agent 节点 部署负载均衡器（可选）以nginx为例，配置转发到9345和后端6443端口创建nginx.conf文件 events &#123; worker_connections 1024; ## Default: 1024 &#125; stream &#123; upstream kube-apiserver &#123; server host1:6443 max_fails=3 fail_timeout=30s; server host2:6443 max_fails=3 fail_timeout=30s; server host3:6443 max_fails=3 fail_timeout=30s; &#125; upstream rke2 &#123; server host1:9345 max_fails=3 fail_timeout=30s; server host2:9345 max_fails=3 fail_timeout=30s; server host3:9345 max_fails=3 fail_timeout=30s; &#125; server &#123; listen 6443; proxy_connect_timeout 2s; proxy_timeout 900s; proxy_pass kube-apiserver; &#125; server &#123; listen 9345; proxy_connect_timeout 2s; proxy_timeout 900s; proxy_pass rke2; &#125; &#125; 将对应的3个ip地址修改为实际server节点ip地址 启动nginx docker run -itd -p 9345:9345 -p 6443:6443 -v ~/nginx.conf:/etc/nginx/nginx.conf nginx 实际生产环境部署建议部署两个nginx，中间通过keepalived维持vip实现统一入口。 部署第一个Server下载rke2二进制可执行文件，和自动配置rke2-server curl -sfL http://rancher-mirror.rancher.cn/rke2/install.sh | INSTALL_RKE2_MIRROR=cn sh - 设置rke2-server开机自启 systemctl enable rke2-server.service 配置config.yaml文件 mkdir /etc/rancher/rke2/ -p touch config.yaml 输入以内容 tls-san: - xxx.xxx.xxx.xxx - www.xxx.com 此处填写LB的统一入口ip地址或域名，如果有多个换行分组方式隔开。 启动rke2-server systemctl start rke2-server.service 将常用CLI配置软链接 ln -s /var/lib/rancher/rke2/bin/kubectl /usr/bin/kubectl ln -s /var/lib/rancher/rke2/bin/ctr /usr/bin/ctr ln -s /var/lib/rancher/rke2/bin/crictl /usr/bin/crictl 配置kubeconfig mkdir -p ~/.kube/ cp /etc/rancher/rke2/rke2.yaml ~/.kube/config 可以将kubeconfig文件中的中的ip地址由127.0.0.1替换为实际LB的IP地址。 获取注册到server的token文件 cat /var/lib/rancher/rke2/server/token 配置其他Server配置rke2-agent服务 mkdir -p /etc/rancher/rke2/ vim /etc/rancher/rke2/config.yaml 配置文件内容如下： server: https://&lt;server&gt;:9345 token: &lt;token from server node&gt; tls-san: - xxx.xxx.xxx.xxx - www.xxx.com 注： server地址可以填写第一台Server的地址，也可以填写外部统一入口的地址，最佳实践是填写统一入口地址，这样当第一个Server出现问题后，agent还可以通过统一入口地址通过其他Server获取集群信息。 token填写第一台server的token tls-san跟第一台server一样，一般填写统一入口的ip地址或域名，用于TLS证书注册。 下载rke2二进制可执行文件，和自动配置rke2-server curl -sfL http://rancher-mirror.rancher.cn/rke2/install.sh | INSTALL_RKE2_MIRROR=cn sh - 设置rke2-server开机自启 systemctl enable rke2-server.service 启动rke2-server systemctl start rke2-server.service 等待注册和集群启动 验证： kubectl get node NAME STATUS ROLES AGE VERSION rke-node4 Ready control-plane,etcd,master 140m v1.22.5+rke2r1 rke-node5 Ready control-plane,etcd,master 138m v1.22.5+rke2r1 rke-node6 Ready control-plane,etcd,master 19h v1.22.5+rke2r1 rke-node7 Ready &lt;none&gt; 19h v1.22.5+rke2r1 进入etcd-pod，查看etcd集群状态。 etcdctl --cert /var/lib/rancher/rke2/server/tls/etcd/server-client.crt --key /var/lib/rancher/rke2/server/tls/etcd/server-client.key --endpoints https://127.0.0.1:2379 --cacert /var/lib/rancher/rke2/server/tls/etcd/server-ca.crt member list e19d2834bb177be1, started, rke-node4-896165c9, https://192.168.0.25:2380, https://192.168.0.25:2379, false ec67af24a94fb07c, started, rke-node6-fed10843, https://192.168.0.32:2380, https://192.168.0.32:2379, false f7e9f28da0a6e5e6, started, rke-node5-4a4b6af5, https://192.168.0.29:2380, https://192.168.0.29:2379, false 按单机操作加入agent节点。 通过RKE2离线部署kubernetes集群Tarball模式RKE2的离线部署方式与k3s比较相似，都是提前将对应的离线介质下载放置到对应的目录，启动二进制进程执行。 在RKE2对应的Release页下载对应的离线安装介质https://github.com/rancher/rke2/releases主要为以下离线安装介质 rke2-images.linux-amd64.tar rke2.linux-amd64.tar.gz sha256sum-amd64.txt根据所需要的不同网络插件，下载对应的镜像包 rke2-images-canal.linux-amd64.tar.gz 离线安装脚本 将这些下载后的安装介质放置在节点的一个统一目录如&#x2F;root&#x2F;images 下载离线安装脚本 curl -sfL https://get.rke2.io --output install.sh 部署安装 INSTALL_RKE2_ARTIFACT_PATH=/root/images sh install.sh 执行此脚本，将自动对离线介质进行解压到对应目录。接下来就跟在线安装一样，启动RKE2的进程，进行部署server和agent启动rke2 设置rke2-server开机自启 systemctl enable rke2-server.service 启动rke2-server systemctl start rke2-server.service 等待注册和集群启动 Private Registry将镜像上传到镜像仓库可以使用rancher的rancher-load-images.sh脚本结合rke2-images-all.linux-amd64.txt文件进行镜像上传。 下载rke2可执行文件rke2.linux-amd64.tar.gz解压，将systemctl文件和rke2可执行文件复制到对应目录 cp lib/systemd/system/* /usr/local/lib/systemd/system/ cp bin/* /usr/local/bin/ cp share/* /usr/local/share/ -rf 配置config.yaml，指定默认拉取镜像 system-default-registry: xxx.xxx.xxx.xxx 若私有镜像仓库为http或自签名https需要在/etc/rancher/rke2 /registries.yaml进行配置但这里我配置的insecure-registry没有生效，具体issue查看：https://github.com/rancher/rke2/issues/2317 通过RKE2部署Kubernetes高可用实现原理RKE2部署的Kubernetes和其他Kubernetes的组件需要HA的方式是一致的.Kubernetes 集群的高可用是针对： etcd controller-manager scheduler apiserver etcd：通过本身的 Raft 算法 Leader 选主机制，组成ETCD集群，实现 etcd 高可用。 controller manager：leader election 选举竞争锁的机制来保证高可用。 scheduler：leader election 选举竞争锁的机制来保证高可用。 apiserver：无状态，通过前端负载均衡实现高可用。 另外一个在于在rke2集群中，containerd、kubelet组件集成到了rke2服务中，这点和k3s非常相式，同时在rke2服务中还集成了nginx服务，主要用于做为kubelet连接api-server的方向代理。 HA的主要区别在于API-server统一入口，因为RKE2会帮助其他组件自动做HA， 当有统一入口时，跟kubeadm和其他原生Kubernetes一样，所有请求都会通过统一负载均衡器连接到后端的rke2-server。 如果api-server没有统一入口，kubelet和rke2-agent去连接rke2-server时，会用一个server地址去注册即可，然后agent会获取 所有rke2 server 的地址，然后存储到 &#x2F;var&#x2F;lib&#x2F;rancher&#x2F;rke2&#x2F;agent&#x2F;etc&#x2F;rke2-api-server-agent-load-balancer.json中，生成nginx反向代理配置比如： cat rke2-agent-load-balancer.json &#123; &quot;ServerURL&quot;: &quot;https://192.168.3.10:9345&quot;, &quot;ServerAddresses&quot;: [ &quot;192.168.3.11:9345&quot;, &quot;192.168.3.12:9345&quot; ], &quot;Listener&quot;: null 当192.168.3.10 挂掉之后，会自动切换到另一个rke2 server 去连接。当192.168.3.10恢复后，回重新连接192.168.3.10。 另外在前面也提到，rke2里面也集成了containerd，那么问题来了，如果rke2-agent进程出现问题down了，是否会影响平台上业务正常运行呢？答案是，不会影响业务正常运行，因为containerd创建容器是通过containerd-shim-runc-v2调用runc创建，当containerd出现问题时containerd-shim-runc-v2会被init进程托管，不会导致退出影响现有业务POD。但需要注意的是rke2-agent退出后kubelet也退出了，对应的业务状态探测就没有了，在默认超时5分钟后，Controller-manager会将业务pod重建。 其他使用技巧使用RKE2部署Kubernetes使用其他网络插件默认情况下rke2部署使用的是canal做为网络插件，还支持calico和cilium网络插件，若想使用其他网络插件只需要进行配置即可。 如ciliumcilium依赖内核bfp特性，在启用前需要先进行挂载。检查是否有进行挂载 mount | grep /sys/fs/bpf 进行挂载 sudo mount bpffs -t bpf /sys/fs/bpf sudo bash -c &#39;cat &lt;&lt;EOF &gt;&gt; /etc/fstab none /sys/fs/bpf bpf rw,relatime 0 0 EOF&#39; 在次检查 mount | grep /sys/fs/bpf bpffs on /sys/fs/bpf type bpf (rw,relatime) bpffs on /sys/fs/bpf type bpf (rw,relatime) 在start rke2-server和agent服务前先配置config.yaml mkdir -p /etc/rancher/rke2/ vim /etc/rancher/rke2/config.yaml 添加以下参数 cni: cilium 启动rke2-server systemctl start rke2-server.service 查看是否部署成功 kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system cilium-6rfzw 1/1 Running 0 52s kube-system cilium-node-init-998vd 1/1 Running 0 52s kube-system cilium-operator-85f67b5cb7-nw7n8 1/1 Running 0 52s kube-system cilium-operator-85f67b5cb7-qc2vh 0/1 Pending 0 52s kube-system cloud-controller-manager-rke-node4 1/1 Running 0 65s kube-system etcd-rke-node4 1/1 Running 0 73s 组件参数配置在&#x2F;etc&#x2F;rancher&#x2F;rke2&#x2F;config.yaml 文件中，按照对应组件，添加对应的参数，如apiserver对应为kube-apiserver-arg，组件对应参数为etcd-arg。kube-controller-manager-arg、kube-scheduler-arg、kubelet-arg、kube-proxy-arg。 12345678etcd-arg: - &quot;quota-backend-bytes=858993459&quot; - &quot;max-request-bytes=33554432&quot;kube-apiserver-arg: - &quot;watch-cache=true&quot;kubelet-arg: - &quot;system-reserved=cpu=1,memory=2048Mi&quot; 配置完成后启动rke2-server。agent节点要同步时配置，否则kubelet和kube-proxy参数将不生效 检查参数是否生效如： ps aux|grep system-reserved 集群备份和还原rke2备份文件保存在每个拥有etcd角色的节点的&#x2F;var&#x2F;lib&#x2F;rancher&#x2F;rke2&#x2F;server&#x2F;db&#x2F;snapshots目录内，拥有多副本保存。默认每隔12小时备份一次，保留5份。 注：目前版本只能通过定时备份，没有立刻备份的选型。将 指定备份文件恢复 关闭rke2-server进程 systemctl stop rke2-server 指定文件恢复 rke2 server \\ --cluster-reset \\ --cluster-reset-restore-path=&lt;PATH-TO-SNAPSHOT&gt; 若是HA集群，还原成功后在其他server节点将执行rm -rf /var/lib/rancher/rke2/server/db然后重新启动server，加入集群。 rke2跟rke1一样也支持将备份文件在一个新集群进行还原。 常见操作参考链接：https://gist.github.com/superseb/3b78f47989e0dbc1295486c186e944bf 查看本机运行的容器ctr命令 /var/lib/rancher/rke2/bin/ctr --address /run/k3s/containerd/containerd.sock --namespace k8s.io container ls crictl命令 export CRI_CONFIG_FILE=/var/lib/rancher/rke2/agent/etc/crictl.yaml /var/lib/rancher/rke2/bin/crictl ps /var/lib/rancher/rke2/bin/crictl --config /var/lib/rancher/rke2/agent/etc/crictl.yaml ps /var/lib/rancher/rke2/bin/crictl --runtime-endpoint unix:///run/k3s/containerd/containerd.sock ps -a 最终都是连接到containerd的socket文件 查看日志journalctl -f -u rke2-server /var/lib/rancher/rke2/agent/containerd/containerd.log /var/lib/rancher/rke2/agent/logs/kubelet.log etcd操作etcdctl check perf for etcdpod in $(kubectl -n kube-system get pod -l component=etcd --no-headers -o custom-columns=NAME:.metadata.name); do kubectl -n kube-system exec $etcdpod -- sh -c &quot;ETCDCTL_ENDPOINTS=&#39;https://127.0.0.1:2379&#39; ETCDCTL_CACERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt&#39; ETCDCTL_CERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.crt&#39; ETCDCTL_KEY=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.key&#39; ETCDCTL_API=3 etcdctl check perf&quot;; done etcdctl endpoint status for etcdpod in $(kubectl -n kube-system get pod -l component=etcd --no-headers -o custom-columns=NAME:.metadata.name); do kubectl -n kube-system exec $etcdpod -- sh -c &quot;ETCDCTL_ENDPOINTS=&#39;https://127.0.0.1:2379&#39; ETCDCTL_CACERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt&#39; ETCDCTL_CERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.crt&#39; ETCDCTL_KEY=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.key&#39; ETCDCTL_API=3 etcdctl endpoint status&quot;; done etcdctl endpoint health for etcdpod in $(kubectl -n kube-system get pod -l component=etcd --no-headers -o custom-columns=NAME:.metadata.name); do kubectl -n kube-system exec $etcdpod -- sh -c &quot;ETCDCTL_ENDPOINTS=&#39;https://127.0.0.1:2379&#39; ETCDCTL_CACERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt&#39; ETCDCTL_CERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.crt&#39; ETCDCTL_KEY=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.key&#39; ETCDCTL_API=3 etcdctl endpoint health&quot;; done etcdctl compact rev=$(kubectl -n kube-system exec $(kubectl -n kube-system get pod -l component=etcd --no-headers -o custom-columns=NAME:.metadata.name | head -1) -- sh -c &quot;ETCDCTL_ENDPOINTS=&#39;https://127.0.0.1:2379&#39; ETCDCTL_CACERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt&#39; ETCDCTL_CERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.crt&#39; ETCDCTL_KEY=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.key&#39; ETCDCTL_API=3 etcdctl endpoint status --write-out fields | grep Revision | cut -d: -f2&quot;) kubectl -n kube-system exec $(kubectl -n kube-system get pod -l component=etcd --no-headers -o custom-columns=NAME:.metadata.name | head -1) -- sh -c &quot;ETCDCTL_ENDPOINTS=&#39;https://127.0.0.1:2379&#39; ETCDCTL_CACERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt&#39; ETCDCTL_CERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.crt&#39; ETCDCTL_KEY=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.key&#39; ETCDCTL_API=3 etcdctl compact \\&quot;$(echo $rev)\\&quot;&quot; etcdctl defrag kubectl -n kube-system exec $(kubectl -n kube-system get pod -l component=etcd --no-headers -o custom-columns=NAME:.metadata.name | head -1) -- sh -c &quot;ETCDCTL_ENDPOINTS=&#39;https://127.0.0.1:2379&#39; ETCDCTL_CACERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt&#39; ETCDCTL_CERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.crt&#39; ETCDCTL_KEY=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.key&#39; ETCDCTL_API=3 etcdctl defrag --cluster&quot; 对应的，直接操作etcdctl参考：https://gist.github.com/superseb/3b78f47989e0dbc1295486c186e944bf","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"Jenkins与外部系统集成","slug":"jenkins_Docking_external_system","date":"2021-10-14T13:45:59.000Z","updated":"2021-10-14T13:45:59.000Z","comments":true,"path":"2021/10/14/jenkins_Docking_external_system/","permalink":"http://yoursite.com/2021/10/14/jenkins_Docking_external_system/","excerpt":"","text":"环境准备软件版本 软件 版本 gitlab 14.3.0 Jenkins 2.303.1 Harbor 1.10.2 Sonar 9.1 Nexus 3.35.0-02 ArgoCD 2.1.3 部署gitlab1docker run --detach --hostname 10.8.242.28 --publish 443:443 --publish 80:80 --publish 1022:22 --name gitlab --restart always --volume /srv/gitlab/config:/etc/gitlab --volume /srv/gitlab/logs:/var/log/gitlab --volume /srv/gitlab/data:/var/opt/gitlab gitlab/gitlab-ce:12.10.3-ce.0 替换hostname为实际节点外网IP 部署HarborHarbor部署与管理部署前先修改docker编辑docker 1234vim /etc/docker/daemon.json&#123; &quot;insecure-registries&quot; : [&quot;0.0.0.0/0&quot;]&#125; 重启docker 1systemctl restart docker 安装docker-compose 12curl -L https://github.com/docker/compose/releases/download/1.24.1/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-compose 下载harbor 1https://github.com/goharbor/harbor/releases/download/v1.10.2/harbor-online-installer-v1.10.2.tgz 配置harbo.yaml 1hostname: 172.31.48.86 //修改为实际节点IP 屏蔽https配置 安装harbor 1./install.sh --with-clair 1234567891011121314docker-compose ps Name Command State Ports ---------------------------------------------------------------------------------------------clair /docker-entrypoint.sh Up (healthy) 6060/tcp, 6061/tcp harbor-core /harbor/start.sh Up (healthy) harbor-db /entrypoint.sh postgres Up (healthy) 5432/tcp harbor-jobservice /harbor/start.sh Up harbor-log /bin/sh -c /usr/local/bin/ ... Up (healthy) 127.0.0.1:1514-&gt;10514/tcpharbor-portal nginx -g daemon off; Up (healthy) 80/tcp nginx nginx -g daemon off; Up (healthy) 0.0.0.0:80-&gt;80/tcp redis docker-entrypoint.sh redis ... Up 6379/tcp registry /entrypoint.sh /etc/regist ... Up (healthy) 5000/tcp registryctl /harbor/start.sh Up (healthy) 访问http://node_ip admin&#x2F;Harbor12345 创建测试项目spring-petclinic官方示例项目地址：https://projects.spring.io/spring-petclinic/ 本次实践针对Spring官方提供的spring-petclinic示例项目进行容器化部署，该项目采用Spring Boot + Thymeleaf开发，数据库可使用MySQL、H2等，本实践为操作方便直接使用内置的H2数据库。 注意：由于本实践采用的是H2内置数据库，所以每个应用实例的数据独立，也使得应用变成了有状态应用，而生产的最佳实践应该是数据采用外部存储，且应用采用无状态方式部署。 国内clone地址：https://gitee.com/wanshaoyuan/spring-petclinic.git 将此项目clone后上传到私有的gitlab中. 与Gitlab集成安装gitlab插件 Gitlab中申请AccessToken 将申请成功的token保存好 配置Jenkins对接gitlab添加凭证测试连接 测试读取gitlab中项目spring-petclinic项目中pom.xml文件 配置连接gitlab私有项目的密钥可以用ssh密钥也可以使用账号密码 分支处修改为main分支 构建 去cat这个文件输出内容 执行立即构建 输出结果为实际我们的pom.xml的文件内容 与Kubernetes集成构建分布式动态编译环境安装Kubernetes插件Jenkins与Kubernetes集成实现动态Slave Pod，需要安装Kubernetes插件： kubernetes 安装Kubernetes Continuous Deploy插件Jenkins访问kubernetes需要依赖于kubeconfig，为支持kubeconfig类型的凭据配置，需要安装Kubernetes Continuous Deploy插件： Kubernetes Continuous Deploy 配置Kubernetes集群配置 系统管理—&gt;系统设置—&gt;新增一个云 配置Jenkins URL，这里可以不配置api-server地址和证书key，连接kubernetes，所以默认会去读取放在JENKINS_HOME的.kube&#x2F;目录的kubeconfig文件，用于连接集群。我这里是通过安装包的方式安装的Jenkins HOME在&#x2F;var&#x2F;lib&#x2F;jenkins&#x2F;目录，如果是通过容器方式启动，将kubeconfig文件直接放~&#x2F;.kube&#x2F;目录。保存到Jenkins主机的config文件中 复制粘贴到Jenkins容器内的~&#x2F;.kube&#x2F;config文件中 12docker exec -it jenkins mkdir /root/.kube/docker cp config jenkins:/root/.kube/config 注意：此方式Jenkins容器重启后，会将目录重新初始化覆盖掉，kubeconfig文件，生产环境可以直接挂载。 验证Pipeline流水线以上Jenkins与Kubernetes的集成配置就基本完成了，下面在正式为Spring Petclinic应用创建Pipeline之前，先简单测试下Jenkins与Kubernetes集成Pipeline流水线是否正常。 新建一个流水线类型的任务test-hello-pipeline 准备流水线测试脚本 1234567891011121314151617181920212223242526272829pipeline &#123; agent &#123; kubernetes &#123; cloud &#x27;Kubernetes&#x27; namespace &#x27;default&#x27; yaml &quot;&quot;&quot;apiVersion: v1kind: Podspec: containers: - name: busybox image: busybox command: - sleep args: - infinity&quot;&quot;&quot; &#125; &#125; stages &#123; stage(&#x27;Test&#x27;) &#123; steps &#123; container(&#x27;busybox&#x27;) &#123; sh &quot;echo &#x27;hello world&#x27;&quot; &#125; &#125; &#125; &#125;&#125; 以上是一个简单的声明式pipeline，利用busybox镜像输出hello world字符串。 添加流水线脚本 把测试脚本添加到任务的流水线脚本框中： 保存流水线，并执行构建 查看JOB运行结果 在Kubernetes中可以看到Jenkins自动创建了Pod来执行任务，任务执行完成以后，Pod自动删除。 Jenkins中查看下构建的控制台输出，正常输出了hello world： 验证结果表明，Jenkins与Kubernetes配置成功，Pipeline运行正常。 Sonar-Qube对接实现代码质量扫描 安装sonarqube初始化 123helm repo add sonarqube https://SonarSource.github.io/helm-chart-sonarqubehelm repo updatekubectl create namespace sonarqube helm安装sonarqube 1helm install sonarqube --namespace sonarqube sonarqube/sonarqube --set postgresql.persistence.enabled=false 注意：这里为了快速部署没有设置postgresql的持久化存储，有数据丢失风险，生产环境postgresql建议设计HA或持久化存储。设置为NodePort对外暴露 1kubectl patch svc sonarqube-sonarqube -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&#x27; -n sonarqube 查看NodePort端口 12345kubectl get svc -n sonarqubeNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEsonarqube-postgresql ClusterIP 10.110.40.18 &lt;none&gt; 5432/TCP 4m41ssonarqube-postgresql-headless ClusterIP None &lt;none&gt; 5432/TCP 4m41ssonarqube-sonarqube NodePort 10.106.78.100 &lt;none&gt; 9000:30005/TCP 4m41s 查看启动成功 1234 kubectl get pod -n sonarqubeNAME READY STATUS RESTARTS AGEsonarqube-postgresql-0 1/1 Running 0 4m7ssonarqube-sonarqube-0 1/1 Running 0 4m7s 访问节点的30005端口 默认密码admin&#x2F;admin 如果需要中文直接安装插件就好administrator—-&gt;Marketplace搜索Chinese—-安装 生成token申请tokenadministrator—&gt;security—&gt;user—&gt;token保存生成token Jenkins配置安装插件系统设置—&gt;插件管理安装SonarQube Scanner for Jenkins 配置插件配置sonarQube-server Server URL填写sonarqube的地址 Server authentication token填写刚刚创建的token，这里创建一个密钥类型为Secret Text。Secret填写token详细信息，ID为此secret的名称 配置sonarQUbe-agent系统管理-&gt;全局工具配置——&gt;SonarQube Scanner此处配置为自动安装 FreeStyle风格任务下配置SonarQube以上面的test-gitlab项目的spring-petclinic为例先执行maven构建出class文件，在进行扫描,因为sonarQube扫描的对象是.class而不是.java文件。 1docker run -i -v /var/jenkins_home/workspace/:/tmp maven:3.6-jdk-8 mvn -f /tmp/spring-petclinic/pom.xml clean package -DskipTests 在构建阶段添加”Execute SonarQube Scanner” 输入以下内容 1234567sonar.projectKey=testsonar.projectName=testsonar.projectVersion=1.0sonar.sources=srcsonar.java.binaries=target/classessonar.language=javasonar.sourceEncoding=UTF-8 注：sonar.projectKey&#x3D;Test #sonar那显示project-keysonar.projectName&#x3D;Test #sonar那显示project名字sonar.projectVersion&#x3D;1.0 ##sonar那显示project版本sonar.sources&#x3D;src #指定要扫描的源码目录。sonar.java.binaries&#x3D;target&#x2F;classes #指定java文件编译后class文件目录。sonar.language&#x3D;java #只扫描的语言。sonar.sourceEncoding&#x3D;UTF-8 #指定源码的编码格式，一般都会去指定为UTF-8。 执行构建Jenkins处查看 sonar处查看 Jenkins-Pipeline风格任务下配置SonarQube使用Pipeline流水线，需要在添加以下步骤 1、在对应的代码库的根目录创建sonar-project.properties 123456789sonar.projectKey=test2sonar.projectName=test2sonar.projectVersion=1.0sonar.sources=srcsonar.java.binaries=target/classessonar.java.source=1.8sonar.java.target=1.8sonar.language=javasonar.sourceEncoding=UTF-8 Pipeline中添加以下步骤 Pipeline中添加以下步骤 12345678910stage(&#x27;SonarQube analysis&#x27;) &#123; steps &#123; script &#123; def sonarqubeScannerHome = tool name: &#x27;SonarQubeScanner&#x27; withSonarQubeEnv(&#x27;sonar&#x27;) &#123; sh &quot;$&#123;sonarqubeScannerHome&#125;/bin/sonar-scanner&quot; &#125; &#125; &#125; &#125; 注：1、SonarQubeScanner为全局工具配置中的SonarQube Scanner的配置名称。2、withSonarQubeEnv配置的sonar变量为全局——&gt;系统配置sonar-server的配置名称 清空workspace 1rm -rf /var/jenkins_home/workspace/spring-petclinic Sonattype-NexusNexus是开源的制品库，可以用来存储一些代码构建后的制品如jar包，npm包和docker镜像等。也可以将存放制品后的仓库做为私服，供给给后面需要内网编译的软件使用。 部署安装软件版本：3.35.0-02本次部署为了更加方便和快捷，采用Docker方式部署创建目录 1mkdir /var/nexus-data &amp;&amp; chown -R 200 /var/nexus-data Docker运行 1docker run -d -p 8081:8081 --name nexus -v /var/nexus-data:/nexus-data sonatype/nexus3 初始账号和密码访问账号：admin密码： 1cat /var/nexus-data/admin.password 创建仓库 仓库分为三种类型，proxy、group、hosted。Proxy：Repository是代理仓库，可以配置上游仓库地址，如阿里云仓库地址。当本地仓库不到时，去向配置的上游仓库查找。hosted：供本地使用的本地仓库。group：仓库组，将多个仓库合成一个组，查找jar包时，会按照仓库组中的仓库顺序下载jar包。 这里创建两个名为spring-petclinic-releases、spring-petclinic-snapshots，类型为hotsted的Maven2仓库。 releases库主要用于存储正式版的制品，snapshots存储持续集成过程中产生的制品。 这里可以根据情况进行修改为release或snapshots maven处配置在spring-petclinic目录下创建conf&#x2F;settings.xml文件用于存放连接Nexus3的凭证信息，正常可以在maven_home或~&#x2F;.m2&#x2F;目录有这文件。因为这里是使用Docker进行构建编译，所以这里直接与业务代码放置在一起。settings.xml文件 123456789101112131415161718192021&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt; &lt;pluginGroups&gt; &lt;/pluginGroups&gt; &lt;proxies&gt; &lt;/proxies&gt; &lt;servers&gt; &lt;server&gt; &lt;id&gt;releases&lt;/id&gt; &lt;username&gt;账号&lt;/username&gt; &lt;password&gt;密码&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;username&gt;账号&lt;/username&gt; &lt;password&gt;密码&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt;&lt;/settings&gt; 关闭https检测，因为Nexus3使用的是http方式对外暴露所以需要关闭maven构建时强行要求https链接 src/checkstyle/nohttp-checkstyle.xml注释&lt;module name=&quot;io.spring.nohttp.checkstyle.check.NoHttpCheck&quot;/&gt;注释后：&lt;!-- &lt;module name=&quot;io.spring.nohttp.checkstyle.check.NoHttpCheck&quot;/&gt; --&gt; pom.xml文件添加以下内容 12345678910111213&lt;distributionManagement&gt; &lt;repository&gt; &lt;!--id的名字可以任意取，但是在setting文件中的属性&lt;server&gt;的ID与这里一致--&gt; &lt;id&gt;releases&lt;/id&gt; &lt;!--指向仓库类型为host(宿主仓库）的储存类型为Release的仓库--&gt; &lt;url&gt;http://172.16.0.195:8081/repository/mspring-petclinic-releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;!--指向仓库类型为host(宿主仓库）的储存类型为Snapshot的仓库--&gt; &lt;url&gt;http://172.16.0.195:8081/repository/spring-petclinic-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; 执行编译 1docker run -i -v /root/spring-petclinic/:/tmp maven:3.6-jdk-8 mvn -f /tmp/pom.xml --settings /tmp/conf/settings.xml clean deploy 编译完成上传成功后 在spring-petclinic-snapshots仓库内可见上传来的jar包，对应的jar包后面也接上了对应的时间戳，方便进行分类。 如果要上传到release仓库，将pom.xml中的\u0010&lt;version&gt;2.5.0-SNAPSHOT&lt;/version&gt;中的-SNAPSHOT字段删除就表示为正式版。 ArgoCD集成实现CD端对接编写并上传部署spring-petclinic的yaml文件和Dockerfile文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758apiVersion: apps/v1kind: Deploymentmetadata: name: spring-petclinic-0-0-1spec: selector: matchLabels: app: spring-petclinic version: 0.0.1 replicas: 1 template: metadata: labels: app: spring-petclinic version: 0.0.1 spec: containers: - name: spring-petclinic image: registry.cn-shenzhen.aliyuncs.com/yedward/spring-petclinic:0.0.1 resources: limits: memory: 2Gi cpu: 1 ports: - containerPort: 8080 livenessProbe: failureThreshold: 3 httpGet: path: /actuator/health/liveness port: 8080 scheme: HTTP initialDelaySeconds: 30 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 2 readinessProbe: failureThreshold: 3 httpGet: path: /actuator/health/readiness port: 8080 scheme: HTTP initialDelaySeconds: 30 periodSeconds: 5 successThreshold: 2 timeoutSeconds: 2---apiVersion: v1kind: Servicemetadata: name: spring-petclinic-svc-0-0-1spec: selector: app: spring-petclinic version: 0.0.1 ports: - port: 8080 targetPort: 8080 type: NodePort 将yaml中的镜像地址改为实际的镜像仓库地址和项目名称。Dockerfile 1234567FROM registry.cn-shenzhen.aliyuncs.com/yedward/openjdk:8-jre-slim# 企业实际场景中应该通过USER指定以非root用户运行USER appuserEXPOSE 8080COPY target/*.jar /app/WORKDIR /appCMD java -jar -Xms1024m -Xmx1024m /app/spring-petclinic.jar 上传到gitlab中的spring-petclinic项目中 部署ArgoCD单节点部署使用官网快速部署 12kubectl create namespace argocdkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml 部署完后产生以下服务 12345678910111213141516171819202122232425262728293031NAME READY STATUS RESTARTS AGEpod/argocd-application-controller-0 1/1 Running 0 5d6hpod/argocd-dex-server-74588646d-sz9g8 1/1 Running 0 2d2hpod/argocd-redis-5ccdd9d4fd-csthm 1/1 Running 1 5d6hpod/argocd-repo-server-5bbb8bdf78-mxkv7 1/1 Running 0 18hpod/argocd-server-789fb45964-82mzx 1/1 Running 0 18hNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/argocd-dex-server ClusterIP 10.43.180.172 &lt;none&gt; 5556/TCP,5557/TCP,5558/TCP 5d6hservice/argocd-metrics ClusterIP 10.43.184.97 &lt;none&gt; 8082/TCP 5d6hservice/argocd-redis ClusterIP 10.43.4.233 &lt;none&gt; 6379/TCP 5d6hservice/argocd-repo-server ClusterIP 10.43.9.45 &lt;none&gt; 8081/TCP,8084/TCP 5d6hservice/argocd-server NodePort 10.43.48.239 &lt;none&gt; 80:31320/TCP,443:31203/TCP 5d6hservice/argocd-server-metrics ClusterIP 10.43.149.186 &lt;none&gt; 8083/TCP 5d6hNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/argocd-dex-server 1/1 1 1 5d6hdeployment.apps/argocd-redis 1/1 1 1 5d6hdeployment.apps/argocd-repo-server 1/1 1 1 5d6hdeployment.apps/argocd-server 1/1 1 1 5d6hNAME DESIRED CURRENT READY AGEreplicaset.apps/argocd-dex-server-74588646d 1 1 1 5d6hreplicaset.apps/argocd-redis-5ccdd9d4fd 1 1 1 5d6hreplicaset.apps/argocd-repo-server-5bbb8bdf78 1 1 1 5d6hreplicaset.apps/argocd-server-789fb45964 1 1 1 5d6hNAME READY AGEstatefulset.apps/argocd-application-controller 1/1 5d6h 使用NodePort方式为对外暴露 1kubectl patch svc argocd-server -n argocd -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&#x27; 访问dashboard 默认帐号为admin，密码通过secret获取 1kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=&quot;&#123;.data.password&#125;&quot; | base64 -d 配置ArgoCD配置对接gitlabsetting——&gt;Repositories-&gt;Connect repo using HTTPS 如果对应的git是私有库，pull需要帐号密码则需要在argo设置中配置repo connect 填写对应的帐号密码，如果是自签名证书需要将CA附上 创建Projectsetting——&gt;Projects项目是argocd中的管理对象，也与之对应的发布权限相关联。创建项目，并配置DESTINATIONS，能够发布到哪些集群和命名空间 创建Application 创建完后 点击sync会自动将yaml文件部署到k8s集群中。可以在Kubernetes集群中查看到 123kubectl get pod NAME READY STATUS RESTARTS AGEspring-petclinic-0-0-1-6695b96956-xx9nw 1/1 Running 0 28m Harbor中创建对应项目在harbor中创建spring-petclinic项目 gitlab Webhook配置当前Jenkins进行CI构建还是基于手动点击运行，可以配置基于gitlab的触发事件进行调用，如push、merge、tag push等事件触发回调Jenkins自动执行CIjenkins处打开项目触发器生成连接Secret token保存下来Gitlab配置：root登录后，需要开放安全配置，允许本地local网络连接在menu选择admin——&gt;settings-——&gt;Network——&gt;Outbound requests 勾选 12Allow requests to the local network from web hooks and services Allow requests to the local network from system hooks 将Jenkins的ip添加到白名单中，保存。 项目——&gt;setting——&gt;webhooks 填写Jenkins对应的回调地址和token 点击Test settings即可在Jenkins处看见已经开始的构建任务。 保存配置 test这里选择基于事件回调。查看Jenkins处是否开始自动执行任务。 Jenkins配置Argo是检查到yaml文件变化会进行自动发布到k8s中，那么我们只需要在Jenkins中增加修改和上传yaml阶段即可。 完整的构建阶段shell编译阶段shell 1docker run -i -v /var/jenkins_home/workspace/:/tmp maven:3.6-jdk-8 mvn -f /tmp/spring-petclinic/pom.xml clean package -DskipTests 代码扫描阶段 1234567sonar.projectKey=testsonar.projectName=testsonar.projectVersion=1.0sonar.sources=srcsonar.java.binaries=target/classessonar.language=javasonar.sourceEncoding=UTF-8 镜像构建阶段 123docker login -u useradmin -p password harbor_ipdocker build -t harbor_ip/spring-petclinic/spring-petclinic:$BUILD_NUMBER .docker push harbor_ip/spring-petclinic/spring-petclinic:$BUILD_NUMBER 注：1、这里使用Jenkins内置BUILD_NUMBER号为镜像tag，跟Jenkins的CI号是匹配的。2、将上传镜像的账号密码修改为实际的账号密码。 发布更新部署yaml阶段 123456789git clone http://username:password@1.13.173.7/root/spring-petclinic.gitgit config --global user.email &quot;root@example.com&quot;git config --global user.name &quot;root&quot;git remote set-url origin http://username:password@1.13.173.7/root/spring-petclinic.gitsed -i &quot;s/spring-petclinic:.*/spring-petclinic:$BUILD_NUMBER/g&quot; spring-petclinic/deployment.yamlcd spring-petclinic/git add deployment.yamlgit commit -m &quot;update yaml&quot;git push origin main 在配置一个构建后删除操作，避免构建后缓存影响下次构建 执行构建，构建成功后查看对应的k8s中的部署的业务镜像版本号是否与实际应用部署的环境变量相同。 123456789kubectl describe pod/spring-petclinic-0-0-1-699954b589-h7n58Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 57m default-scheduler Successfully assigned default/spring-petclinic-0-0-1-7969df6996-dn2cc to rke-node2 Normal Pulled 57m kubelet Container image &quot;1.13.173.7:8080/spring-petclinic/spring-petclinic:15&quot; already present on machine Normal Created 57m kubelet Created container spring-petclinic Normal Started 57m kubelet Started container spring-petclinic 访问节点ip+spring-petclinic服务暴露出来的NodePort端口 这是一个宠物医院的管理系统，可以通过此页面进行宠物管理。 备注Jenkins内置环境变量直接访问${YOUR_JENKINS_HOST}&#x2F;env-vars.html即可 12345678910111213BUILD_NUMBER， 唯一标识一次build，例如23；BUILD_ID，基本上等同于BUILD_NUMBER，但是是字符串，例如2011-11-15_16-06-21；JOB_NAME， job的名字，例如JavaHelloWorld；BUILD_TAG，作用同BUILD_ID,BUILD_NUMBER,用来全局地唯一标识一此build，例如jenkins-JavaHelloWorld-23；EXECUTOR_NUMBER， 例如0；NODE_NAME，slave的名字，例如MyServer01；NODE_LABELS，slave的label，标识slave的用处，例如JavaHelloWorld MyServer01；JAVA_HOME， java的home目录，例如C:\\Program Files (x86)\\Java\\jdk1.7.0_01；WORKSPACE，job的当前工作目录，例如c:\\jenkins\\workspace\\JavaHelloWorld；HUDSON_URL = JENKINS_URL， jenkins的url，例如http://localhost:8000/ ；BUILD_URL，build的url 例如http://localhost:8000/job/JavaHelloWorld/23/；JOB_URL， job的url，例如http://localhost:8000/job/JavaHelloWorld/；SVN_REVISION，svn 的revison， 例如4；","categories":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/categories/CI-CD/"}],"tags":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/tags/CI-CD/"}]},{"title":"Jenkins-pipeline讲解和使用","slug":"jenkins_pipeline","date":"2021-10-14T13:45:59.000Z","updated":"2021-10-14T13:45:59.000Z","comments":true,"path":"2021/10/14/jenkins_pipeline/","permalink":"http://yoursite.com/2021/10/14/jenkins_pipeline/","excerpt":"","text":"什么是Jenkins-PipelinePipeline是一套运行在Jenkins上的工作流框架，2.X版本Jenkins的核心功能，主要是将一个大的工作流分拆成多个独立的功能模块，实现单个任务难以完成的复杂流程编排和可视化。Jenkins Pipeline也是实现CICD As file的一个重要工具，将Pipeline编写成Jenkinsfile与业务代码一起存放。 Pipeline支持两种语法：1、声明式语法 Jenkins新加入的语法规则在Jenkinsfile固定的关键字之内，所采用的语法风格大多与shell类似，这种风格更加符合日常的阅读习惯，也更简单，以后我都将采用这种方式进行介绍以及深入。 2、脚本式语法不是shell脚本形式，而是基于Groovy语言的语法风格，学习成本相对较高 建议直接使用声明式语法清晰简单明了，合适大部分人入门 Pipeline和FreeStyle对比| | 灵活方式 | 显示形式 ||—|—|—|—|| FreeStyle | 图形化操作，合适入门操作，后期流程多后，不易于快速快速构建 | 只有统一日志展示，没有完整阶段流程信息展示 | || Pipeline | 结构化代码语法，易于阅读和管理，可以实现CICD as Code | 阶段流程信息展示清晰，每个阶段构建时间和对应的构建日志清晰可读 | | Jenkins-Pipline语法介绍 图片来源:https://wiki.eryajf.net/pages/3298.html#_1-%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D%E3%80%82 Aagent: 一个 Aagent 就是一个 Jenkins执行 Step 的具体运行环境。 Stage: 表示Pipeline的一个阶段，如clone code阶段，Build阶段。一个Pipeline中至少需要一个Stage。 Step: 表示实际的执行步骤，小到执行一个 Shell 脚本，大到构建一个 Docker 镜像，由各类 Jenkins Plugin 提供，当插件扩展Pipeline DSL 时，通常意味着插件已经实现了一个新的步骤，在Stage下有且只能有一个step。 environment环境变量，可以定义在全局变量或者步骤中的局部变量，取决于所定义位置。 12345678910111213pipeline &#123; agent any environment &#123; DISABLE_AUTH = &#x27;true&#x27; &#125; stages &#123; stage(“Build”) &#123; steps &#123; echo env.DISABLE_AUTH &#125; &#125; &#125;&#125; 运行结果，会输出对应的环境变量。 options选项，定义流水线运行时的配置选项。如历史构建记录数量保留，超时时间等操作。以下例子定义重试次数. 12345678910111213pipeline &#123; agent any options &#123; retry(3) &#125; stages &#123; stage(&#x27;Example&#x27;) &#123; steps &#123; sh &quot;dwdwe&quot; &#125; &#125; &#125;&#125; 编写一个无法执行的命令，可以通过Console Output看见会Retrying 3次才停止。 parameters参数，为流水线运行时设置相关的参数，不需要在UI界面上额外定义。出现在Pipeline块内，并且只有一次。常用参数：string：字符串类型的参数，例如： parameters &#123; string(name: &#39;DEPLOY_ENV&#39;, defaultValue: &#39;staging&#39;, description: &#39;&#39;) &#125;文本: 一个text参数，可以包含多行，例如： parameters &#123; text(name: &#39;DEPLOY_TEXT&#39;, defaultValue: &#39;One\\nTwo\\nThree\\n&#39;, description: &#39;&#39;) &#125;booleanParam: 一个布尔参数，例如： parameters &#123; booleanParam(name: &#39;DEBUG_BUILD&#39;, defaultValue: true, description: &#39;&#39;) &#125;choice： 选择参数，例如： parameters &#123; choice(name: &#39;CHOICES&#39;, choices: [&#39;one&#39;, &#39;two&#39;, &#39;three&#39;], description: &#39;&#39;) &#125; password: 在 Jenkins 参数化构建 UI 提供一个暗文密码输入框，所有需要脱敏的信息，都可以通过这个参数来配置。 parameters &#123; password(name: &#39;PASSWORD&#39;, defaultValue: &#39;SECRET&#39;, description: &#39;A secret password&#39;) &#125; 注意：这种声明定义之后，需要手动构建一次，然后才会自动落位到配置好的参数化构建中了。例子： 1234567891011121314151617181920212223242526272829pipeline &#123; agent any parameters &#123; string(name: &#x27;PERSON&#x27;, defaultValue: &#x27;Mr Jenkins&#x27;, description: &#x27;Who should I say hello to?&#x27;) text(name: &#x27;BIOGRAPHY&#x27;, defaultValue: &#x27;&#x27;, description: &#x27;Enter some information about the person&#x27;) booleanParam(name: &#x27;TOGGLE&#x27;, defaultValue: true, description: &#x27;Toggle this value&#x27;) choice(name: &#x27;CHOICE&#x27;, choices: [&#x27;One&#x27;, &#x27;Two&#x27;, &#x27;Three&#x27;], description: &#x27;Pick something&#x27;) password(name: &#x27;PASSWORD&#x27;, defaultValue: &#x27;SECRET&#x27;, description: &#x27;Enter a password&#x27;) &#125; stages &#123; stage(&#x27;Example&#x27;) &#123; steps &#123; echo &quot;Hello $&#123;params.PERSON&#125;&quot; echo &quot;Biography: $&#123;params.BIOGRAPHY&#125;&quot; echo &quot;Toggle: $&#123;params.TOGGLE&#125;&quot; echo &quot;Choice: $&#123;params.CHOICE&#125;&quot; echo &quot;Password: $&#123;params.PASSWORD&#125;&quot; &#125; &#125; &#125;&#125; 在执行Build时会多出这些选项 post运行后处理，当流水线完成后根据配置的条件做一些动作，如：构建失败后邮件通知。条件：always：无论怎么样总是执行。changed: 当前Pipeline状态与先前不一致情况下执行。failuer: 失败情况下。success: 成功情况下。unstable: 不稳定情况下，Pipeline状态标识为黄色。aborted: Pipeline中止情况下。cleanup: 无论怎么样，执行目录清理 例如：无论怎么样，将执行情况邮件发送。 12345678post&#123; always&#123; mail to : ‘team@example.com’, subject:”Pipeline statue:$&#123;currentBuild.fullDisplayName&#125;”, body:”The execution result$&#123;env.Build_url&#125;” &#125;&#125; 定义三种执行状态，此时shell命令无法执行，输出always和success输出信息。 12345678910111213141516171819202122pipeline &#123; agent any stages &#123; stage(&#x27;Example&#x27;) &#123; steps &#123; sh &#x27;dwd&#x27; &#125; &#125; &#125; post &#123; always &#123; echo &#x27;already exec&#x27; &#125; success &#123; echo &#x27;exec success &#x27; &#125; failure &#123; echo &#x27;exec failure&#x27; &#125; &#125;&#125; tool构建工具，获取通过自动安装或手动安装工具的环境变量，支持maven、jdk、gradle，工具的名称必须预先在Jenkins的系统设置-&gt;全局工具配置中定义。例如在Jenkins—&gt;Global Tool Configuration中添加工具对应的环境变量，然后在项目中引用。 12345678910111213pipeline &#123; agent any tools &#123; maven &#x27;maven&#x27; &#125; stages &#123; stage(&#x27;Example&#x27;) &#123; steps &#123; sh &#x27;mvn --version&#x27; &#125; &#125; &#125;&#125; input交互输入，在流水线执行各个阶段的时候，由人工确认是否继续执行。 123456789101112131415161718192021222324252627282930pipeline&#123; agent any environment&#123; approvalMap = &#x27;&#x27; &#125; stages &#123; stage(&#x27;pre deploy&#x27;)&#123; steps&#123; script&#123; approvalMap = input ( message: &#x27;发布到哪个环境？&#x27;, ok: &#x27;确定&#x27;, parameters:[ choice(name: &#x27;ENV&#x27;,choices: &#x27;test\\npre\\nprod&#x27;,description: &#x27;发布到什么环境？&#x27;), string(name: &#x27;username&#x27;,defaultValue: &#x27;&#x27;,description: &#x27;输入用户名&#x27;) ], submitter: &#x27;admin&#x27;, ) &#125; &#125; &#125; stage(&#x27;deploy&#x27;)&#123; steps&#123; echo &quot;操作者是 $&#123;approvalMap[&#x27;username&#x27;]&#125;&quot; echo &quot;发布到什么环境 $&#123;approvalMap[&#x27;ENV&#x27;]&#125;&quot; &#125; &#125; &#125;&#125; 点击构建，程序将会在input的步骤停住，等待用户进行相应输入和选择。message ：必填。页面展示信息 ok：input表单上“ok”按钮的可选文本。 submitter：允许提交此input选项的用户或外部组名列表，用逗号分隔。默认允许任何用户。 例子结果如图所示： when条件判断，允许流水线根据给定的条件决定是否应该执行阶段，when 指令必须包含至少一个条件.内置条件branch：分支匹配，如when &#123; branch &#39;release-v2.5&#39; &#125;environment：环境变量匹配如when &#123; environment name: &#39;DEPLOY_TO&#39;, value: &#39;production&#39; &#125; not：当嵌套条件为false时执行stage。必须包含至少一个条件例如：when &#123; not &#123; branch &#39;master&#39; &#125; &#125;allOf：当所有嵌套条件都为真时，执行舞台。必须包含至少一个条件。例如：when &#123; allOf &#123; branch &#39;master&#39;; environment name: &#39;DEPLOY_TO&#39;, value: &#39;production&#39; &#125; &#125;anyOf: 当至少一个嵌套条件为真时执行。必须至少包含一个条件。例如：when &#123; anyOf &#123; branch &#39;master&#39;; branch &#39;staging&#39; &#125; &#125; 以上例子支持通配符配置，如when &#123; branch &#39;release-v2.*&#39; &#125;例子: 1234567891011121314151617181920pipeline &#123; agent any stages &#123; stage(&#x27;Example&#x27;) &#123; steps &#123; git branch: &#x27;main&#x27;, url: &#x27;https://gitee.com/wanshaoyuan/spring-petclinic.git&#x27; &#125; &#125; stage(&#x27;deploy to proc&#x27;)&#123; when &#123; branch &#x27;main&#x27; &#125; steps&#123; echo &quot;deploy to proc env&quot; &#125; &#125; &#125; &#125; 执行后因为是clone main分支，将输出deploy to proc env。 parallel默认Pipeline是串行，可以通过parallel配置并行构建，阶段可以在他们内部声明多嵌套阶段, 它们将并行执行，一个阶段只能有一个 steps 或 parallel的阶段。 1234567891011121314151617181920212223242526pipeline &#123; agent any stages &#123; stage(&#x27;one&#x27;) &#123; steps &#123; echo &quot;stage1&quot; &#125; &#125; stage(&#x27;two&#x27;) &#123; failFast true parallel &#123; stage(&#x27;并行1&#x27;) &#123; steps &#123; echo &quot;并行一&quot; &#125; &#125; stage(&#x27;并行2&#x27;) &#123; steps &#123; echo &quot;并行二&quot; &#125; &#125; &#125; &#125; &#125;&#125; 注:添加failFast true到包含parallel的stage中，其中一个失败时中止所有parallel内的stage。本阶段会执行多个步骤。 通过BlueOcean查看Pipeline效果图 script脚本标签，需要执行一些系统命令语法。 12345678910pipeline &#123; agent any stages &#123; stage(&#x27;Example&#x27;) &#123; steps &#123; sh &#x27;date&#x27; &#125; &#125; &#125;&#125; 通过sh执行对应的shell命令。 trigger触发器，设置构建触发器，比如根据周期计划定时构建。cron定时构建根Linux内crontab对应的，分、时、日、月、周。这里例子定义每分钟执行一次。 123456789101112131415pipeline &#123; agent any triggers&#123; cron(&#x27;* * * * 1&#x27;) &#125; stages &#123; stage(&#x27;cat文件&#x27;)&#123; steps&#123; sh &#x27;&#x27;&#x27;cat README.md&#x27;&#x27;&#x27; &#125; &#125; &#125; &#125; 注意点：H关键字为Hash，表示当前设置的时间跨度范围内随机一值例如triggers&#123; cron(&#39;H/15 * * * *&#39;) &#125;每15分钟执行一次，可能在 :07,:22,:37,:52执行 withCredentials将secret与变量对应起来。在jenkins中创建的密钥，在Pipeline中希望通过变量方式引用，可以通过withCredentials进行。 例子在全局凭据中创建个Username with password的凭证，输入用户名和密码 123456789101112pipeline &#123; agent any stages &#123; stage(&#x27;部署到测试环境&#x27;)&#123; steps&#123; withCredentials([usernamePassword(credentialsId: &#x27;harbor_account&#x27;, passwordVariable: &#x27;password&#x27;, usernameVariable: &#x27;username&#x27;)]) &#123; sh &#x27;echo $username&#x27; &#125; &#125; &#125; &#125; &#125; 执行构建后会输出密钥中的用户名注：usernamePassword：withCredentials的类型，相应的还支持sshUserPrivateKey(ssh key)和certificate（证书）。credentialsId：Jenkins中对应的配置名称。passwordVariable： 密码项目转成对应的变量。usernameVariable： 用户名项转换成对应的变量。 SSH User Private Key 示例 123456withCredentials(bindings: [sshUserPrivateKey(credentialsId: &#x27;jenkins-ssh-key-for-abc&#x27;, \\ keyFileVariable: &#x27;SSH_KEY_FOR_ABC&#x27;, \\ passphraseVariable: &#x27;&#x27;, \\ usernameVariable: &#x27;&#x27;)]) &#123; // some block&#125; Certificate 示例 123456withCredentials(bindings: [certificate(aliasVariable: &#x27;&#x27;, \\ credentialsId: &#x27;jenkins-certificate-for-xyz&#x27;, \\ keystoreVariable: &#x27;CERTIFICATE_FOR_XYZ&#x27;, \\ passwordVariable: &#x27;XYZ-CERTIFICATE-PASSWORD&#x27;)]) &#123; // some block&#125; 参考： https://www.jenkins.io/zh/doc/book/pipeline/jenkinsfile/#usernames-and-passwords BlueOcean使用Jenkins针对pipeline提供了全新的Blue Ocean界面，可以清晰的查看流水线的执行情况： 安装blueocean插件 安装完以后在对应的项目处会多出一个打开Blue Ocean按钮， 界面美化后的扁平化风格，可以通过此界面，进行一些常规的配置和操作 阶段性Pipeline重跑 日志统一下载 Jenkinsfile目前主流大部分CI工具都支持Pipeline as Code，就是将整CI流程通过代码方式实现，然后将对应的代码和业务代码放置在一起，对应的CI工具在拉取业务代码后可以直接解析CI的流程代码进行执行。Jenkins也是支持这种方式的，通过将写好的Pipeline写在Jenkinsfile中存放在代码仓库中，Jenkins配置读取指定目录的Jenkinsfile文件即可。 实例Demo最佳实践完整CICD步骤 1、clone 源码。2、编译源码。3、编译后进行代码扫描和编译后可执行文件扫描。4、将编译后生成的制品上传到制品库。5、镜像构建，将构建后的镜像上传到Harbor。6、更新Gitlab中的部署文件。7、触发ArgoCD同步。手动或自动部署到Kubernetes环境中。 Jenkins创建连接账号因为要进行镜像上传和修改gitlab中的部署yaml，需要进行修改。系统管理——&gt;Manage Credentials——&gt;创建全局域的认证信息 添加凭据 类型为Username with password 创建id为harbor_account和gitlab_account的凭证用于Pipeline连接。 代码库配置连接SonarQube信息使用Pipeline流水线，需要在添加以下步骤 1、在对应的代码库的根目录创建sonar-project.properties文件 123456789sonar.projectKey=test2sonar.projectName=test2sonar.projectVersion=1.0sonar.sources=srcsonar.java.binaries=target/classessonar.java.source=1.8sonar.java.target=1.8sonar.language=javasonar.sourceEncoding=UTF-8 Pipeline中添加以下步骤 12345678910stage(&#x27;SonarQube analysis&#x27;) &#123; steps &#123; script &#123; def sonarqubeScannerHome = tool name: &#x27;SonarQubeScanner&#x27; withSonarQubeEnv(&#x27;sonar&#x27;) &#123; sh &quot;$&#123;sonarqubeScannerHome&#125;/bin/sonar-scanner&quot; &#125; &#125; &#125; &#125; 注：1、SonarQubeScanner为全局工具配置中的SonarQube Scanner的配置名称。2、withSonarQubeEnv配置的sonar变量为全局——&gt;系统配置sonar-server的配置名称 Pipeline创建创建个名称为spring-petclini的Pipeline配置构建触发器 贴入以下Pipeline 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100pipeline &#123; agent &#123; kubernetes &#123; cloud &#x27;kubernetes&#x27; namespace &#x27;default&#x27; yaml &quot;&quot;&quot;apiVersion: v1kind: Podspec: containers: - name: git image: alpine/git:v2.26.2 command: - cat tty: true - name: maven image: maven:3.6.3-openjdk-8 command: - cat tty: true volumeMounts: - mountPath: /root/.m2/repository name: jenkins-maven-m2-pvc - name: docker image: docker:19.03-dind command: - cat tty: true volumeMounts: - mountPath: /var/run/docker.sock name: docker-sock - name: helm-kubectl image: registry.cn-shenzhen.aliyuncs.com/yedward/helm-kubectl:3.3.1-1.18.8 command: - cat tty: true volumes: - name: jenkins-maven-m2-pvc persistentVolumeClaim: claimName: jenkins-maven-m2-pvc - name: docker-sock hostPath: path: /var/run/docker.sock type: &quot;&quot;&quot;&quot;&quot; &#125; &#125; stages &#123; stage(&#x27;Clone&#x27;) &#123; steps &#123; container(&#x27;git&#x27;) &#123; git branch: &#x27;main&#x27;, credentialsId: &#x27;gitlab&#x27;, url: &#x27;http://172.16.1.184/root/spring-petclinic.git&#x27; &#125; &#125; &#125; stage(&#x27;Build&#x27;) &#123; steps &#123; container(&#x27;maven&#x27;) &#123; sh &#x27;mvn clean package -DskipTests&#x27; &#125; &#125; &#125; stage(&#x27;SonarQube analysis&#x27;) &#123; steps &#123; script &#123; def sonarqubeScannerHome = tool name: &#x27;SonarQubeScanner&#x27; withSonarQubeEnv(&#x27;sonar&#x27;) &#123; sh &quot;$&#123;sonarqubeScannerHome&#125;/bin/sonar-scanner&quot; &#125; &#125; &#125; &#125; stage(&#x27;Publish&#x27;) &#123; steps &#123; container(&#x27;docker&#x27;) &#123; withCredentials([usernamePassword(credentialsId: &#x27;harbor_account&#x27;, passwordVariable: &#x27;USERPWD&#x27;, usernameVariable: &#x27;USERNAME&#x27;)]) &#123; sh &#x27;echo &quot;$USERPWD&quot; | docker login --username=&quot;$USERNAME&quot; 172.16.1.31 --password-stdin&#x27; sh &#x27;docker build -t 172.16.1.31/spring-petclinic/spring-petclinic:$BUILD_NUMBER .&#x27; sh &#x27;docker push 172.16.1.31/spring-petclinic/spring-petclinic:$BUILD_NUMBER&#x27; &#125; &#125; &#125; &#125; stage(&#x27;Deploy&#x27;) &#123; steps &#123; container(&#x27;git&#x27;) &#123; withCredentials([usernamePassword(credentialsId: &#x27;gitlab_account&#x27;, passwordVariable: &#x27;USERPWD&#x27;, usernameVariable: &#x27;USERNAME&#x27;)]) &#123; sh &#x27;git config --global user.email &quot;root@example.com&quot;&#x27; sh &#x27;git config --global user.name &quot;root&quot;&#x27; sh &#x27;git remote set-url origin http://$USERNAME:$USERPWD@172.16.1.184/root/spring-petclinic.git&#x27; sh &#x27;sed -i &quot;s/spring-petclinic:.*/spring-petclinic:$BUILD_NUMBER/g&quot; deployment.yaml&#x27; sh &#x27;git add deployment.yaml&#x27; sh &#x27;git commit -m &quot;update yaml&quot;&#x27; sh &#x27;git push origin main&#x27; &#125; &#125; &#125; &#125; &#125;&#125; 注意点：1、clone阶段如果是私有代码仓库，需要配置凭证，可以通过流水线语法生成对应的执行命令。 输入对应的git地址和分支，选择gitlab密钥，生成对应的执行代码 执行修改spring-petclinic项目的代码注释掉主页的图片，提交代码，触发自动CICD查看效果。 注释掉首页小狗图片spring-petclinic/src/main/resources/templates/welcome.html 1&lt;!--&lt;img class=&quot;img-responsive&quot; src=&quot;../static/resources/images/pets.png&quot; th:src=&quot;@&#123;/resources/images/pets.png&#125;&quot;/&gt;--&gt; 重新提交代码 查看已经没有对应logo 参考链接：https://www.jenkins.io/doc/book/pipeline/syntax/https://wiki.eryajf.net/pages/3298.html","categories":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/categories/CI-CD/"}],"tags":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/tags/CI-CD/"}]},{"title":"应用性能监控1-Skywalking","slug":"apm_1","date":"2021-08-26T13:45:59.000Z","updated":"2021-08-26T13:45:59.000Z","comments":true,"path":"2021/08/26/apm_1/","permalink":"http://yoursite.com/2021/08/26/apm_1/","excerpt":"","text":"概述随着应用功能越来越多，从单体架构发展到现在微服务架构，拆分的模块越来越细粒化，需要定位应用模块之间的问题困难越来越大，需要通过一些第三方工具去帮助我们快速定位和发现应用模块的问题，实现以下功能：1、监控模块间响应时间展示2、应用模块间调用链路展示3、慢响应识别市面上也有非常多的APM软件提供。主流开源的如SkyWalking、ZipKin、CAT、PinPoint、ElasticAPM。这些都是根开发语言有强绑定关系，并且需要业务加载对应的开发包和引入SDK，对业务具有一定的侵入性。目前还有新兴的解决方案，基于云原生ServiceMesh方式，对应用没有侵入性和开发语言绑定。 SkyWalking介绍基于Google分布式链路追踪论文Dapper开发，由中国工程师吴晟开发并开源贡献给Apache基金会，支持多种开发语言如Java、PHP、Go、C++、Node.js、Python、.NET、Lua…… SkyWalking组件介绍总体架构如下 SkyWalking架构总体分为四部分： Agent：探针负责与各类开发语言和平台集成如ServiceMesh，进行Tracing和Metric数据收集。发送给Server端。Server端（OAP)：接受Agent采集发送过来的数据，进行数据分析、处理、聚合、查询以及将数据发送到后端存储。Storage：支持多种后端存储（ElasticSearch、Mysql、Tidb…)，接收Server端发送过来的数据。UI: 进行数据计算后的结果统一展示和调用链路展示。 SkyWalking安装环境信息 软件 版本 Kubernetes v1.18.20 SkyWalking v8.1.0 SkyWalking官方支持多种安装方式，这里为了快速部署，使用的是在Kubernetes上用Helm安装，后端存储使用ElasticSearch。使用官方Helm安装方式最小化安装，后端存储使用ElasticSearch。 参考部署手册https://github.com/apache/skywalking-kubernetes环境 配置环境变量 12export SKYWALKING_RELEASE_NAME=skywalkingexport SKYWALKING_RELEASE_NAMESPACE=default 配置repo 123export REPO=skywalkinghelm repo add $&#123;REPO&#125; https://apache.jfrog.io/artifactory/skywalking-helm helm repo update 安装skywalking，这里安装会自动帮你部署一个ElasticSearch，如果需要对接已经存在的ElasticSearch集群或使用其他的后端存储，可以使用其他参数进行部署安装。 12345helm install &quot;$&#123;SKYWALKING_RELEASE_NAME&#125;&quot; $&#123;REPO&#125;/skywalking -n &quot;$&#123;SKYWALKING_RELEASE_NAMESPACE&#125;&quot; \\ --set oap.image.tag=8.1.0-es7 \\ --set oap.storageType=elasticsearch7 \\ --set ui.image.tag=8.1.0 \\ --set elasticsearch.imageTag=7.5.1 部署完后查看 123456789kubectl get pod NAME READY STATUS RESTARTS AGEelasticsearch-master-0 1/1 Running 0 8m54selasticsearch-master-1 1/1 Running 0 8m54selasticsearch-master-2 1/1 Running 0 8m54sskywalking-es-init-vl8c7 0/1 Completed 0 8m54sskywalking-oap-64df9d4b8c-dvksd 1/1 Running 0 3m50sskywalking-oap-64df9d4b8c-p6thl 1/1 Running 0 8m54sskywalking-ui-649dc77bd7-t9d7m 1/1 Running 0 8m54s 部署了一个ElasticSearch集群和skywalking对应的组件 为了方便访问，我们将Skywalking的UI通过NodePort对外暴露出来。 1kubectl patch svc skywalking-ui --type=&#x27;json&#x27; -p &#x27;[&#123;&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;/spec/type&quot;,&quot;value&quot;:&quot;NodePort&quot;&#125;,&#123;&quot;op&quot;:&quot;add&quot;,&quot;path&quot;:&quot;/spec/ports/0/nodePort&quot;,&quot;value&quot;:30930&#125;]&#x27; 访问http:&#x2F;&#x2F;节点ip:30930，此时默认UI界面如下： 与应用集成方式方式一：应用启动加载agent依赖包。比如通过http://skywalking.apache.org/downloads/下载对应发行版本tar包里面包含的agent文件，然后应用启动命令加载此agent依赖文件即可。如以下通过容器应用构建Dockerfile方式加载。 123456FROM registry.cn-shenzhen.aliyuncs.com/yedward/openjdk:8-jre-slimUSER appuserEXPOSE 8080COPY --from=build /usr/src/app/target/*.jar /app/WORKDIR /appCMD java -jar -Xms1024m -Xmx1024m /app/spring-petclinic.jar -javaagent:/opt/skywalking/agent/skywalking-agent.jar 方式二：通过外部挂载和参数引用方式。下面Demo主要就是对这种方式的演示。 这两种方式最大的区别在于，方式一需要改动应用启动命令，方式二对应用本身不需要进行改动，就需要进行升级即可。 应用Demo演示以spring-petclinic为Demo进行演示，一个简单的应用，前面有一个Gateway做为统一流量入口，通过Web模块将对应的服务请求转发到后端不同的其他服务上，进行服务调用。 https://github.com/wanshaoyuan/spring-petclinic-msa.git 部署Demo应用 123456下载git clone https://github.com/wanshaoyuan/spring-petclinic-msa.git部署yamlkubectl apply -f k8s/local-skywalking/ 访问服务 http://host_ip:31080 一个宠物医院系统，可以点击进行一些资料的添加和修改。 查看Skywalking的数据，点击右上角自动的按钮进行自动的刷新 调用Top耗时显示和响应耗时范围展示 服务响应时间和调用成功率 全局调用链路展示 调用关系和路径耗时展示 总结：Skywalking UI做的非常精美，做为一款开源产品功能覆盖还是非常全面的，APM系统对于目前微服务体系的应用进行故障排查还是有非常大的帮助。但这种非常对开发语言还是有一定依赖性，另外一种不需要开发语言依赖的方式就是ServiceMesh的实现完全不侵入应用，也不需要加载Jar包，ServiceMesh主要是通过做应用透明代理和流量劫持去实现链路追踪，如Istio，但他的缺点是只能追踪HTTP请求，覆盖范围有限，并且相对追踪的数据也比埋点的要少一些。","categories":[{"name":"应用上云","slug":"应用上云","permalink":"http://yoursite.com/categories/%E5%BA%94%E7%94%A8%E4%B8%8A%E4%BA%91/"}],"tags":[{"name":"应用上云","slug":"应用上云","permalink":"http://yoursite.com/tags/%E5%BA%94%E7%94%A8%E4%B8%8A%E4%BA%91/"}]},{"title":"OPA概述和介绍","slug":"opa_1","date":"2021-07-08T13:45:59.000Z","updated":"2021-07-08T13:45:59.000Z","comments":true,"path":"2021/07/08/opa_1/","permalink":"http://yoursite.com/2021/07/08/opa_1/","excerpt":"","text":"OPA是什么？OPA是一个开源的通用策略引擎，由Styra公司贡献给CNCF基金会，目前也已经成CNCF基金会毕业了。 OPA通过声明式开发语言Rego，来编写控制策略，对支持的平台进行策略控制。类似于在请求入口和后端服务中间部署的一个组件，对请求的指令进行判断是否与配置的策略相匹配。 OPA能做什么？通过OPA编写的策略可以对microservices, Kubernetes, CI&#x2F;CD pipelines, API gateways进行一些策略控制。例如与K8s集成，实现灵活的安全策略控制，在Kubernetes中也是可以取代Pod Security Policy实现更灵活的控制，目前k8s社区也在后面版本将Pod Security Policy去除，计划用OPA进行取代。 OPA与Kubernetes结合是发展历程和实现方式Gatekeeper v1.0 - 使用 OPA 作为带有 kube-mgmt sidecar 的许可控制器，用来强制执行基于 configmap 的策略。这种方法实现了验证和转换许可控制(admission controller)。贡献方：Styra Gatekeeper v2.0 - 使用 Kubernetes 策略控制器（admission controller)作为许可控制器，OPA 和 kube-mgmt sidecar 实施基于 configmap 的策略。这种方法实现了验证和转换准入控制和审核功能。贡献方：Microsoft Gatekeeper v3.0 - 准入控制器(adminission controller)与 OPA Constraint Framework 集成在一起，用来实施基于 CRD 的策略，并可以可靠地共享已完成声明配置的策略。使用 kubebuilder 进行构建，实现了验证以及最终转换（待完成）为许可控制和审核功能。这样就可以为 Rego 策略创建策略模板，将策略创建为 CRD 并存储审核结果到策略 CRD 上。该项目是 Google，Microsoft，Red Hat 和 Styra 合作完成的。 上面提到了多次准入控制器(adminission controller)这里也简单一起介绍一下，为什么有了准入控制器还需要OPA。首先准入(Admission Control)机制是一种请求拦截机制，用于对请求的资源对象进行校验。包含两个控制器： 变更（Mutating）准入控制：修改请求的对象 验证（Validating）准入控制：验证请求的对象当请求到达 API Server 的时候首先执行变更准入控制，然后再执行验证准入控制。 准入控制器实现的策略 为什么有了准入控制器还需要OPA-Gatekeeper ？在Kubernetes中，admission在创建，更新和删除操作期间强制执行对象的语义验证，需要重新编译或重新配置Kubernetes API服务器。但使用OPA，直接可以在Kubernetes对象上实施自定义策略 配合强大的声明式策略语言Rego，直接通过K8S对象配置规则。 Service可以是以下任意： Kubernetes API server API gateway Custom service CI&#x2F;CD pipeline 总结来说，以前玩admission Controller太复杂了，要定义很多多个webhook-server，但现在Gatekeeper就相当于一个总的webhook-server，根据Rego+CRD配置对应规则即可。 Gatekeeper与Kubernetes集成实现方式 1、Gatekeeper部署在Kubernetes 中启动后注册 API Server 的注册 Dynamic Admission Controller。 2、将Gatekeeper做为总的Webhook-server，当连接Api-server时，通过RBAC认证后，请求到Admission controller，发送到Gatekeeper进行规则匹配和决策。 3、然后将响应结果在返回到Api-server。 参考链接： https://www.openpolicyagent.org/https://kubernetes.io/zh/blog/2019/08/06/opa-gatekeeper-policy-and-governance-for-kubernetes/","categories":[{"name":"安全","slug":"安全","permalink":"http://yoursite.com/categories/%E5%AE%89%E5%85%A8/"}],"tags":[{"name":"安全","slug":"安全","permalink":"http://yoursite.com/tags/%E5%AE%89%E5%85%A8/"}]},{"title":"kubernetes pod的弹性伸缩(三)(基于事件驱动伸缩)","slug":"kubernetes_keda","date":"2021-06-18T13:45:59.000Z","updated":"2021-06-18T13:45:59.000Z","comments":true,"path":"2021/06/18/kubernetes_keda/","permalink":"http://yoursite.com/2021/06/18/kubernetes_keda/","excerpt":"","text":"软件 版本 keda 2.3.0 Kubernetes 1.20.7 概述目前在Kubernetes中做POD弹性伸缩HPA有基于CPU、memory这种资源指标实现和通过custom-metrics-apiserver和Prometheus-adapter实现的自定义业务监控指标的弹性伸缩，但这些都不是很灵活有效，基于cpu、memory实际对业务带来价值并不大，并不能很准确代表业务实际是否需要扩缩容。基于自定义业务监控指标配置起来相当繁琐。所以社区这些问题背景下开发了KEDA，基于事件的伸缩 KEDA是什么？KEDA是红帽和微软联合开发的一个开源项目，后面贡献给了CNCF基金会，目前处于sandbox阶段，实现KEDA的目的主要为了更好和更加灵活使用HPA。 项目地址https://github.com/kedacore/keda 文档地址https://keda.sh/ 什么是事件驱动？举个简单例子，常见的消息队列系统，有生产者和消费者，生产者往消息队列中吐数据，消费者从消息队列中消费数据，当出现消息堆积时，此时是消费者处理不过来了，应该及时扩容消费者。这就是keda可以实现的，通过对应的接口去查看消息队列系统的情况，进行判断，调用HPA进行扩缩容。 keda能做什么？ 图片来源：https://developer.ibm.com/technologies/messaging/articles/introduction-to-keda/ 图片来源：https://keda.sh/docs/1.4/concepts/ KEDA 监控定义的事件源，并定期检查是否有任何事件。达到设置的阈值后，KEDA 会根据部署的复制件计数设置为 1 或 0，根据配置最小副本计数，激活或停用POD，具体取决于是否有任何事件。通过上面两幅架构图可以看见keda和HPA是相辅相成的，keda实现的是事件收集和pod副本数0-&gt;1，1-&gt;0的调整，HPA实现1-&gt;n,n-&gt;1的POD副本调整. keda组成keda由两个组件组成：keda-operator：创建维护hpa资源对象，同时激活hpa伸缩(0-&gt;1,1-&gt;0)。 keda-metrics-apiserver: 实现了hpa中external metrics，根据事件源配置返回计算结果,推动hpa进行。 keda定义的CRD对象主要有三个 ScaledObjects：定义事件源和资源映射以及控制的范围。 ScaledJobs：定义事件源和job资源映射以及控制的范围。 TriggerAuthentication：监控事件源的认证配置 测试使用部署keda部署非常简单，有三种方式 Helm charts Operator Hub YAML declarations 这里，我们直接用yaml进行，一个yaml即可搞定 12kubectl apply -f https://github.com/kedacore/keda/releases/download/v2.3.0/keda-2.3.0.yaml 部署完后检查 1234kubectl get pod -n kedaNAME READY STATUS RESTARTS AGEkeda-metrics-apiserver-84d8fc689d-62kc2 1/1 Running 0 5h23mkeda-operator-54f95d8598-l5vjg 1/1 Running 0 5h23m 看状态是否正常，也建议看看这两个组件的log是否有报错。 接下来进行测试使用，直接使用官方github里面的例子。 RabbitMQ弹性实现效果图片来源:(https://jstobigdata.com/rabbitmq/direct-exchange-in-amqp-rabbitmq/)当rabbimq队列达到设定的长度后扩容consumer副本数，当队列长度降下去后将副本数缩小为对应的配置值。 部署Rabbimq-server 1helm repo add bitnami https://charts.bitnami.com/bitnami 部署rabbitmq 1helm install rabbitmq --set auth.username=user --set auth.password=PASSWORD bitnami/rabbitmq 查看是否部署成功 123kubectl get pod NAME READY STATUS RESTARTS AGErabbitmq-0 1/1 Running 0 7h37m 部署 RabbitMQ consumer repo里面是配置rabbimq-consumer和keda规则 1git clone https://github.com/kedacore/sample-go-rabbitmq.git 12cd sample-go-rabbitmq 部署 123456kubectl apply -f deploy/deploy-consumer.yamlsecret/rabbitmq-consumer-secret createddeployment.apps/rabbitmq-consumer createdscaledobject.keda.sh/rabbitmq-consumer createdtriggerauthentication.keda.sh/rabbitmq-consumer-trigger created 主要看看ScaledObject 123456789101112131415161718apiVersion: keda.sh/v1alpha1kind: ScaledObjectmetadata: name: rabbitmq-consumer namespace: defaultspec: scaleTargetRef: name: rabbitmq-consumer pollingInterval: 5 # Optional. Default: 30 seconds cooldownPeriod: 30 # Optional. Default: 300 seconds maxReplicaCount: 30 # Optional. Default: 100 triggers: - type: rabbitmq metadata: queueName: hello queueLength: &quot;5&quot; authenticationRef: name: rabbitmq-consumer-trigger 参数 含义 pollingInterval 定时获取scaler的时间间隔，默认30s cooldownPeriod 上次active至缩容为0需要等待的时间，默认300s maxReplicaCount 最大POD副本数 minReplicaCount 最小POD副本数，默认为0 triggers.type 事件类型是keda支持的哪种，如是redis就写redis，是kafka是写kafka trigger.metadata.queueName 这个根实际事件类型有关的触发器指标了，这里定义的是rabbitmq的队列名 trigger.metadata.queueLength 触发器定义的队列长度 trigger.authenticationRef 认证关联，关联TriggerAuthentication 部署后会发现rabbitmq-consumer副本数会自动变为0，因为此时队列是空的 123kubectl get deploymentNAME READY UP-TO-DATE AVAILABLE AGErabbitmq-consumer 0/0 0 0 1h53m 此时get hpa也创建出来了 123kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEkeda-hpa-rabbitmq-consumer Deployment/rabbitmq-consumer &lt;unknown&gt;/5 (avg) 1 30 0 1h 部署produce往Rabbitmq里面推数据制造拥堵 这个job将推送 300信息到 “hello” 这个队列 keda将在2分种扩容 deployment到30个 1kubectl apply -f deploy/deploy-publisher-job.yaml 对应的我们也可以观察hpa和pod的扩缩容 已经在迅速扩容了 1234567891011121314151617181920212223242526272829303132333435ubectl get pod NAME READY STATUS RESTARTS AGEnfs-client-provisioner-68df844cc-dclwb 1/1 Running 0 8hrabbitmq-0 1/1 Running 0 8hrabbitmq-consumer-5c486c869c-4pwxj 0/1 ContainerCreating 0 40srabbitmq-consumer-5c486c869c-5wd9k 0/1 ContainerCreating 0 40srabbitmq-consumer-5c486c869c-6zgs9 1/1 Running 0 56srabbitmq-consumer-5c486c869c-78lx9 0/1 ContainerCreating 0 40srabbitmq-consumer-5c486c869c-7g4wf 1/1 Running 0 71srabbitmq-consumer-5c486c869c-7qhqn 0/1 ContainerCreating 0 40srabbitmq-consumer-5c486c869c-8rj6l 1/1 Running 0 56srabbitmq-consumer-5c486c869c-bmjqx 0/1 ContainerCreating 0 40srabbitmq-consumer-5c486c869c-db2ts 1/1 Running 0 71srabbitmq-consumer-5c486c869c-ddzhp 1/1 Running 0 56srabbitmq-consumer-5c486c869c-dgwn4 0/1 ContainerCreating 0 40srabbitmq-consumer-5c486c869c-f4k8l 1/1 Running 0 87srabbitmq-consumer-5c486c869c-jqbhl 1/1 Running 0 87srabbitmq-consumer-5c486c869c-k2mdw 0/1 ContainerCreating 0 40srabbitmq-consumer-5c486c869c-kpgc2 0/1 ContainerCreating 0 56srabbitmq-consumer-5c486c869c-kzlwl 0/1 ContainerCreating 0 56srabbitmq-consumer-5c486c869c-ls4dm 1/1 Running 0 56srabbitmq-consumer-5c486c869c-lvtx8 0/1 ContainerCreating 0 40srabbitmq-consumer-5c486c869c-ngmnj 1/1 Running 0 71srabbitmq-consumer-5c486c869c-nxp4n 0/1 ContainerCreating 0 40srabbitmq-consumer-5c486c869c-p27lk 1/1 Running 0 71srabbitmq-consumer-5c486c869c-p4d59 0/1 ContainerCreating 0 40srabbitmq-consumer-5c486c869c-q9wdl 0/1 ContainerCreating 0 40srabbitmq-consumer-5c486c869c-qqgdw 1/1 Running 0 56srabbitmq-consumer-5c486c869c-rpghb 1/1 Running 0 89srabbitmq-consumer-5c486c869c-sr7hd 0/1 ContainerCreating 0 40srabbitmq-consumer-5c486c869c-sxl8t 1/1 Running 0 87srabbitmq-consumer-5c486c869c-vk9wp 0/1 ContainerCreating 0 56srabbitmq-consumer-5c486c869c-xgnbf 0/1 ContainerCreating 0 40srabbitmq-consumer-5c486c869c-xn76k 0/1 ContainerCreating 0 40srabbitmq-publish-rxjpl 0/1 Completed 0 109s 等待队列数据消费完了，等待冷却期，pod会自行销毁。 基于Prometheus自定义监控指标弹性使用自定义HPA监控的例子测试 clone代码 1git clone https://github.com/stefanprodan/k8s-prom-hpa 切换到k8s-prom-hpa目录 12kubectl create namespace monitoring kubectl create -f ./prometheus 查看prometheus 1234567kubectl get pod -n monitoringNAME READY STATUS RESTARTS AGEprometheus-64bc56989f-qcs4p 1/1 Running 1 22hkubectl get svc -n monitoringNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEprometheus NodePort 10.104.215.207 &lt;none&gt; 9090:31190/TCP 22h 访问prometheushttp://node_ip:31190 prometheus内prometheus-cfg.yaml 配置了自动发生规则，会自动将组件注册。 部署Podinfo应用测试custom-metric autoscale 1kubectl create -f ./podinfo/podinfo-svc.yaml,./podinfo/podinfo-dep.yaml prometheus配置了自动发现规则，在podinfo-dep.yaml里面配置了对应的规则 12annotations: prometheus.io/scrape: &#x27;true&#x27; 所以应用一启动就能直接在prometheus-target中发现。 配置keda 1234567891011121314151617181920apiVersion: keda.sh/v1alpha1kind: ScaledObjectmetadata: name: prometheus-scaledobject namespace: defaultspec: scaleTargetRef: name: podinfo pollingInterval: 5# Optional. Default: 30 seconds cooldownPeriod: 30 # Optional. Default: 300 seconds maxReplicaCount: 30 # Optional. Default: 100 triggers: - type: prometheus metadata: serverAddress: http://192.168.0.32:31190/ metricName: http_requests_total threshold: &#x27;10&#x27; query: sum(rate(http_requests_total[1m])) 这里10指的是每秒10个请求，按照定义的规则metricsQuery中的时间范围1分钟，这就意味着过去1分钟内每秒如果达到10个请求则会进行扩容。 配置完后使用ab进行压力测试 使用ab进行压力测试，模拟1000个并发量 安装ab 1apt install apache2-utils -y ab命令最基本的参数是-n和-c： 12345678-n 执行的请求数量-c 并发请求个数其他参数：-t 测试所进行的最大秒数-p 包含了需要POST的数据的文件-T POST数据所使用的Content-type头信息-k 启用HTTP KeepAlive功能，即在一个HTTP会话中执行多个请求，默认时，不启用KeepAlive功能 12ab -n 10000 -c 1000 http://192.168.0.32:31198/ 查看是否进行弹性伸缩。 12345678910kubectl get pod NAME READY STATUS RESTARTS AGEnfs-client-provisioner-68df844cc-dclwb 1/1 Running 3 3d8hpodinfo-56874dc7f8-4n9m4 0/1 Running 4 74mpodinfo-56874dc7f8-g89vr 0/1 Running 0 43spodinfo-56874dc7f8-v2pn8 1/1 Running 4 74mpodinfo-56874dc7f8-xgrz6 0/1 ContainerCreating 0 12srabbitmq-0 1/1 Running 0 3d8hrabbitmq-publish-rxjpl 0/1 Completed 0 2d23h keda会根据实际的值计算需要弹性的副本数，保证业务可用性。 123root@cka01:~# kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEkeda-hpa-prometheus-scaledobject Deployment/podinfo 13/10 (avg) 1 30 3 3m3s 总结总的感受下来，还是非常好用的，像之前配一些业务自定义指标的HPA监控还需要配置Prometheus-adapter配置规则，但通过keda直接可以对接Prometheus，配置对应表达式尽快。其他一些集成的事件触发器也直接使用就可以了。 keda支持的业务事件触发器 参考链接： https://keda.sh","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"容器应用优雅关闭","slug":"graceful_close","date":"2021-05-26T13:45:59.000Z","updated":"2021-05-26T13:45:59.000Z","comments":true,"path":"2021/05/26/graceful_close/","permalink":"http://yoursite.com/2021/05/26/graceful_close/","excerpt":"","text":"概述优雅关闭： 在关闭前，执行正常的关闭过程，释放连接和资源，如我们操作系统执行shutdown。 目前业务系统组件众多，互相之间调用关系也比较复杂，一个组件的下线、关闭会涉及到多个组件对于任何一个线上应用，如何保证服务更新部署过程中从应用停止到重启恢复服务这个过程中不影响正常的业务请求，这是应用开发运维团队必须要解决的问题。传统的解决方式是通过将应用更新流程划分为手工摘流量、停应用、更新重启三个步骤，由人工操作实现客户端不对更新感知。这种方式简单而有效，但是限制较多：不仅需要使用借助网关的支持来摘流量，还需要在停应用前人工判断来保证在途请求已经处理完毕。 同时，在应用层也有一些保障应用优雅停机的机制，目前Tomcat、Spring Boot、Dubbo等框架都有提供相关的内置实现，如SpringBoot 2.3内置graceful shutdown可以很方便的直接实现优雅停机时的资源处理，同时一个普通的Java应用也可以基于Runtime.getRuntime().addShutdownHook()来自定义实现，它们的实现原理都基本一致，通过等待操作系统发送的SIGTERM信号，然后针对监听到该信号做一些处理动作。优雅停机是指在停止应用时，执行的一系列保证应用正常关闭的操作。这些操作往往包括等待已有请求执行完成、关闭线程、关闭连接和释放资源等，优雅停机可以避免非正常关闭程序可能造成数据异常或丢失，应用异常等问题。优雅停机本质上是JVM即将关闭前执行的一些额外的处理代码。 现状分析现阶段，业务容器化后业务启动是通过shell脚本启动业务，对应的在容器内PID为1的进程为shell进程但shell 程序不转发signals，也不响应退出信号。所以在容器应用中如果应用容器中启动 shell，占据了 pid&#x3D;1 的位置，那么就无法接收k8s发送的SIGTERM信号，只能等超时后被强行杀死了。 案例分析go开发的一个Demo 1234567891011121314151617181920212223242526272829303132333435package mainimport ( &quot;fmt&quot; &quot;os&quot; &quot;os/signal&quot; &quot;syscall&quot; &quot;time&quot;)func main() &#123; c := make(chan os.Signal) signal.Notify(c, syscall.SIGTERM, syscall.SIGINT) go func() &#123; for s := range c &#123; switch s &#123; case syscall.SIGINT, syscall.SIGTERM: fmt.Println(&quot;退出&quot;, s) ExitFunc() default: fmt.Println(&quot;other&quot;, s) &#125; &#125; &#125;() fmt.Println(&quot;进程启动...&quot;) time.Sleep(time.Duration(200000)*time.Second)&#125;func ExitFunc() &#123; fmt.Println(&quot;正在退出...&quot;) fmt.Println(&quot;执行清理...&quot;) fmt.Println(&quot;退出完成...&quot;) os.Exit(0)&#125; 代码参考：https://www.jianshu.com/p/ae72ad58ecb6 1、Signal.Notify会监听括号内指定的信号，若没有指定，则监听所有信号。2、通过switch对监听到信号进行判断，如果是SININT和SIGTERM则条用Exitfunc函数执行退出。 SHELL模式和CMD模式带来的差异性编写应用Dockerfile文件 概述在Dockerfile中CMD和ENTRYPOINT用来启动应用，有shell模式和exec模式，对应的使用shell模式，PID为1的进程为shell，使用exec模式PID为1的进程为业务本身。SHELL模式 12345678FROM golang as builderWORKDIR /go/COPY app.go .RUN go build app.goFROM ubuntuWORKDIR /root/COPY --from=builder /go/app .CMD ./app 构建镜像 1docker build -t app:v1.0-shell ./ 运行查看 12345docker exec -it app-shell ps auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.7 0.0 2608 548 pts/0 Ss+ 03:22 0:00 /bin/sh -c ./root 6 0.0 0.0 704368 1684 pts/0 Sl+ 03:22 0:00 ./approot 24 0.0 0.0 5896 2868 pts/1 Rs+ 03:23 0:00 ps aux 可以看见PID为1的进程是sh进程 此时执行docker stop，业务进程是接收不到SIGTERM信号的，要等待一个超时时间后被KILL 日志没有输出SIGTERM关闭指令 12345docker stop app-shellapp-shelldocker logs app-shell进程启动... EXEC模式 123456789FROM golang as builderWORKDIR /go/COPY app.go .RUN go build app.goFROM ubuntuWORKDIR /root/COPY --from=builder /go/app .CMD [&quot;./app&quot;] 构建镜像 1docker build -t app:v1.0-exec ./ 运行查看 1234docker exec -it app-exec ps auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 2.0 0.0 703472 1772 pts/0 Ssl+ 03:33 0:00 ./approot 14 0.0 0.0 5896 2908 pts/1 Rs+ 03:34 0:00 ps aux 可以看见PID为1的进程是应用进程 此时执行docker stop，业务进程是可以接收SIGTERM信号的，会优雅退出 123456789docker stop app-execapp-execdocker logs app-exec进程启动...退出 terminated正在退出...执行清理...退出完成... 注意：1、以下测试在ubuntu做为应用启动base镜像测试成功，在alpine做为应用启动base镜像时shell模式和exec模式都一样，都是应用进程为PID 1的进程。 直接启动应用和通过脚本启动区别在实际生产环境中，因为应用启动命令后会接很多启动参数，所以通常我们会使用一个启动脚本来启动应用，方便我们启动应用。对应的在容器内PID为1的进程为shell进程但shell 程序不转发signals，也不响应退出信号。所以在容器应用中如果应用容器中启动 shell，占据了 pid&#x3D;1 的位置，那么就无法接收k8s发送的SIGTERM信号，只能等超时后被强行杀死了。启动脚本start.sh 123cat &gt; start.sh&lt;&lt; EOF #!/bin/shsh -c /root/app 123456789FROM golang as builderWORKDIR /go/COPY app.go .RUN go build app.goFROM alpineWORKDIR /root/COPY --from=builder /go/app .ADD start.sh /root/CMD [&quot;/bin/sh&quot;,&quot;/root/start.sh&quot;] 构建应用 1docker build -t app:v1.0-script ./ 查看 12345docker exec -it app-script ps auxPID USER TIME COMMAND1 root 0:00 /bin/sh /root/start.sh6 root 0:00 /root/app19 root 0:00 ps aux docker stop关闭应用 1docker stop app-script 是登待超时后被强行KILL 12docker logs app-script进程启动... 容器应用优雅关闭方案介绍方案介绍正常的优雅停机可以简单的认为包括两个部分：- 应用：应用自身需要实现优雅停机的处理逻辑，确保处理中的请求可以继续完成，资源得到有效的关闭释放，等等。针对应用层，不管是Java应用还是其他语言编写的应用，其实现原理基本一致，都提供了类似的监听处理接口，根据规范要求实现即可。- 平台：平台层要能够将应用从负载均衡中去掉，确保应用不会再接受到新的请求连接，并且能够通知到应用要进行优雅停机处理。在传统的部署模式下，这部分工作可能需要人工处理，但是在K8s容器平台中，K8s的Pod删除默认就会向容器中的主进程发送优雅停机命令，并提供了默认30s的等待时长，若优雅停机处理超出30s以后就会强制终止。同时，有些应用在容器中部署时，并不是通过容器主进程的形式进行部署，那么K8s也提供了PreStop的回调函数来在Pod停止前进行指定处理，可以是一段命令，也可以是一个HTTP的请求，从而具备了较强的灵活性。通过以上分析，理论上应用容器化部署以后仍然可以很好的支持优雅停机，甚至相比于传统方式实现了更多的自动化操作，本文档后面会针对该方案进行详细的方案验证。 容器应用中第三方Init：在构建应用中使用第三方init如tini或dumb-init 方案一：通过k8s的prestop参数调用容器内进程关闭脚本，实现优雅关闭。 方案二：通过第三方init进程传递SIGTERM到进程中。 方案验证方案一：通过k8s Prestop参数调用在前面脚本启动的dockerfile基础上，定义一个优雅关闭的脚本，通过k8s-prestop在关闭POD前调用优雅关闭脚本，实现pod优雅关闭。 启动脚本start.sh 123cat &gt; start.sh&lt;&lt; EOF #!/bin/sh./app stop.sh优雅关闭脚本 12#!/bin/shps -ef|grep app|grep -v grep|awk &#x27;&#123;print $1&#125;&#x27;|xargs kill -15 12345678910FROM golang as builderWORKDIR /go/COPY app.go .RUN go build app.goFROM alpineWORKDIR /root/COPY --from=builder /go/app .ADD start.sh /root/CMD [&quot;/bin/sh&quot;,&quot;/root/start.sh&quot;] 构建镜像 1docker build -t app:v1.0-prestop ./ 通过yaml部署到k8s中 12345678910111213141516171819202122232425apiVersion: apps/v1kind: Deploymentmetadata: name: app-prestop labels: app: prestopspec: replicas: 1 selector: matchLabels: app: prestop template: metadata: labels: app: prestop spec: containers: - name: prestop image: 172.16.1.31/library/app:v1.0-prestop lifecycle: preStop: exec: command: - sh - /root/stop.sh 查看POD日志，然后删除pod副本 123kubectl get pod NAME READY STATUS RESTARTS AGEapp-prestop-847f5c4db8-mrbqr 1/1 Running 0 73s 查看日志 12kubectl logs app-prestop-847f5c4db8-mrbqr -f进程启动... 另外窗口删除POD 12345678kubectl logs app-prestop-847f5c4db8-mrbqr -f进程启动...退出 terminated正在退出...执行清理...退出完成... 可以看见执行了Prestop脚本进行优雅关闭。同样的可以将yaml文件中的Prestop脚本取消进行对比测试可以发现就会进行强制删除。 方案二：shell脚本修改为exec执行修改start.sh脚本 12#!/bin/shexec ./app shell中添加一个 exec 即可让应用进程替代当前shell进程,可将SIGTERM信号传递到业务层，让业务实现优雅关闭。 可使用上面例子，进行修改测试。 方案三：通过第三init工具启动使用dump-init或tini做为容器的主进程，在收到退出信号的时候，会将退出信号转发给进程组所有进程。，主要适用应用本身无关闭信号处理的场景。docker –init本身也是集成的tini。 1234567891011FROM golang as builderWORKDIR /go/COPY app.go .RUN go build app.goFROM alpineWORKDIR /root/COPY --from=builder /go/app .ADD start.sh tini /root/RUN chmoad a+x start.sh &amp;&amp; apk add --no-cache tiniENTRYPOINT [&quot;/sbin/tini&quot;, &quot;--&quot;]CMD [&quot;/root/tini&quot;, &quot;--&quot;, /root/start.sh&quot;] 构建镜像 1docker build -t app:v1.0-tini ./ 测试运行 1docker run -itd --name app-tini app:v1.0-tini 查看日志 123docker logs app-tini进程启动... 发现容器快速停止了，但没有输出应用关闭和清理的日志 后面查阅相关资料发现 使用tini或dump-init做为应用启动的主进程。tini和dumb-init会将关闭信号向子进程传递，但不会等待子进程完全退出后自己在退出。而是传递完后直接就退出了。 相关issue：https://github.com/krallin/tini/issues/180 后面又查到另外一个第三方的组件smell-baron能实现等待子进程优雅关闭后在关闭本身功能。 但这个项目本身热度不是特别高，并且有很久没有维护了。 123456789101112FROM golang as builderWORKDIR /go/COPY app.go .RUN go build app.goFROM ubuntuWORKDIR /root/COPY --from=builder /go/app .ADD start.sh /root/ADD smell-baron /bin/smell-baronRUN chmod a+x /bin/smell-baron &amp;&amp; chmod a+x start.shENTRYPOINT [&quot;/bin/smell-baron&quot;]CMD [&quot;/root/start.sh&quot;] 构建镜像 1docker build -t app:v1.0-smell-baron ./ 测试 123456789docker run -itd --name app-smell-baron app:v1.0-smell-barondocker stop app-smell-baron进程启动...退出 terminated正在退出...执行清理...退出完成... 总结：1、对于容器化应用启动命令建议使用EXEC模式。2、对于应用本身代码层面已经实现了优雅关闭的业务，但有shell启动脚本，容器化后部署到k8s上建议使方案一和方案二。3、对于应用本身代码层面没有实现优雅关闭的业务，建议使用方案三。 项目地址：https://github.com/insidewhy/smell-baronhttps://github.com/Yelp/dumb-inithttps://github.com/krallin/tini","categories":[{"name":"应用上云","slug":"应用上云","permalink":"http://yoursite.com/categories/%E5%BA%94%E7%94%A8%E4%B8%8A%E4%BA%91/"}],"tags":[{"name":"应用上云","slug":"应用上云","permalink":"http://yoursite.com/tags/%E5%BA%94%E7%94%A8%E4%B8%8A%E4%BA%91/"}]},{"title":"ArgoCD二：一个完整的CICD流程例子","slug":"argo_cd_2","date":"2021-01-27T13:45:59.000Z","updated":"2021-01-27T13:45:59.000Z","comments":true,"path":"2021/01/27/argo_cd_2/","permalink":"http://yoursite.com/2021/01/27/argo_cd_2/","excerpt":"","text":"概述此篇文章演示一个完整的GitOps工作流程，使用Gitlab-CI+ArgoCD来实现，其中Gitlab-CI主要用于实现代码编译、镜像构建、修改部署的yaml文件。ArgoCD用于将修改后的部署yaml文件推送到各个集群中。 软件 版本 Gitlab 13.7.0 ArgoCD 1.8.1 Kubernetes 1.17.4 整体流程 1、将应用代码和构建Docker镜像的Dockerfile文件放置到Gitlab对应项目中 2、在Gitlab中创建用于专门用于存放部署yaml的项目 3、配置Gitlab-CI用于代码编译镜像构建和业务yaml文件修改 4、配置ArgoCD检测存放部署yaml的项目，有更新后自动部署到对应环境中 Gitlab配置以一个Go-server项目为例 代码文件server.go文件 12345678910111213141516171819package mainimport ( &quot;fmt&quot; &quot;log&quot; &quot;net/http&quot;)func hello(w http.ResponseWriter, r *http.Request) &#123; fmt.Fprintf(w, &quot;Hello world&quot;)&#125;func main() &#123; http.HandleFunc(&quot;/&quot;, hello) if err := http.ListenAndServe(&quot;:8080&quot;, nil); err != nil &#123; log.Fatal(err) &#125;&#125; Dockerfile文件 12345FROM golang:alpineWORKDIR /goADD server /goCMD [&quot;./server&quot;] 在gitlab中创建proc-yaml的项目，用于集中存放到Kubernetes集群中的业务的yaml文件在proc-yaml项目中创建go-server项目的文件夹，专门存放go-server项目的yaml文件 deployment.yaml 12345678910111213141516171819202122232425262728293031323334apiVersion: apps/v1kind: Deploymentmetadata: name: go-demo labels: app: go-demospec: replicas: 1 selector: matchLabels: app: go-demo template: metadata: labels: app: go-demo spec: containers: - name: go-demo image: 172.16.1.31/library/go-server-demo:63 ports: - containerPort: 8080---apiVersion: v1kind: Servicemetadata: name: go-demospec: type: NodePort selector: app: go-demo ports: - port: 8080 targetPort: 8080 nodePort: 30007 Gitlab-CI配置Gitlab-CI配置和基本使用可以参考本博客的Gitlab-CI时用章节这里体现的主要是和之前的差异性部分Gitlab-CI环境变量配置 主要添加连接镜像仓库和代码仓库的凭证 创建.gitlab-ci.yml文件，用于配置gitlab-ci 123456789101112131415161718192021222324252627282930313233343536373839404142434445stages: - package - docker_build - modify_yamlbuild_job: image: golang:alpine stage: package tags: - k8s-runner script: - go build server.go artifacts: paths: - /builds/root/go-server docker_build_job: image: docker:19.03.0 services: - name: docker:19.03.0-dind command: [&quot;--insecure-registry=0.0.0.0/0&quot;,&quot;--registry-mirror=https://vqgjby9l.mirror.aliyuncs.com&quot;] variables: DOCKER_HOST: tcp://127.0.0.1:2375 DOCKER_TLS_CERTDIR: &quot;&quot; stage: docker_build tags: - k8s-runner script: - docker info - docker login -u $REGISTRY_USERNAME -p $REGISTRY_PASSWORD 172.16.1.31 - docker build -t 172.16.1.31/library/go-server-demo:$CI_PIPELINE_ID . - docker push 172.16.1.31/library/go-server-demo:$CI_PIPELINE_IDmodify_yaml: image: wanshaoyuan/git-client:v1.0 stage: modify_yaml tags: - k8s-runner script: - git clone http://$GIT_USERNAME:$GIT_PASSWORD@172.16.1.184/root/proc-yaml.git - git config --global user.email &quot;root@example.com&quot; - git config --global user.name &quot;root&quot; - git remote set-url origin http://$GIT_USERNAME:$GIT_PASSWORD@172.16.1.184/root/proc-yaml.git - sed -i &quot;s/go-server-demo:.*/go-server-demo:$CI_PIPELINE_ID/g&quot; proc-yaml/go-server/deployment.yaml - cd proc-yaml/ - git add * - git commit -m &quot;update go-server&quot; - git push origin master 主要包含3个阶段，代码编译阶段、镜像构建阶端、部署yaml文件修改阶段 代码编译阶段：启动一个编译容器，根据代码编译出对应的制品，这里为了能让下个镜像构建阶段直接使用编译好的制品，需要配置artifacts将目录共享给下个阶段。 镜像构建阶段：使用DinD的方式构建Docker镜像，然后镜像tag以我们Pipeline的ID为tag，这里好处在于可以根据对应的Pipeline的迅速定位到对应的镜像版本 部署yaml修改阶段: 通过git命令将对部署yaml项目的repo clone下来然后通过sed进行修改并commit回去，通过此步ArgoCD检测到变化后会拉取新的yaml进行部署 ArgoCD配置在ArgoCD中配置对接pro-yaml的repo认证 创建App 选择自动同步模式 PATH这配置我们go-server这个项目的路径，就会检测这里面的yaml文件，并自动更新到集群中 最后创建完成 测试 修改server.go的Hello world为Hello Argo 等待git-ci执行 检查proc-yaml项目是否更新 可以看见对应的image的tag修改成了，我们实际上Pipeline的ID 等待片刻后ArgoCD检测到yaml变化，自动部署，此时Kubernetes中应用进行了自动更新 访问 12curl 192.168.0.6:30007Hello argo 总结通过GitlabCI与ArgoCD结合确实非常方便的将我们应用进行部署到Kubernetes中，并且后续也非常容易进行回滚，在之前通过Gitlab-CI中集成kubectl镜像，进行部署容易造成kubeconfig文件的泄漏，也不方便做权限的管理和应用状态控制，但结合ArgoCD后一切都美好了起来。","categories":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/categories/CI-CD/"}],"tags":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/tags/CI-CD/"}]},{"title":"ArgoCD一：基本介绍和使用","slug":"argo_1","date":"2020-12-27T13:45:59.000Z","updated":"2020-12-27T13:45:59.000Z","comments":true,"path":"2020/12/27/argo_1/","permalink":"http://yoursite.com/2020/12/27/argo_1/","excerpt":"","text":"ArgoCD是什么？ArgoCD是一个专为Kubernetes而生，遵循GitOps理念的持续部署工具。这里指的Gitops简单来说就是1、以Git为核心。2、声明式定义各类对象。3、配置一致性管理。具体参考(https://www.hwchiu.com/gitops-book-ch2.html) 软件版本 软件 版本 ArgoCD 1.8.1 Kubernetes 1.18 ArgoCD是是Intuit公司开源出来的属于整个Argo项目中的其中一个子项目，整个Argo项目中还包括Argo-event、argo-workflow、Argo-Rollout https://github.com/argoproj 2020年Argo正式加入CNCF孵化项目中。 ArgoCD功能有哪些？ 将应用程序自动部署到指定环境中 可通过CRD方式定义执行对象 多租户管理和RBAC权限集成管控 多集应用管理和部署 可视化UI 支持多种Kubernetes配置对象(Kustomize，Helm，Ksonnet，Jsonnet，plain-YAML) 支持SSO集成(OIDC，OAuth2，LDAP，SAML 2.0，GitHub，GitLab，Microsoft，LinkedIn) Webhook集成可实现通知 自带metric指标暴露 为什么需要ArgoCD现有的CICD工具对在CD到Kubernetes中时通过提供kubeconfig文件方式对接到Kubernetes集群，这样非常容易造成kubeconfig文件泄漏，并且造成安全引患。 通过ArgoCD可以实现应用快速发布到Kubernetes中，并且能够根据版本标识快速跟踪和多集群部署功能，实现多个集群之间同一应用部署问题。 多种方式实现持续部署配置，如通过UI方式，同时也可以通过CRD方式使用Kubernetes自定义对象实现持续部署。 ArgoCD部署安装和基本使用软件架构 Argo API Server提供介面給外界操控 ArgoCD 服務，本身提供了 CLI, GUI 以及 gRPC&#x2F;REST 等介面，最簡單的 Demo 可以使用 GUI 來操作。Repository Service.与远端Git Repo进行同步，缓存在本地 ArgoCD-redis用作本地缓存 ArgoCd-dex-server实现token认证服务和SSO Application Controller与Kubernetes进行通信对部署的workload进行状态检测与Git Repository进行对比发现变化，进行更新。 部署单节点部署使用官网快速部署 12kubectl create namespace argocdkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml 高可用部署参考 1https://github.com/argoproj/argo-cd/tree/master/manifests 部署完后产生以下服务 12345678910111213141516171819202122232425262728293031NAME READY STATUS RESTARTS AGEpod/argocd-application-controller-0 1/1 Running 0 5d6hpod/argocd-dex-server-74588646d-sz9g8 1/1 Running 0 2d2hpod/argocd-redis-5ccdd9d4fd-csthm 1/1 Running 1 5d6hpod/argocd-repo-server-5bbb8bdf78-mxkv7 1/1 Running 0 18hpod/argocd-server-789fb45964-82mzx 1/1 Running 0 18hNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/argocd-dex-server ClusterIP 10.43.180.172 &lt;none&gt; 5556/TCP,5557/TCP,5558/TCP 5d6hservice/argocd-metrics ClusterIP 10.43.184.97 &lt;none&gt; 8082/TCP 5d6hservice/argocd-redis ClusterIP 10.43.4.233 &lt;none&gt; 6379/TCP 5d6hservice/argocd-repo-server ClusterIP 10.43.9.45 &lt;none&gt; 8081/TCP,8084/TCP 5d6hservice/argocd-server NodePort 10.43.48.239 &lt;none&gt; 80:31320/TCP,443:31203/TCP 5d6hservice/argocd-server-metrics ClusterIP 10.43.149.186 &lt;none&gt; 8083/TCP 5d6hNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/argocd-dex-server 1/1 1 1 5d6hdeployment.apps/argocd-redis 1/1 1 1 5d6hdeployment.apps/argocd-repo-server 1/1 1 1 5d6hdeployment.apps/argocd-server 1/1 1 1 5d6hNAME DESIRED CURRENT READY AGEreplicaset.apps/argocd-dex-server-74588646d 1 1 1 5d6hreplicaset.apps/argocd-redis-5ccdd9d4fd 1 1 1 5d6hreplicaset.apps/argocd-repo-server-5bbb8bdf78 1 1 1 5d6hreplicaset.apps/argocd-server-789fb45964 1 1 1 5d6hNAME READY AGEstatefulset.apps/argocd-application-controller 1/1 5d6h 使用NodePort方式为对外暴露 1kubectl patch svc argocd-server -n argocd -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&#x27; 访问Dashboard 默认帐号为admin，密码通过secret获取 1kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=&quot;&#123;.data.password&#125;&quot; | base64 -d ArgoCD使用方式有两种，方式一：通过UI-dashboard使用，方式二：通过CLI工具使用 使用CLI工具，需要提前下载 1https://github.com/argoproj/argo-cd/releases/download/v1.8.1/argocd-linux-amd64 123mv argocd-linux-amd64 /usr/local/bin/argocdchmod a+x /usr/local/bin/argocd 登录环境 1argocd login &lt;ARGOCD_SERVER&gt; 更新密码 1argocd account update-password 以一个简单的例子讲解ArgoCD的基本使用 在git上创建个argo-example项目用于存放我们的部署文件，存放以下yaml文件 12345678910111213141516171819202122232425262728293031323334apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2kind: Deploymentmetadata: name: bootcamptspec: selector: matchLabels: app: bootcampt replicas: 1 # tells deployment to run 2 pods matching the template template: metadata: labels: app: bootcampt spec: containers: - name: bootcampt image: docker.io/jocatalin/kubernetes-bootcamp:v1 ports: - containerPort: 8080---apiVersion: v1kind: Servicemetadata: name: bootcamptspec: selector: app: bootcampt ports: - protocol: TCP port: 8080 targetPort: 8080 nodePort: 30062 type: NodePort 点击创建 同步部署 如果对应的git是私有库，pull需要帐号密码则需要在argo设置中配置repo connect 填写对应的帐号密码，如果是自签名证书需要将CA附上 查看部署后的应用 在Kubernetes集群中查看部署后的应用 123kubectl get pod NAME READY STATUS RESTARTS AGEbootcampt-544d66b664-ddhgd 1/1 Running 0 2m18s 访问 12curl 172.16.1.6:30062Hello Kubernetes bootcamp! | Running on: bootcampt-544d66b664-ddhgd | v=1 更新yaml文件将docker.io/jocatalin/kubernetes-bootcamp:v1改成 docker.io/jocatalin/kubernetes-bootcamp:v2然后git push到git中 等待2分钟左右，ArgoCD会自动将新的yaml文件部署到Kubernetes中再次访问 12curl 172.16.1.6:30062Hello Kubernetes bootcamp! | Running on: bootcampt-7687b6c957-42vjw | v=2 通过CLI方式就是直接使用下面yaml文件部署或直接使用argocd cli create 1argocd app create --project default --name bootcamp --repo http://172.16.1.184/root/argo-example.git --path . --dest-server https://kubernetes.default.svc --dest-namespace default --revision master --sync-policy automated yaml文件 12345678910111213141516171819apiVersion: argoproj.io/v1alpha1kind: Applicationmetadata: name: bootcampspec: destination: name: &#x27;&#x27; namespace: default server: &#x27;https://kubernetes.default.svc&#x27; source: path: . repoURL: &#x27;http://172.16.1.184/root/argo-example.git&#x27; targetRevision: HEAD project: default syncPolicy: automated: prune: false selfHeal: false 因为ArgoCD有在部署集群中创建CRD对象所以我们同时也可以使用kubectl查看到创建好的Application 123kubectl get Application -n argocdNAME SYNC STATUS HEALTH STATUSbootcamp Synced Healthy 选中对应的app可以进行历史发布查看和回滚 对应的ID与实际git库里面提交的commit是匹配的 可以选择任意一个历史版本进行回滚 常用CLI命令 列出部署的应用 123argocd app listNAME CLUSTER NAMESPACE PROJECT STATUS HEALTH SYNCPOLICY CONDITIONS REPO PATH TARGETbootcamp https://kubernetes.default.svc default default Synced Healthy Auto &lt;none&gt; http://172.16.1.184/root/argo-example.git . HEAD 查看指定部署的应用详细情况 12345678910111213141516Name: bootcampProject: defaultServer: https://kubernetes.default.svcNamespace: defaultURL: https://172.16.1.7:31203/applications/bootcampRepo: http://172.16.1.184/root/argo-example.gitTarget: HEADPath: .SyncWindow: Sync AllowedSync Policy: AutomatedSync Status: Synced to HEAD (b1696a3)Health Status: HealthyGROUP KIND NAMESPACE NAME STATUS HEALTH HOOK MESSAGE Service default bootcampt Synced Healthy service/bootcampt unchangedapps Deployment default bootcampt Synced Healthy deployment.apps/bootcampt configured 同步部署的应用 1argocd app sync appname 设置为自动同步 1argocd app set appname --sync-policy automated 多集群应用部署添加集群首先需要将你需要添加的集群的config文件追加放到~&#x2F;.kube&#x2F;config 然后执行 12345kubectl config get-contextsCURRENT NAME CLUSTER AUTHINFO NAMESPACE* cluster-1 cluster-1 cluster-1 cluster-1-rke-node2 cluster-1-rke-node2 cluster-1 cluster-2 cluster-2 cluster-2 看当前在哪个context，如我要添加cluster-2 则执行 1argocd cluster add cluster-2 argocd命令会自动读取你config文件内的context信息包含证书和token执行成功后查看集群 1234argocd cluster listSERVER NAME VERSION STATUS MESSAGEhttps://192.168.0.29:6443 cluster-2 1.17 Successful https://kubernetes.default.svc in-cluster 1.17 Successful 如果是Rancher建的集群，你希望通过连接rancher在转发到api-server需要通过另外方式参考：https://gist.github.com/janeczku/b16154194f7f03f772645303af8e9f80 遗憾的是目前argocd多集群部署不能在一个Application内选择多个部署集群，只能通过建立多个相同Application对应不同部署目的集群方式 如上Application要部署到uat和sit环境只能 12345678910111213141516171819apiVersion: argoproj.io/v1alpha1kind: Applicationmetadata: name: sit-bootcampspec: destination: name: &#x27;&#x27; namespace: default server: &#x27;https://kubernetes.default.svc&#x27; source: path: . repoURL: &#x27;http://172.16.1.184/root/argo-example.git&#x27; targetRevision: HEAD project: default syncPolicy: automated: prune: false selfHeal: false 123456789101112131415161718apiVersion: argoproj.io/v1alpha1kind: Applicationmetadata: name: uat-bootcampspec: destination: name: &#x27;&#x27; namespace: default server: &#x27;https://192.168.0.29:6443&#x27; source: path: . repoURL: &#x27;http://172.16.1.184/root/argo-example.git&#x27; targetRevision: HEAD project: default syncPolicy: automated: prune: false selfHeal: false 社区内也进行了相关讨论，ISSUE如下https://github.com/argoproj/argo-cd/issues/1673 参考链接： https://www.jianshu.com/p/eec8e201b7e9","categories":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/categories/CI-CD/"}],"tags":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/tags/CI-CD/"}]},{"title":"Gitlab-CI使用","slug":"gitlab_ci","date":"2020-11-27T13:45:59.000Z","updated":"2020-11-27T13:45:59.000Z","comments":true,"path":"2020/11/27/gitlab_ci/","permalink":"http://yoursite.com/2020/11/27/gitlab_ci/","excerpt":"","text":"概述GitLab CI &#x2F; CD是GitLab的一部分,gitlab 8.0版本开始新增的功能，是用Ruby和Go语言编写的。根我们通常的CI系统不一样通常的是一个master-slave架构，即使没有slave，master一样可以做CI，slave只是做为一个压力分担功能，gitlab是gitlab-server本身是不执行的，是通过api与GitLab Runner交互让gitlab-runner去执行CI。 GitLab Runner是一个go语言编写程序，它可以运行在任何可以运行go环境的平台上(二进制包、docker、k8s) 易用性方面同样也是Everything as dockerGitlab CI 整个流程和 Drone 以及流行的 Travis CI 都是比较类似的，通过在项目中添加一个 .gitlab-ci.yml 的配置文件，配置文件中描述构建流水线来执行任务，对不同编程语言的编译通过不同的docker image实现。让CI工作所需的步骤可归纳为添加.gitlab-ci.yml到存储库的根目录配置一个Runner Runner分两种类型Specific Runners （独享的runner）Shared Runners（共享的runner） 软件版本 gitlab：GitLab Community Edition 13.7.0Kubernetes：1.17.4 部署gitlab-runner创建测试项目代码和Dockerfile在gitlab上创建一个项目名为go-server，编写个应用server.go package main import ( &quot;fmt&quot; &quot;log&quot; &quot;net/http&quot; ) func hello(w http.ResponseWriter, r *http.Request) &#123; fmt.Fprintf(w, &quot;Hello World&quot;) &#125; func main() &#123; http.HandleFunc(&quot;/&quot;, hello) if err := http.ListenAndServe(&quot;:8080&quot;, nil); err != nil &#123; log.Fatal(err) &#125; &#125; 创建DockerfileDockerfile FROM golang WORKDIR /go ADD server /go CMD [&quot;./server&quot;] deployment.yaml 部署到Kubernetes的yaml文件 apiVersion: apps/v1 kind: Deployment metadata: name: go-demo labels: app: go-demo spec: replicas: 1 selector: matchLabels: app: go-demo template: metadata: labels: app: go-demo spec: containers: - name: go-demo image: 172.16.1.31/library/go-server-demo:IMAGE_TAG ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: go-demo spec: type: NodePort selector: app: go-demo ports: - port: 8080 targetPort: 8080 nodePort: 30007 在Kubernetes集群中部署gitlab-runner整体流程 先执行代码构建，构建出制品 基于制品构建应用的容器镜像 部署到Kubernetes集群中 在gitlab项目页申请runner连接gitlab的地址和token，注意保存 git clone部署runner到Kubernetes中的helm文件 git clone https://gitlab.com/gitlab-org/charts/gitlab-runner.git 修改values.yaml 以下几个地方： gitlabUrl: //填写在项目页申请的gitlab地址和token runnerRegistrationToken: rbac: create: true //启用RBAC runners: # runner configuration, where the multi line strings is evaluated as # template so you can specify helm values inside of it. # # tpl: https://helm.sh/docs/howto/charts_tips_and_tricks/#using-the-tpl-function # runner configuration: https://docs.gitlab.com/runner/configuration/advanced-configuration.html config: | [[runners]] [runners.kubernetes] namespace = &quot;&#123;&#123;.Release.Namespace&#125;&#125;&quot; image = &quot;ubuntu:20.04&quot; privileged = true tags: &quot;k8s-runner&quot;//执行CI任务时可以通过tags匹配对应的runner，这里指定为k8s-runner 通过helm部署runner到Kubernetes中 1helm install --namespace default gitlab-runner gitlab-runner/ 查看是否部署成功，主要是看POD是否正常启动 kubectl get pod NAME READY STATUS RESTARTS AGE gitlab-runner-gitlab-runner-b5744f77c-jwnjk 1/1 Running 0 5d 默认是部署一个副本，如果需要多个runner实例，扩容对应的Deployment副本数即可。同样是刚刚项目CI&#x2F;CD设置页可以看见已经可用的runner了 在gitlab-ci中创建环境变量用于存放一些连接认证信息环境变量 将对应的信息填写到的对应的环境变量中，包括连接镜像仓库的帐号和密码和连接Kubernetes集群的config文件 KUBECONFIG文件进行base64编码存入，为了防止重定向后格式损坏 base64 /root/.kube/config -w0 测试案例编写.gitlab-ci.yaml stages: - package - docker_build - deploy_k8s build_job: image: golang:alpine stage: package tags: - k8s-runner script: - go build server.go artifacts: paths: - /builds/root/go-server docker_build_job: image: docker:19.03.0 services: - name: docker:19.03.0-dind command: [&quot;--insecure-registry=0.0.0.0/0&quot;,&quot;--registry-mirror=https://vqgjby9l.mirror.aliyuncs.com&quot;] variables: DOCKER_HOST: tcp://127.0.0.1:2375 DOCKER_TLS_CERTDIR: &quot;&quot; stage: docker_build tags: - k8s-runner script: - docker info - docker login -u $REGISTRY_USERNAME -p $REGISTRY_PASSWORD 172.16.1.31 - docker build -t 172.16.1.31/library/go-server-demo:$CI_PIPELINE_ID . - docker push 172.16.1.31/library/go-server-demo:$CI_PIPELINE_ID deploy_k8s_job: image: bitnami/kubectl:1.17 stage: deploy_k8s tags: - k8s-runner script: - echo $KUBECONFIG |base64 -d &gt; config - sed -i &quot;s/IMAGE_TAG/$CI_PIPELINE_ID/g&quot; deployment.yaml - cat deployment.yaml - kubectl apply -f deployment.yaml --kubeconfig config 注：1、因为Docker这边我们需要配置insecure-Registry需要这里通过Service的方式配置，所以这里在stage启用的是安装docker-cli的镜像，然后在server里面启用的是docker-dind的镜像，连接通过tcp连接使用这里需要注意新版docker需要使用DOCKER_TLS_CERTDIR参数设置为空，不然无法连接。 2、kubeconfig文件使用base64编码主要为了重定向后格式不会造成损坏。 3、因为gitlab-ci每个阶段都是启动一个容器去执行构建任务，所以在每个阶段产生的制品如何给下个阶段使用是一个很大问题，这里可以配置artifacts参数将对应需要保留的制品保存到下个阶段。 4、构建的缓存配置，可以参考以下链接https://docs.gitlab.com/runner/configuration/advanced-configuration.html 5、Docker镜像构建这里使用的是docker-dind的方式，实际上也可以直接透传主机的docker.sock文件到POD内使用，但不是特别安全，也可以使用一些其他解决方案如kaniko 上传后gitlab-ci会自动执行cicd部署到Kubernetes集群中 自动部署完成，查看Kubernetes集群中部署对象 kubectl get pod NAME READY STATUS RESTARTS AGE go-demo-7f697958d4-7fbcd 1/1 Running 0 32h 访问节点30007端口 curl 192.168.0.6:30007 Hello World 总结：优点： 与gitlab集成度非常高 不需要单独部署有gitlab&gt;&#x3D;8.0 就能直接使用 runner支持Autoscale UI可视化，可操作性强，可针对但个流程进行重复执行及报表展示 CI完全对应你这个代码库，每个项目对应自己CI 缺点： 没有插件，对接第三方系统需要自己实现 只能支持gitlab代码仓库","categories":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/categories/CI-CD/"}],"tags":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/tags/CI-CD/"}]},{"title":"calico网络插件运维","slug":"calico_ops","date":"2020-11-18T13:45:59.000Z","updated":"2020-11-18T13:45:59.000Z","comments":true,"path":"2020/11/18/calico_ops/","permalink":"http://yoursite.com/2020/11/18/calico_ops/","excerpt":"","text":"适用范围本文档测试范围 软件 版本 Kubernetes v1.14.x,v1.15.x,v1.16.x calico v3.13.4 概述Calico是一种开源网络和网络安全解决方案，适用于容器，虚拟机和基于主机的本机工作负载。Calico支持广泛的平台，包括Kubernetes，docker，OpenStack和裸机服务。Calico后端支持多种网络模式。- BGP模式：将节点做为虚拟路由器通过BGP路由协议来实现集群内容器之间的网络访问。- IPIP模式：在原有IP报文中封装一个新的IP报文，新的IP报文中将源地址IP和目的地址IP都修改为对端宿主机IP。- cross-subnet：Calico-ipip模式和calico-bgp模式都有对应的局限性，对于一些主机跨子网而又无法使网络设备使用BGP的场景可以使用cross-subnet模式，实现同子网机器使用calico-BGP模式，跨子网机器使用calico-ipip模式。 calico切换BGP模式 部署完成后默认使用calico-ipip的模式，通过在节点的路由即可得知，通往其他节点路由通过tunl0网卡出去 修改为BGP网络模式，在system项目中修改calico-node daemonset 修改CALICO_IPV4POOL_IPIP改为off，添加新环境变量FELIX_IPINIPENABLED为false 修改完成后对节点进行重启，等待恢复后查看主机路由，与ipip最大区别在于去往其他节点的路由，由Tunnel0走向网络网卡。 calico切换cross-subnet模式Calico-ipip模式和calico-bgp模式都有对应的局限性，对于一些主机跨子网而又无法使网络设备使用BGP的场景可以使用cross-subnet模式，实现同子网机器使用calico-BGP模式，跨子网机器使用calico-ipip模式。 部署集群网络选择calico网络插件 默认部署出来是calico的ip-in-ip的模式查看宿主机网卡，会发现多了个tunl0网卡，这个是建立ip隧道的网卡 去其他主机的路由都是走tunl0网卡出去 切换到cross-subnet模式 1kubectl edit ipPool/default-ipv4-ippool 将ipipMode改为crossSubnet 在UI将calico-node的POD删了重建 重启检查calico网络 可以看见同子网的主机出口走的是bgp，不同子网主机走的是tunl0网卡走ipip模式验证创建应用测试跨主机网络，在不同主机上互相ping测试，看看跨主机网络是否正常。 配置Route reflector安装calicoctl安装方式Single host上面binary安装Single host上面continer安装作为k8s pod运行实际经验：Binary方式在集群里面的一台worker节点安装（比如RR）calicoctl会检测bird&#x2F;felix的运行状态在非calico node节点运行只能使用部分命令，不能运行calico node相关命令通过配置calicoctl来对calico进行控制，通常情况下建议将 1curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.13.3/calicoctl 配置可执行权限 1chmod +x calicoctl 复制的&#x2F;usr&#x2F;bin&#x2F;目录 1cp calicoctl /usr/bin/ 配置calicoctl连接Kubernetes集群 123export CALICO_DATASTORE_TYPE=kubernetesexport CALICO_KUBECONFIG=~/.kube/configcalicoctl node status calico node-to-node mesh默认情况下calico采用node-to-node mesh方式 ，为了防止BGP路由环路，BGP协议规定在一个AS（自治系统）内部，IBGP路由器之间只能传一跳路由信息，所以在一个AS内部，IBGP路由器之间为了学习路由信息需要建立全互联的对等体关系，但是当一个AS规模很大的时候，这种全互联的对等体关系维护会大量消耗网络和CPU资源，所以这种情况下就需要建立路由反射器以减少IBGP路由器之间的对等体关系数量。 Route reflector角色介绍早期calico版本提供专门的route reflector镜像，在新版本calico node内置集成route reflector功能。Route reflector可以是以下角色： 集群内部的node节点 集群外部节点运行calico node 其他支持route reflector的软件或者设备。 这里以一个集群内部的node节点为例： 关闭node-to-node mesh123456789101112cat &lt;&lt;EOF | calicoctl apply -f -apiVersion: projectcalico.org/v3kind: BGPConfigurationmetadata: name: defaultspec: logSeverityScreen: Info nodeToNodeMeshEnabled: false asNumber: 63400EOF 设置Route reflector配置Route reflector支持多种配置方式如：1、支持配置全局BGP peer，。2、支持针对单个节点进行配置BGP Peer。也可以将calico节点充当Route reflector 这里以配置calico节点充当Router reflector为例。 配置节点充当BGP Route Reflector 可将Calico节点配置为充当路由反射器。为此，要用作路由反射器的每个节点必须具有群集ID-通常是未使用的IPv4地址。 要将节点配置为集群ID为244.0.0.1的路由反射器，请运行以下命令。这里将节点名为rke-node4的节点配置为Route Reflector，若一个集群中要配置主备rr，为了防止rr之间的路由环路，需要将集群ID配置成一样 1calicoctl patch node rke-node4 -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;bgp&quot;: &#123;&quot;routeReflectorClusterID&quot;: &quot;244.0.0.1&quot;&#125;&#125;&#125;&#x27; 给节点打上对应的label标记该节点以表明它是Route Reflector，从而允许BGPPeer资源选择它。 1kubectl label node rke-node4 route-reflector=true 创建BGPPeer 1234567891011export CALICO_DATASTORE_TYPE=kubernetesexport CALICO_KUBECONFIG=~/.kube/configcat &lt;&lt;EOF | calicoctl apply -f -kind: BGPPeerapiVersion: projectcalico.org/v3metadata: name: peer-with-route-reflectorsspec: nodeSelector: all() peerSelector: route-reflector == &#x27;true&#x27;EOF 查看BGP节点状态node上查看，peer type由node-to-node mesh变为 node specific Route Reflector上节点查看，节点已正常建立连接 设置veth网卡mtu通常，通过使用最高MTU值（不会在路径上引起碎片或丢包）来实现最高性能。对于给定的流量速率，最大带宽增加，CPU消耗可能下降。对于一些支持jumbo frames的网络设备，可以配置calico支持使用下表列举了，常见几种MTU配置下calico对应的网卡mtu的配置 IPIP和VXLAN协议中的IP中使用的额外报文头，通过头的大小减小了最小MTU。（IP中的IP使用20字节的标头，而VXLAN使用50字节的标头）。如果在Pod网络中的任何地方使用VXLAN，请将MTU大小配置为“物理网络MTU大小减去50”。如果仅在IP中使用IP，则将MTU大小配置为“物理网络MTU大小减去20” 。将工作负载端点MTU和隧道MTU设置为相同的值 配置方法： 升级集群 配置网卡MTU此时通过system项目下calico-config文件可以看见对应的mtu设置 创建workload查看POD网卡MTU为9001 设置全局AS号默认情况下，除非已为节点指定每个节点的AS，否则所有Calico节点都使用64512自治系统。可以通过修改默认的BGPConfiguration资源来更改所有节点的全局默认值。以下示例命令将全局默认AS编号设置为64513。 12345678910cat &lt;&lt;EOF | calicoctl apply -f -apiVersion: projectcalico.org/v3kind: BGPConfigurationmetadata: name: defaultspec: logSeverityScreen: Info nodeToNodeMeshEnabled: false asNumber: 64513EOF 设置单个主机和AS号例如，以下命令将名为node-1的节点更改为属于AS 64514。 1calicoctl patch node node-1 -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;bgp&quot;: &#123;“asNumber”: “64514”&#125;&#125;&#125;&#x27; 修改节点地址范围此操作建议在部署完集群后立刻进行。 默认情况下calico在集群层面分配一个10.42.0.0&#x2F;16的CIDR网段，在这基础上在单独为每个主机划分一个单独子网采用26位子网掩码对应的集群支持的节点数为2^10&#x3D;1024节点，单个子网最大支持64个POD，当单个子网对应IP消耗后，calico会重新在本机上划分一个新的子网如下，在集群对端主机可以看见对应的多个CIDR路由信息。 注意：块大小将影响节点POD的IP地址分配和路由条目数量，如果主机在一个CIDR中分配所有地址，则将为其分配一个附加CIDR。如果没有更多可用的块，则主机可以从分配给其他主机的CIDR中获取地址。为借用的地址添加了特定的路由，这会影响路由表的大小。 将块大小从默认值增加（例如，使用&#x2F;24则为每个块提供256个地址）意味着每个主机更少的块，会减少路由。但是对应的集群可容纳主机数也对应减少为2^8 从默认值减小CIDR大小（例如，使用&#x2F;28为每个块提供16个地址）意味着每个主机有更多CIDR，因此会有更多路由。 calico允许用户修改对应的IP池和集群CIDR 创建和替换步骤注意：删除Pod时，应用程序会出现暂时不可用 添加一个新的IP池。 注意：新IP池必须在同一群集CIDR中。 禁用旧的IP池（注意：禁用IP池只会阻止分配新的IP地址。它不会影响现有POD的联网） 从旧的IP池中删除Pod。 验证新的Pod是否从新的IP池中获取地址。 删除旧的IP池。 定义ippool资源 123456789apiVersion: projectcalico.org/v3kind: IPPoolmetadata: name: my-ippoolspec: blockSize: 24 cidr: 192.0.0.0/16 ipipMode: Always natOutgoing: true 修改对应的blockSize号 创建新的 1calicoctl apply -f pool.yaml 将旧的ippool禁用 1calicoctl patch ippool default-ipv4-ippool -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;disabled&quot;: “true”&#125;&#125;&#x27; 创建workload测试 根据节点标签定义对应的ippoolCalico 能够进行配置，为不同拓扑指定 IP 地址池。例如可能希望某些机架、地区、或者区域能够从同一个 IP 池中获取地址。这对于降低路由数量或者配合防火墙策略的要求会很有帮助。 给节点配置对应label 12kubectl label nodes kube-node-0 rack=0kubectl label nodes kube-node-1 rack=1 通过标签定义对应的节点IPpool 1234567891011calicoctl create -f -&lt;&lt;EOFapiVersion: projectcalico.org/v3kind: IPPoolmetadata: name: rack-0-ippoolspec: cidr: 192.168.0.0/24 ipipMode: Always natOutgoing: true nodeSelector: rack == &quot;0&quot;EOF 1234567891011calicoctl create -f -&lt;&lt;EOFapiVersion: projectcalico.org/v3kind: IPPoolmetadata: name: rack-1-ippoolspec: cidr: 192.168.1.0/24 ipipMode: Always natOutgoing: true nodeSelector: rack == &quot;1&quot;EOF 关闭SNAT默认情况下，calico访问集群外网络是通过SNAT成宿主机ip方式，在一些金融客户环境中为了能实现防火墙规则，需要直接针对POD ip进行进行规则配置，所以需要关闭natOutgoing 12kubectl edit ippool/default-ipv4-ippool 将 natOutgoing: true修改为natOutgoing: false 此时，calico网络访问集群外的ip源ip就不会snat成宿主机的ip地址。 固定POD IP固定单个ip 12345678910111213141516171819202122apiVersion: apps/v1 kind: Deploymentmetadata: name: nginx-testspec: selector: matchLabels: app: nginx replicas: 1 # tells deployment to run 1 pods matching the template template: metadata: labels: app: nginx annotations: &quot;cni.projectcalico.org/ipAddrs&quot;: &quot;[\\&quot;10.42.210.135\\&quot;]&quot; spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 固定多个ip，只能通过ippool的方式 1cat ippool1.yaml 123456789apiVersion: projectcalico.org/v3kind: IPPoolmetadata: name: pool-1spec: blockSize: 31 cidr: 10.21.0.0/31 ipipMode: Never natOutgoing: true 123456789101112131415161718192021apiVersion: apps/v1 kind: Deploymentmetadata: name: nginx-testspec: selector: matchLabels: app: nginx replicas: 1 # tells deployment to run 1 pods matching the template template: metadata: labels: app: nginx annotations: &quot;cni.projectcalico.org/ipv4pools&quot;: &quot;[\\&quot;pool-1\\&quot;]&quot; spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80","categories":[{"name":"Network","slug":"Network","permalink":"http://yoursite.com/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://yoursite.com/tags/Network/"}]},{"title":"Kubernetes安全机制(二)","slug":"kubernetes_security_2","date":"2020-10-11T13:45:59.000Z","updated":"2020-10-11T13:45:59.000Z","comments":true,"path":"2020/10/11/kubernetes_security_2/","permalink":"http://yoursite.com/2020/10/11/kubernetes_security_2/","excerpt":"","text":"1. 网络安全策略1.1 相同namespace的NetworkPolicy的隔离性创建一个namespace 1kubectl create ns policy-demo 创建pod 1kubectl create deployment --namespace=policy-demo nginx --image=nginx 创建service 1kubectl expose --namespace=policy-demo deployment nginx --port=80 测试访问(可以正常访问) 12kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/shwget -q nginx -O - 创建NetworkPolicy规则 12345678910kubectl create -f - &lt;&lt;EOFkind: NetworkPolicyapiVersion: networking.k8s.io/v1metadata: name: default-deny namespace: policy-demospec: podSelector: matchLabels: &#123;&#125;EOF 此规则表示拒绝pod连接policy-demo namespace下的pod 在次测试 123456kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/shwget -q nginx -O - wget: Download time out 可以看见被拒绝访问了 添加允许规则 12345678910111213141516kubectl create -f - &lt;&lt;EOFkind: NetworkPolicyapiVersion: networking.k8s.io/v1metadata: name: access-nginx namespace: policy-demospec: podSelector: matchLabels: app: nginx ingress: - from: - podSelector: matchLabels: run: accessEOF 这条规则意思，允许，label为 run:access的pod访问policy-demo namespace下label为run：nginx的pod 刚刚我们执行 1kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/sh pod名称为access会自动会这个deployment创建run:access这个label 在次测试,可以访问成功 123456789101112131415161718kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/sh wget -q nginx -O -&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 创建一个不是run:access的pod去测试访问 1234567kubectl run --namespace=policy-demo cannot-access --rm -ti --image busybox /bin/sh测试 wget -q nginx -O -wget：Download time out 结论：同namespace下可以使用Policy在限制pod与pod之间的访问 清空环境 1kubectl delete ns policy-demo 1.2 不同namespace pod的隔离性创建两个namespace policy-demo、policy-demo2，然后在policy-demo里面创建nginx-pod和对应的service和busybox，在policy-demo2里面创建busybox，两个namespace的busybox去访问policy-demo里面的nginx 12kubectl create ns policy-demokubectl create ns policy-demo2 1234kubectl create deployment --namespace=policy-demo nginx --image=nginxkubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/shkubectl run --namespace=policy-demo2 access --rm -ti --image busybox /bin/shkubectl expose --namespace=policy-demo deployment nginx --port=80 还没设置NetworkPolicy时分别从policy-demo和policy-demo2两个namespace去busybox去访问nginx,访问成功。 需要注意的是policy-demo2去访问要接上namespace名 1wget -q nginx.policy-demo -O - 配置NetworkPolicy 12345678910kubectl create -f - &lt;&lt;EOFkind: NetworkPolicyapiVersion: networking.k8s.io/v1metadata: name: default-deny namespace: policy-demospec: podSelector: matchLabels: &#123;&#125;EOF 配置拒绝所有Policy，此时两个namespace的busybox都不能访问了 在添加允许run:access label的pod访问Policy 12345678910111213141516kubectl create -f - &lt;&lt;EOFkind: NetworkPolicyapiVersion: networking.k8s.io/v1metadata: name: access-nginx namespace: policy-demospec: podSelector: matchLabels: app: nginx ingress: - from: - podSelector: matchLabels: run: accessEOF 此时policy-demo这个namespace下的busybox可以访问本namespace下的这个nginxpolicy-demo2这个namespace下的busybox访问不了policy-demo这个namespace下的nginx 配置允许policy-demo2下的run:access标签的POD访问policy-demo namespace下的app：nginx服务 给policy-demo2命名空间打上label 1kubectl label ns/policy-demo2 project=policy-demo2 12345678910111213141516171819kubectl create -f - &lt;&lt;EOFkind: NetworkPolicyapiVersion: networking.k8s.io/v1metadata: name: access-nginx2 namespace: policy-demospec: podSelector: matchLabels: app: nginx ingress: - from: - namespaceSelector: matchLabels: project: policy-demo2 podSelector: matchLabels: run: accessEOF 此时policy-demo2下的run:access标签的POD访问policy-demo namespace下的app：nginx服务，但其他标签不可以。 运行个run：access2标签的busybox去访问policy-demo namespace下的app：nginx服务 12345kubectl run --namespace=policy-demo2 access2 --rm -ti --image busybox /bin/sh wget -q nginx.policy-demo -O - wget：Download time out 注意： 12345678910...ingress:- from: - namespaceSelector: matchLabels: user: alice - podSelector: matchLabels: role: client... 像上面这样定义的 namespaceSelector 和 podSelector，是“或”（OR）的关系。所以说，这个 from 字段定义了两种情况，无论是 Namespace 满足条件，还是 Pod 满足条件，这个 NetworkPolicy 都会生效。 1234567891011... ingress: - from: - namespaceSelector: matchLabels: user: alice podSelector: matchLabels: role: client ... 这样定义的 namespaceSelector 和 podSelector，其实是“与”（AND）的关系。所以说，这个 from 字段只定义了一种情况，只有 Namespace 和 Pod 同时满足条件，这个 NetworkPolicy 才会生效。 清空环境 123kubectl delete ns policy-demokubectl delete ns policy-demo2 1.3 南北向流量隔离实战创建namespace 12kubectl create ns policy-demokubectl create ns policy-demo2 在policy-demo命名空间内创建两个测试POD 123kubectl run --namespace=policy-demo test-network1 --command sleep 1000000 --image=busyboxkubectl run --namespace=policy-demo test-network2 --command sleep 1000000 --image=busybox 在policy-demo2命名空间内创建一个测试pod 1kubectl run --namespace=policy-demo2 test-network3 --command sleep 1000000 --image=busybox 创建全局禁止外访规则 12345678910111213kubectl create -f - &lt;&lt;EOFapiVersion: crd.projectcalico.org/v1kind: GlobalNetworkPolicymetadata: name: global-deny-all-egressspec: selector: all() types: - Egress egress: - action: DenyEOF 单个POD外访白名单以允许policy-demo命名空间中的test-network pod为例 123456789101112131415161718192021kubectl create -f - &lt;&lt;EOFapiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: allow-testnetwork-egress-ssh namespace: policy-demospec: podSelector: matchLabels: run: test-network1 #通过Label Selector匹配到具体某一类Pod policyTypes: - Egress egress: - to: - ipBlock: cidr: 172.16.0.5/32 #白名单IP ports: - protocol: TCP port: 22 #白名单端口EOF 查看NetworkPolicy 123kubectl get networkpolicy -n policy-demoNAME POD-SELECTOR AGEallow-testnetwork-egress-ssh run=test-network1 16s 测试访问此时test-network1可以访问其他pod无法访问 Namespace外访白名单 允许policy-demo命名空间下全部POD都访问172.16.0.5的22端口 12345678910111213141516171819kubectl create -f - &lt;&lt;EOFapiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: allow-egress-to-ssh-policy-demo namespace: policy-demospec: podSelector: &#123;&#125; policyTypes: - Egress egress: - to: - ipBlock: cidr: 172.16.0.5/32 #白名单IP ports: - protocol: TCP port: 22 #白名单端口EOF 此时test-network1、test-networ2点可以访问其它命名空间的pod无法访问 全局外访白名单 1234567891011121314151617181920kubectl create -f - &lt;&lt;EOFapiVersion: crd.projectcalico.org/v1kind: GlobalNetworkPolicymetadata: name: global-allow-all-egress-to-sshspec: selector: all() types: - Egress egress: - action: Allow source: &#123;&#125; destination: nets: - 172.16.0.5 #白名单IP ports: - 22 #白名单端口 protocol: TCPEOF 配置此规则后，集群内全部pod都可以访问172.16.0.5的22端口 2. RBAC安装cfssl 1234curl -s -L -o /bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64curl -s -L -o /bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64curl -s -L -o /bin/cfssl-certinfo https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64chmod +x /bin/cfssl* 2.1. user创建创建名为test-cka的平台用户 1cd /etc/kubernetes/pki 确认kubernetes证书目录是否有以下文件 若没有ca-config.json 1234567891011121314151617181920cat &gt; ca-config.json &lt;&lt;EOF&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;kubernetes&quot;: &#123; &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ], &quot;expiry&quot;: &quot;87600h&quot; &#125; &#125; &#125;&#125;EOF ca-config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile； 创建一个创建证书签名请求 12345678910111213141516171819cat &gt; test-cka-csr.json &lt;&lt;EOF&#123; &quot;CN&quot;: &quot;test-cka&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;EOF 1cfssl gencert -ca=ca.crt -ca-key=ca.key -config=ca-config.json -profile=kubernetes test-cka-csr.json | cfssljson -bare test-cka 创建kubeconfig文件 1export KUBE_APISERVER=&quot;https://192.168.1.10:6443&quot; KUBE_APISERVER写你master节点IP地址 1kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true --server=$&#123;KUBE_APISERVER&#125; --kubeconfig=test-cka.kubeconfig 1kubectl config set-credentials test-cka --client-certificate=/etc/kubernetes/pki/test-cka.pem --client-key=/etc/kubernetes/pki/test-cka-key.pem --embed-certs=true --kubeconfig=test-cka.kubeconfig 设置context 1kubectl config set-context kubernetes --cluster=kubernetes --user=test-cka --kubeconfig=test-cka.kubeconfig 设置默认context,将集群参数和用户参数关联起来，如果配置了多个集群，可以通过集群名来切换不同的环境 1kubectl config use-context kubernetes --kubeconfig=test-cka.kubeconfig 查看kubectl的context 1234kubectl config get-contextsCURRENT NAME CLUSTER AUTHINFO NAMESPACE* kubernetes-admin@kubernetes kubernetes kubernetes-admin 用户目前还是kubernetes-admin，切换到test-cka 查看用户切换 123kubectl config get-contexts --kubeconfig=/etc/kubernetes/pki/test-cka.kubeconfigCURRENT NAME CLUSTER AUTHINFO NAMESPACE* kubernetes kubernetes test-cka 此时去get pod，get node 123kubectl get pod --kubeconfig=test-cka.kubeconfig No resources found.Error from server (Forbidden): pods is forbidden: User &quot;test-cka&quot; cannot list pods in the namespace &quot;default&quot; 123kubectl get node --kubeconfig=test-cka.kubeconfig No resources found.Error from server (Forbidden): nodes is forbidden: User &quot;test-cka&quot; cannot list nodes at the cluster scope4 2.2. Role和RoleBinding创建创建角色定义这个角色只能对default这个namespace 执行get、watch、list权限定义角色role.yaml 123456789101112kubectl create -f - &lt;&lt;EOFkind: RoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: namespace: default name: pod-readerrules:- apiGroups: [&quot;&quot;] # 空字符串&quot;&quot;表明使用core API group resources: [&quot;pods&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]EOF role_bind.yaml 1234567891011121314151617kubectl create -f - &lt;&lt;EOFkind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: read-pods namespace: defaultsubjects:- kind: User name: test-cka apiGroup: rbac.authorization.k8s.ioroleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io EOF 使用admin创建 1kubectl apply -f role.yaml --kubeconfig=/root/.kube/config 1kubectl apply -f role_bind.yaml --kubeconfig=/root/.kube/config check一下 此时用test-cka这个用户去get pod 123456kubectl get pod --kubeconfig /etc/kubernetes/pki/test-cka.kubeconfig NAME READY STATUS RESTARTS AGEhttp-app-844765cb6c-nfp7l 1/1 Running 0 10hhttp-app2-58d4c447c5-qzg99 1/1 Running 0 10htest-679b667858-pzdn2 1/1 Running 0 1h[root@master pki]# 123kubectl get node --kubeconfig=test-cka.kubeconfig No resources found.Error from server (Forbidden): nodes is forbidden: User &quot;test-cka&quot; cannot list nodes at the cluster scope get pod可以但get node不行，因为我们刚刚配置role只有pod权限 删除pod看看 12kubectl delete pod/http-app-844765cb6c-nfp7lError from server (Forbidden): pods &quot;http-app-844765cb6c-nfp7l&quot; is forbidden: User &quot;test-cka&quot; cannot delete pods in the namespace &quot;default&quot; 你会发现也删不掉，因为我们role里面配置的权限是watch和list和get 1234 kubectl get pod -n kube-system --kubeconfig=test-cka.kubeconfig No resources found.Error from server (Forbidden): pods is forbidden: User &quot;test-cka&quot;&quot; cannot list pods in the namespace &quot;kube-system&quot;[root@master pki]# 可以看见test-cka这个用户只能访问default这个namespace的pod资源，其他的namespace都访问不了，同样namespace的其他资源也访问不了 2.3. ClusterRole和ClusterRoleBinding创建cluster_role.yaml 1234567891011kubectl create -f - &lt;&lt;EOFkind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: secret-readerrules:- apiGroups: [&quot;&quot;] resources: [&quot;nodes&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]EOF 再定义一个ClusterRoleBinding，将上面的clusterrole和用户rancher绑定起来cluster_role_bind.yaml 123456789101112131415kubectl create -f - &lt;&lt;EOFkind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: read-secrets-globalsubjects:- kind: User name: test-cka apiGroup: rbac.authorization.k8s.ioroleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.ioEOF 应用 1kubectl apply -f cluster_role.yaml --kubeconfig=/root/.kube/config 1kubectl apply -f cluster_role_bind.yaml --kubeconfig=/root/.kube/config 此时去get 节点 123kubectl get node --kubeconfig /etc/kubernetes/pki/test-cka.kubeconfig NAME STATUS ROLES AGE VERSIONmaster Ready master 9d v1.15.5 3. Security ContextSecurity Context 的目的是限制容器的行为，保护操作系统和其他容器不受其影响。 Kubernetes 提供了三种配置 Security Context 的方法： Container-level Security Context：仅应用到指定的容器 Pod-level Security Context：应用到 Pod 内所有容器以及 Volume Pod Security Policies（PSP）：应用到集群内部所有 Pod 以及 Volume 3.1 Container-level Security Context1234567891011121314151617181920cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Podmetadata: name: test-container labels: app: webspec: containers: - name: test1 image: busybox command: [ &quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1h&quot; ] securityContext: runAsUser: 1000 - name: test2 image: busybox command: [ &quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1h&quot; ]EOF 1234567kubectl exec -it test-container -c test1 id uid=1000 gid=0(root)kubectl exec -it test-container -c test2 iduid=0(root) gid=0(root) groups=10(wheel) 通过securityContext将test1 container的运行user自动修改为1000了，test2仍然保持不变为root user。 3.2 Pod-level Security Context创建POD层面securityContext 1234567891011121314151617181920cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Podmetadata: name: test-container2 labels: app: web2spec: securityContext: runAsUser: 1000 containers: - name: test1 image: busybox command: [ &quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1h&quot; ] - name: test2 image: busybox command: [ &quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1h&quot; ]EOF 1234kubectl exec -it test-container2 -c test1 iduid=1000 gid=0(root)kubectl exec -it test-container2 -c test2 iduid=1000 gid=0(root) 通过securityContext将POD内 container的运行user都自动修改为1000了 3.3 Pod Security Policies（PSP）当一个pod安全策略资源被创建,它本身是一种kubernetes资源,但此时它什么都不会做.为了使用它,需要通过RBAC将user或ServiceAccount与它进行绑定 PSP 的用法和 RBAC 是紧密相关的，换句话说，应用 PSP 的基础要求是： 不同运维人员的操作账号需要互相隔离并进行单独授权。 不同命名空间，不同 ServiceAccount 也同样要纳入管理流程。 创建POD安全策略，这个策略主要限制POD使用特权模式 12345678910111213141516171819202122232425cat &lt;&lt;EOF | kubectl apply -f -apiVersion: policy/v1beta1kind: PodSecurityPolicymetadata: name: privileged annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: &#x27;*&#x27;spec: seLinux: rule: RunAsAny supplementalGroups: rule: RunAsAny runAsUser: rule: RunAsAny fsGroup: rule: RunAsAny privileged: false allowPrivilegeEscalation: true allowedCapabilities: - &#x27;*&#x27; volumes: - &#x27;*&#x27;EOF 创建一个名称空间和一个serviceaccount.使用这个serviceaccount来模拟一个非管理员用户 1kubectl create namespace psp-demo 授权psp-demo Namespace中默认ServiceAccount的使用privileged这个PodSecurityPolicy 123kubectl create rolebinding default:psp:privileged \\ --role=psp:privileged \\ --serviceaccount=psp-demo:default 创建特权模式的workload测试 1234567891011121314151617181920212223242526cat &lt;&lt;EOF | kubectl --as=system:serviceaccount:psp-demo:default applyapply -f -apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 securityContext: privileged: trueEOF from server for: “2.yaml”: pods “privileged” is forbidden: User “system:serviceaccount:psp-demo:default” cannot get resource “pods” in API group “” in the namespace “development”&#96;&#96;&#96; 提示无法创建，因为被Pod Security Policy给限制住了 去除securityContext：privileged: true字段，在次测试，可以正常创建，符合预期。 4. 手动注册节点到Kubernetes集群中先决条件1、集群中安装好docker和kubelet 使用TLS bootstrap的自动注册节点到k8s集群中 kubelet需要申请那些证书 集群启用RBAC后各组件之间的通信是基于TLS加密的，client和server需要通过证书中的CN，O来确定用户的user和group，因此client和server都需要持有有效证书 node节点的kubelet需要和master节点的apiserver通信，此时kubelet是一个client需要申请证书 node节点的kubelet启动为守住进程通过10250端口暴露自己的状态，此时kubelet是一个server需要申请证书 kubelet申请证书的步骤 1、集群产生一个低权账号用户组，并通过TOKEN进行认证2、创建ClusterRole使其具有创建证书申请CSR的权限3、给这个组添加ClusterRoleBinding，使得具有这个组的账号的kubelet具有上述权限4、给添加ClusterRoleBinding，使得controller-manager自动同意上述两个证书的下发5、调整 Controller Manager确保启动tokencleaner和bootstrapsigner（4中自动证书下发的功能）6、基于上述TOKEN生成bootstrap.kubeconfig文件，并下发给node节点7、node节点的kubelet拿着这个bootstrap.kubeconfig向master的apiserver发起CSR7、master自动同意并下发第一个证书node记得点的kubelet自动拿着第一个证书与master的apiserver通信申请第二个证书8、master自动同意并下发第二个证书node节点加入集群 创建token创建类型为”bootstrap.kubernetes.io&#x2F;token”的secret 12echo &quot;$(head -c 6 /dev/urandom | md5sum | head -c 6)&quot;.&quot;$(head -c 16 /dev/urandom | md5sum | head -c 16)&quot;485bd8.711b717a196f47f4 执行上述命令得到一个TOKEN值”485bd8.711b717a196f47f4” 这个 485bd8.711b717a196f47f4 就是生成的 Bootstrap Token，保存好 token，因为后续要用；关于这个 token 解释如下: Token 必须满足 [a-z0-9]{6}.[a-z0-9]{16} 格式；以 . 分割，前面的部分被称作 Token ID ， Token ID 并不是 “机密信息”，它可以暴露出去；相对的后面的部分称为 Token Secret ，它应该是保密的。 基于token创建secret将下列secret对应的字段修改为刚刚申请的token值 12345678910111213141516171819202122232425262728cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Secretmetadata: # Name MUST be of form &quot;bootstrap-token-&lt;token id&gt;&quot; name: bootstrap-token-xxx namespace: kube-system# Type MUST be &#x27;bootstrap.kubernetes.io/token&#x27;type: bootstrap.kubernetes.io/tokenstringData: # Human readable description. Optional. description: &quot;The default bootstrap token generated by &#x27;kubeadm init&#x27;.&quot; # Token ID and secret. Required. token-id: token-secret: # Allowed usages. usage-bootstrap-authentication: &quot;true&quot; usage-bootstrap-signing: &quot;true&quot; # Extra groups to authenticate the token as. Must start with &quot;system:bootstrappers:&quot; auth-extra-groups: system:bootstrappers:cka:default-node-tokenEOF 配置RBAC12345678910111213141516171819202122232425262728293031323334353637cat &lt;&lt;EOF | kubectl apply -f -kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: system:certificates.k8s.io:certificatesigningrequests:nodeclientrules:- apiGroups: [&quot;certificates.k8s.io&quot;] resources: [&quot;certificatesigningrequests/nodeclient&quot;] verbs: [&quot;create&quot;]---# A ClusterRole which instructs the CSR approver to approve a node renewing its# own client credentials.kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclientrules:- apiGroups: [&quot;certificates.k8s.io&quot;] resources: [&quot;certificatesigningrequests/selfnodeclient&quot;] verbs: [&quot;create&quot;]---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: system:node-bootstrapperrules:- apiGroups: - certificates.k8s.io resources: - certificatesigningrequests verbs: - create - get - list - watchEOF 配置ClusterRoleBinding 1234567891011121314151617181920212223242526272829cat &lt;&lt;EOF | kubectl apply -f -apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: cka:kubelet-bootstraproleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:node-bootstrappersubjects:- apiGroup: rbac.authorization.k8s.io kind: Group name: system:bootstrappers:cka:default-node-token---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: cka:node-autoapprove-bootstraproleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:nodeclientsubjects:- apiGroup: rbac.authorization.k8s.io kind: Group name: system:bootstrappers:cka:default-node-token EOF 确认controller-manager是否开启bootstrapsigner 12cat /etc/kubernetes/manifests/kube-controller-manager.yaml|grep bootstrapsigner - --controllers=*,bootstrapsigner,tokencleaner 生成bootstrap.kubeconfig文件我这里的apiserver的地址为”https://172.16.0.7:6443“ 设置集群参数 12345kubectl config set-cluster cka \\ --certificate-authority=/etc/kubernetes/pki/ca.crt \\ --embed-certs=true \\ --server=https://172.16.0.7:6443 \\ --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf 设置客户端认证参数，替换token 123kubectl config set-credentials system:bootstrap:485bd8 \\ --token=485bd8.711b717a196f47f4 \\ --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf 设置上下文，替换token id 1234kubectl config set-context default \\ --cluster=cka \\ --user=system:bootstrap:485bd8 \\ --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf 设置默认上下文 1kubectl config use-context default --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf 将其拷贝到node节点 12scp /etc/kubernetes/bootstrap-kubelet.conf rke-node2:/etc/kubernetes/bootstrap-kubelet.conf 拷贝config.yaml到node节点 1scp /var/lib/kubelet/config.yaml rke-node2:/var/lib/kubelet/ 配置node节点的kubelet创建证书目录 1mkdir /etc/kubernetes/pki/ 将master节点ca证书拷贝过来 1234scp /etc/kubernetes/pki/ca.crt rke-node2:/etc/kubernetes/pki/scp /etc/kubernetes/pki/ca.key rke-node2:/etc/kubernetes/pki/ 修改Address为实际上节点ip 12vim /etc/kubernetes/kubelet.config 123456789101112kind: KubeletConfigurationapiVersion: kubelet.config.k8s.io/v1beta1address: 172.16.0.5port: 10250readOnlyPort: 10255cgroupDriver: cgroupfsclusterDNS: [&quot;10.96.0.10&quot;]clusterDomain: cluster.local.failSwapOn: falseauthentication: anonymous: enabled: true 编辑kubelet.service修改hostname-override为实际节点ip,pause镜像地址按实际镜像仓库地址修改。 1vim /etc/systemd/system/kubelet.service 123456789101112131415161718192021[Unit]Description=Kubernetes KubeletAfter=docker.serviceRequires=docker.service[Service]#EnvironmentFile=/k8s/kubernetes/cfg/kubeletExecStart=/usr/bin/kubelet \\--logtostderr=true \\--v=4 \\--hostname-override=172.16.0.5 \\--kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf \\--config=/etc/kubernetes/kubelet.config \\--cert-dir=/etc/kubernetes/pki \\--pod-infra-container-image=k8s.gcr.io/pause:3.1Restart=on-failureKillMode=process[Install]WantedBy=multi-user.target 启动服务 123systemctl daemon-reload systemctl restart kubelet.servicesystemctl status kubelet.service 检查 1234kubectl get nodeNAME STATUS ROLES AGE VERSIONrke-node1 Ready master 37m v1.19.1rke-node2 Ready &lt;none&gt; 115s v1.19.1 5. 使用临时容器调试现有POD截止到目前k8s1.18版本，k8s已经支持四种类型的container：标准容器，sidecar容器，init容器，ephemeral容器。 什么是ephemeral容器临时容器与其他容器的不同之处在于，它们缺少对资源或执行的保证，并且永远不会自动重启，因此不适用于构建应用程序。临时容器使用与常规容器相同的 ContainerSpec 段进行描述，但许多字段是不相容且不允许的。 临时容器没有端口配置，因此像 ports，livenessProbe，readinessProbe 这样的字段是不允许的。Pod 资源分配是不可变的，因此 resources 配置是不允许的。有关允许字段的完整列表， ephemeral容器的用途 当由于容器崩溃或容器镜像不包含调试实用程序而导致 kubectl exec 无用时，临时容器对于交互式故障排查很有用。 尤其是，distroless 镜像能够使得部署最小的容器镜像，从而减少攻击面并减少故障和漏洞的暴露。由于 distroless 镜像不包含 shell 或任何的调试工具，因此很难单独使用 kubectl exec 命令进行故障排查。 使用临时容器时，启用进程命名空间共享很有帮助，可以查看其他容器中的进程。 使用ephemeral容器ephemeral容器目前还是个alpha的功能所以需要在Kubernetes的api-server、scheduler、controller-manager和节点的kubelet开启对应的参数 修改/etc/kubernetes/manifests/kube-apiserver.yaml、kube-controller-manager.yaml、kube-scheduler.yaml添加组件的参数--feature-gates=EphemeralContainers=true 修改kubelet参数 &#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;config.yaml 底部的以下几行： 12featureGates: EphemeralContainers: true 保存文件并运行以下命令： 1systemctl restart kubelet 创建个nginx用于模拟正常pod，打开shareProcessNamespace，让同个pod内的不同container可以查看共同进程空间 12345678910111213cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Podmetadata: name: nginxspec: shareProcessNamespace: true containers: - name: nginx image: nginx stdin: true tty: trueEOF 目前使用ephemeral容器可以直接通过kubectl进行，在Kubernetes 1.18版本加入了kubectl debug的特性，方便用户进行trouble shooting,目前还是alpha特性 使用kubectl创建ephemeral容器 ，附加个busybox到刚刚创建的nginx容器中 1kubectl alpha debug nginx -it --image=busybox 因为打开了shareProcessNamespace所以在同一个pod中的不同container可以看见对应互相的进程。 1234567891011Defaulting debug container name to debugger-mbzbp.If you don&#x27;t see a command prompt, try pressing enter./ # ps auxPID USER TIME COMMAND 1 root 0:00 /pause 6 root 0:00 nginx: master process nginx -g daemon off; 33 101 0:00 nginx: worker process 46 root 0:00 sh 51 root 0:00 ps aux","categories":[{"name":"安全","slug":"安全","permalink":"http://yoursite.com/categories/%E5%AE%89%E5%85%A8/"}],"tags":[{"name":"安全","slug":"安全","permalink":"http://yoursite.com/tags/%E5%AE%89%E5%85%A8/"}]},{"title":"使用ECK部署ElasticSearch到Kubernetes中","slug":"logging_1","date":"2020-09-25T15:27:44.000Z","updated":"2020-09-25T15:27:43.000Z","comments":true,"path":"2020/09/25/logging_1/","permalink":"http://yoursite.com/2020/09/25/logging_1/","excerpt":"","text":"概述通过operator对有状态应用可以快速进行部署，将ElasticSearch集群快速部署在Kubernetes集群上，并且可以通过operator快速对集群进行管理和扩容。 环境信息 软件 版本 Rancher 2.3.8-ent Kubernetes 1.17.6 OS centos7.6 ElasticSearch 7.9 ECK-Opera 1.2 架构规划 详细部署调整宿主机操作系统内核参数限制一个进程可以拥有的VMA(虚拟内存区域)的数量要超过262144，不然elasticsearch会报max virtual memory areas vm.max_map_count [65535] is too low, increase to at least [262144] 1echo &quot;vm.max_map_count=655360&quot;&gt;&gt;/etc/sysctl.conf 执行生效 1sysctl -p 部署Local-pv使用Rancher开源的local-path-provisioner驱动将节点本地存储用于存储ElasticSearch的数据 https://github.com/rancher/local-path-provisioner https://www.bladewan.com/2018/05/19/kubernetes_storage/ 部署es-operator1kubectl apply -f https://download.elastic.co/downloads/eck/1.2.1/all-in-one.yaml 检查es-operator状态 1kubectl -n elastic-system logs -f statefulset.apps/elastic-operator 123kubectl get pod/elastic-operator-0 -n elastic-systemNAME READY STATUS RESTARTS AGEelastic-operator-0 1/1 Running 0 2d23h 部署ElasticSearch12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273cat &lt;&lt;EOF | kubectl apply -f -apiVersion: elasticsearch.k8s.elastic.co/v1kind: Elasticsearchmetadata: name: quickstartspec: version: 7.9.0 nodeSets: - name: master-nodes count: 3 podTemplate: spec: containers: - name: elasticsearch env: - name: ES_JAVA_OPTS value: -Xms2g -Xmx2g resources: requests: memory: 4Gi cpu: 0.5 limits: memory: 4Gi cpu: 2 image: docker.elastic.co/elasticsearch/elasticsearch:7.9.0 config: node.master: true node.data: false node.ingest: false volumeClaimTemplates: - metadata: name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: local-path - name: data-nodes count: 3 podTemplate: spec: containers: - name: elasticsearch env: - name: ES_JAVA_OPTS value: -Xms2g -Xmx2g resources: requests: memory: 4Gi cpu: 0.5 limits: memory: 4Gi cpu: 2 image: docker.elastic.co/elasticsearch/elasticsearch:7.9.0 config: node.master: false node.data: true node.ingest: false volumeClaimTemplates: - metadata: name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: local-pathEOF 注意点：1、修改实际的Request和limit,Xms和Xmx设置为实际limit的一半。2、storageClassName修改为实际的存储名称。3、镜像地址修改为实际镜像地址。 查看部署状态 123kubectl get elasticsearchNAME HEALTH NODES VERSION PHASE AGEquickstart green 6 7.9.0 Ready 41m 查看部署的POD状态信息，部署了3个Master+3个Data节点 12345678kubectl get pod NAME READY STATUS RESTARTS AGEquickstart-es-data-nodes-0 1/1 Running 0 10mquickstart-es-data-nodes-1 1/1 Running 0 12mquickstart-es-data-nodes-2 1/1 Running 0 14mquickstart-es-master-nodes-0 1/1 Running 0 5m57squickstart-es-master-nodes-1 1/1 Running 0 7m23squickstart-es-master-nodes-2 1/1 Running 0 8m36s 默认开启了x-pack插件有认证系统，帐户默认为elastic，密码保存在secret密钥中 获取访问密钥 12PASSWORD=$(kubectl get secret quickstart-es-elastic-user -o go-template=&#x27;&#123;&#123;.data.elastic | base64decode&#125;&#125;&#x27;) 使用NodePort方式对外暴露 1kubectl patch svc quickstart-es-http -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&#x27; 访问ElasticSearch 查看集群状态 1234curl --insecure -u &quot;elastic:$PASSWORD&quot; -k https://node_ip:NodePort/_cat/health?v epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1599476283 10:58:03 quickstart green 6 3 0 0 0 0 0 0 - 100.0% 查看集群节点信息 123456789curl --insecure -u &quot;elastic:$PASSWORD&quot; -k https://172.16.1.6:30620/_cat/nodes?vip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name10.42.1.92 27 58 16 2.97 2.15 1.66 dlrt - quickstart-es-data-nodes-110.42.4.15 8 58 15 2.00 1.03 0.95 lmr - quickstart-es-master-nodes-010.42.0.41 5 58 15 1.88 2.82 2.92 dlrt - quickstart-es-data-nodes-010.42.2.183 35 58 6 0.12 0.09 0.09 lmr * quickstart-es-master-nodes-210.42.5.8 8 58 12 0.85 0.45 0.34 dlrt - quickstart-es-data-nodes-210.42.3.20 56 58 8 0.05 0.18 0.38 lmr - quickstart-es-master-nodes-1 角色说明： d (data node)i (ingest node)m (master-eligible node)l (machine learning node)v (voting-only node)t (transform node)r (remote cluster client node)‘-‘ (coordinating node only). master处带*表示为3个master节点中选举出来的leader节点。 部署Kibana1234567891011121314151617181920212223cat &lt;&lt;EOF | kubectl apply -f -apiVersion: kibana.k8s.elastic.co/v1kind: Kibanametadata: name: quickstartspec: version: 7.9.1 count: 1 elasticsearchRef: name: quickstart podTemplate: spec: containers: - name: kibana resources: requests: memory: 1Gi cpu: 0.5 limits: memory: 2Gi cpu: 2 image: docker.elastic.co/kibana/kibana:7.9.1EOF 检查kibana部署状态 1234kubectl get kibanaNAME HEALTH NODES VERSION AGEquickstart green 7.9.1 6m21s 使用NodePort对外暴露 1kubectl patch svc quickstart-kb-http -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&#x27; 获取密码，默认情况下kibana的密码也是存储在secret中 1kubectl get secret quickstart-es-elastic-user -o=jsonpath=&#x27;&#123;.data.elastic&#125;&#x27; | base64 --decode; echo 通过https://host_ip:nodeport访问kibana 帐号为elastic，密码为刚刚通过secret获取的密码 部署Fluentd部署FluentdFluentd是使用c+Ruby语言开发的一个日志收集端，目前也是CNCF官方项目，拥有非常丰富的插件生态。 优点： 1、插件生态丰富，可以使用目前社区比较丰富的插件避免重复造轮子。 2、插件使用Ruby语言自定义编写方便 缺点： 1、受限于Ruby的GIL无法实现真正高性能 部署Fluentd 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106---apiVersion: v1kind: ServiceAccountmetadata: name: fluentd namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: name: fluentd namespace: kube-systemrules:- apiGroups: - &quot;&quot; resources: - pods - namespaces verbs: - get - list - watch---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: fluentdroleRef: kind: ClusterRole name: fluentd apiGroup: rbac.authorization.k8s.iosubjects:- kind: ServiceAccount name: fluentd namespace: kube-system---apiVersion: apps/v1kind: DaemonSetmetadata: name: fluentd namespace: kube-system labels: k8s-app: fluentd-logging version: v1spec: selector: matchLabels: k8s-app: fluentd-logging version: v1 template: metadata: labels: k8s-app: fluentd-logging version: v1 spec: serviceAccount: fluentd serviceAccountName: fluentd tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch env: - name: FLUENT_ELASTICSEARCH_HOST value: &quot;10.43.7.26&quot; - name: FLUENT_ELASTICSEARCH_PORT value: &quot;9200&quot; - name: FLUENT_ELASTICSEARCH_SCHEME value: &quot;https&quot; # Option to configure elasticsearch plugin with self signed certs # ================================================================ - name: FLUENT_ELASTICSEARCH_SSL_VERIFY value: &quot;false&quot; # Option to configure elasticsearch plugin with tls # ================================================================ - name: FLUENT_ELASTICSEARCH_SSL_VERSION value: &quot;TLSv1_2&quot; # X-Pack Authentication # ===================== - name: FLUENT_ELASTICSEARCH_USER value: &quot;elastic&quot; - name: FLUENT_ELASTICSEARCH_PASSWORD value: &quot;g38SlQ408i5SI6E24SoqdE3N&quot; resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers 注意将以下几个环境变量改为实际得ElasticSearch地址和配置帐号 123456789FLUENT_ELASTICSEARCH_HOST #修改为实际的ElasticSearch地址FLUENT_ELASTICSEARCH_PORT#修改为实际的ElasticSearch的端口FLUENT_ELASTICSEARCH_SSL_VERIFY#因为采用的是自签名证书，所以这里关闭SSL认证FLUENT_ELASTICSEARCH_USER#连接的x-pack的帐号FLUENT_ELASTICSEARCH_PASSWORD#连接的x-pack的密码 采用Daemonset方式进行部署，部署后会将主机的&#x2F;var&#x2F;log目录映射到了容器内，在fluentd内配置文件对应的/fluentd/etc/kubernetes.conf文件中，tail插件对&#x2F;var&#x2F;log&#x2F;container&#x2F;目录的*.log文件进行收集，此目录存储的是对应的容器标准输出的日志文件的软链接。 配置kibana，选择对应的Index，查看","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"CICD_DEMO完整版示例e","slug":"cicd_demo","date":"2020-09-17T13:45:59.000Z","updated":"2020-09-17T13:45:59.000Z","comments":true,"path":"2020/09/17/cicd_demo/","permalink":"http://yoursite.com/2020/09/17/cicd_demo/","excerpt":"","text":"环境准备部署gitlab1docker run --detach --hostname 10.8.242.28 --publish 443:443 --publish 80:80 --publish 1022:22 --name gitlab --restart always --volume /srv/gitlab/config:/etc/gitlab --volume /srv/gitlab/logs:/var/log/gitlab --volume /srv/gitlab/data:/var/opt/gitlab gitlab/gitlab-ce:12.10.3-ce.0 替换hostname为实际节点外网IP 部署HarborHarbor部署与管理部署前先修改docker编辑docker 1234vim /etc/docker/daemon.json&#123; &quot;insecure-registries&quot; : [&quot;0.0.0.0/0&quot;]&#125; 重启docker 1systemctl restart docker 安装docker-compose 12curl -L https://github.com/docker/compose/releases/download/1.24.1/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-compose 下载harbor 1https://github.com/goharbor/harbor/releases/download/v1.10.2/harbor-online-installer-v1.10.2.tgz 配置harbo.yaml 1hostname: 172.31.48.86 //修改为实际节点IP 屏蔽https配置 安装harbor 1./install.sh --with-clair 1234567891011121314docker-compose ps Name Command State Ports ---------------------------------------------------------------------------------------------clair /docker-entrypoint.sh Up (healthy) 6060/tcp, 6061/tcp harbor-core /harbor/start.sh Up (healthy) harbor-db /entrypoint.sh postgres Up (healthy) 5432/tcp harbor-jobservice /harbor/start.sh Up harbor-log /bin/sh -c /usr/local/bin/ ... Up (healthy) 127.0.0.1:1514-&gt;10514/tcpharbor-portal nginx -g daemon off; Up (healthy) 80/tcp nginx nginx -g daemon off; Up (healthy) 0.0.0.0:80-&gt;80/tcp redis docker-entrypoint.sh redis ... Up 6379/tcp registry /entrypoint.sh /etc/regist ... Up (healthy) 5000/tcp registryctl /harbor/start.sh Up (healthy) 访问http://node_ip admin&#x2F;Harbor12345 Drone配置使用Rancher应用商店已经提供了Drone的部署安装及配置，通过Rancher应用商店即可将Drone部署并配置对接 在gitlab上创建一个项目名为go-server，编写个应用server.go 123456789101112131415161718package mainimport ( &quot;fmt&quot; &quot;log&quot; &quot;net/http&quot;)func hello(w http.ResponseWriter, r *http.Request) &#123; fmt.Fprintf(w, &quot;Hello World&quot;)&#125;func main() &#123; http.HandleFunc(&quot;/&quot;, hello) if err := http.ListenAndServe(&quot;:8080&quot;, nil); err != nil &#123; log.Fatal(err) &#125;&#125; 创建DockerfileDockerfile 1234FROM golangWORKDIR /goADD server /goCMD [&quot;./server&quot;] 1、配置gitlab外部认证 回调接口填写其中一个节点的http://ip/login如下 保存好对应的ApplicationID和secret 部署Drone 12345678910111213141516171819docker run \\ --volume=/var/run/docker.sock:/var/run/docker.sock \\ --volume=/var/lib/drone:/data \\ --env=DRONE_LOGS_DEBUG=true \\ --env=DRONE_GIT_ALWAYS_AUTH=false \\ --env=DRONE_GITLAB_SERVER=http://114.215.25.58 \\ --env=DRONE_GITLAB_CLIENT_ID=d6272993ac02c3bb4069d73bf0ff8dabeaff47c0739ae27d1a23e8b80e33faa5 \\ --env=DRONE_GITLAB_CLIENT_SECRET=01f454fe0a55256a974d420b8ca023df6efc80b33d8a917dd16138b152b73253 \\ --env=DRONE_RPC_SECRET=12345678\\ --env=DRONE_RUNNER_CAPACITY=3 \\ --env=DRONE_SERVER_HOST=114.215.130.36\\ --env=DRONE_SERVER_PROTO=http \\ --env=DRONE_TLS_AUTOCERT=false \\ --publish=80:80 \\ --publish=443:443 \\ --restart=always \\ --detach=true \\ --name=drone \\ drone/drone:1 参数说明： 参数 说明 DRONE_RPC_SECRET runner连接server的凭证 DRONE_RUNNER_CAPACITY server调用runner的并发数 DRONE_SERVER_HOST server的ip DRONE_SERVER_PROTO server对外提供的协议可选http和https 部署Drone-runner采用Docker的方式 123456789101112docker run -d \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -e DRONE_RPC_PROTO=http \\ -e DRONE_RPC_HOST=172.31.227.147 \\ -e DRONE_RPC_SECRET=12345678 \\ -e DRONE_RUNNER_CAPACITY=3 \\ -e DRONE_RUNNER_NAME=$&#123;HOSTNAME&#125; \\ -p 3000:3000 \\ --restart always \\ --name runner \\ drone/drone-runner-docker:1 检查日志查看是否部署成功 在gitlab go-server项目创建名为.drone文件填入以下内容 123456789101112131415161718192021222324252627282930kind: pipelinetype: dockername: buildsteps:- name: build-code image: golang:alpine pull: if-not-exists # always never commands: - pwd - ls - go build server.go - ls- name: build-image image: plugins/docker settings: repo: 172.31.227.151/go-server/go-server registry: 172.31.227.151 use_cache: true username: from_secret: registry_username password: from_secret: registry_password tags: $&#123;DRONE_BUILD_NUMBER&#125; insecure: true mirror: https://yefnfc9c.mirror.aliyuncs.com/trigger: branch: - master event: - push 连接镜像仓库的凭证环境变量可在Drone对应的项目中定义好 参数说明： 参数 说明 DRONE_RPC_PROTO runner连接server协议 DRONE_RPC_HOST server的地址 DRONE_RPC_SECRET 连接server的凭证 commit代码后会自动运行CI Jenkins配置使用应用概述 整体CICD流程 push code to trigger Gitlab webhook compiling the code and build image test upload image to harbor Generate a new YML file deploy&#x2F;upgrade 先决条件1、clone front-end项目 1git clone https://gitee.com/wanshaoyuan/front-end.git 2、并上传到内部gitlab上 3、在harbor中创建front-ent项目 4、在jenkins中安装好gitlab、docker、Kubernetes插件 5、部署sock-shap 1git clone https://gitee.com/wanshaoyuan/microservices-demo.git 12kubectl create namespace sock-shopkubectl apply -f microservices-demo/deploy/kubernetes/manifests/. 访问 1http://node_ip:30001 Jenkins安装 1docker run -itd --name jenkins -u root -p 50000:50000 -p 8080:8080 -v /var/jenkins_home:/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock jenkins/jenkins:lts 节点安装git 1yum install git 获取jenkins密码 1docker logs jenkins |grep password 安装推荐的Jenkins插件 Jenkins对接Gitlab安装gitlab插件 Gitlab中申请AccessToken 将申请成功的token保存好 配置Jenkins对接gitlab 测试连接 测试 在gitlab中创建项目上传个test.txt文件通过Jenkins读取 配置连接gitlab私有项目的密钥可以用ssh密钥也可以使用账号密码 构建 去cat这个文件输出内容 执行立即构建 输出结果 Jenkins对接Kubernetes安装Kubernetes插件 Jenkins-CI对接Kubernetes配置系统管理—&gt;系统设置—&gt;新增一个云 配置Jenkins URL，这里我们没有配置api-server地址和证书key，连接kubernetes，所以默认会去读取放在JENKINS_HOME的.kube&#x2F;目录的kubeconfig文件，用于连接集群。我这里是通过安装包的方式安装的Jenkins HOME在&#x2F;var&#x2F;lib&#x2F;jenkins&#x2F;目录，如果是通过容器方式启动，将kubeconfig文件直接放~&#x2F;.kube&#x2F;目录。 从RancherUI上复制配置文件 保存到Jenkins主机的config文件中复制粘贴到Jenkins容器内的~&#x2F;.kube&#x2F;config文件中 12docker exec -it jenkins mkdir /root/.kube/docker cp config jenkins:/root/.kube/config 在编译集群对应项目下创建secret 在jenkins系统设置中创建一个Kubernetes云 演示效果 修改前，先展示前端。(https://github.com/microservices-demo/front-end)这里我们演示修改前端logo，然后git push到代码仓库，触发Jenkins进行cicd操作，修改front-end&#x2F;public&#x2F;navbar.html的&lt;img src&#x3D;”img&#x2F;logo.png”为新的图片地址，然后将代码提交触发cicd重新打开前端看见logo变了。 创建cicd Pipeline生成连接gitlab的凭证Grove语句 添加Harbor凭证 创建流水线任务 添加以下Pipeline任务 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273pipeline &#123; environment &#123; registry = &quot;172.31.227.151&quot; project_name = &quot;/front-end&quot; app_name = &quot;/front-end&quot; registryCredential = &#x27;harbor&#x27; &#125; agent &#123; kubernetes &#123; defaultContainer &#x27;wanshaoyuan/jnlp-slave:3.27-1-alpine&#x27; yaml &quot;&quot;&quot;apiVersion: v1kind: Podmetadata: labels: some-label: some-label-valuespec: containers: - name: jnlp image: &#x27;wanshaoyuan/jnlp-slave:3.27-1-alpine&#x27; args: [&#x27;\\$(JENKINS_SECRET)&#x27;, &#x27;\\$(JENKINS_NAME)&#x27;] - name : docker image: wanshaoyuan/docker:19.03.2 tty: true volumeMounts: - name: repo-docker-sock mountPath: /var/run/docker.sock - name: kubectl image: wanshaoyuan/jenkins-tools:v1.0 tty: true volumeMounts: - mountPath: &quot;/root/.kube&quot; name: &quot;volume-0&quot; readOnly: false volumes: - name: &quot;volume-0&quot; secret: secretName: &quot;kubeconfig&quot; - name: repo-docker-sock hostPath: path: /var/run/docker.sock&quot;&quot;&quot; &#125; &#125; stages &#123; stage(&#x27;clone code&#x27;) &#123; steps &#123; git credentialsId: &#x27;9371d980-6bc2-47aa-a47f-04cf78871e7a&#x27;, url: &#x27;http://114.215.25.58/root/front-end.git&#x27; &#125; &#125; stage(&#x27;image build &#x27;) &#123; steps &#123; container(&#x27;docker&#x27;) &#123; script &#123; docker.withRegistry( &#x27;http://172.31.227.151&#x27;, registryCredential ) &#123; def dockerImage=docker.build registry + project_name + app_name +&quot;:$BUILD_NUMBER&quot; dockerImage.push() &#125; &#125; &#125; &#125; &#125; stage(&#x27;deploy app &#x27;) &#123; steps &#123; container(&#x27;kubectl&#x27;) &#123; sh &#x27;sed -i &quot;s/image: .*front-end:.*/image: 172.31.227.151\\\\/front-end\\\\/front-end:$BUILD_ID/g&quot; front-end-dep.yaml&#x27; sh &#x27;kubectl apply -f front-end-dep.yaml&#x27; &#125; &#125; &#125; &#125;&#125; 定义环境变量：镜像仓库地址、项目名称、应用名称、连接镜像仓库的认证信息。 定义jnlp-agent POD所启动的container，包括jnlp容器，用于根jenkins进行通信，maven容器用于代码编译、docker 容器，用于容器镜像编译，kubectl容器用于连接对应的kubernetes集群用于应用创建和更新。 定义四个阶段，每个阶段定义对应的步骤，调用对应的容器实现功能（clone代码阶段、代码编译阶段、容器镜像构建阶段、应用部署阶段） 每个阶段调用对应的容器实现，如代码编译阶段调用编译容器进行、容器镜像构建阶段调用docker镜像映射宿主机socket进行。 配置gitlab自动触发Jenkins CICD 编辑jenkins项目配置使用Build when a change is pushed to GitLab. GitLab CI Service URL: http://191.8.2.112:12000/project/test-go-dev 当代码有更新的时候触发，通过GitLab CI 打开gitlab，配置webhook回掉地址，填入jenkins对应项目地址gitlab对应的项目设置页 默认触发器是push事件时，触发webhook，需要注意的是因为我们这里的Jenkins配置了用户名和密码所以url需要在原来基础上添加用户名和密码其他不变格式为http://username:password@192.168.1.4:1080/project/dashboard-build 默认测试webhook调用 默认一个代码commit的请求下载logo放置 1wget https://rancher.com/img/brand-guidelines/assets/logos/png/cow/rancher-logo-cow-blue.png 修改前端logo，然后git push到代码仓库，触发Jenkins进行cicd操作，修改front-end&#x2F;public&#x2F;navbar.html的&lt;img src&#x3D;”img&#x2F;logo.png”为新的图片地址，然后将代码提交触发cicd重新打开前端看见logo变了。 Rancher-Pipeline配置使用概述Rancher2.x Pipeline支持在发布阶段直接将应用发布到应用商店，部署阶段直接从应用商店部署应用，以下为操作步骤 先决条件 提前创建好存放统一chart的目录目录结构如下：.&#x2F;charts&#x2F;app_name&#x2F;version 将app_name和Version替换为实际应用名和版本号 CICD到应用商店流程12345678graph LRA[源码库]-- &quot;Jenkins clone源码完成其他CI环节&quot; --&gt; B[clone catalog 模板并通过Jenkins修改模板]B -- &quot;git push&quot; --&gt; C[push到统一存放catalog目录]C-- &quot;Rancher-server自动同步更新&quot; --&gt; c[UI显示新版本可更新] 应用chart制作继续使用go-server项目的例子 创建应用源码目录创建专属chart目录 目录结构如下 创建目录 1mkdir /go-server/go-server-chart 在目录go-server-chartt内创建以下文件和文件夹 1234567891011go-server-chart│ ├── 0│ │ ├── Chart.yaml│ │ ├── questions.yml│ │ ├── templates│ │ │ ├── _helpers.tpl│ │ │ ├── go-server-dep.yaml│ │ │ └── go-server-svc.yaml│ │ └── values.yaml│ └── README.md 0表示版本文件夹，如果要发布新版本只需要创建个1或者另外文件夹，然后修改Chart.yaml文件里面的版本号。 README.md 内定义对应用的说明Chart.yaml 12345apiVersion: v1description: go-servername: go-serverversion: $&#123;CICD_EXECUTION_SEQUENCE&#125;icon: http://ohx02qrb8.bkt.clouddn.com/micheling.png 详细环境变量参考 1https://rancher.com/docs/rancher/v2.x/en/k8s-in-rancher/pipelines/ 定义软件的描述、名字、版本、图标地址 questions.yml 123456789101112131415161718192021222324252627282930questions:- variable: replicaCount default: &quot;1&quot; description: &quot;Replica count&quot; type: string required: true label: Replicas- variable: imageName default: &quot;172.31.227.151/go-server/go-server&quot; description: &quot;image name&quot; type: string required: true label: image_name- variable: imageTage default: &quot;latest&quot; description: &quot;image version&quot; type: string required: true label: image_version- variable: nodeport default: &quot;30000&quot; description: &quot;NodePort port number(to set explicitly, choose port between 30000-32767)&quot; type: string required: true label: nginx Service NodePort number 定义变量，主要与ui上对应variable： 对应template里面的.Values.xxx值default：当用户没有设置自定义值时的默认值description： ui上用户自定义值的描述type: 自定义值的数据类型required： 是否必填label：标签 values.yaml: values.yaml 则提供了这些配置参数的默认值。 templates :放置实际要执行的应用的yaml文件 go-server-dep.yaml 1234567891011121314151617181920212223242526272829apiVersion: apps/v1kind: Deploymentmetadata: labels: cattle.io/creator: norman workload.user.cattle.io/workloadselector: deployment-default-test name: test namespace: defaultspec: replicas: &#123;&#123; .Values.replicaCount &#125;&#125; selector: matchLabels: workload.user.cattle.io/workloadselector: deployment-default-test template: metadata: labels: workload.user.cattle.io/workloadselector: deployment-default-test spec: containers: - image: &#123;&#123; .Values.imageName &#125;&#125;:&#123;&#123; .Values.imageTage &#125;&#125; imagePullPolicy: Always name: test ports: - containerPort: 8080 name: 8080tcp01 protocol: TCP stdin: true tty: true go-server-svc.yaml 1234567891011121314151617apiVersion: v1kind: Servicemetadata: labels: cattle.io/creator: norman name: test-nodeport namespace: defaultspec: ports: - name: 8080tcp01 nodePort: &#123;&#123; .Values.nodeport &#125;&#125; port: 8080 protocol: TCP targetPort: 8080 selector: workload.user.cattle.io/workloadselector: deployment-default-test type: NodePort _helpers.tpl 定义一些系统通用的继承变量一般不需要，直接用默认就好。 1234567891011121314151617&#123;&#123;/* vim: set filetype=mustache: */&#125;&#125;&#123;&#123;/*Expand the name of the chart.*/&#125;&#125;&#123;&#123;- define &quot;name&quot; -&#125;&#125;&#123;&#123;- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix &quot;-&quot; -&#125;&#125;&#123;&#123;- end -&#125;&#125;&#123;&#123;/*Create a default fully qualified app name.We truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec).*/&#125;&#125;&#123;&#123;- define &quot;fullname&quot; -&#125;&#125;&#123;&#123;- $name := default .Chart.Name .Values.nameOverride -&#125;&#125;&#123;&#123;- printf &quot;%s-%s&quot; .Release.Name $name | trunc 63 | trimSuffix &quot;-&quot; -&#125;&#125;&#123;&#123;- end -&#125;&#125; 同时创建实际的catalog目录 1mkdir test-chart 继续按上面方式创建一个go-server的chart应用，需要修改Chart.yaml文件这里就不能在用变量了，因为使用添加私有应用商店时是读取不出变量的，在源码库内的chart.yaml用变量是因为需要动态更新。 Chart.yaml 12345apiVersion: v1description: go-servername: go-serverversion: v1.0 icon: http://ohx02qrb8.bkt.clouddn.com/micheling.png 然后push到代码仓库的chart项目 Pipeline配置创建secret用于push时的代码仓库的帐号密码 如果push是ssh方式这里的secret就放置公钥 USERNAME和PASSWORD两个key 创建镜像仓库凭证用于上传镜像 选择代码库创建Pipeline 创建代码编译步骤 创建编译上传镜像步骤 创建发布应用模板步骤 类型选择Publish Catalog Template Chart Folder：填写源码库内的chart的相对路径 Catalog Template Version：这里填写Rancher Pipeline环境变量根在代码库内chart目录Chart.yaml Version一致 Catalog Template Name：为你的catalog应用的名称，如果你是生成一个新的catalog。统一存放catalog目录没有这个应用则可以根据实际需求取名字，如果是在原有基础上更新则有已有的catalog，则用原目录名 运行Pipeline catalog自动更新 对应代码目录下 .rancher-pipeline.yml文件 1234567891011121314151617181920212223242526272829303132333435stages:- name: cod_build steps: - runScriptConfig: image: golang:latest shellScript: go build server.go- name: image_build steps: - publishImageConfig: dockerfilePath: ./Dockerfile buildContext: . tag: go-server/go-server:$&#123;CICD_GIT_COMMIT&#125; pushRemote: true registry: 172.31.227.151 env: PLUGIN_INSECURE: &quot;true&quot; PLUGIN_MIRROR: https://yefnfc9c.mirror.aliyuncs.com- name: update_catalog steps: - publishCatalogConfig: path: ./go-server-chart/0 catalogTemplate: go-server version: $&#123;CICD_EXECUTION_SEQUENCE&#125; gitUrl: http://114.215.25.58/root/chart.git gitBranch: master gitAuthor: root gitEmail: shaoyua@rancher.com envFrom: - sourceName: gitlab sourceKey: USERNAME targetKey: USERNAME - sourceName: gitlab sourceKey: PASSWORD targetKey: PASSWORDtimeout: 60","categories":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/categories/CI-CD/"}],"tags":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/tags/CI-CD/"}]},{"title":"Kubernetes资源模型概述","slug":"kubernetes_request_limit","date":"2020-08-24T13:45:59.000Z","updated":"2020-08-24T13:45:59.000Z","comments":true,"path":"2020/08/24/kubernetes_request_limit/","permalink":"http://yoursite.com/2020/08/24/kubernetes_request_limit/","excerpt":"","text":"为业务配置Request和Limit容器本身共享宿主机的资源，通过配置资源限制，能够更大程度利用宿主机的硬件资源，必免因为部分应用的问题导致影响其他运行的业务，在Kubernetes中资源限制方式主要通过Request和Limit。其中Request表示应用使用资源的最小值，Limit表示应用使用资源的最大值。资源管理的对象主要分为两种类型，可压缩资源和不可压缩资源，其中可压缩资源一般都以CPU资源为例，不可压缩资源主要对应的是内存。 配置Request 1234567891011121314151617181920212223242526cat &lt;&lt;EOF | kubectl apply -f - apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment-requestspec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 resources: requests: memory: &quot;500m&quot; cpu: &quot;100m&quot;EOF 配置Limit 1234567891011121314151617181920212223242526cat &lt;&lt;EOF | kubectl apply -f -apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment-limitspec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 resources: limits: memory: &quot;1Gi&quot; cpu: &quot;500m&quot;EOF CPU和RAM单位CPU资源以cpus为单位。允许小数值。单位为millicores ，你可以用后缀m来表示mili。例如100m， 意思是使用单个核心的1&#x2F;100。1core&#x3D;1000m，若节点是4cpu对应的为4*1000&#x3D;4000m。 1000m (milicores) &#x3D; 1 core100m (milicores) &#x3D; 0.1 core RAM资源以bytes为单位。你可以将RAM表示为纯整数或具有这些后缀之一的定点整数：E, P, T, G, M, K, Ei, Pi, Ti, Gi, Mi, Ki。例如，以下代表大约相同的数值： k8s中内存M和Mi的区别M&#x3D;10001000Mi&#x3D;10241024 CPU Request的实现原理request则主要用以声明最小的CPU核数。一方面则体现在设置cpushare上。比如request.cpu&#x3D;3，则cpushare&#x3D;1024*3&#x3D;3072。 以刚刚部署的nginx-deployment-request和nginx-deployment-limit为例查看对应的操作系统上的cgroup 查看对应POD所在节点 12345kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-deployment-limit-6f555f68df-pt45x 1/1 Running 0 18s 10.42.1.41 rke-node3 &lt;none&gt; &lt;none&gt;nginx-deployment-request-76d9f4d69d-7nh6j 1/1 Running 0 46s 10.42.1.40 rke-node3 &lt;none&gt; &lt;none&gt;test-7fd4b897b8-592fl 1/1 Running 0 64m 10.42.0.30 rke-node2 &lt;none&gt; &lt;none&gt; ssh连接到节点，通过docker查看对应的cgroup目录 123docker ps | grep request|grep -v pause | cut -d&#x27; &#x27; -f1fa82dcca40b4 123docker inspect fa82dcca40b4 --format &#x27;&#123;&#123;.HostConfig.CgroupParent&#125;&#125;&#x27;/kubepods/burstable/pod51932f96-0e7f-4528-92bd-c01023e97abd 查看对应cgroup目录 1234cat /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pod51932f96-0e7f-4528-92bd-c01023e97abd/cpu.shares 102 比如nginx-deployment-request设置request.cpu&#x3D;100m，则102 &#x3D; (request.cpu * 1024) &#x2F;1000 通过cpu.shares实现在cpu繁忙情况下仍然能给优先级高的应用分配到对应的CPU计算量。 Memory、cpu Limit的实现原理cpu limit主要用以声明使用的最大的CPU核数。通过设置cfs_quota_us和cfs_period_us。比如limits.cpu&#x3D;3，则cfs_quota_us&#x3D;300000。cfs_period_us值一般都使用默认的100000在cgroup，cpu子系统中通过cfs_quota_us&#x2F;cfs_period_us严格限制cpu的的使用量 继续以部署的nginx-deployment-limit为例讲解，找到对应的cgroup目录 cpu.cfs_quota_us参数为50000 cfs_period_us参数为 100000 cfs_quota_us&#x2F;cfs_period_us&#x3D;0.5 对应的500m memory limit通过对应的cgroup子目录的memory.limit_in_bytes字段进行限制。 到达资源限制后的响应措施当pod 内存超过limit时，会被oom。当cpu超过limit时，不会被kill，但是会限制不超过limit值。 内存资源限制 123456789101112131415161718192021222324252627282930313233cat &lt;&lt;EOF | kubectl apply -f -apiVersion: apps/v1kind: Deploymentmetadata: name: deployment-oomspec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: polinux/stress command: - stress - --vm - &quot;1&quot; - --vm-bytes - 250M - --vm-hang - &quot;1&quot; ports: - containerPort: 80 resources: limits: memory: &quot;100Mi&quot;EOF 部署一个压测容器，压测时会分配250M内存，但实际pod的内存limit为100Mi。 一达到设置的阈值，便会被OOMKILL掉。 123kubectl get pod NAME READY STATUS RESTARTS AGEdeployment-oom-75d47b57c4-sst77 0/1 OOMKilled 0 33s 将limits值调整到1000Mi查看可以看见内存使用值稳定在250MB左右，因将limit值调大了，所以它不会被杀死。 123kubectl top podNAME CPU(cores) MEMORY(bytes) deployment-oom-76cd9448cc-gxprn 230m 251Mi cpu使用率超过limit时，不会被kill，但是会限制不超过limit值，影响实际业务性能。 可以看见cpu使用在快速增长 配置cpu limit限制cpu使用为1000 查看资源监控 123456789101112131415161718192021222324252627cat &lt;&lt;EOF | kubectl apply -f -apiVersion: apps/v1kind: Deploymentmetadata: name: test-cpu namespace: defaultspec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx args: - -cpus - &quot;2&quot; image: vish/stress resources: limits: cpu: &quot;1&quot;EOF 查看监控，cpu usage为cpu使用率，当使用率达到限制值1000后，便不在增长，保持平稳。 123kubectl top pod NAME CPU(cores) MEMORY(bytes) test-cpu-6c945c9469-bnprz 999m 1Mi 配置的最佳实践POD对应多个不同的QOS级别，不同的QOS级别在资源抢占时对应不同的策略 配置方式最佳实践：Guaranteed 级别1、 pod 中的每一个 container 都必须包含cpu和内存资源的 limit、request 信息，并且这两个值必须相等 Burstable 级别1、资源申请信息不满足 Guaranteed 级别的要求• 2、pod 中至少有一个 container 指定了 cpu 或者 memory 的 request。 BestEffort级别1、 pod 中任何一个container都没有配置cpu或者 memory的request，limit信息 所以建议对于一些关键型业务为避免因为资源不够被杀，建议配置好cpu和内存的Request和limit并使他们相等。 配置值的最佳实际实际上应用容器化改造上线时，切不可以当时申请虚拟机的cpu和内存值做为参考值，因为这部分值是偏大的，容易造成资源浪费，实际上我们可以用两种方式获取业务的实际资源值。 方式一：通过是旧的业务，我们可以通过历史的监控信息看见对应的cpu和内存在不同业务量下小的消耗值，这个值可以做为我们实际的配置值。 方式二：新业务可以通过性能压测，模拟业务请求量得到对应业务量下的资源消耗，这样往往得到的值是准确的。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"k3s集群部署和使用cilium网络插件","slug":"k3s_install_cilium","date":"2020-08-16T13:45:59.000Z","updated":"2020-08-16T13:45:59.000Z","comments":true,"path":"2020/08/16/k3s_install_cilium/","permalink":"http://yoursite.com/2020/08/16/k3s_install_cilium/","excerpt":"","text":"概述cilium都近期比较火的一个网络插件，它通过将ebpf技术引入网络插件中用于满足容器工作负载的新可伸缩性，安全性和可见性要求。 前置条件ebpf依赖内核技术，所以需要保证以下内核版本1 、kernel版本 &gt;&#x3D; 4.9.17 软件 版本 k3s v1.18.6+k3s1 cilium 1.8 部署k3s采用两节点部署k3s，节点一角色为master+worker，节点二角色为worker 部署master 1curl -sfL https://docs.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh -s - --write-kubeconfig ~/.kube/config --write-kubeconfig-mode 666 --docker --no-flannel 获取连接master的token信息 1cat /var/lib/rancher/k3s/server/node-token 部署worker 1curl -sfL https://docs.rancher.cn/k3s/k3s-install.sh| INSTALL_K3S_EXEC=&#x27;--docker --no-flannel&#x27; INSTALL_K3S_MIRROR=cn K3S_URL=https://master-ip:6443 K3S_TOKEN=xxx sh - 查看节点部署 1234 kubectl get nodeNAME STATUS ROLES AGE VERSIONrke-k3s-node1.novalocal NotReady master 24m v1.18.6+k3s1rke-k3s-node2 NotReady &lt;none&gt; 10s v1.18.6+k3s1 此时因为没有部署网络插件所以节点状态为NotReady状态 所有节点执行挂载bpf文件系统 1sudo mount bpffs -t bpf /sys/fs/bpf 方式一：快速部署cilium网络插件12kubectl create -f https://raw.githubusercontent.com/cilium/cilium/v1.8/install/kubernetes/quick-install.yaml 查看组件状态 12345678910111213kubectl get pod -n kube-systemNAME READY STATUS RESTARTS AGEcilium-p2c2p 1/1 Running 0 28mcilium-operator-868c78f7b5-bvgrl 1/1 Running 0 28mcoredns-8655855d6-9nvn9 1/1 Running 0 27mlocal-path-provisioner-6d59f47c7-k2454 1/1 Running 0 21mmetrics-server-7566d596c8-k4dzz 1/1 Running 0 21mhelm-install-traefik-cmlzv 0/1 Completed 3 21msvclb-traefik-wz685 2/2 Running 0 20mtraefik-758cd5fc85-t2mzc 1/1 Running 0 8m49scilium-operator-868c78f7b5-tjm27 1/1 Running 0 2m49ssvclb-traefik-n2b8m 2/2 Running 0 71scilium-97j69 1/1 Running 0 91s 测试网络连通性 12kubectl apply -f https://raw.githubusercontent.com/cilium/cilium/v1.8/examples/kubernetes/connectivity-check/connectivity-check.yaml 主要测试二层网络联通性和三层网络联通性 ，状态都为running表示正常cilium默认跨主机网络通信是通过VXLAN实现的 1234567891011121314151617kubectl get podNAME READY STATUS RESTARTS AGEpod-to-a-6cf58894b7-lh9pz 1/1 Running 0 5m11spod-to-b-intra-node-77b485d996-6bm8d 1/1 Running 0 5m10shost-to-b-multi-node-clusterip-c7557d4f8-89lj6 1/1 Running 0 5m11secho-b-659766fb56-c54fm 1/1 Running 0 5m11secho-a-5f555bbc8b-fck6k 1/1 Running 0 5m12spod-to-a-l3-denied-cnp-74b9566cc7-ct4kg 1/1 Running 0 5m11secho-b-host-65d7db76d8-kv277 1/1 Running 0 5m11shost-to-b-multi-node-headless-5dfcdf9b76-wsz7m 1/1 Running 0 5m11spod-to-b-intra-node-nodeport-56975db6c7-sqwq8 1/1 Running 0 5m11spod-to-b-multi-node-clusterip-75f5c78f68-4ljhc 1/1 Running 0 5m10spod-to-b-multi-node-headless-5df88f9bd4-gfjws 1/1 Running 0 5m10spod-to-b-multi-node-nodeport-55b9769455-dv6pv 1/1 Running 0 5m10spod-to-a-allowed-cnp-5898f7d8c9-bjch7 1/1 Running 0 5m11spod-to-a-external-1111-5779fb7cb9-qf8mq 0/1 Running 4 5m9spod-to-external-fqdn-allow-google-cnp-74466b4c6f-sfchn 0/1 Running 3 5m9s 删除测试用例 1kubectl delete -f https://raw.githubusercontent.com/cilium/cilium/v1.8/examples/kubernetes/connectivity-check/connectivity-check.yaml 方式二：Helm部署方式cilium网络插件因为cilium1.8开始hubble功能集成到Cilium的helm-chart中了，所以在1.8版本cilium需要使用hubble需要在chart中启用，部署方式分两种 方式一：Cilium + cilium-etcd-operator 方式二：cilium+自带etcd 因为k3s默认使用SQLite3做为后端数据库所以这里使用方式一方式进行安装 hubble主要用于下载helm客户端 1wget https://get.helm.sh/helm-v3.3.0-linux-amd64.tar.gz 添加cilium的repo 1helm repo add cilium https://helm.cilium.io/ 部署cilium和hubble 123456789helm install cilium cilium/cilium --version 1.8.2 \\ --namespace kube-system \\ --set global.etcd.enabled=true \\ --set global.etcd.managed=true \\ --set global.hubble.enabled=true \\ --set global.hubble.listenAddress=&quot;:4244&quot; \\ --set global.hubble.metrics.enabled=&quot;&#123;dns,drop,tcp,flow,port-distribution,icmp,http&#125;&quot; \\ --set global.hubble.relay.enabled=true \\ --set global.hubble.ui.enabled=true 使用NodePort方式对外暴露 1kubectl patch svc hubble-ui -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&#x27; -n kube-system 访问hubble-UI 通过hubble可以非常清晰看到各个应用的互相连接关系和访问信息。 总结：整体部署和使用下来功能是非常强大，目前看社区都有往ebpf方向走，包括向calico目前也有个ebpf的模式，通过ebpf可以提升整体网络的性能和细粒化网络流量的控制和可视化，后续将以这个为方向深入理解。但带来的问题就是对于排错对比与calico-bgp或flannel-vxlan会更加复杂，在部署后出现过一次大面积瘫痪情况，没有找到是k3s问题和cilium问题，后面将集群和cilium重新部署就好了。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"Kubernetes安全机制(一)","slug":"kubernetes_security_1","date":"2020-07-09T13:45:59.000Z","updated":"2020-07-09T13:45:59.000Z","comments":true,"path":"2020/07/09/kubernetes_security_1/","permalink":"http://yoursite.com/2020/07/09/kubernetes_security_1/","excerpt":"","text":"常见安全攻击面描述安全攻击面图片来源网络 外部访问攻击：核心组件非安全端口对外暴露，以及连接集群关键凭证泄漏。 平台组件攻击： 利用平台组件docker、kube-apiserver、etcd、kubelet 非安全端口对外暴露，以及集群组件间连接安全，以及组件的安全漏洞进行攻击。 POD层面攻击：利用运行在集群内的恶意POD进行攻击。 安全矩阵图片来源网络 安全问题案例图片来源网络 2018年黑客入侵了特斯拉在亚马逊上的Kubernetes容器集群。由于该集群控制台未设置密码保护，黑客便得以在一个Kubernetes pod中获取到访问凭证，然后据此访问其网络存储桶S3，通过S3获取到了一些敏感数据，比如遥测技术，并且还在特斯拉的Kubernetes pod中进行挖矿。 安全攻击点—DockerDaemon公网暴露 Docker本身是C&#x2F;S架构，Docker-client连接Docker-Daemon对docker进行操作。 Client连接Daemon有以下常见方式：1、tcp:&#x2F;&#x2F;host:port2、unix:&#x2F;&#x2F;path_to_socket3、fd:&#x2F;&#x2F;socketfd。 若通过TCP方式将dockerDaemon对外暴露，攻击者可以通过Api接口接管本地Docker服务。 安全攻击点—包含恶意程序的镜像图片来源网络 图片来源网络 安全攻击点—组件安全漏洞利用风险图片来源网络 利用平台docker、k8s组件漏洞进行攻击如：Docker Runc逃逸漏洞此漏洞允许以root身份运行的容器以特权用户身份在主机上执行任意代码。这意味着容器可能会破坏Docker主机（覆盖Runc CLI），而所需要的只是能够使用root来运行容器。攻击者可以使用受感染的Docker镜像或对未受感染的正在运行的容器运行exec命令。 漏洞影响:运行那些从不受信任的来源处获取到的镜像，那么其主机很有可能被攻击者所全面接收。 安全攻击点—容器特权模式逃逸图片来源网络 特权模式容器允许允许容器内的root拥有外部物理机的root权限， ，可以直接获取宿主机设备文件访问权限。 可以通过mount命令将宿主机磁盘设备挂载进容器内部，获取对整个宿主机的文件读写权限。 安全攻击点—利用挂载目录逃逸图片来源网络 将宿主机部分核心目录以写权限挂载进容器内，攻击者进入容器内可以对这些目录进行修改，如利用crontab定义一些执行脚本，来达到破坏目的。 安全攻击点—利用Linux Capabilities逃逸图片来源网络 Linux Capabilities将root用户关联的特权拆分为一个个单元，每个单元可以独立启用和禁用，来达到细粒化权限控制。 容器通过cgroup和Namespace实现资源隔离同时也支持通过Capabilities进行细粒化权限管理，docker和k8s都是通过白名单方式添加Capabilities。如添加CAP_SYS_PTRACE特性在配合pid&#x3D;host参数可以实现在容器内使用strance进行宿主机的进程追踪。 安全攻击点—利用Linux Capabilities逃逸图片来源网络 图片来源网络 安全攻击点—凭证泄漏图片来源网络 公有云上平台凭证泄漏，导致可以直接获取访问Kubernetes平台的连接信息，对Kubernetes进行控制和操作。 防范机制平台安全框架认证(Authenticating)是服务端对客户端请求的合法性鉴别。 授权(Authorization)是对访问资源的授权，当一个请求经过认证后，需要访问某一个资源（比如创建一个pod），授权检查都会通过访问策略比较该请求上下文的属性，（比如用户，资源和Namespace），根据授权规则判定该资源（比如某namespace下的pod）是否是该客户可访问的。 准入(Admission Control)机制是一种请求拦截机制，用于对请求的资源对象进行校验，修改。如Istio中的自动在对应namespace的POD中自动注入sidecar。 Kubernetes组件安全机制图片来源网络 1、全部组件都需要连接Api-server，默认情况下使用 TLS 对集群中其他组件到 Api-server 通信进行加密。 2、组件通过对应的ServiceAccount运行，通过RBAC进行授权限制。 3、限制etcd访问，集群中只有api-server能够访问ETCD，并且对于etcd的访问采用TLS客户端证书相互认证。 4、kubelet做为apiserver获取节点pod信息的入口，启用身份验证和授权。 认证机制 X509 Client Certs Static Token File Bootstrap Tokens Static Password File Service Account Tokens OpenID Connect Tokens X509 Client Certs图片来源网络 双向数字证书认证，HTTPS证书认证，是基于CA根证书签名的双向数字证书认证方式，是所有认证方式中最严格的认证。也是Kubernetes默认认证方式，组件之间以及外部客户端认证方式。 Api-Server相关的三个启动参数： client-ca-file: 指定CA根证书文件为&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.pem tls-private-key-file: 指定ApiServer私钥文件为&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;apiserver-key.pem tls-cert-file：指定ApiServer证书文件为&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;apiserver.pem Service Account Tokens图片来源网络 用于控制集群内部POD对象访问API-server. Kubernetes会为每个namespace自动创建一个默认的service account资源，并命名为”default”。 创建POD时没有明确指定ServiceAccount，将会以default的ServiceAccount挂载到POD中。 授权机制ABAC: ABAC(基于属性的访问控制)：ABAC控制权限的粒度非常细，非常灵活；属性通常来说分为四类：用户属性、环境属性、操作属性、对象属性；ABAC通过动态的控制一组属性是否满足条件来进行授权判断。优点：权限控制的粒度非常细，非常灵活。缺点：太过于复杂。例如规则：“禁止所有学生在上课时间进出校门”这条规则，其中，“学生”是用户的角色属性，“上课时间”是环境属性，“进出”是操作属性，而“校门”就是对象属性了。 RBAC:引入角色概念，通过将权限和角色关联，来实现用户根权限的解耦，角色可以看做是权限的一个集合。通过权限与角色关联，角色与用户的关联可以实现用户和角色多对多的对应关系。 RBAC的模型 Who是否可以对What进行How的访问操作kubernetes之前的版本都是使用ABAC做为权限管理控制，在kubernetes1.3发布alpha版本RBAC,在kubernetes1.6版本RBAC提升为Beta版，kubernetes1.8版本正式提升为GA版，为什么用RBAC取代ABAC是因为在kubernetes中ABAC过于复杂，不好管理，难以理解，RBAC相对功能和可管理性来说更加合适kubernetes。所以说没有更好，只有更合适。 RBAC（Role base access control)： k8s上发展过程：1.3 Alpha，1.6 Beta 1.8 GA 概念：引入角色概念，通过将权限和角色关联，来实现用户根权限的解耦，角色可以看做是权限的一个集合。通过权限与角色关联，角色与用户的关联可以实现用户和角色多对多的对应关系。 优点 :将权限与角色关联，用户关联角色，根传统用户关联权限相比这样权限控制更加灵活 。 控制粒度更细，可以对单个资源对象的做权限的分配。缺点：复杂度较高，有一定学习成本。 角色：Kubernetes中的RBAC定义了两类角色 Role：role是一组权限的集合， 例如一个角色可以包含读取 Pod 的权限和列出 Pod 的权限。Role 只能用来给某个特定 namespace 中的资源作鉴权 。 ClusterRole：clusterrole根role一样也是一组权限的集合，cluster范围的Role，但clusterrole的有效范围是整个集群的权限和一些集群级别的资源如：集群的node节点、非资源类型endpoint、跨所有命名空间的命名空间范围资源（例如pod，需要运行命令kubectl get pods –all-namespaces来查询集群中所有的pod）。 Role：Resource：表示控制的资源对象，可以是pod，当然也可以是namespace级别的其他资源如：Service，secrets，pvc，configmap等。 Verbs：表示控制的权限操作，因为安全认证是通过api-server来完成的，api-server支持的10种权限操作为”get”, “ list “, “watch “,”create”,”update”, “patch”, “delete”,”deletecollection”,”redirect”,”proxy”] ClusterRole： Resource：ClusterRole定义的资源对象是集群层面的或需要进行跨多个namespace的资源控制 Verbs：根role一样 有了角色后，需要将这些角色根对应对象绑定起来，这样才能完成一次权限的分配，根Role和ClusterRole是对应的角色的棒定也分两种RoleBinding和ClusterRoleBinding 。 RoleBinding &amp; ClusterRoleBinding ： Subjects：关联的对象类型，在k8s主要分为三种user，group，ServiceAccount roleRef：为bind的对象，如果你是role，kind就写role是Clusterrolekind就写ClusterRole，name为对象的名字。 默认的Role和ClusterRole api-service会默认创建一些role和rolebinding 和clusterrole和clusterrolebind，他们默认都是以system开头的，以system开头的都是为kubernetes系统使用而保留。 clusterrole里面cluster-admin、admin、edit、view这几个clusterrole，直接参考https://v1-8.docs.kubernetes.io/docs/admin/authorization/rbac/ cluster-admin、admin、edit、view这几个角色是不包含system前缀的，他们是面向用户的角色，可以直接使用rolebind直接使用。 User和ServiceAccount Kubernetes 集群中包含两类用户：一类是由 Kubernetes 管理的 service account，另一类是普通用户。普通用户假定为由外部独立服务管理。管理员分发私钥，用户存储（如 Keystone 或 Google 帐户），甚至包含用户名和密码列表的文件。在这方面，Kubernetes 没有代表普通用户帐户的对象。无法通过 API 调用的方式向集群中添加普通用户。 Service Account是由 Kubernetes API 管理的帐户。是用于POD中的进程访问API-Service的，为POD中的进程提供一个身份标识。 为什么用ServiceAccount？通过ServiceAccount可以更精确控制POD内应用访问集群权限。 User和ServiceAccount kubernetes在每个namespace内都有一个默认的service account,当在创建pod或其他资源时，没有手工指定Service account时会默认使用namespace里面这个default这个Service account 一般来说为了安全起见，建议一个应用使用一个独立的Service account运行，这样RoleBindings就不会无意中授予其他应用程序的权限，如果需要这样使用的话在部署yaml文件中添加 准入控制机制—Admission Control 准入(Admission Control)机制是一种请求拦截机制，用于对请求的资源对象进行校验。 包含两个控制器：变更（Mutating）准入控制：修改请求的对象验证（Validating）准入控制：验证请求的对象 当请求到达 API Server 的时候首先执行变更准入控制，然后再执行验证准入控制。 默认的控制策略 OPA介绍 开源的通用策略引擎，可以统一进行请求策略控制。OPA通过一种高级声明性语言（Rego）来定义策略配置规则，实现对请求的控制。 在Kubernetes中，admission在创建，更新和删除操作期间强制执行对象的语义验证，需要重新编译或重新配置Kubernetes API服务器。但使用OPA，直接可以在Kubernetes对象上实施自定义策略 配合强大的声明式策略语言Rego，直接通过K8S对象配置规则。 Service可以是以下任意： Kubernetes API server API gateway Custom service CI&#x2F;CD pipeline OPA介绍—Gatekeeper Gatekeeper的创建是为了让用户能够通过配置（而不是代码）定制许可控制。 通过CRD和模板定义， 可分享，可重复使用， 参数化配置 集群当前状态的可知和反馈，并非仅仅操作或下发策略的人员知晓更方便与其他系统集成，如CI&#x2F;CD POD安全策略 POD直接运行在宿主机上可以映射宿主机的端口和使用宿主机网络，共享宿主机的内核，这样情况下会一定的安全风险，可以通过配置POD安全策略限制并约束POD的一些行为。PodSecurityPolicy对象定义了一组条件，指示 Pod 必须按系统所能接受的设定运行。 网络安全策略 概述： Kubernetes的一个重要特性就是要把不同node节点的pod（container）连接起来，无视物理节点的限制。但是在某些应用环境中，比如公有云，不同租户的pod不应该互通，这个时候就需要网络隔离。Kubernetes提供了NetworkPolicy，支持按Namespace级别的网络隔离。Network Policy提供了基于策略的网络控制，用于隔离应用并减少攻击面。它使用标签选择器模拟传统的分段网络，并通过策略控制它们之间的流量以及来自外部的流量 先决条件kubernetes NetworkPolicy是通过调用对应的网络组件的policy功能来实现的，像weave和calico、cilium自带policy组件来实现功能，但flannel本身是没有这个的，但flannel可以通过直接使用calico的calico-controler组件来实现这个功能 网络策略是用来约束pod组与彼此和其他网络端点通信的规范通过标签来筛选pods定义Ingress与Egress流量与规则应用场景：1、多租户网络隔离、不同级别业务系统策略控制2、内外部业务系统请求细粒化控制限制 镜像安全扫描 镜像仓库集成镜像漏洞扫描，镜像仓库内镜像进行CVE漏洞扫描 配置扫描策略，上传后立刻扫描，超过定义的高风险漏洞就禁止pull镜像。 定期同步外网漏洞仓库 镜像签名 通过在容器镜像中可以对镜像进行加密签名用来保证镜像件来源和镜像内容防篡改。 ca证书进行镜像的签发密钥和验证。 最小镜像原则，降低攻击面 最小镜像原则，使用体积小的base镜像降低攻击面Google发布的Distroless镜像仅包含应用程序及其运行时依赖项。不包含程序包管理器，shell和在标准Linux发行版中找到的任何其他程序。 Distroless提供java、python、Nodejs、dotnet等开发语言的基础镜像 优点：1、降低镜像体积、减少磁盘空间占用。2、只安装应用所依赖的组件，无其他组件降低攻击面。 缺点：1、没有shell和一些调试工具，出故障时无法方便调试，但可以通过临时容器进行调试 集群CIS安全扫描 互联网安全中心（CIS）发布的全面的Kubernetes Benchmark 建立Kubernetes 的安全配置规范 1、集群组件配置检查（Api-server、controller-manager、scheduler、etcd）是否符合安全配置参数。 2、集群组件配置文件权限检查。 3、策略检查（pod安全策略、网络安全策略检查）。 使用Rootless Container提升容器的安全性 Docker Daemon因为需要创建Namespace和挂载分层文件系统等所以一直以来是以root用户来运行的。这也导致了有Docker访问权限的用户可以通过连接Docker Engine获取root权限，而且可以绕开系统的审计能力对系统进行攻击。 Rootless Container实现使用非特权用户运行docker engine。 Docker 19.03中发布了“Rootless Container”特性 实现原理：1、通过user Namespace实现docker demon运行的用户重映射 2、利用用户态的网络SLiRP通过一个TAP设备连接到非特权用户名空间，为容器提供外网连接能力 集群及运行安全 – 实践https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/ 容器和应用安全 – 实践限制对Kubernetes API的访问 对所有API通讯使用TLS加密 API认证 API授权限制对Kubelet的访问 关闭匿名访问 限制运行时Workload和用户具备的能力 限制资源使用 限制特权容器 限制容器加载特定内核模块 限制容器网络访问 限制云元数据API访问 限制Pod能够使用的node保护集群组件不受侵害 限制对etcd的访问 启用审计日志 限制对无关特性或alpha&#x2F;beta特性的使用 定期更新基础设施credential 评估第三方集成安全性 加密Secrets资源 接收安全更新和漏洞报告 限制资源使用 配置Namespace配额 配置容器默认资源限制镜像安全 最小镜像原则，减少攻击面 基础镜像规划化避免乱使用 镜像内使用非特权用户 镜像漏洞扫描，定义风险阈值 代码安全 通过TLS访问，对传输内容加密 限制通信端口范围 确保第三方依赖安全性 静态代码分析 动态探测攻击","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"POD间网络不通的排查思路","slug":"pod_network","date":"2020-06-26T13:45:59.000Z","updated":"2020-06-26T13:45:59.000Z","comments":true,"path":"2020/06/26/pod_network/","permalink":"http://yoursite.com/2020/06/26/pod_network/","excerpt":"","text":"POD间网络不通VXLAN网络模式canal网络插件测试以下场景 通过Rancher UI检查system项目下canal网络插件是否运行正常。 在对应项目下以daemonset方式启动多个POD，分布到不同节点，POD内进行ping测试。 POD内直接ping对端宿主机的flannel.1网卡。 在POD所在宿主机上ping对应POD ip。 主机上udp 8472端口是否监听 macvlan网络插件 端口单vlan通过场景验证找2台机器worker1和worker2，在2台机器中，分别给需要支持macvlan的网卡手动添加一个IP，假如网卡为eth0。 添加IP 在worker1中执行如下命令 1ip address add 192.168.1.2/24 dev eth0 在worker2中执行如下命令 1ip address add 192.168.1.3/24 dev eth0 检查IP是否添加成功 以worker1为例，添加前 12345678910111213ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc mq state UP group default qlen 1000 link/ether 0a:06:7c:64:b7:f6 brd ff:ff:ff:ff:ff:ff inet 172.31.15.100/20 brd 172.31.15.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::806:7cff:fe64:b7f6/64 scope link valid_lft forever preferred_lft forever 添加后 12345678910111213141: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc mq state UP group default qlen 1000 link/ether 0a:06:7c:64:b7:f6 brd ff:ff:ff:ff:ff:ff inet 172.31.15.100/20 brd 172.31.15.255 scope global eth0 valid_lft forever preferred_lft forever inet 192.168.1.2/24 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::806:7cff:fe64:b7f6/64 scope link valid_lft forever preferred_lft forever 检查路由规则 给网卡添加IP时，会自动添加路由规则，可通过route -n查看。 添加前 12345route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 172.31.0.1 0.0.0.0 UG 0 0 0 eth0172.31.0.0 0.0.0.0 255.255.240.0 U 0 0 0 eth0 添加后 123456route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 172.31.0.1 0.0.0.0 UG 0 0 0 eth0172.31.0.0 0.0.0.0 255.255.240.0 U 0 0 0 eth0192.168.1.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 跨主机互ping验证 只要跨主机在网卡上添加ip且互相可以ping通，说明iaas层就没问题，此时worker1和worker2上的eth0网卡详情如下 worker1 eth0 123456782: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc mq state UP group default qlen 1000 link/ether 0a:06:7c:64:b7:f6 brd ff:ff:ff:ff:ff:ff inet 172.31.15.100/20 brd 172.31.15.255 scope global eth0 valid_lft forever preferred_lft forever inet 192.168.1.2/24 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::806:7cff:fe64:b7f6/64 scope link valid_lft forever preferred_lft forever worker2 eth0 123456782: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc mq state UP group default qlen 1000 link/ether 0a:16:e4:de:a5:68 brd ff:ff:ff:ff:ff:ff inet 172.31.1.41/20 brd 172.31.15.255 scope global eth0 valid_lft forever preferred_lft forever inet 192.168.1.3/24 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::816:e4ff:fede:a568/64 scope link valid_lft forever preferred_lft forever 从worker1上ping worker2机器使用macvlan ip进行互ping 端口多vlan通过场景验证验证方式与单端口一致，唯一区别在于多vlan情况下，交换机端口配置的是trunk模式，允许多个vlan通过，也就是交换机本身不会对端口打vlan-tag了，需要通过软件层面给对应的网卡的数据包打上vlan-tag，否则不能通信，在linux操作系统中可以通过建立子vlan口的方式将IP地址配置在子vlan口上，对应的数据包经过子vlan口后会自动打上vlan-tag 实现使用以下方式： 在需要测试的两台worker节点上按下述方法创建好子vlan口，然后在子vlan口上配置ip，互相ping测试。 测试跨子网通信，则需要提前配置好路由，手动添加静态路由由即可。 安装vconfig命令 1yum install -y vlan 给eth0设置vlan 404 12vconfig add eth0 404 //添加eth0.404子接口 1ip address add 192.168.1.101 dev eth0.404 //设置IP 测试路由 1ip route add 192.168.1.0/24 dev eth0.404 &#x2F;&#x2F;设置路由,192.168.1.0网段的从eth0.404发出 1ifconfig eth0.404 up 删除方式 1vconfig rem eth0.404 实际生产环境中建议配置使用多网卡做Bond方式，保证网卡的高可用，一般推荐模式为Bond1模式即为主备模式，需要提前验证Bond配置是否生效。 验证 查看bond状态 123456789101112131415161718cat /proc/net/bonding/bond0 Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)Bonding Mode: fault-tolerance (active-backup)Primary Slave: NoneCurrently Active Slave: eth1 #当前主接口MII Status: upMII Polling Interval (ms): 100Up Delay (ms): 0Down Delay (ms): 0Slave Interface: eth1MII Status: upSpeed: Unknown //openstack虚拟化网卡默认不显示速率Duplex: Unknown //openstack虚拟化网卡默认不显示双工Link Failure Count: 0 Permanent HW addr: fa:16:3e:47:c7:a0Slave queue ID: 0 测试方法 Down掉当前主接口，进行测试，看是否会进行网卡切换，网络是否会中断。 模拟容器内macvlan网络测试 12345678910111213141516171819202122232425262728293031# 创建namespaceip netns add ns1ip netns add ns2# 基于物理网卡创建macvlan网卡ip link add mv1 link eno1 type macvlan mode bridgeip link add mv2 link eno1 type macvlan mode bridge# 将网卡挂载到对应namespace中(若trunk模式，需要配置子vlan口，将子vlan口加入) ip link set mv1 netns ns1ip link set mv2 netns ns2# 启动网卡 ip netns exec ns1 ip link set dev mv1 upip netns exec ns2 ip link set dev mv2 up# 设置IP地址 ip netns exec ns1 ifconfig mv1 192.168.1.50/24 upip netns exec ns2 ifconfig mv2 192.168.1.60/24 up# 查看端口详细情况 ip netns exec ns1 ip aip netns exec ns2 ip a# ping测试ip netns exec ns1 ping -c 4 192.168.1.60# 清除ip netns del ns1ip netns del ns2","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"Longhorn部署安装和管理","slug":"longhorn_install","date":"2020-06-16T13:45:59.000Z","updated":"2020-06-16T13:45:59.000Z","comments":true,"path":"2020/06/16/longhorn_install/","permalink":"http://yoursite.com/2020/06/16/longhorn_install/","excerpt":"","text":"Longhorn部署环境信息 软件 版本 操作系统 centos7.8 rancher v2.3.8-ent2 Kubernetes 1.17.4 Longhorn 初始化节点上安装iscsi插件 1yum install iscsi-initiator-utils 将需要添加到Longhorn的磁盘进行格式化和挂载到&#x2F;var&#x2F;lib&#x2F;longhorn目录 1mkfs.xfs /dev/vdb 1mount /dev/vdb /var/lib/longhorn 部署Longhorn通过Rancher应用商店启动Longhorn 根据实际情况配置参数 启动后访问Longhorn 在rancher中创建workload绑定PVC进行测试 扩容磁盘在Longhorn界面上选择对应的节点 添加磁盘挂载路径即可 存储卷扩容有两种扩展Longhorn体积的方法：使用PersistentVolumeClaim（PVC）和Longhorn UI。 如果使用的是Kubernetes v1.14或v1.15，则只能使用Longhorn UI扩展卷。 通过PVC仅在以下情况下应用此方法： Kubernetes版本v1.16或更高版本。 Kubernetes使用Longhorn StorageClass动态设置PVC。 StorageClass中的allowVolumeExpansion字段设置为true。 用法：将workload副本设置为0，在Longhorn卷找到相应的PVC，然后通过kubectl修改对应的pvc中的spec.resources.requests.字段，在Longhorn中对应的卷大小会自动更新。 存储卷快照选择对应的存储卷，创建快照 还原快照1、将挂载此卷的workload副本数设置为02、重新将此卷使用维护模式挂载到对应的节点。 3、选择卷快照还原，还原后将磁盘从对应节点卸载。 4、应用启动查看数据。 Longhorn备份和灾难恢复概述Longhorn做为分布式块存储，本身能支持针对存储卷的备份和数据卷的灾难恢复。用以下示例讲解Longhorn存储卷备份和灾难恢复。 数据卷备份Longhorn支持存储卷的备份，将数据卷备份到AWS S3、NFSv4，采用快照方式增量备份到后端的backend存储上。以NFSv4做为后端backend为例配置后端存储。 节点安装NFS套件 1yum install nfs-utils 创建共享目录 1mkdir /nfsdata 配置exports 1vim /etc/exports 1/nfsdata *(rw,sync,no_root_squash) 启动服务并设置开机启动 12345678systemctl enable rpcbindsystemctl enable nfs-serversystemctl enable nfs-locksystemctl enable nfs-idmapsystemctl start rpcbindsystemctl start nfs-serversystemctl start nfs-locksystemctl start nfs-idmap NFSV4挂载测试 1mount -t nfs host_ip:/nfsdata /mnt/ 在Longhorn界面上配置存储对接 创建备份 在对应的backup选项内可以看见创建出来的备份 对应nfs服务器查看数据，可以看见数据被分成了一个个块，以分块的方式存储。 1234567891011121314151617181920212223242526272829303132333435363738[root@iZ2zegq8vbaetetmgkyq4pZ /]# tree /nfsdata//nfsdata/└── backupstore └── volumes └── 5d └── d6 └── pvc-0853fa8c-512a-11ea-98d5-00163e122063 ├── backups │ └── backup_backup-b39c1feeeef94bf5.cfg ├── blocks │ ├── 2e │ │ └── 96 │ │ └── 2e96c43b271147b95c36f12d09a284062e7ca746185a99f203d6e857cb62721a.blk │ ├── 4b │ │ └── f3 │ │ └── 4bf324201d1805d2c15fe5f97158ac376c7e01ffb21eaca00094eb70ecf673b5.blk │ ├── 73 │ │ └── 18 │ │ └── 731859029215873fdac1c9f2f8bd25a334abf0f3a9e1b057cf2cacc2826d86b0.blk │ ├── a6 │ │ └── b3 │ │ └── a6b3b084f3a14e2557a4185d34fd07707ece662570298a02cf3fd745c6521725.blk │ ├── b2 │ │ └── 2c │ │ └── b22c71a142d7c1f8b8ba287e27bf62752087cec2bc0c8b58db5a96047d138a73.blk │ ├── bd │ │ └── 90 │ │ └── bd90eab7f660b8ae3c97a3a4cd4808d361518730282befc8e2113ae31b0a45ee.blk │ ├── d9 │ │ └── 46 │ │ └── d9462886143128037ae7c2e07bebd980c87ae4f1590c2dc4307962cc6d9f9f9f.blk │ └── ea │ └── 2c │ └── ea2c43cad3a82e7cc76695b0fa04bacced0c5bb4d041785eef85580d120fb383.blk └── volume.cfg23 directories, 10 files 数据卷还原数据卷还原步骤：1、workload暂停使用,副本设置为0。2、基于备份卷创建一个还原卷。 3、基于还原卷可创建新的PV和PVC或覆盖原有pvc。 基于备份卷创建新的PVC填写卷名 基于卷创建对应的PV和PVC 若覆盖现用的PVC1、覆盖原有pvc需要在Longhorn界面将原有volume删除或删除对应的PVC。 2、若覆盖现用的PVC,需要勾选 Use Previous Name，就创建与之前对应的卷名。 2、基于volume创建对应PV&#x2F;PVC 3、将POD副本重新设置为正常,启动后查看卷内数据。 数据卷的灾难恢复Longhorn数据卷的灾难恢复步骤： 1、A集群做为主集群对数据卷进行备份到远端backend。2、B集群做为备集群指向与A集群一样的远端backend。 3、在B集群基于备份卷创建Create Disaster Recovery Volume。 4、基于创建的Create Disaster Recovery Volume创建PVC到对应的namespace。 激活Disaster Recovery 5、创建POD挂载新建的PVC。","categories":[{"name":"分布式存储","slug":"分布式存储","permalink":"http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"分布式存储","slug":"分布式存储","permalink":"http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"}]},{"title":"Rook-ceph","slug":"rook_ceph","date":"2020-05-25T13:45:59.000Z","updated":"2020-05-25T13:45:59.000Z","comments":true,"path":"2020/05/25/rook_ceph/","permalink":"http://yoursite.com/2020/05/25/rook_ceph/","excerpt":"","text":"Rook概述Rook（https://rook.io/）是由CNCF社区管理的云原生存储编排系统，rook并不是一个实际的存储软件，它做的是将存储软件的部署和运维动作通过Kubernetes来实现自动化。比如rook-ceph项目实际上是在Kubernetes中定义了对应的operator和CRD资源对象来对ceph集群进行操作。 rook目前支持的存储 Ceph EdgeFS CockroachDB Cassandra NFS Yugabyte DB 目前比较成熟的是rook-ceph。通过rook-ceph可以将ceph非常简单方便的部署到Kubernetes，通过Kubernetes的资源对象来对ceph进行控制。 rook架构 使用Rook部署ceph环境概述 软件 版本 centos 7.7 Kubernetes 1.17.4 rook v1.3 每个节点预留了块50G的磁盘做为osd节点 节点磁盘信息如下： 12345 lsblk -fNAME FSTYPE LABEL UUID MOUNTPOINTvda └─vda1 ext4 995d4542-f0dd-47e6-90eb-690de3b64430 /vdb 将节点vdb磁盘做为ceph-osd节点 1git clone https://github.com/rook/rook.git -b release-1.3 若clone慢也可以使用 1git clone https://gitee.com/wanshaoyuan/rook.git -b release-1.3 部署 1cd rook/cluster/examples/kubernetes/ceph 12kubectl create -f common.yamlkubectl create -f operator.yaml cluster.yaml文件内包含对ceph初始化的配置 1kubectl create -f cluster.yaml 参数：默认情况会下rook-ceph会将集群内全部节点以及节点上全部磁盘做为osd节点，生产环境不建议这样使用，建议指定节点和指定节点设备。 12useAllDevices: true //将host上全部空余设备做为ceph-osd磁盘 useAllNodes: true //将集群内全部节点做为ceph节点 使用指定节点的指定设备配置将useAllNodes和useAllDevices设置为false 12345678910nodes:- name: &quot;172.24.234.128&quot; devices: # specific devices to use for storage can be specified for each node - name: &quot;vdb&quot;- name: &quot;172.24.234.147&quot; devices: - name: &quot;vdb&quot;- name: &quot;172.24.234.156&quot; devices: - name: &quot;vdb&quot; 注意：nodes：name处要与kubectl get node出来的显示一致，若为ip显示ip若为主机名显示主机名 ceph-dashboard访问1kubectl apply -f dashboard-external-https.yaml 获取访问端口 1234kubectl get svc/rook-ceph-mgr-dashboard-external-https -n rook-ceph NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGErook-ceph-mgr-dashboard-external-https NodePort 10.43.117.2 &lt;none&gt; 8443:30519/TCP 53s 获取访问密码 1kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=&#x27;&#123;.data.password&#125;&#x27; | base64 --decode 查看集群健康状态123kubectl get CephCluster -n rook-cephNAME DATADIRHOSTPATH MONCOUNT AGE PHASE MESSAGE HEALTHrook-ceph /var/lib/rook 3 20m Ready Cluster updated successfully HEALTH_OK 创建存储池和storageclass12345678910111213141516171819202122232425262728293031323334353637383940apiVersion: ceph.rook.io/v1kind: CephBlockPoolmetadata: name: replicapool namespace: rook-cephspec: failureDomain: host replicated: size: 3---apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: rook-ceph-block# Change &quot;rook-ceph&quot; provisioner prefix to match the operator namespace if neededprovisioner: rook-ceph.rbd.csi.ceph.comparameters: # clusterID is the namespace where the rook cluster is running clusterID: rook-ceph # Ceph pool into which the RBD image shall be created pool: replicapool # RBD image format. Defaults to &quot;2&quot;. imageFormat: &quot;2&quot; # RBD image features. Available for imageFormat: &quot;2&quot;. CSI RBD currently supports only `layering` feature. imageFeatures: layering # The secrets contain Ceph admin credentials. csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph # Specify the filesystem type of the volume. If not specified, csi-provisioner # will set default as `ext4`. csi.storage.k8s.io/fstype: xfs# Delete the rbd volume when a PVC is deletedreclaimPolicy: Delete 部署应用测试1kubectl apply -f /root/rook/cluster/examples/kubernetes/mysql.yaml -f /root/rook/cluster/examples/kubernetes/wordpress.yaml 扩容存储节点需要保证磁盘的gpt分区表类型，先对磁盘进行初始化 1234parted -s /dev/xxxx mklabel gptsgdisk --zap-all /dev/xxx 编辑集群资源 1kubectl edit CephCluster/rook-ceph -n rook-ceph 添加对应节点磁盘字段 123456789101112131415161718- config: null devices: - config: null name: vdb name: rke-node5 resources: &#123;&#125;- config: null devices: - config: null name: vdb name: rke-node6 resources: &#123;&#125;- config: null devices: - config: null name: vdb name: rke-node7 resources: &#123;&#125; 常见问题：1、RKE部署问题1、通过Rancher RKE部署的Kubernetes集群因为rancher rke部署的集群kubelet是运行在容器中的，所以需要将flexvolume插件映射到kubelet容器中，不然无法挂载pvc到workload中。 为kubelt添加以下参数： 1234extra_args: volume-plugin-dir: /usr/libexec/kubernetes/kubelet-plugins/volume/exec extra_binds: /usr/libexec/kubernetes/kubelet-plugins/volume/exec:/usr/libexec/kubernetes/kubelet-plugins/volume/exec 2、ubuntu16.04操作系统部署问题ubuntu16.04默认4.4内核无法挂载rbd块到workload中，提示缺少特性，需要将内核升级到4.15。升级步骤如下： 升级内核到4.15 12uname -aLinux kworker2 4.4.0-142-generic #168-Ubuntu SMP Wed Jan 16 21:00:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux 1apt-get install --install-recommends linux-generic-hwe-16.04 1reboot 12uname -aLinux kworker2 4.15.0-60-generic #67~16.04.1-Ubuntu SMP Mon Aug 26 08:57:33 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux 清理集群123kubectl delete -f cluster.yaml kubectl delete -f operator.yaml kubectl delete -f common.yaml 清理宿主机目录 123456789101112131415#!/usr/bin/env bashDISK=&quot;/dev/sdb&quot;# Zap the disk to a fresh, usable state (zap-all is important, b/c MBR has to be clean)# You will have to run this step for all disks.sgdisk --zap-all $DISKdd if=/dev/zero of=&quot;$DISK&quot; bs=1M count=100 oflag=direct,dsync# These steps only have to be run once on each node# If rook sets up osds using ceph-volume, teardown leaves some devices mapped that lock the disks.ls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove %# ceph-volume setup can leave ceph-&lt;UUID&gt; directories in /dev (unnecessary clutter)rm -rf /dev/ceph-*rm /var/lib/rook/ -rfrm /var/lib/kubelet/plugins/ -rfrm /var/lib/kubelet/plugins_registry/ -rf","categories":[{"name":"分布式存储","slug":"分布式存储","permalink":"http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"分布式存储","slug":"分布式存储","permalink":"http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"}]},{"title":"Rancher容器集群跨主机网络模式切换","slug":"rancher_network","date":"2020-04-28T13:45:59.000Z","updated":"2020-04-28T13:45:59.000Z","comments":true,"path":"2020/04/28/rancher_network/","permalink":"http://yoursite.com/2020/04/28/rancher_network/","excerpt":"","text":"Flannel或canal网络插件切换host-gw模式默认部署出来是canal的vxlan的模式查看宿主机网卡，会发现多了个flannel.1网卡，这个是建立vxlan隧道的网卡 跨主机的网卡都是走flannel.1网卡出去，因为要经过vxlan封包 切换到host-gw模式的方法编辑集群 将flannel_backend_type改为host-gw保存，等待集群进行更新 继续修改canal默认的configmap文件 切换到System项目——&gt;资源——&gt;配置映射修改canal-config 保存，然后将机器重启在次查看flannel.1网卡已经消失了,所有流量出去通过对应的明细路由进行访问 。 验证创建workload，多个副本，分布到不同主机上，进行跨主机ping测试 在不同主机上互相ping测试，看看跨主机网络是否正常，这里需要注意的是，在公有云和openstack上是ping不通的因为host-gw模式公有云默认都开启了防ARP欺骗，分配的Mac地址和IP都绑定了，非平台分配的mac地址和IP出去会被drop掉。 calico网络插件Calico是一种开源网络和网络安全解决方案，适用于容器，虚拟机和基于主机的本机工作负载。Calico支持广泛的平台，包括Kubernetes，docker，OpenStack和裸机服务。Calico后端支持多种网络模式。 BGP模式：将节点做为虚拟路由器通过BGP路由协议来实现集群内容器之间的网络访问。 IPIP模式：在原有IP报文中封装一个新的IP报文，新的IP报文中将源地址IP和目的地址IP都修改为对端宿主机IP。 cross-subnet：Calico-ipip模式和calico-bgp模式都有对应的局限性，对于一些主机跨子网而又无法使网络设备使用BGP的场景可以使用cross-subnet模式，实现同子网机器使用calico-BGP模式，跨子网机器使用calico-ipip模式。 calico切换BGP模式部署完成后默认使用calico-ipip的模式，通过在节点的路由即可得知，通往其他节点路由通过tunl0网卡出去 修改为BGP网络模式，在system项目中修改calico-node daemonset 修改CALICO_IPV4POOL_IPIP改为off，添加新环境变量FELIX_IPINIPENABLED为false 修改完成后对节点进行重启，等待恢复后查看主机路由，与ipip最大区别在于去往其他节点的路由，由Tunnel0走向网络网卡。 calico切换cross-subne模式Calico-ipip模式和calico-bgp模式都有对应的局限性，对于一些主机跨子网而又无法使网络设备使用BGP的场景可以使用cross-subnet模式，实现同子网机器使用calico-BGP模式，跨子网机器使用calico-ipip模式。 部署集群网络选择calico网络插件 默认部署出来是calico的ip-in-ip的模式查看宿主机网卡，会发现多了个tunl0网卡，这个是建立ip隧道的网卡 去其他主机的路由都是走tunl0网卡出去 切换到cross-subnet模式 1kubectl edit ipPool/default-ipv4-ippool 将ipipMode改为crossSubnet 在UI将calico-node的POD删了重建 重启检查calico网络 可以看见同子网的主机出口走的是bgp，不同子网主机走的是tunl0网卡走ipip模式验证创建应用测试跨主机网络，在不同主机上互相ping测试，看看跨主机网络是否正常。 配置Route reflector安装方式Single host上面binary安装Single host上面continer安装作为k8s pod运行实际经验：Binary方式在集群里面的一台worker节点安装（比如RR）calicoctl会检测bird&#x2F;felix的运行状态在非calico node节点运行只能使用部分命令，不能运行calico node相关命令通过配置calicoctl来对calico进行控制，通常情况下建议将 1curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.13.3/calicoctl 配置可执行权限 1chmod +x calicoctl 复制的&#x2F;usr&#x2F;bin&#x2F;目录 1cp calicoctl /usr/bin/ 配置calicoctl连接Kubernetes集群 123export CALICO_DATASTORE_TYPE=kubernetesexport CALICO_KUBECONFIG=~/.kube/configcalicoctl node status calico node-to-node mesh 默认情况下calico采用node-to-node mesh方式 ，为了防止BGP路由环路，BGP协议规定在一个AS（自治系统）内部，IBGP路由器之间只能传一跳路由信息，所以在一个AS内部，IBGP路由器之间为了学习路由信息需要建立全互联的对等体关系，但是当一个AS规模很大的时候，这种全互联的对等体关系维护会大量消耗网络和CPU资源，所以这种情况下就需要建立路由反射器以减少IBGP路由器之间的对等体关系数量。 Route reflector角色介绍 早期calico版本提供专门的route reflector镜像，在新版本calico node内置集成route reflector功能。Route reflector可以是以下角色： 集群内部的node节点 集群外部节点运行calico node 其他支持route reflector的软件或者设备。 这里以一个集群内部的node节点为例： 关闭node-to-node mesh 123456789101112cat &lt;&lt;EOF | calicoctl apply -f -apiVersion: projectcalico.org/v3kind: BGPConfigurationmetadata: name: defaultspec: logSeverityScreen: Info nodeToNodeMeshEnabled: false asNumber: 63400EOF 设置Route reflector配置Route reflector支持多种配置方式如：1、支持配置全局BGP peer，。2、支持针对单个节点进行配置BGP Peer。也可以将calico节点充当Route reflector 这里以配置calico节点充当Router reflector为例。 配置节点充当BGP Route Reflector 可将Calico节点配置为充当路由反射器。为此，要用作路由反射器的每个节点必须具有群集ID-通常是未使用的IPv4地址。 要将节点配置为集群ID为244.0.0.1的路由反射器，请运行以下命令。这里将节点名为rke-node4的节点配置为Route Reflector，若一个集群中要配置主备rr，为了防止rr之间的路由环路，需要将集群ID配置成一样 1calicoctl patch node rke-node4 -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;bgp&quot;: &#123;&quot;routeReflectorClusterID&quot;: &quot;244.0.0.1&quot;&#125;&#125;&#125;&#x27; 给节点打上对应的label标记该节点以表明它是Route Reflector，从而允许BGPPeer资源选择它。 1kubectl label node rke-node4 route-reflector=true 创建BGPPeer 1234567891011export CALICO_DATASTORE_TYPE=kubernetesexport CALICO_KUBECONFIG=~/.kube/configcat &lt;&lt;EOF | calicoctl apply -f -kind: BGPPeerapiVersion: projectcalico.org/v3metadata: name: peer-with-route-reflectorsspec: nodeSelector: all() peerSelector: route-reflector == &#x27;true&#x27;EOF 查看BGP节点状态node上查看，peer type由node-to-node mesh变为 node specific Route Reflector上节点查看，节点已正常建立连接","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"Thanos初体验","slug":"thanos","date":"2020-03-14T13:45:59.000Z","updated":"2020-03-14T13:45:59.000Z","comments":true,"path":"2020/03/14/thanos/","permalink":"http://yoursite.com/2020/03/14/thanos/","excerpt":"","text":"概述：Thanos主要用于解决大规模prometheus部署、增强prometheus高可用的工具，由Improbable团队开源，项目地址https://github.com/thanos-io/thanos 架构： 组件：thanos-sidecar： 通过Prometheus附加件与Prometheus进行连接，通过http方式在Prometheus的remote-read API基础之上实现了storeAPI接口，query组件可以直接从此接口读取监控数据，并且还支持将数据上传到对应的对象存储。 thanos-query： 通过thanos-sidecar组件的store-api的grpc接口抓取监控数据，并对监控数据进行聚合去重处理。 thanos-storage-gateway： 对接后端对象存储，当需要查询对象存储中的历史监控数据时，query与其相连查看。 thanos-compact： 对存储在对象存储中的监控数据多个较小的块连续合并为较大的块。这显着减少了存储桶中的总存储大小。提高查询效率。 thanos-rules： 对监控数据进行告警，通知altermanager，并且可以预先计算经常需要或计算量大的表达式，并将其结果保存为一组新的时间序列据提供给query查询和对象存储进行存储。 Thanos能解决什么问题？大规模集群部署问题prometheus本身只支持单机部署，没有自带集群模式，所以在目前大规模集群监控主要通过prometheus联邦机制或者做监控指标服务拆分方式去实现并且最难以接受的是prometheus对于监控历史数据的存储问题，在本地不能存储过久的监控数据，只能通过远端存储接口，存储到支持prometheus远端存储接口的数据库中，这带来的问题就是引入新的组件会增加对应的运维工作量。 prometheus高可用问题和扩展问题prometheus官方高可用的方式是通过部署多个prometheus实例采集同一个target，前端通过LB设备做为统一入口，这带来的问题就是，两个prometheus实例内存储的数据会存在差异，特别是当其中一个prometheus宕机后，另外一个prometheus接管服务，此时宕机的prometheus就会丢失宕机期间的监控数据，当LB的请求转发过来会出现数据不一致情况。 Thanos能够解决上述问题，thanos能够将多个prometheus实例的数据进行聚合去重，来支持prometheus横向扩展和提高prometheus的高可用性，同时也支持将历史监控数据存储到对象存储中，提供监控数据的可靠性，降低运维难度。 部署架构 部署多个prometheus实例，采集相同或不同的targets。 thanos-sidecar通过sidecar方式与prometheus部署在一起，将数据提供给query查询和将本地落盘的数据上传到兼容S3协议的对象存储中。 Query进行数据汇总和去重做数据查询的统一入口，grafana通过query接口进行监控展示。 历史监控数据查询通过store-Gateway进行查询。 compactor组件对存储在对象存储的数据进行压缩，降低采样率，后续查看长时间监控数据提高效率。 部署使用：这里以一个快速demo的方式进行thanos的功能展示和部署，实际生产可以结合prometheus-operator的方式去部署更佳。 提前准备：创建一个storageclass，用于提供给prometheus实例使用 部署prometheus创建ServiceAccount并做权限绑定 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: ServiceAccountmetadata: name: prometheus---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: prometheusrules:- apiGroups: - &quot;&quot; resources: - nodes - services - endpoints - pods - nodes/proxy verbs: - get - list - watch- apiGroups: - &quot;extensions&quot; resources: - ingresses verbs: - get - list - watch- apiGroups: - &quot;&quot; resources: - configmaps - nodes/metrics verbs: - get- nonResourceURLs: - /metrics verbs: - get---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: prometheusroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheussubjects:- kind: ServiceAccount name: prometheus namespace: defaultEOF 创建configmapprometheus-configmap.yaml 将targets改为实际运行node-exporter节点地址 1234567891011121314151617apiVersion: v1kind: ConfigMapmetadata: name: prometheus-configdata: prometheus.yaml.tmpl: | global: scrape_interval: 15s scrape_timeout: 15s external_labels: cluster: test-thanos replica: $(POD_NAME) scrape_configs: - job_name: &quot;node&quot; static_configs: - targets: [&quot;172.16.1.6:9100&quot;] 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485cat &lt;&lt;EOF | kubectl apply -f -apiVersion: apps/v1kind: StatefulSetmetadata: name: prometheus labels: app: prometheusspec: serviceName: &quot;prometheus&quot; replicas: 2 selector: matchLabels: app: prometheus thanos-store-api: &quot;true&quot; template: metadata: labels: app: prometheus thanos-store-api: &quot;true&quot; spec: serviceAccountName: prometheus volumes: - name: prometheus-config configMap: name: prometheus-config - name: prometheus-config-shared emptyDir: &#123;&#125; containers: - name: prometheus image: prom/prometheus:v2.14.0 args: - &quot;--config.file=/etc/prometheus-shared/prometheus.yaml&quot; - &quot;--storage.tsdb.path=/prometheus&quot; - &quot;--storage.tsdb.retention.time=6h&quot; - &quot;--storage.tsdb.no-lockfile&quot; - &quot;--storage.tsdb.min-block-duration=2h&quot; # 每隔2小时将数据压缩成一个block，持久化到硬盘中 - &quot;--storage.tsdb.max-block-duration=2h&quot; - &quot;--web.enable-admin-api&quot; # thanos可以通过prometheus admin api管理数据 - &quot;--web.enable-lifecycle&quot; # 支持热更新 localhost:9090/-/reload 加载 ports: - name: http containerPort: 9090 volumeMounts: - name: prometheus-config-shared mountPath: /etc/prometheus-shared/ - name: data mountPath: &quot;/prometheus&quot; - name: thanos image: thanosio/thanos:v0.11.0 args: - sidecar - --tsdb.path=/prometheus - --prometheus.url=http://localhost:9090 - --reloader.config-file=/etc/prometheus/prometheus.yaml.tmpl #配置对应的prometheus.yaml.tmpl文件路径 - --reloader.config-envsubst-file=/etc/prometheus-shared/prometheus.yaml # 基于配置文件模板生成配置文件 ports: - name: http-sidecar containerPort: 10902 - name: grpc containerPort: 10901 env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name volumeMounts: - name: prometheus-config mountPath: /etc/prometheus - name: prometheus-config-shared mountPath: /etc/prometheus-shared/ - name: data mountPath: &quot;/prometheus&quot; volumeClaimTemplates: #提供volumesclaimTemplate为每个prometheus实例分配一个PVC - metadata: name: data labels: app: prometheus spec: storageClassName: managed-nfs-storage accessModes: - ReadWriteOnce resources: requests: storage: 2GiEOF 注意以下参数： 123456--storage.tsdb.min-block-duration=2h --storage.tsdb.max-block-duration=2h #这两个值使用thanos需要配置成一致，禁用prometheus对监控数据进行压缩，因为使用thanos，compactor组件会对监控数据进行压缩。 --web.enable-admin-api&quot; # thanos可以通过prometheus admin api管理数据--web.enable-lifecycle&quot; #配置prometheus配置热加载 创建Service 12345678910111213141516cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Servicemetadata: name: thanos-store-gatewayspec: type: ClusterIP clusterIP: None ports: - name: grpc port: 10901 targetPort: grpc selector: thanos-store-api: &quot;true&quot;EOF 部署exporter用于采集监控数据 部署node-exporter 在节点上部署node-exporter用于收集主机资源信息 1docker run -d --net=&quot;host&quot; --pid=&quot;host&quot; -v &quot;/:/host:ro,rslave&quot; bitnami/node-exporter --path.rootfs=/host 部署Query12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667cat &lt;&lt;EOF | kubectl apply -f -apiVersion: apps/v1kind: Deploymentmetadata: name: thanos-querier namespace: kube-mon labels: app: thanos-querierspec: selector: matchLabels: app: thanos-querier template: metadata: labels: app: thanos-querier spec: containers: - name: thanos image: thanosio/thanos:v0.11.0 args: - query - --log.level=debug - --query.replica-label=replica # Discover local store APIs using DNS SRV. - --store=dnssrv+thanos-store-gateway:10901 ports: - name: http containerPort: 10902 - name: grpc containerPort: 10901 resources: requests: memory: &quot;2Gi&quot; cpu: &quot;1&quot; limits: memory: &quot;2Gi&quot; cpu: &quot;1&quot; livenessProbe: httpGet: path: /-/healthy port: http initialDelaySeconds: 10 readinessProbe: httpGet: path: /-/healthy port: http initialDelaySeconds: 15---apiVersion: v1kind: Servicemetadata: name: thanos-querier labels: app: thanos-querierspec: ports: - port: 9090 protocol: TCP targetPort: http nodePort: 30001 name: http selector: app: thanos-querier type: NodePortEOF 访问节点http://ip:30001端口即可打开Query界面 可以看见通过prometheus接口抓取过来的数据 可以看见对应的监控指标和对应的thanos-sidecar节点 通过Stores页面可以看见对应的组件的状态信息 以查看节点负载为例 默认情况下会出现两个值，因为有两个prometheus实例对数据进行收集 勾选deduplication选项将对监控数据进行去重处理，query是根据 prometheus.yaml.tmpl内的external_labels标签进行去重处理。Querier 会取时间戳新的数据进行展示。 部署对象存储Thanos目前支持多种对象存储接口，包括像国内的阿里OSS、腾讯COS、openstack-swift和兼容S3接口的对象存储如minio等，下图为官方支持的对象存储列表 这里我们主要以Minio为例演示对接 创建pvc 12345678910111213141516171819cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: PersistentVolumeClaimmetadata: # This name uniquely identifies the PVC. This is used in deployment. name: minio-pv-claimspec: # Read more about access modes here: http://kubernetes.io/docs/user-guide/persistent-volumes/#access-modes storageClassName: managed-nfs-storage accessModes: # The volume is mounted as read-write by a single node - ReadWriteOnce resources: # This is the request for storage. Should be available in the cluster. requests: storage: 10Gi EOF 创建Minio 1kubectl create -f https://raw.githubusercontent.com/minio/minio/master/docs/orchestration/kubernetes/minio-standalone-deployment.yaml 创建对外暴露Service 12345678910111213141516cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Servicemetadata: name: minio-servicespec: type: NodePort ports: - port: 9000 targetPort: 9000 protocol: TCP nodePort: 30002 selector: app: minioEOF 访问Minio http://ip:30002 使用minio&#x2F;minio123访问Minio 创建名为prometheus的bucket 将落盘的监控数据存储到对象存储中创建secret 1234567891011121314151617cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Secretmetadata: name: minio-secrettype: OpaquestringData: thanos-secret.yaml: |- type: S3 config: bucket: &quot;prometheus&quot; endpoint: &quot;172.16.1.6:30002&quot; access_key: &quot;minio&quot; insecure: true secret_key: &quot;minio123&quot;EOF 在prometheus启动的yaml文件中引用此secret添加以下部分 1234567891011121314151617181920spec: serviceAccountName: prometheus volumes: - name: minio-secret secret: secretName: minio-secret containers: - name: thanos image: thanosio/thanos:v0.11.0 args: - sidecar - --tsdb.path=/prometheus - --prometheus.url=http://localhost:9090 - --reloader.config-file=/etc/prometheus/prometheus.yaml.tmpl #配置对应的prometheus.yaml.tmpl文件路径 - --reloader.config-envsubst-file=/etc/prometheus-shared/prometheus.yaml # 基于配置文件模板生成配置文件 - --objstore.config-file=/etc/secret/thanos-secret.yaml volumeMounts: - name: minio-secret mountPath: &quot;/etc/secret/&quot; 等待片刻后可以在Minio上看见thanos-sidecar上传过来的prometheus监控数据 thanos-secret.yaml 部署Store-Gateway部署Store-Gateway方便查看历史存储数据 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172cat &lt;&lt;EOF | kubectl apply -f -apiVersion: apps/v1kind: StatefulSetmetadata: labels: app.kubernetes.io/name: thanos-store name: thanos-storespec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: thanos-store serviceName: thanos-store-gateway template: metadata: labels: thanos-store-api: &quot;true&quot; app.kubernetes.io/name: thanos-store spec: containers: - args: - store - --data-dir=/var/thanos/store - --grpc-address=0.0.0.0:10901 - --http-address=0.0.0.0:10902 - --objstore.config-file=/etc/secret/thanos-secret.yaml - --experimental.enable-index-header image: quay.io/thanos/thanos:v0.11.0 livenessProbe: failureThreshold: 8 httpGet: path: /-/healthy port: 10902 scheme: HTTP periodSeconds: 30 name: thanos-store ports: - containerPort: 10901 name: grpc - containerPort: 10902 name: http readinessProbe: failureThreshold: 20 httpGet: path: /-/ready port: 10902 scheme: HTTP periodSeconds: 5 volumeMounts: - mountPath: /var/thanos/store name: data readOnly: false - mountPath: /etc/secret/ name: minio-secret volumes: - name: minio-secret secret: secretName: minio-secret volumeClaimTemplates: - metadata: labels: app.kubernetes.io/name: thanos-store name: data spec: storageClassName: managed-nfs-storage accessModes: - ReadWriteOnce resources: requests: storage: 10GiEOF 注意label thanos-store-api: “true”这样才能与thanos-store-gateway这个headless service关联，另外与它关联的Querier才能发现它。 部署Compactcompact需要本地磁盘空间来存储中间数据以进行处理，建议使用大约100GB的空间以保持正常工作。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879cat &lt;&lt;EOF | kubectl apply -f -apiVersion: apps/v1kind: StatefulSetmetadata: labels: app.kubernetes.io/component: database-compactor app.kubernetes.io/instance: thanos-compact app.kubernetes.io/name: thanos-compact app.kubernetes.io/version: v0.11.0 name: thanos-compactspec: replicas: 1 selector: matchLabels: app.kubernetes.io/component: database-compactor app.kubernetes.io/instance: thanos-compact app.kubernetes.io/name: thanos-compact serviceName: thanos-compact template: metadata: labels: app.kubernetes.io/component: database-compactor app.kubernetes.io/instance: thanos-compact app.kubernetes.io/name: thanos-compact app.kubernetes.io/version: v0.11.0 spec: containers: - args: - compact - --wait - --objstore.config-file=/etc/secret/thanos-secret.yaml - --data-dir=/var/thanos/compact - --debug.accept-malformed-index image: quay.io/thanos/thanos:v0.11.0 livenessProbe: failureThreshold: 4 httpGet: path: /-/healthy port: 10902 scheme: HTTP periodSeconds: 30 name: thanos-compact ports: - containerPort: 10902 name: http readinessProbe: failureThreshold: 20 httpGet: path: /-/ready port: 10902 scheme: HTTP periodSeconds: 5 terminationMessagePolicy: FallbackToLogsOnError volumeMounts: - mountPath: /var/thanos/compact name: data readOnly: false - mountPath: /etc/secret/ name: minio-secret volumes: - name: minio-secret secret: secretName: minio-secret volumeClaimTemplates: - metadata: labels: app.kubernetes.io/component: database-compactor app.kubernetes.io/instance: thanos-compact app.kubernetes.io/name: thanos-compact name: data spec: storageClassName: managed-nfs-storage accessModes: - ReadWriteOnce resources: requests: storage: 10GiEOF 需要本地磁盘空间来存储中间数据以进行处理，建议使用大约100GB的空间以保持正常工作。在重新启动之间可以安全地删除磁盘上的数据 Receiver介绍主要解决大规模场景下 Query都调所有 thanos-Sidecar会消耗很多资源，所以统一通过prometheus的remote-write接口将数据传送给thanos-Receiver，Query从thanos-Receiver获取数据https://thanos.io/proposals/201812_thanos-remote-receive.md/ Grafana对接使用thanos后grafana对接直接对接Query就可以了 启动grafana 1docker run -d --name=grafana -p 3000:3000 grafana/grafana 添加DataSource这填写Query的地址 添加dashboard，因为我们之有node-exporter的数据所以这块需要使直接使用主机节点的dashboard，导入dashboard输入id号8919 总结 Thanos做为prometheus的一个附加组件，还是能解决目前在使用prometheus时的一些高可用，历史数据存储和大规模集群的问题，目前社区也在大力发展。 参考链接：https://github.com/thanos-io/thanos/blob/master/docs/quick-tutorial.mdhttps://engineering.hellofresh.com/monitoring-at-hellofresh-part-1-architecture-677b4bd6b728https://github.com/thanos-io/kube-thanos/tree/master/manifestshttps://github.com/thanos-io/thanos/blob/master/docs/components/sidecar.mdhttps://www.qikqiak.com/k8strain/monitor/thanos/#rulerhttps://github.com/thanos-io/kube-thanos/tree/release-0.11/examples/all/manifests","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"容器镜像签名","slug":"harbor_notary","date":"2020-02-22T15:27:44.000Z","updated":"2020-02-22T15:27:44.000Z","comments":true,"path":"2020/02/22/harbor_notary/","permalink":"http://yoursite.com/2020/02/22/harbor_notary/","excerpt":"","text":"概述Notary 是一个用于建立内容之间信任的平台。在容器镜像中可以对镜像进行加密签名用来保证镜像件来源和镜像内容防篡改。软件版本 软件 版本 Harbor 1.10.1 Docker 18.09.9 Harbor打开NotaryHarbor部署时可以选择启用notray，需要Harbor使用Https模式 1./install --with-notary 部署完后会在harbor中启用notary-server-photon和notary-signer-photon组件，服务监听4443端口 新建个notary项目配置内容信任，阻止没有签名的镜像下载 上传镜像默认不启用docker的内容信任参数上传镜像 1docker push 172.16.1.31/notary/hyperkube:v1.15.5-rancher1 此时在Harbor中看见镜像是未签名状态 因为项目开启了仅允许通过认证的镜像下载，所以未签名的镜像无法pull下来。 docker开启镜像签名使用以下两个环境变量 12export DOCKER_CONTENT_TRUST=1export DOCKER_CONTENT_TRUST_SERVER=&quot;https://172.16.1.31:4443&quot; DOCKER_CONTENT_TRUST&#x3D;1：表示开启Docker内容信任模式DOCKER_CONTENT_TRUST_SERVER：指定认证服务器，其实就是对应harbor notary服务的地址和端口 若Harbor使用自签名证书，则需要将对应的ca证书放置到主机的~&#x2F;.docker&#x2F;tls&#x2F;172.16.1.31:4443&#x2F;目录，证书名为ca.crt。 172.16.1.31:443为你的notary节点ip和端口 原理就是使用目录下ca证书进行镜像的签发密钥和验证。 继续push镜像 1234567891011121314151617181920docker push 172.16.1.31/notary/hyperkube:v1.15.5-rancher1The push refers to repository [172.16.1.31/notary/hyperkube]2ea80d1d4b24: Layer already exists3892a0baf222: Layer already exists39b3ac6d96e9: Layer already exists7bbae4dddb88: Layer already existsa00defcfe869: Layer already exists2ab0ae805c74: Layer already exists43a8fe7d2382: Layer already exists3f6a6f542637: Layer already exists5ba3be777c2d: Layer already existsv1.15.5-rancher1: digest: sha256:19c919ef37b634919bd08429ee1fde0f3d8ed83c7e75929f7e259ba35f88d9b7 size: 2211Signing and pushing trust metadataYou are about to create a new root signing key passphrase. This passphrasewill be used to protect the most sensitive key in your signing system. Pleasechoose a long, complex passphrase and be careful to keep the password and thekey file itself secure and backed up. It is highly recommended that you use apassword manager to generate the passphrase and keep it safe. There will be noway to recover this key. You can find the key in your config directory.Enter passphrase for new root key with ID cd80889: 输入密码对镜像进行签名，后续镜像上传都是使用此密码进行镜像签署。根密钥的密码在 ~&#x2F;.docker&#x2F;trust 目录中生成一个根密钥镜像签署密钥的密码在 ~&#x2F;.docker&#x2F;trust 目录中生成一个镜像签署密钥 上传成功后在harbor中查看 查看镜像的签名 1docker trust inspect 172.16.1.31/notary/hyperkube:v1.15.5-rancher1 12345678910111213141516171819202122232425262728293031323334docker trust inspect 172.16.1.31/notary/hyperkube:v1.15.5-rancher1[ &#123; &quot;Name&quot;: &quot;172.16.1.31/notary/hyperkube:v1.15.5-rancher1&quot;, &quot;SignedTags&quot;: [ &#123; &quot;SignedTag&quot;: &quot;v1.15.5-rancher1&quot;, &quot;Digest&quot;: &quot;19c919ef37b634919bd08429ee1fde0f3d8ed83c7e75929f7e259ba35f88d9b7&quot;, &quot;Signers&quot;: [ &quot;Repo Admin&quot; ] &#125; ], &quot;Signers&quot;: [], &quot;AdministrativeKeys&quot;: [ &#123; &quot;Name&quot;: &quot;Root&quot;, &quot;Keys&quot;: [ &#123; &quot;ID&quot;: &quot;4d8fcd2d60d87115dff15e43a649ff0c95fa26f7fd9cf1756ce4775c1a0f94c8&quot; &#125; ] &#125;, &#123; &quot;Name&quot;: &quot;Repository&quot;, &quot;Keys&quot;: [ &#123; &quot;ID&quot;: &quot;7254ee1ad59f87b21cd1af6ec847f95a41b84702cd745e8eb9c47abadb5145e5&quot; &#125; ] &#125; ] &#125;] 镜像拉取1、若节点docker配置了docker内容信任 12export DOCKER_CONTENT_TRUST=1export DOCKER_CONTENT_TRUST_SERVER=&quot;https://172.16.1.31:4443&quot; 拉取镜像需要进行证书签名比对，若harbor使用的是自签名证书，此时需要节点~&#x2F;.docker&#x2F;tls&#x2F;上放置了CA证书 若节点没有配置docker内容信任，则拉取镜像时不需要做签名验证。 2、Harbor项目开启了只允许信任镜像下载，则未签名的镜像无法下载 镜像删除签名了的镜像无法直接通过Harbor进行删除，需要在notary中将镜像签名去除 1notary -s https://172.16.1.31:4443 -d ~/.docker/trust --tlscacert /root/.docker/tls/172.16.1.31\\:4443/ca.crt remove -p 172.16.1.31/notary/hyperkube v1.15.5-rancher1 signed 注意点：1、tag和镜像是用空格分开的2、notary命令可以通过https://github.com/theupdateframework/notary/releases下载","categories":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"prometheus监控部署与使用","slug":"prometheus_1","date":"2020-01-03T15:27:44.000Z","updated":"2020-01-03T15:27:43.000Z","comments":true,"path":"2020/01/03/prometheus_1/","permalink":"http://yoursite.com/2020/01/03/prometheus_1/","excerpt":"","text":"prometheus部署整体架构: 组件作用：prometheus：用于收集监控指标。grafana：对接prometheus，将收集到的监控指标，图标化展示。altermanager：用于对触发阈值的监控指标进行告警。kube-state-metrics：部署在kubernetes集群中，用于收集kubernetes组件信息和对应的POD信息监控信息。node-export：用于收集节点监控指标。 1git clone https://github.com/stefanprodan/k8s-prom-hpa 切换到k8s-prom-hpa目录 12kubectl create -f ./prometheus 目录内包含了prometheus的配置文件和部署文件查看prometheus 1234567kubectl get pod -n monitoringNAME READY STATUS RESTARTS AGEprometheus-64bc56989f-qcs4p 1/1 Running 1 22hkubectl get svc -n monitoringNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEprometheus NodePort 10.104.215.207 &lt;none&gt; 9090:31190/TCP 22h 访问prometheushttp://node_ip:31190 prometheus内prometheus-cfg.yaml 配置了自动发生规则，会自动将组件注册。 部署kube-state-metric监控Kubernetes集群参考：https://github.com/kubernetes/kube-state-metrics/tree/master/docs kube-state-metric对应的版本支持的Kubernetes版本信息 clone代码 1git clone https://github.com/kubernetes/kube-state-metrics.git -b v1.8.0 部署kube-state-metric在kube-state-metrics&#x2F;kubernetes目录执行 1kubectl apply -f kube-state-metrics-service-account.yaml,kube-state-metrics-cluster-role.yaml,kube-state-metrics-cluster-role-binding.yaml,kube-state-metrics-deployment.yaml,kube-state-metrics-service.yaml 检查kube-state-metrices部署 12345kubectl get pod -n kube-system -o widemetrics-server-7575d9677b-fl5rc 1/1 Running 1 21h 10.244.0.14 k8s-master curl --insecure https://10.244.0.14/healthzok 默认prometheus会自动发现kube-state-metrices创建的Service对应的端口和指标获取路径 在prometheus上可以targets上可以看见对应的点 部署node-export用于节点监控数据收集使用helm-chart部署 1helm install --name node-exporter --namespace kube-system prometheus-node-exporter/ 验证是否能取到指标 1curl http://127.0.0.1:9100/metrics grafana部署对接prometheus添加对应的监控指标展示模板配置grafana对监控指标进行展示安装grafan 1docker run -d --name=grafana -p 3000:3000 grafana/grafana 访问主机3000端口默认帐号密码：admin&#x2F;admin 添加DataSource，类型选择prometheus，添加prometheus的地址 添加监控展示模板，添加主机监控模板和Kubernetes集群监控模板 导入ID为8919的node监控dashboard 导入ID为6417的Kubernetes集群监控dashboard","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"Habor高可用的实现方式","slug":"habor_ha","date":"2019-12-22T13:45:59.000Z","updated":"2019-12-22T13:45:59.000Z","comments":true,"path":"2019/12/22/habor_ha/","permalink":"http://yoursite.com/2019/12/22/habor_ha/","excerpt":"","text":"Harbor架构图片来源harbor官网 组件介绍：proxy: 通过nginx服务器来做反向代理 registry: docker官方的镜像存储服务。 ui: 前端UI入口 adminserver: 作为Harbor工程的配置数据管理器使用 DB: 存放用户信息、项目信息、用户权限和安全漏洞库，早期使用Mysql，目前采用Postgres。 job services: 通过状态机的形式将镜像复制到远程Harbor实例。镜像删除同样也可以被同步到远程Harbor实例中。 log: 运行rsyslogd的容器，主要用于收集其他容器的日志 Notary： 通过镜像签名方式方式，用于确保镜像来源真实性。 需要实现高可用的组件Redis：用于存储session会话信息。 Postgres：用于存储用户信息，项目信息，用户权限信息和安全漏洞库。 Registry：实际镜像管理组件。 双主HA模式通过共享存储实现高可用Harbor默认全部数据存储在&#x2F;data目录中，所以通过共享存储实际上就是将data目录挂载到共享存储中，目前常用方式是使用一些开源的分布式文件系统存储如cephfs，glusterfs或放置在对象存储S3中。 底层使用共享存储用于保证镜像数据可靠性。 将postgres独立出来，通过pgpool-II实现高可用集群。 外部通过LB+keepalived方式转发到两个harbor中实现统一入口。 优点： 由于多Harbor实例共享存储，因此可以保持数据是实时一致的； 缺点： 门槛高，需要具备共享存储； 需要搭建和维护postgres集群，维护成本高； 主备模式 底层使用共享存储用于保证镜像和postgres的数据可靠性。 外部通过在两个Habor间通过Keepalived维持VIP，保证统一入口，但一个宕机后可以通过VIP漂移方式在将请求迅速转发到另外一个可用节点。 优点： 架构简单，部署两个harbor，只需要通过共享存储和keepalived就可以实现HA； 缺点：- 只能实现主备，没有全主，每次只能有一个harbor可以对外提供服务； 基于镜像复制方式实现高可用但没有共享存储时可以使用Harbor本身的镜像同步方式 Harbor使用底层使用各自的本地存储； 两个Harbor之间配置双向同步镜像策略； 优点： 架构简单，只需要部署两个Harbor，不需要通过其他额外组件。 配置文件，只需要配置两个Harbor间的镜像同步。 缺点： 只能实现主备，没有全主，每次只能有一个harbor可以对外提供服务； 但其中一台宕机后，镜像push请求到另外一台harbor后，待另外一台harbor恢复后需要手工执行一下同步策略进行同步。 只能同步镜像，用户和用户权限信息无法进行同步，因为这些都保存到postgres数据库中，所以需要通过导表方式进行同步用户和权限。 若VIP进行频繁飘移，容易造成两边postgres数据不一致。 导表方式如下：需要导出以下表： 表名 用途 harbor_user 用户信息表 project 项目信息表 project_member 项目成员表 project_metadata 项目元数据表，数据主要为项目创建和更新时间 repository 项目镜像信息元数据 假设有A节点和B节点两个Harbor，目前vip在A节点上，需要将A节点以下表导出同步到B节点上，若后续VIP漂移到B节点上后也需要将B节点上数据重新同步到A节点。 在主节点上操作：备份registry数据库进入 harbor-db容器 1pg_dump -d registry -f /var/lib/postgresql/registry.sql 导出原表123456pg_dump -t harbor_user registry &gt; /var/lib/postgresql/harbor_user.sqlpg_dump -t project_member registry &gt; /var/lib/postgresql/project_member.sqlpg_dump -t project registry &gt; /var/lib/postgresql/project.sqlpg_dump -t project_metadata registry &gt; /var/lib/postgresql/project_metadata.sqlpg_dump -t repository registry &gt;/var/lib/postgresql/repository.sqlpg_dump -t oidc_user registry &gt;/var/lib/postgresql/oidc_user.sql 将主节点导出的表文件发送到备节点上在备节点操作进入 harbor-db容器执行 备份registry数据库1pg_dump -d registry -f /var/lib/postgresql/registry.sql 删除原表（解决导入时外键依赖问题）123456psql registry -c &quot;drop table project_metadata;&quot;psql registry -c &quot;drop table project;&quot;psql registry -c &quot;drop table oidc_user;&quot;psql registry -c &quot;drop table harbor_user;&quot;psql registry -c &quot;drop table project_member;&quot;psql registry -c &quot;drop table repository;&quot; 导入123456psql registry -f /tmp/harbor/harbor_user.sql psql registry -f /tmp/harbor/project.sqlpsql registry -f /tmp/harbor/project_metadata.sql psql registry -f /tmp/harbor/oidc_user.sqlpsql registry -f /tmp/harbor/project_member.sql psql registry -f /tmp/harbor/repository.sql 查看harbor用户和项目成员信息，同步即可 注意： 若VIP进行频繁飘移，容易造成两边postgres数据不一致。","categories":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"ceph部署","slug":"ceph_deploy","date":"2019-12-01T13:45:59.000Z","updated":"2019-12-01T13:45:59.000Z","comments":true,"path":"2019/12/01/ceph_deploy/","permalink":"http://yoursite.com/2019/12/01/ceph_deploy/","excerpt":"","text":"非常简单的ceph部署手册，主要方便大家用于学习研究，非生产可用！！ 基础环境信息版本信息操作系统：ubuntu16.04内核版本：4.4.0-87-genericceph版本：13.2.7(mimic) 主机角色信息 主机名 IP地址 角色 配置 rke-node1 192.168.0.5 ceph-mon — rke-node2 192.168.0.6 ceph-osd1 100G HDD rke-node3 192.168.0.7 ceph-osd2 100G HDD rke-node4 192.168.0.8 ceph-osd3 100G HDD 部署安装配置源1echo deb https://download.ceph.com/debian-&#123;ceph-stable-release&#125;/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list 1apt update 安装python-minimal 12apt install 安装ceph-deploy12sudo apt install ceph-deploy 创建普通用户在集群中所有节点创建ceph用户，主要用于ceph-deploy部署（虽然可以用root，但不建议这样做) 123456useradd -d /home/ceph -m cephpasswd cephecho &quot;ceph ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/cephchmod 0440 /etc/sudoers.d/ceph 在ceph-deploy节点使用ceph用户登录，配置节点免密码ssh 1su - ceph 1ssh-keygen 1ssh-copy-id host_name 建立集群创建集群文件夹 1sudo mkdir /etc/ceph 1cd etc/ceph 创建集群部署ceph-monitor12345sudo ceph-deploy new rke-node1注：rke-node1为我要部署monitor的实际的FQDN主机名 12sudo ceph-deploy install rke-node1 rke-node2 rke-node3 rke-node4 --repo-url=http://mirrors.aliyun.com/ceph/debian-mimic 注：这里指定使用安装源，因为连国外网慢 初始化ceph-mon 1ceph-deploy mon create-initial 注意： 1、若遇到没有权限创建&#x2F;var&#x2F;run&#x2F;ceph目录时可以手动创建然后将用户和属主改为ceph。 同步权限配置文件 1sudo ceph-deploy admin rke-node1 rke-node2 rke-node3 rke-node4 部署ceph-mgr 1sudo ceph-deploy mgr create rke-node1 初始化并启动OSD 123sudo ceph-deploy osd create --data /dev/vdb rke-node2sudo ceph-deploy osd create --data /dev/vdb rke-node3sudo ceph-deploy osd create --data /dev/vdb rke-node4 查看是否加入成功和ceph集群状态 12345678910111213141516sudo ceph -s cluster: id: dc33e445-6157-47cd-81b2-23f9b233839e health: HEALTH_OK services: mon: 1 daemons, quorum rke-node1 mgr: rke-node1(active) osd: 3 osds: 3 up, 3 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 3.0 GiB used, 294 GiB / 297 GiB avail pgs: 查看osd状态12345678910 sudo ceph osd treeID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF-1 0.29008 root default-3 0.09669 host rke-node2 0 hdd 0.09669 osd.0 up 1.00000 1.00000-5 0.09669 host rke-node3 1 hdd 0.09669 osd.1 up 1.00000 1.00000-7 0.09669 host rke-node4 2 hdd 0.09669 osd.2 up 1.00000 1.00000 启用ceph-dashboard通过ceph-mgr启用dashboard功能 1ceph mgr module enable dashboard 设置访问ip和端口 123ceph config set mgr mgr/dashboard/server_addr 172.16.0.195ceph config set mgr mgr/dashboard/server_port 8800 设置管理员帐号和密码 1ceph dashboard set-login-credentials admin admin@2019 访问http:&#x2F;&#x2F;主机ip:8800 测试创建pool因为我们osd数量小于4设置pg为128 1ceph osd pool create test-pool 128 在pool内创建文件在test-pool内创建一个名为rbdtest的文件 1rbd -p test-pool create rbdtest -s 1G 查看是否创建成功 12rbd ls -p test-poolrbdtest 使用nbd将块映射到宿主机因为ceph比较新，ubuntu16.04默认内核不支持一些新feture，所以这里就不用rbd进行map了。安装 1apt-get install rbd-nbd 12345rbd-nbd map test-pool/test-rbdrbd-nbd list-mappedid pool image snap device31755 test-pool rbdtest - /dev/nbd0 格式化 1mkfs.xfs /dev/nbd0 挂载并测试写入数据1mount /dev/nbd0 /mnt/ 1echo test &gt; /mnt/test 卸载 12umount /mntrbd-nbd unmap /dev/nbd0","categories":[{"name":"分布式存储","slug":"分布式存储","permalink":"http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"分布式存储","slug":"分布式存储","permalink":"http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"}]},{"title":"kubernetes pod的弹性伸缩(二)(基于自定义监控指标的弹性伸缩)","slug":"kubernetes_hpa2","date":"2019-11-25T13:45:59.000Z","updated":"2019-11-25T13:45:59.000Z","comments":true,"path":"2019/11/25/kubernetes_hpa2/","permalink":"http://yoursite.com/2019/11/25/kubernetes_hpa2/","excerpt":"","text":"实际生产环境中，通过CPU和内存的监控指标弹性伸缩不能很好的反映应用真实的状态，所以我们需要根据应用本身自定义一些监控指标来进行弹性伸缩，如web应用，根据当前QPS来进行弹性，在这里Kubernetes HPA本身也支持自定义监控指标。 自定义监控指标收集过程： pod 内置一个metrics 或者挂一个sidecar 当作exporter对外暴露。 Prometheus收集对应的监控指标。 Prometheus-adapter定期从prometheus收集指标对抓取的监控指标进行过滤和晒算，通过custom-metrics-apiserver将指标对外暴露。 HPA控制器从custom-metrics-apiserver获取数据。 部署prometheusclone代码 1git clone https://github.com/stefanprodan/k8s-prom-hpa 切换到k8s-prom-hpa目录 12kubectl create namespace monitoring kubectl create -f ./prometheus 查看prometheus 1234567kubectl get pod -n monitoringNAME READY STATUS RESTARTS AGEprometheus-64bc56989f-qcs4p 1/1 Running 1 22hkubectl get svc -n monitoringNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEprometheus NodePort 10.104.215.207 &lt;none&gt; 9090:31190/TCP 22h 访问prometheushttp://node_ip:31190 prometheus内prometheus-cfg.yaml 配置了自动发生规则，会自动将组件注册。 监控指标被prometheus收集 –&gt;Prometheus adapter变换指标格式–&gt;custom metrics apiserver–&gt;k8s hpa 部署kubernetes-prometheus-adapter进入k8s-prom-hpa目录生成Prometheus-adapter所需的TLS证书： 1make certs custom-metrics-api&#x2F;custom-metrics-apiservice.yaml中配置了insecureSkipTLSVerify: true选项，所以生成的证书是否受信任都无所谓。 查看output文件夹有以下文件 12ls output/apiserver.csr apiserver-key.pem apiserver.pem 部署k8s-prometheus-adapter 1kubectl create -f ./custom-metrics-api prometheus-adapter配置文件查看custom-metrics-apiserver-deployment.yaml文件 12345678910args: - /adapter - --secure-port=6443 - --tls-cert-file=/var/run/serving-cert/serving.crt //连接custom-metrics-apiserver的证书 - --tls-private-key-file=/var/run/serving-cert/serving.key - --logtostderr=true //日志标准错误输出 - --prometheus-url=http://prometheus.monitoring.svc:9090/ //连接prometheus的地址，因为prometheus部署在集群内，所以可以直接用Service地址 - --metrics-relist-interval=30s - --v=10 //日志debug等级，值越高日志越详细，可以适当调小 - --config=/etc/adapter/config.yaml //prometheus-adapter收集到prometheus监控指标后的过滤规则 custom-metrics-config-map.yaml文件 custom-metrics-config-map.yaml文件主要定义的是prometheus-adapter去prometheus获取指标的规则，因为prometheus的指标不能直接拿来用，需要通过prometheus-adapter进行一层中转和修改，然后将重新组装的指标通过自己接口暴露给custom-metrics-apiserver。 所以这个文件内的值，实际上定义的就是从prometheus抓取规则的语句。 123456789- seriesQuery: &#x27;&#123;namespace!=&quot;&quot;,__name__!~&quot;^container_.*&quot;&#125;&#x27; seriesFilters: - isNot: .*_seconds_total resources: template: &lt;&lt;.Resource&gt;&gt; name: matches: ^(.*)_total$ as: &quot;&quot; metricsQuery: sum(rate(&lt;&lt;.Series&gt;&gt;&#123;&lt;&lt;.LabelMatchers&gt;&gt;&#125;[1m])) by (&lt;&lt;.GroupBy&gt;&gt;) 这个文件内容分为两部分第一个部分是 rules，用于 custom metrics；第二部分是 resourceRules，用于 metrics。 Prometheus adapter，可以将 Prometheus 中的任何一个指标都用于 HPA，但需要在prometheus-adapter内定义查询语句将它拿到。如果只需要使用一个指标做 HPA，可以只写一条查询，而不需要像这里面使用了多个查询。 字段解释： seriesQuery：prometheus的查询语句 seriesFilters：指标过滤 is：需要筛选保留下来的。 isNot：需要过滤掉的。 resource：将指标中的标签和k8s资源对应起来有两种方式一种是用overrides方式 name：用来给对应的指重命名的，有些指标是递增的http_request但采集的原始指标是http_request_total，需要进行一层计算然后过滤掉_total matches：通过正则表达式来匹配指标名，可以进行分组； as：默认值为 $1，也就是第一个分组。as 为空就是使用默认值的意思,也就是去.*对应的值。 metricsQuery： metricsQuery字段是一个Go模板，对调用出来的prometheus指标进行特定的处理。 Series：指标名称 LabelMatchers：标签匹配列表。 1md：定义时间范围 总体来说就是获取多次http_request_total指标，然后进行处理计算过去1分钟内每秒http_request，最后结果返回为http_request指标。 查看k8s-prometheus-adapter部署情况 1234kubectl get pod -n monitoringNAME READY STATUS RESTARTS AGEcustom-metrics-apiserver-6c6c7f67d8-9vdkt 1/1 Running 0 4h58mprometheus-64bc56989f-qcs4p 1/1 Running 0 4h59m 查看创建的api组 123kubectl api-versions | grep metricscustom.metrics.k8s.io/v1beta1metrics.k8s.io/v1beta1 获取自定义监控指标 1yum install jq -y 1kubectl get --raw &quot;/apis/custom.metrics.k8s.io/v1beta1&quot; | jq . 123456789101112131415&#123; &quot;kind&quot;: &quot;APIResourceList&quot;, &quot;apiVersion&quot;: &quot;v1&quot;, &quot;groupVersion&quot;: &quot;custom.metrics.k8s.io/v1beta1&quot;, &quot;resources&quot;: [ &#123; &quot;name&quot;: &quot;namespaces/go_gc_duration_seconds_count&quot;, &quot;singularName&quot;: &quot;&quot;, &quot;namespaced&quot;: false, &quot;kind&quot;: &quot;MetricValueList&quot;, &quot;verbs&quot;: [ &quot;get&quot; ] &#125;, 部署Podinfo应用测试custom-metric autoscale 12kubectl create -f ./podinfo/podinfo-svc.yaml,./podinfo/podinfo-dep.yaml prometheus配置了自动发现规则，在podinfo-dep.yaml里面配置了对应的规则 123456789annotations: prometheus.io/scrape: &#x27;true&#x27;``` 所以应用一启动就能直接在prometheus-target中发现。 获取自定义监控指标 kubectl get –raw “&#x2F;apis&#x2F;custom.metrics.k8s.io&#x2F;v1beta1&#x2F;namespaces&#x2F;default&#x2F;pods&#x2F;*&#x2F;http_requests” | jq .{ “kind”: “MetricValueList”, “apiVersion”: “custom.metrics.k8s.io&#x2F;v1beta1”, “metadata”: { “selfLink”: “&#x2F;apis&#x2F;custom.metrics.k8s.io&#x2F;v1beta1&#x2F;namespaces&#x2F;default&#x2F;pods&#x2F;%2A&#x2F;http_requests” }, “items”: [ { “describedObject”: { “kind”: “Pod”, “namespace”: “default”, “name”: “podinfo-58b68656c9-b9cmg”, “apiVersion”: “&#x2F;v1” }, “metricName”: “http_requests”, “timestamp”: “2019-11-09T09:00:09Z”, “value”: “888m” }, { “describedObject”: { “kind”: “Pod”, “namespace”: “default”, “name”: “podinfo-58b68656c9-mr265”, “apiVersion”: “&#x2F;v1” }, “metricName”: “http_requests”, “timestamp”: “2019-11-09T09:00:09Z”, “value”: “911m” } ]} 1234配置podinfo custom hpa kubectl create -f .&#x2F;podinfo&#x2F;podinfo-hpa-custom.yaml 1234567891011121314151617```apiVersion: autoscaling/v2beta1kind: HorizontalPodAutoscalermetadata: name: podinfospec: scaleTargetRef: apiVersion: extensions/v1beta1 kind: Deployment name: podinfo minReplicas: 2 maxReplicas: 10 metrics: - type: Pods pods: metricName: http_requests targetAverageValue: 10 这里10指的是每秒10个请求，按照定义的规则metricsQuery中的时间范围1分钟，这就意味着过去1分钟内每秒如果达到10个请求则会进行扩容。 1234kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEpodinfo Deployment/podinfo 713m/10 2 10 8 18h 713m是什么意思？自定义API SERVER收到请求后会从Prometheus里面查询http_requests_total的值，然后把这个值换算成一个以时间为单位的请求率。713m的m就是milli-requests，按照定义的规则metricsQuery中的时间范围1分钟，大概每秒为0.71个请求 使用webbench进行压测 123webbench -c 100 http://172.31.48.86:31198/-c表示发送100个请求 查看hpa，可以看见请求数暴涨 123kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEpodinfo Deployment/podinfo 57610m/10 2 10 8 12h 查看HPA事件 123456kubectl describe hpa/podinfoType Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulRescale 33s horizontal-pod-autoscaler New size: 4; reason: pods metric http_requests above target Normal SuccessfulRescale 17s horizontal-pod-autoscaler New size: 8; reason: pods metric http_requests above target Normal SuccessfulRescale 2s horizontal-pod-autoscaler New size: 10; reason: pods metric http_requests above target","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"K3S安装","slug":"k3s_install","date":"2019-10-28T13:45:59.000Z","updated":"2019-10-28T13:45:59.000Z","comments":true,"path":"2019/10/28/k3s_install/","permalink":"http://yoursite.com/2019/10/28/k3s_install/","excerpt":"","text":"环境信息|软件名称 | 软件版本|| :—-: | :—-: | :—-:| k3s| v0.9.1 | 安装k3s在线安装执行在线安装脚本，会自动识别环境，拉取镜像，安装k3s安装server端： 1curl -sfL https://get.k3s.io | sh - 安装agent端：获取连接server端的token 12/var/lib/rancher/k3s/server/node-token k3s agent -s https://master_ip:6443 -t token agent连接server端 1curl -sfL https://get.k3s.io | K3S_URL=https://myserver:6443 K3S_TOKEN=XXX sh - 验证 123456k3s kubectl get nodeNAME STATUS ROLES AGE VERSIONubuntu-server Ready master 86m v1.15.4-k3s.1ubuntu-agent Ready worker 86m v1.15.4-k3s.1 卸载执行 1/usr/local/bin/k3s-uninstall.sh 或k3s-agent-uninstall.sh 注意：在线安装有pause镜像会去k8s.gcr.io仓库拉取，在国内网络环境连接会受阻。 离线安装准备下载离线包在github，k3s release页下载对应的离线镜像 1https://github.com/rancher/k3s/releases/ 根据机器类型，下载对应的文件 文件 作用 机器类型 k3s k3s 可执行文件 x86 CPU架构64位系统 k3s-arm64 k3s可执行文件 ARM CPU架构 64位系统 k3s-armhf k3s可执行文件 ARM CPU架构 32位系统 k3s-airgap-images-amd64.tar 离线镜像包 x86 CPU架构64位系统 k3s-airgap-images-arm64.tar 离线镜像包 ARM CPU架构 64位系统 k3s-airgap-images-arm.tar 离线镜像包 ARM CPU架构 32位系统 选择对应的操作系统版本，下载对应的k3s可执行文件和离线镜像包。 注：官方的Raspbian Buster Lite系统仍然是32位。 需要提前在打开操作系统cgroup ubuntu18.04方法： 123sudo vi /boot/firmware/cmdline.txtcgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory Raspbian Buster Lite系统方法： 12vi /boot/cmdline.txtcgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory 重启生效。 创建镜像存储目录 1mkdir -p /var/lib/rancher/k3s/agent/images/ 拷贝离线镜像件 1sudo cp ./k3s-airgap-images-$ARCH.tar /var/lib/rancher/k3s/agent/images/ 拷贝可执行文到&#x2F;usr&#x2F;bin目录，并配置可执行权限 12cp k3sxxx /usr/bin/k3schmoad a+x /usr/bin/k3s 启动 k3s会自动解压并导入镜像文件 1k3s server 12rm /etc/rancher/k3s/k3s.yaml rm /var/lib/rancher/ -rf 查看集群 123k3s kubectl get nodeNAME STATUS ROLES AGE VERSIONubuntu Ready master 14s v1.15.4-k3s.1 agent连接在server端获取agent连接server的token 1cat /var/lib/rancher/k3s/server/node-token 常见命令列出当前集群镜像 1k3s crictl images 列出当前集群容器 1k3s crictl ps -a 1k3s crictl ps 查看pod占用cpu 内存资源 1k3s crictl stats FAQ不使用containerd直接docker安装docker后 将离线镜像导入 1docker load &lt; k3s-airgap-images-arm64.tar 执行k3s server 1k3s server --docker 使用systemd启动k3s server创建systemd文件 1vim /etc/systemd/system/k3s.service 12345678910111213141516171819202122[Unit]Description=Lightweight KubernetesDocumentation=https://k3s.ioAfter=network-online.target[Service]Type=$&#123;SYSTEMD_TYPE&#125;EnvironmentFile=$&#123;FILE_K3S_ENV&#125;ExecStartPre=-/sbin/modprobe br_netfilterExecStartPre=-/sbin/modprobe overlayExecStart=$&#123;BIN_DIR&#125;/k3s \\\\ $&#123;CMD_K3S_EXEC&#125;KillMode=processDelegate=yesLimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinityTasksMax=infinityTimeoutStartSec=0Restart=alwaysRestartSec=5s[Install]WantedBy=multi-user.target 根据实际情况修改ExecStar,server用server对应的命令，agent用agent对应指令。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"Istio(三)通过Rancher使用Istio","slug":"istio-3","date":"2019-10-01T13:45:59.000Z","updated":"2019-10-01T13:45:59.000Z","comments":true,"path":"2019/10/01/istio-3/","permalink":"http://yoursite.com/2019/10/01/istio-3/","excerpt":"","text":"环境信息： 组件 版本 Kubernetes 1.15.5 Istio 1.4.3 Rancher 2.3.4 概述Rancher2.3版本已经集成Istio了，通过Rancher2.3可以非常简单方便和直观的对Istio规则进行配置和管理。 部署Rancher参考官方手册，进行单节点部署，或HA部署。 https://docs.rancher.cn/rancher2x/quick-start.html 启用Istio进入Rancher后在对集群工具选项中启用Istio。 默认是没有启用Istio-gateway，可以自行启用 启用Istio后Rancher会在System项目下部署Istio组件 在对应项目的资源下可以看见已经启用的Istio，在这可以配置Istio规则对应图表的查看 测试使用继续以Bookinfo为例，通过Rancher UI配置对应的规则部署bookinfo 1kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml 在对应的default命名空间启用sidecar自动注入 部署bookinfo 1kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml 因为本次没有启用Istio-Gateway，所以直接将bookinfo的productpage通过nodeport方式暴露出来 1kubectl patch svc productpage -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&#x27; 访问 http://172.16.1.6:32632/productpage 流量监控查看，这个主要从prometheus处获取 内嵌kiali监控图 其他对应的如grafana、kiali、jaeger、prometheus的访问界面可以直接点击左上方图标即可进入。 实现动态请求路由配置路由请求策略将全部访问流量指向reviews v1版本定义destinationrule配置对应的版本和label关联 创建VirtualService服务，定义http路由访问V1版本，权重设置100 访问http://172.16.1.6:32632/productpage不断刷新页面可以看见一直显示v1版本 实现灰度发布在这里实现两种灰度发布的方式 方式一：通过http的head将请求路由到不同的后端服务上比如我们这里将用户标识进行请求路由，比如将用户名为jason的请求路由到Reviews service v2的版本上其他用户还是轮询访问 删除之前定义的VirtualService规则 定义新规则根据http的头部headers配置其匹配规则 创建两个http路由，一个定义匹配规则，规则为基于http的头的end-user，精确匹配为jason则，访问到reviews v2版本，当http的头的end-user字段非jason时访问V1版本。 规则定义在请求头的end-user字段为Jason时将请求转发到Reviews-server v2版本在UI上通过sign in登录输入用户名jason，不用输入密码刷新页面可以看见无论怎么刷新都还是黑星星退出登录或用其他用户名登录访问都还是Reviews-server v1版本(没星星) 方式二：将访问流量分比例，比如百分之80流量访问Reviews-server v1版本，百分之20流量访问Reviews-server v2版本 删除刚刚定义的virtualService规则，建立新规则，在http路由中添加另外一个hosts，设置流量访问权重，V1版本权重为80，V2版本权重为20 不断刷新页面，你会发现Review只会显示没有星星和黑星星，并且黑星星的显示次数比没星星要少很多。 查看kiali看见流量走向Reviews-server v1和v2版本 熔断配置部署httpbin 1kubectl apply -f samples/httpbin/httpbin.yaml 创destinationrule断路器 定义连接池HTTP1最大等待请求设置为1每个连接的HTTP最大请求设置为1TCP最大连接数设置为1 异常检测基本驱逐时间设置为1连续错误设置为1时间间隔为1s最大驱逐百分比为100% 部署fortio进行测试 1kubectl apply -f samples/httpbin/sample-client/fortio-deploy.yaml 1FORTIO_POD=$(kubectl get pod | grep fortio | awk &#x27;&#123; print $1 &#125;&#x27;) 执行fortio用3个连接20个请求进行测试 1kubectl exec -it $FORTIO_POD -c fortio /usr/bin/fortio -- load -c 3 -qps 0 -n 20 -loglevel Warning http://httpbin:8000/get 多连接几次，可以看见服务降级生效，请求都被拒绝了。 12345678&gt; 0.001 &lt;= 0.00147963 , 0.00123982 , 100.00, 3# target 50% 0.00072548# target 75% 0.000921566# target 90% 0.00115988# target 99% 0.00144765# target 99.9% 0.00147643Sockets used: 20 (for perfect keepalive, would be 3)Code 503 : 20 (100.0 %) 总结通过rancher可以相对简单的把Istio用起来，但实际上Istio本身的配置复杂对于大部分使用者还是有一定门槛，相信在后期的版本中应该能得到一定解决。","categories":[{"name":"ServiceMesh","slug":"ServiceMesh","permalink":"http://yoursite.com/categories/ServiceMesh/"}],"tags":[{"name":"ServiceMesh","slug":"ServiceMesh","permalink":"http://yoursite.com/tags/ServiceMesh/"}]},{"title":"Istio(二)通过Istio实现服务治理","slug":"istio-2","date":"2019-08-31T13:45:59.000Z","updated":"2019-08-31T13:45:59.000Z","comments":true,"path":"2019/08/31/istio-2/","permalink":"http://yoursite.com/2019/08/31/istio-2/","excerpt":"","text":"环境信息： 组件 版本 Kubernetes 1.15.5 Istio 1.4.2 helm 2.16.1 服务治理sidecar的注入前面提到，通过ServiceMesh进行微服务治理和传统的SDK的区别在于，ServiceMesh主要是依靠在应用POD中附加透明代理方式实现。下面介绍下istio是如何实现透明代理注入的。 sidecar的注入方式有两种：1、手动方式注入手动注入其实就是修改deployment的pod template添加另外一个sidecar容器 1istioctl kube-inject -f samples/sleep/sleep.yaml | kubectl apply -f - 2、使用自动方式注入原理：Kubernetes 调用时，admissionregistration.k8s.io&#x2F;v1beta1会进行配置。会在带有 istio-injection&#x3D;enabled 标签的命名空间中选择 Pod进行sidercar注入配置sidecar自动注入是使用了mutating webhook admission controller实现的，在Kubernetes1.9+的版本才实现此功能,需要对pod实现此功能需要给对应的namespace打上istio-injection&#x3D;enabled 的标签 1kubectl label namespace default istio-injection=enabled 查看是否生效 123456kubectl get namespace -L istio-injectionNAME STATUS AGE ISTIO-INJECTIONdefault Active 20h enabledistio-system Active 19h kube-public Active 20h kube-system Active 20h 这样就会在 Pod 创建时触发 Sidecar 的注入过程了。运行测试容器 1kubectl apply -f samples/sleep/sleep.yaml 查看POD内是否有两个container 123kubectl get pod NAME READY STATUS RESTARTS AGEsleep-86cf99dfd6-ffmct 2/2 Running 0 13m 查看POD的详细信息，发现多了个istio-proxy的container及其配置信息 1kubectl describe pod/sleep-86cf99dfd6-ffmct 需要关闭自动注入功能，只需要将这个标签去掉即可 1kubectl label namespace default istio-injection- 然后新建的POD就不带sidercar容器了 概念讲解Gateway： 服务入口流量控制，可与VirtualService绑定然后通过Istio规则来控制流量，早期使用kubernetes的ingress，后因为ingress的功能原因，又独立弄了套istio-Gateway，Istio Gateway 只配置四层到六层的功能，通常也会绑定VirtualServer实现七层转发规则。 VirtualService： 定义转发到后端应用的路由规则，比如流量拆分百分比。 DestinationRule：所定义了经过VirtualService处理之后的流量的访问策略。如定义负载均衡配置、连接池大小以及外部检测（用于在负载均衡池中对不健康主机进行识别和驱逐）配置。 bookinfo应用部署以官方微服务应用Bookinfo为例演示istio的应用治理能力bookinfo 应用程序显示的有关书籍的信息，类似于在线书店的单个商品。应用页面上显示的是书籍的描述、书籍详细信息（ISBN，页数等）以及书评。 bookinfo 应用一共包含四个微服务：Productpage、Details、Reviews、Ratings。 Productpage： 使用 Python 开发，负责展示书籍的名称和书籍的简介。Details： 使用 Ruby 开发，负责展示书籍的详细信息。Reviews： 使用 Java开发，负责显示书评（Reviews分为3个版本分别对应V1、V2、V3对应的显示就是没有星星，黑星星、红星星。Ratings： 使用 Node.js 开发,负责显示书籍的评星。 部署bookinfo 1kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml 12345678kubectl get deploymentNAME READY UP-TO-DATE AVAILABLE AGEdetails-v1 1/1 1 1 13mproductpage-v1 1/1 1 1 13mratings-v1 1/1 1 1 13mreviews-v1 1/1 1 1 13mreviews-v2 1/1 1 1 13mreviews-v3 1/1 1 1 13m 通过Istio实现bookinfo服务治理通过istio-Gateway暴露bookinfo12345678910111213141516171819202122232425262728293031323334353637383940414243cat &lt;&lt;EOF | kubectl apply -f -apiVersion: networking.istio.io/v1alpha3kind: Gatewaymetadata: name: bookinfo-gatewayspec: selector: istio: ingressgateway # 关联istio-system namespace下istio-ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - &quot;*&quot;---apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: bookinfospec: hosts: - &quot;*&quot; gateways: - bookinfo-gateway #关联Gateway对象 http: - match: - uri: exact: /productpage #精确匹配 - uri: prefix: /static #前缀匹配 - uri: exact: /login - uri: exact: /logout - uri: prefix: /api/v1/products route: - destination: host: productpage.default.svc.cluster.local #对应kubernetes的Service，注意使用服务的短名称时Istio会根据规则所在的命名空间来处理这一名称，而非服务所在的命名空间。 port: number: 9080EOF 访问istio-gateway对应的nodeport端口 123kubectl get svc/istio-ingressgateway -n istio-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEistio-ingressgateway NodePort 10.43.187.106 &lt;none&gt; 15020:30485/TCP,80:31380/TCP,443:31390/TCP,31400:31400/TCP,15029:31464/TCP,15030:31827/TCP,15031:30522/TCP,15032:31907/TCP,15443:32151/TCP 5h38m 如访问：http://172.16.1.6:31380/productpage，出现bookinfo页 左侧评Reviews分为3个版本分别对应V1、V2、V3对应的显示就是没有星星，黑星星、红星星。可以通过刷新页面进行切换显示 V1 空星星显示页 V2 黑星星显示页 V3 红星星显示页 通过kiali查看流量拓扑创建kiali登录的帐号和密码 1kubectl create secret generic kiali-secret -n istio-system --from-literal &quot;username=admin&quot; --from-literal &quot;passphrase=admin&quot; 动态请求路由配置路由请求策略将全部访问流量指向reviews v1版本 123456789101112131415161718192021222324252627282930313233cat &lt;&lt;EOF | kubectl apply -f -apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: reviewsspec: hosts: - reviews http: - route: - destination: host: reviews.default.svc.cluster.local subset: v1 ---apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata: name: reviewsspec: host: reviews.default.svc.cluster.local subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 - name: v3 labels: version: v3EOF DestinationRule内定义与后端服务的关联，这里的v1、v2、v3对应的label实际上就是对应kubernetes的workload的label 12345678kubectl get deployment --show-labelsNAME READY UP-TO-DATE AVAILABLE AGE LABELSratings-v1 1/1 1 1 23h app=ratings,version=v1reviews-v1 1/1 1 1 23h app=reviews,version=v1reviews-v2 1/1 1 1 23h app=reviews,version=v2reviews-v3 1/1 1 1 23h app=reviews,version=v3 然后在去访问bookinfo页面发现Book Reviews处怎么刷新都还是空白打开Kiali可以看见绿色代表流量走向的箭头已经指向reviews v1，对应的是 查看kiali查看流量拓扑，流量都指向V1 实现灰度发布在这里实现两种灰度发布的方式 方式一：通过http的head将请求路由到不同的后端服务上比如我们这里将用户标识进行请求路由，比如将用户名为jason的请求路由到Reviews service v2的版本上其他用户还是轮询访问 清空之前定义的规则 1kubectl delete VirtualService/reviews 然后定义新规则 12345678910111213141516171819202122cat &lt;&lt;EOF | kubectl apply -f -apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: reviewsspec: hosts: - reviews http: - match: - headers: end-user: exact: jason route: - destination: host: reviews subset: v2 - route: - destination: host: reviews subset: v1EOF 规则定义在请求头的end-user字段为Jason时将请求转发到Reviews-server v2版本在UI上通过sign in登录输入用户名jason，不用输入密码刷新页面可以看见无论怎么刷新都还是黑星星退出登录或用其他用户名登录访问都还是Reviews-server v1版本(没星星) 方式二：将访问流量分比例，比如百分之80流量访问Reviews-server v1版本，百分之20流量访问Reviews-server v2版本先将刚才demo的规则清除 1kubectl delete virtualservice/reviews 定义新的路由规则 12345678910111213141516171819cat &lt;&lt;EOF | kubectl apply -f -apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: reviewsspec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 weight: 80 - destination: host: reviews subset: v2 weight: 20EOF 不断刷新页面，你会发现Review只会显示没有星星和黑星星，并且黑星星的显示次数比没星星要少很多。查看kiali看见流量走向Reviews-server v1和v2版本 熔断在微服务中，服务被拆成一个个服务模块，各个服务模块之间有大量的调用，为了保证不会因为某个服务访问流量过载而影响其他服务导致大规模集群故障，所以通过熔断机制来保证服务的可靠性。 熔断的实现根我们日常生活中电路中的保险开关是一样的，通过熔断器对应用服务进行检测，当下游服务因访问压力过大而响应变慢或失败，上游服务为了保护系统整体的可用性，可以暂时切断对下游服务的调用。 熔断器配置：在时间窗口内，接口调用超时比率达到一个阈值，会开启熔断。进入熔断状态后，后续对该服务接口的调用会被拒绝，直接进入服务降级模式。 熔断恢复：当经过了对应的时间之后，服务将从熔断状态恢复过来，再次接受调用。 通过istio实现熔断启动httpbin测试应用httpbin是一个web服务，我们将它是部署到kubernetes中然后通过fortio模拟请求访问 12kubectl apply -f samples/httpbin/httpbin.yaml 创建断路器 1234567891011121314151617181920cat &lt;&lt;EOF | kubectl create -f -apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata: name: httpbinspec: host: httpbin trafficPolicy: connectionPool: tcp: maxConnections: 1 http: http1MaxPendingRequests: 1 maxRequestsPerConnection: 1 outlierDetection: consecutiveErrors: 1 interval: 1s baseEjectionTime: 3m maxEjectionPercent: 100EOF 在本步骤中，我们可以理解为Istio的熔断功能主要是通过在链接池中加入上述三个参数：· MaxConnections定义了到目标主机的 HTTP1&#x2F;TCP 最大连接数；· http1MaxPendingRequests定义了针对一个目标的 HTTP 请求的最大排队数量；· maxRequestsPerConnection定义了对某一后端的请求中，一个连接内能够发出的最大请求数量。如果将这一参数设置为 1 则会禁止 keep alive 特性。 接下来创建一个客户端，用来向后端服务发送请求，观察是否会触发熔断策略。这里要使用一个简单的负载测试客户端，名字叫 fortio。这个客户端可以控制连接数量、并发数以及发送 HTTP 请求的延迟。这里我们会把给客户端也进行 Sidecar 的注入，以此保证 Istio 对网络交互的控制： 1kubectl apply -f samples/httpbin/sample-client/fortio-deploy.yaml 接下来就可以登入客户端 Pod 并使用 Fortio 工具来调用 httpbin。-curl 参数表明只调用一次 1FORTIO_POD=$(kubectl get pod | grep fortio | awk &#x27;&#123; print $1 &#125;&#x27;) 1kubectl exec -it $FORTIO_POD -c fortio /usr/bin/fortio -- load -curl http://httpbin:8000/get 123456789101112131415161718192021222324HTTP/1.1 200 OKserver: envoydate: Sun, 02 Feb 2020 08:49:27 GMTcontent-type: application/jsoncontent-length: 371access-control-allow-origin: *access-control-allow-credentials: truex-envoy-upstream-service-time: 3&#123; &quot;args&quot;: &#123;&#125;, &quot;headers&quot;: &#123; &quot;Content-Length&quot;: &quot;0&quot;, &quot;Host&quot;: &quot;httpbin:8000&quot;, &quot;User-Agent&quot;: &quot;fortio.org/fortio-1.3.1&quot;, &quot;X-B3-Parentspanid&quot;: &quot;ea1d6bd8e1ac8621&quot;, &quot;X-B3-Sampled&quot;: &quot;0&quot;, &quot;X-B3-Spanid&quot;: &quot;cef333d2e32ce6b4&quot;, &quot;X-B3-Traceid&quot;: &quot;d89ae2b658f513b3ea1d6bd8e1ac8621&quot; &#125;, &quot;origin&quot;: &quot;127.0.0.1&quot;, &quot;url&quot;: &quot;http://httpbin:8000/get&quot;&#125; 这表明我们创建的客户端已经成功与服务端进行了一次通信。在kiali上可以看见httpbin连接路由图。请求都是返回正常的状态码为200 验证熔断在上面的熔断设置中指定了 maxConnections&#x3D;1 以及 http1MaxPendingRequests&#x3D;1。这意味着如果超过了一个连接同时发起请求，Istio 就会熔断，阻止后续的请求或连接。我们不妨尝试通过并发2个连接(-c 2)发送20个请求数(-n 20)来看一下结果。 1kubectl exec -it $FORTIO_POD -c fortio /usr/bin/fortio -- load -c 2 -qps 0 -n 20 -loglevel Warning http://httpbin:8000/get 12345678...Sockets used: 17 (for perfect keepalive, would be 2)Code 200 : 3 (15.0 %)Code 503 : 17 (85.0 %)Response Header Sizes : count 20 avg 34.5 +/- 82.13 min 0 max 230 sum 690Response Body/Total Sizes : count 20 avg 226.3 +/- 159.6 min 153 max 601 sum 4526All done 20 calls (plus 0 warmup) 5.768 ms avg, 344.2 qps 基本上所有的请求都发送成功了。明明我们设置的最大连接数是1，而我们模拟了两个并发连接，理论上应该只有一半的请求能成功才对，难道熔断没有成功？这里别忘了我们还设置了http1MaxPendingRequests&#x3D;1，正如在前文中介绍的，这个参数的功能类似于为最大连接数提供了一级缓存，所以虽然我们的最大连接数是1，但是因为这个参数也为1，所以两个并发连接的请求都可以发送成功。 接下来我们需要修改请求连接数，将连接数提高到3(-c 3)，请求数不变 1kubectl exec -it $FORTIO_POD -c fortio /usr/bin/fortio -- load -c 3 -qps 0 -n 20 -loglevel Warning http://httpbin:8000/get ­这时候可以观察到熔断行为生效了 1234...Code 200 : 13 (65.0 %)Code 503 : 7 (35.0 %) 有百分之65的请求通过了，百分之35的请求被拒绝了 重复执行上述测试命令后，进入服务降级模式 123...Sockets used: 20 (for perfect keepalive, would be 3)Code 503 : 20 (100.0 %) 黑白名单定义个黑名单，禁止Review-v3访问后端ratings服务，所以就不会显示红星星 123456789101112131415161718192021222324252627282930cat &lt;&lt;EOF | kubectl apply -f -apiVersion: &quot;config.istio.io/v1alpha2&quot;kind: handlermetadata: name: denyreviewsv3handlerspec: compiledAdapter: denier params: status: code: 7 message: Not allowed---apiVersion: &quot;config.istio.io/v1alpha2&quot;kind: instancemetadata: name: denyreviewsv3requestspec: compiledTemplate: checknothing---apiVersion: &quot;config.istio.io/v1alpha2&quot;kind: rulemetadata: name: denyreviewsv3spec: match: destination.labels[&quot;app&quot;] == &quot;ratings&quot; &amp;&amp; source.labels[&quot;app&quot;]==&quot;reviews&quot; &amp;&amp; source.labels[&quot;version&quot;] == &quot;v3&quot; actions: - handler: denyreviewsv3handler instances: [ denyreviewsv3request ]EOF 重点关注match: destination.labels[&quot;app&quot;] == &quot;ratings&quot; &amp;&amp; source.labels[&quot;app&quot;]==&quot;reviews&quot; &amp;&amp; source.labels[&quot;version&quot;] == &quot;v3&quot;这句配置，定义了源和目标，源为label为version&#x3D;&#x3D;v3的workload，目标为label为app&#x3D;&#x3D;reviews的workload。应用后去访问bookinfo发现,不断刷新，访问到v3时你会发现如下图显示。 https://jimmysong.io/istio-handbook/concepts/traffic-management-basic.htmlhttps://www.servicemesher.com/istio-handbook/best-practices/how-to-implement-ingress-gateway.html","categories":[{"name":"ServiceMesh","slug":"ServiceMesh","permalink":"http://yoursite.com/categories/ServiceMesh/"}],"tags":[{"name":"ServiceMesh","slug":"ServiceMesh","permalink":"http://yoursite.com/tags/ServiceMesh/"}]},{"title":"Istio(一)部署安装体验","slug":"Istio-1","date":"2019-07-31T13:45:59.000Z","updated":"2019-07-31T13:45:59.000Z","comments":true,"path":"2019/07/31/Istio-1/","permalink":"http://yoursite.com/2019/07/31/Istio-1/","excerpt":"","text":"环境信息： 组件 版本 Kubernetes 1.15.5 Istio 1.4.2 helm 2.16.1 Helm方式安装下载对应的release版本 1wget https://github.com/istio/istio/releases/download/1.4.2/istio-1.4.2-linux.tar.gz 解压 1tar -xvf istio-1.4.2-linux.tar.gz 将解压后的文件客户端工具放置到&#x2F;usr&#x2F;local&#x2F;bin目录下 1cp istio-1.0.5/bin/istioctl /usr/local/bin/ 使用helm方式安装 https://istio.io/docs/setup/install/helm/有多种安装参数选择，我们这里使用自定义方式，需要安装一些如grafana、Jaeger、kiali、Gateway组件。 1、创建namespace 1kubectl create namespace istio-system 2、安装和配置istio所需要的CRD 1helm install install/kubernetes/helm/istio-init --name istio-init --namespace istio-system 3、确认CRD资源创建完成 1kubectl -n istio-system wait --for=condition=complete job --all 4、使用Helm安装istio 1helm install install/kubernetes/helm/istio --set kiali.enabled=true --set gateways.istio-ingressgateway.type=NodePort --set tracing.enabled=true --set grafana.enabled=true --set gateways.istio-egressgateway.type=NodePort --name istio --namespace istio-system 5、复制并使用istioclt 12cp bin/istioctl /usr/bin/ chmod a+x /usr/bin/istioctl https://istio.io/docs/setup/additional-setup/config-profiles/ 安装完以后查看deployment和service 1234567891011121314kubectl get deployment -n istio-systemNAME READY UP-TO-DATE AVAILABLE AGEgrafana 1/1 1 1 2d20histio-citadel 1/1 1 1 2d20histio-galley 1/1 1 1 2d20histio-ingressgateway 1/1 1 1 2d20histio-pilot 1/1 1 1 2d20histio-policy 1/1 1 1 2d20histio-sidecar-injector 1/1 1 1 2d20histio-telemetry 1/1 1 1 2d20histio-tracing 1/1 1 1 2d20hkiali 1/1 1 1 2d20hprometheus 1/1 1 1 2d20h 1234567891011121314151617kubectl get svc -n istio-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEgrafana ClusterIP 10.43.187.158 &lt;none&gt; 3000/TCP 2d20histio-citadel ClusterIP 10.43.142.36 &lt;none&gt; 8060/TCP,15014/TCP 2d20histio-galley ClusterIP 10.43.124.145 &lt;none&gt; 443/TCP,15014/TCP,9901/TCP 2d20histio-ingressgateway NodePort 10.43.73.211 &lt;none&gt; 15020:32240/TCP,80:31380/TCP,443:31390/TCP,31400:31400/TCP,15029:32216/TCP,15030:31943/TCP,15031:32118/TCP,15032:32435/TCP,15443:31343/TCP 2d20histio-pilot ClusterIP 10.43.72.148 &lt;none&gt; 15010/TCP,15011/TCP,8080/TCP,15014/TCP 2d20histio-policy ClusterIP 10.43.19.133 &lt;none&gt; 9091/TCP,15004/TCP,15014/TCP 2d20histio-sidecar-injector ClusterIP 10.43.144.147 &lt;none&gt; 443/TCP,15014/TCP 2d20histio-telemetry ClusterIP 10.43.89.78 &lt;none&gt; 9091/TCP,15004/TCP,15014/TCP,42422/TCP 2d20hjaeger-agent ClusterIP None &lt;none&gt; 5775/UDP,6831/UDP,6832/UDP 2d20hjaeger-collector ClusterIP 10.43.29.41 &lt;none&gt; 14267/TCP,14268/TCP,14250/TCP 2d20hjaeger-query ClusterIP 10.43.145.48 &lt;none&gt; 16686/TCP 2d20hkiali ClusterIP 10.43.215.248 &lt;none&gt; 20001/TCP 2d20hprometheus ClusterIP 10.43.161.98 &lt;none&gt; 9090/TCP 2d20htracing ClusterIP 10.43.204.201 &lt;none&gt; 80/TCP 2d20hzipkin ClusterIP 10.43.45.55 &lt;none&gt; 9411/TCP 2d20h 默认grafana、Jaeger、kiali的service是cluster-ip类型的，若需要方便外部访问，可以将其修改为nodeport类型或使用ingress对外暴露 123456789kubectl patch svc grafana -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&#x27; -n istio-system kubectl patch svc kiali -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&#x27; -n istio-system kubectl patch svc kiali -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&#x27; -n istio-system kubectl patch svc jaeger-query -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&#x27; -n istio-system 访问 grafana:在这里将prometheus收集过来的各类监控指标图表化，包含对各个后端的请求速率、访问成功率，资源使用统计等。 Kialikiali是一个Redhat开源的Service mesh可视化工具，它可以看见应用模块之间的拓扑图、流量走向图、健康检查状态等 Jaegerjaeger是CNCF基金会管理项目主要用于istio中的分布式链路追踪，服务依赖分析等 卸载 1234helm delete --purge istiohelm delete --purge istio-inithelm delete --purge istio-cnikubectl delete namespace istio-system 1kubectl delete -f install/kubernetes/helm/istio-init/files 参考链接： https://jimmysong.io/istio-handbook/setup/istio-installation.html https://istio.io/docs/setup/install/helm/","categories":[{"name":"ServiceMesh","slug":"ServiceMesh","permalink":"http://yoursite.com/categories/ServiceMesh/"}],"tags":[{"name":"ServiceMesh","slug":"ServiceMesh","permalink":"http://yoursite.com/tags/ServiceMesh/"}]},{"title":"ServiceMesh简介","slug":"ServiceMesh","date":"2019-03-31T13:45:59.000Z","updated":"2019-03-31T13:45:59.000Z","comments":true,"path":"2019/03/31/ServiceMesh/","permalink":"http://yoursite.com/2019/03/31/ServiceMesh/","excerpt":"","text":"什么是ServiceMeshService mesh中文名服务网格，最早由Buoyant公司的CEO Willian Morgan提出(2016年)是一个专门用于处理服务之间通信的基础设施层，主要来解决应用微服务化后应用治理问题。 特点 基础设施：完全根应用分离服务治理下沉到基础设施层； 透明无侵入：以sidecar模式插入轻量级的透明网络代理，应用程序间通讯的中间； Service mesh能做什么？优缺点?当传统的单体架构不能满足日益增涨的功能需求，我们需要将应用模块拆分应用架构调整转向微服务架构；微服务能极大的提高应用的灵活性和伸缩性，但同时带来一些新的问题 服务调用链路增长，出现问题时跟踪和分析难度加大； 不同服务之前存在互相交互，需要更好控制和协调能力； 拆分了多个服务，需要将工作流分担到多个模块上； 当一个服务模块出现问题时，能尽量不影响其他不影响其他服务模块； 所以我们需要一个微服务治理的东西，来解决引入微服务后所带来的问题。 SpringCloud顺势登场 ,Spring Cloud出自Pivotal公司，它整合了Netflix OSS套件，开发人员写代码时只需要去使用对应的SDK ，就可以实现服务注册、服务发现、负载均衡、熔断等功能。SpringCloud缺点: 组件繁多，学习成本很高(用户需求不是掌握和精通各类微服务框架，而是开发微服务应用。实现应用治理)； 服务治理功能不全； 代码侵入性强； 可维护性差（升级需要重新编译)； 只支持JAVA语言不支持其他编程语言； Service mesh作用： 服务模块链路追踪 流量负载均衡 服务熔断机制 动态路由 微服务之间安全加密通信 流量控制… 主要解决应用微服务化后，应用治理问题。 优点： 透明代理代码无侵入; 不绑定编程语言，支持多种编程语言； 与代码解耦，可单独维护和升级； 云原生，完美兼容Kubernetes和其他云原生应用； 将之前通过代码和封装的类实现的功能，抽象出来下降变成了基础设施层，这样做的好处是，开发者可以专注写代码而不用关心服务模块之间治理。 缺点： 性能问题，通过sidecar调用，多了层转发; 学习成本，功能繁多，学习成本高; 运维成本，经过多次转发出现问题后排查后更复杂; Service mesh功能的实现Service mesh通过sidecar方式(车斗)，Sidecar 作为一个单独的进程与业务服务部署在一起，在Kubernetes中部署在同一个POD中，不同container，共享网络栈。对应用来说它是透明的,所有的流量都会通过它，所以对应用程序流量的控制都可以在控制平面下发，在数据平面的sidecar中实现。 大量服务互相调用访问时，它们之间的连接关系就形成了一个网格; sidecar方式和微服务框架SDK方式优缺点对比 特性 SDK sidecar 代码侵入性 需要修改代码，使用对应的类 透明代理方式，对应用无感知 多开发语言支持 只支持SDK支持的语言，比如springcloud只支持java 支持多种编程语言 学习成本 成熟的微服务框架，文档丰富 服务治理，下层到基础设施，运维学习成本高 性能 代码层接入，性能好，基本上没有性能损耗 多了层透明代理转发，性能相对更差 Service mesh的演进史Linkerdhttps://github.com/linkerd/linkerdBuoyant公司产品，scale语言编写运行于JVM，底层是基于Twitter的Finagle库做扩展，2016年发布最早一个0.0.7版本也是业内最早将Service mesh概念实现的产品，linkerd算是数据面产品，控制面由Namerd实现。 2016年1月15日，发布第一个0.07版本，2017年1月23日，Linkerd加入CNCF，2017年4月25日，Linkerd1.0版本发布2017 年 4 月 25 日，Linkerd 1.0 版本发布2017 年 7 月 11 日，Linkerd 发布版本 1.1.1，宣布和 Istio 项目集成，做istio数据面 最早的Service mesh实现，随后因为scale语言编写运行于JVM的性能问题和和缺少控制平面敌不过基于istio被放弃，转而用Rust开发Conduit。 ConduitConduit https://github.com/linkerd/linkerd2Buoyant公司产品，采用Rust+go语言编写，2017年12月5日， Conduit诞生 0.1.0版本发布，主要面向Kubernetes。 linkerd原作者抛弃了 Linkerd, 使用Rust重新编写了sidecar, conduit 各方面的设计理念与 Istio 非常类似，分外控制面和数据面控制面。数据面叫做 Conduit Data Plane由Rust语言编写，控制面则由Go编写叫Conduit Control Plane。 Conduit的优势在于整个数据平面采用Rust语编写，非常轻量级和占用资源极低。 2017年12月5日，发布第一个0.1.0版本；2018年7月6日，发布0.5版本，宣布这将是Conduit的最后一个版本，将合并到Linkerd2； Envoyhttps://github.com/envoyproxy/envoyLyft公司2016年9月开源的高性能网络代理采用C++语言编写，业内第二个开源的Service mesh产品，根linkerd1.0一样也是一个数据平面产品， 2016年9月13日，发布第一个1.0.0版本2017年9月14日，Envoy加入CNCF2016年中，istio诞生google、IBM、lyft联手开始istio项目，lyft以贡献envoy的方式加入。 Istiohttps://github.com/istio/istio google联合IBM和Lyft发布istio，控制面采用go语言编写，数据面采用C++语言编写的Envoy实现。2016年中，Istio诞生2017年5月24日，第一个0.1.0产品发布2018年7月31日，Istio1.0 release版本发布，可正式生产用 在社区热度 连接：通过简单规则配置可以进行流量细粒度控制、设置负载均衡、熔断、灰度发布、重试；安全：服务间的通信加密、认证；控制：服务通信的速率限制；观测：分布式链路追踪，服务监控指标收集； Istio 是独立于平台的，可以运行在各种环境中，包括跨云、内部部署、Kubernetes、Mesos 等Istio 目前支持：在 Kubernetes 上部署的服务；使用 Consul 注册的服务；在虚拟机上部署的服务； Istio架构分为控制平面和数据平面 控制平面组功能及组件：下发各类策略，同时也收集数据平面返回的一些数据。组件： Pilot、Mixer、Citadel； 数据平面组件功能及组件： 透明proxy（Envoy）劫持应用容器流量，接受控制平面组件指令下发生成对应规则同时将采集到部分信息上报到控制平面； 控制平面 Pilot: 负责流量管理和智能路由(AB测试、金丝雀发布)及错误处理(超时、重试、熔断)，Pilot会从k8s中读取服务信息，完成服务发现，Pilot会将用户和配置将网络流量管理请求下发到全部的sidecar中。 Mixer: 前期调用时条件检查和后期报告汇报，每次服务调用时mixer都需要检测调用者请求是否能正确认证，满足ACL规则，后期还需要收集服务上报的监控和跟踪数据。 Citadel：为服务间提供认证和证书管理，可以让服务自动升级为TLS。 Galleey: 整个控制平面的配置中心同时也负责配置管理和分发。 数据平面 Envoy: 扮演sidecar的功能，协调服务网格中所有服务的出入站流量，并提供服务发现、负载均衡、限流熔断等能力，还可以收集大量与流量相关的性能指标。 Sidecar流量劫持原理 istio会给服务配置一个init container ，这个init container会生成很多对应的iptables规则，然后这些规则会将对应inbound和outbound流量都转发给envoy。 但这种方式有个问题因为iptables采用的是线性匹配即一个数据包过来以线性的方式遍历整个规则集，直到找到匹配的否则退出，这种带来的问题，就是当iptables规则量很大时，性能会急剧下降，因为对应的匹配延时会增加。所以目前社区也在考虑IPVS的方案，这点根k8s Service模式一样。 Service mesh局势分析","categories":[{"name":"ServiceMesh","slug":"ServiceMesh","permalink":"http://yoursite.com/categories/ServiceMesh/"}],"tags":[{"name":"ServiceMesh","slug":"ServiceMesh","permalink":"http://yoursite.com/tags/ServiceMesh/"}]},{"title":"flexvolume简介","slug":"flexvolume","date":"2019-02-07T13:45:59.000Z","updated":"2019-02-07T13:45:59.000Z","comments":true,"path":"2019/02/07/flexvolume/","permalink":"http://yoursite.com/2019/02/07/flexvolume/","excerpt":"","text":"概述：在实际使用中，我们的Kubernetes集群需要与外部存储进行对接，这时需要使用使用存储驱动，通过存储驱动去对接存储，Kubernetes有内置一些驱动能对接常见存储如nfs、ceph-rbd、ceph-fs、iscsi等，当我们需要对接的存储内置不支持时，这时我们就需要使用扩展存储驱动的方式去对接，在Kubernetes中自定义扩展存储驱动方式有两种一种是通过flexvolume另外一种是CSI。 flexvolume是在Kubernetes1.2版本中存在于Kubernetes集群Kubernetes1.8版本正式GA的，用户将编写好的插件的二进制可执行文件放置到宿主机的指定目录即可正常调用使用。二进制可执行文件可以使用任意开发语言开发，但需要定义好几个指定接口接口如下：init：kubelet&#x2F;kube-controller-manager 初始化存储插件时调用，插件需要返回是否需要要 attach 和 detach 操作attach：将存储卷挂载到 Node 上detach：将存储卷从 Node 上卸载waitforattach： 等待 attach 操作成功（超时时间为 10 分钟）isattached：检查存储卷是否已经挂载mountdevice：将设备挂载到指定目录中以便后续 bind mount 使用unmountdevice：将设备取消挂载mount：将存储卷挂载到指定目录中umount：将存储卷取消挂载 Flexvolume从v1.8开始支持动态检测驱动程序的能力，初始化和更新驱动生效只需要重启kubelet即可生效。 flexvolume驱动开发的几个关键点flexvolume的开发主要注意几点：1、flexvolume插件的详细日志在kubelet的日志中，kubectl看的只是pod运行的状态。2、插件脚本中任何echo到前台的语句都要注释掉，返回给kubelet的只能是标准的json格式。3、如果需要调试可以通过echo将输出的调试信息&gt;&gt;重定向到外部文本。 这里以官方的LVM例子和两种常见的Kubernetes集群部署方式为例两种讲解一下flexvolume驱动的使用将下例驱动从github中clone下来。https://github.com/kubernetes/examples/tree/master/staging/volumes/flexvolume 在进行验证前请保证测试机器有一块空闲的磁盘。在使用flexvolume时需要配置kubelet参数–enable-controller-attach-detach&#x3D;false RKE部署集群方式这里以LVM为例，其他插件类式 创建lvm创建pv 1pvcreate /dev/sdx 创建vg 1vgcreate /dev/sdx vg0 创建LV 1lvcreate -L 19G vg0 //根据实际情况修改命令 修改rke部署Kubernetes集群yaml文件12345678kubernetes_version: v1.11.3-rancher1-1services: kubelet: extra_args: enable-controller-attach-detach: false extra_binds: - &quot;/usr/libexec/kubernetes/kubelet-plugins:/var/lib/kubelet/volumeplugins&quot; 主要有两点： 配置enable-controller-attach-detach: false 这样是直接让kubelet去attache设备到pod不然会报错Volume has not been added to the list of VolumesInUse in the node’s volume status for volume.表示该节点的附加&#x2F;分离操作仅委派给controller-manager 因为rke部署的Kubernetes集群kubelet是容器化的所以需要将存储插件目录映射到宿主机上。 将flexvolume驱动从github上clone下来并修改https://github.com/kubernetes/examples/tree/master/staging/volumes/flexvolume 在deploy目录创建drivers文件夹，将父目录的lvm驱动复制到drivers文件夹，整体目录结构如下 123456789101112131415161718|-- deploy| |-- deploy.sh| |-- Dockerfile| |-- drivers| | -- lvm| |-- ds.yaml| `-- README.md|-- dummy|-- dummy-attachable|-- nfs|-- lvm|-- nginx-dummy-attachable.yaml|-- nginx-dummy.yaml|-- nginx-lvm.yaml|-- nginx-nfs.yaml|-- nginx.yaml`-- README.md 修改deploy.sh（主要为了生成的文件夹名为k8s~lvm，pod调用驱动时更加清晰）改成如下VENDOR&#x3D;${VENDOR:-k8s}DRIVER&#x3D;${DRIVER:-lvm} 生成驱动镜像切换到deploy目录生成驱动部署镜像 1docker build -t image_name:tag . 修改ds.yaml文件Image修改为我们刚刚生成镜像名和tagHostpath修改为 123hostPath: # TODO Change to the Flexvolume plugin directory of your cluster. path: /usr/libexec/kubernetes/kubelet-plugins/ 这个的作用主要是部署这个daemonset后会自动将驱动放到这个目录，这样就不用我们一台一台主要去复制了，另外修改启动也直接改这个yaml那全部节点也都会自动修改。这个目录是根我们刚刚在rke目录根kubelet映射那个目录是一样的，因为kubelet是容器化的要将这个目录映射到kubelet容器内。 部署这个daemonset 1kubectl apply -f ds.yaml 检查驱动 123kubectl get dsNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEflex-ds 1 1 1 1 1 &lt;none&gt; 9d 在宿主机上检查是否生成k8s~lvm驱动文件夹 12ls /usr/libexec/kubernetes/kubelet-plugins/k8s~lvm 测试使用如下测试案例 123456789101112131415161718192021222324apiVersion: v1kind: Podmetadata: name: nginx namespace: defaultspec: containers: - name: nginx image: nginx volumeMounts: - name: test mountPath: /data ports: - containerPort: 80 volumes: - name: test flexVolume: driver: &quot;k8s/lvm&quot; fsType: &quot;ext4&quot; options: volumeID: &quot;lvol0&quot; size: &quot;19.00g&quot; volumegroup: &quot;vg0&quot; driver就是我们刚刚那个插件目录的k8slvm这里将换成&#x2F;. 格式化文件系统，这里推荐ext4，xfs在大环境下驱动有问题，需要改驱动代码，参考：https://www.jianshu.com/p/3ecc7f72cf9f VolumeId：就是我们刚刚创建的lv的名字，若不记得lvs则显示 Volumegroup：就是vg的名字，vgs可以看见 创建测试应用 123kubectl get podNAME READY STATUS RESTARTS AGEflex-ds-5qnwj 1/1 Running 0 4m 成功运行 12df -h查看挂载/dev/mapper/vg0-lvol0 19G 45M 18G 1% /var/lib/kubelet/plugins/kubernetes.io/flexvolume/k8s/lvm/mounts/test kubeadm部署集群方式1、创建lvm方式同上。 2、配置kubelet的–enable-controller-attach-detach&#x3D; false参数编辑vim &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;kubelet.service.d&#x2F;10-kubeadm.conf在Environment选项后面加上–enable-controller-attach-detach&#x3D;false参数重启kubelet使kubelet生效 12systemctl daemon-reloadsystemctl restart kubelet 剩下部署方式根rke一样 修改flexvolumeq驱动官方lvm驱动案例，功能太弱了，需要提前将pv、vg、lv这些创建好，我们可以做一些修改，比如增加自动创建LV卷的功能，这样我们就不需要将LV卷一个个提前创建好，可以更省事。通过对驱动程序分析attach函数主要负责块设备检测，如果在yaml中定义的块不存在则直接退出 12345678910111213attach() &#123; JSON_PARAMS=$1 SIZE=$(echo $1 | jq -r &#x27;.size&#x27;) DMDEV=$(getdevice) if [ ! -b &quot;$&#123;DMDEV&#125;&quot; ]; then err &quot;&#123;\\&quot;status\\&quot;: \\&quot;Failure\\&quot;, \\&quot;message\\&quot;: \\&quot;Volume $&#123;VOLUMEID&#125; does not exist\\&quot;&#125;&quot; exit 1 fi log &quot;&#123;\\&quot;status\\&quot;: \\&quot;Success\\&quot;, \\&quot;device\\&quot;:\\&quot;$&#123;DMDEV&#125;\\&quot;&#125;&quot; exit 0&#125; domountdevice函数主要负责进行格式化块设备，和mount操作 123456789101112131415161718192021222324252627282930313233343536domountdevice() &#123; MNTPATH=$1 DMDEV=$2 FSTYPE=$(echo $3|jq -r &#x27;.[&quot;kubernetes.io/fsType&quot;]&#x27;) if [ ! -b &quot;$&#123;DMDEV&#125;&quot; ]; then err &quot;&#123;\\&quot;status\\&quot;: \\&quot;Failure\\&quot;, \\&quot;message\\&quot;: \\&quot;$&#123;DMDEV&#125; does not exist\\&quot;&#125;&quot; exit 1 fi if [ $(ismounted) -eq 1 ] ; then log &quot;&#123;\\&quot;status\\&quot;: \\&quot;Success\\&quot;&#125;&quot; exit 0 fi VOLFSTYPE=`blkid -o udev $&#123;DMDEV&#125; 2&gt;/dev/null|grep &quot;ID_FS_TYPE&quot;|cut -d&quot;=&quot; -f2` if [ &quot;$&#123;VOLFSTYPE&#125;&quot; == &quot;&quot; ]; then mkfs -t $&#123;FSTYPE&#125; $&#123;DMDEV&#125; &gt;/dev/null 2&gt;&amp;1 if [ $? -ne 0 ]; then err &quot;&#123; \\&quot;status\\&quot;: \\&quot;Failure\\&quot;, \\&quot;message\\&quot;: \\&quot;Failed to create fs $&#123;FSTYPE&#125; on device $&#123;DMDEV&#125;\\&quot;&#125;&quot; exit 1 fi fi mkdir -p $&#123;MNTPATH&#125; &amp;&gt; /dev/null mount $&#123;DMDEV&#125; $&#123;MNTPATH&#125; &amp;&gt; /dev/null if [ $? -ne 0 ]; then err &quot;&#123; \\&quot;status\\&quot;: \\&quot;Failure\\&quot;, \\&quot;message\\&quot;: \\&quot;Failed to mount device $&#123;DMDEV&#125; at $&#123;MNTPATH&#125;\\&quot;&#125;&quot; exit 1 fi log &quot;&#123;\\&quot;status\\&quot;: \\&quot;Success\\&quot;&#125;&quot; exit 0&#125; 如果要实现自动基于VG创建LV卷，在自动挂载，那需要在格式化和mount前完成，所以需要我们把这个功能在attach函数中实现 attach() &#123; JSON_PARAMS=$1 VOLUMEID=$(echo $&#123;JSON_PARAMS&#125; | jq -r &#39;.volumeID&#39;) VG=$(echo $&#123;JSON_PARAMS&#125; | jq -r &#39;.volumegroup&#39;) THINPOOL=$(echo $&#123;JSON_PARAMS&#125; | jq -r &#39;.thinpool | select(type == &quot;string&quot;)&#39;) SIZE=$(echo $&#123;JSON_PARAMS&#125; | jq -r &#39;.size&#39;) DMDEV=$(getdevice) if [ ! -b &quot;$&#123;DMDEV&#125;&quot; ]; then LVCREATE_OPTS=&quot;-n $&#123;VOLUMEID&#125; -L $&#123;SIZE&#125; $&#123;VG&#125;&quot; lvcreate $&#123;LVCREATE_OPTS&#125; 2&gt;&amp;1 &gt; /dev/null if [ &quot;$?&quot; != &quot;0&quot; ]; then err &quot;&#123;\\&quot;status\\&quot;: \\&quot;Failure\\&quot;, \\&quot;message\\&quot;: \\&quot;Could not attach $&#123;VG&#125;/$&#123;VOLUMEID&#125;\\&quot;&#125;&quot; exit 1 fi fi log &quot;&#123;\\&quot;status\\&quot;: \\&quot;Success\\&quot;, \\&quot;device\\&quot;:\\&quot;$&#123;DMDEV&#125;\\&quot;&#125;&quot; exit 0 &#125; 最终实现如上。 参考链接 https://kubernetes.feisky.xyz/cha-jian-kuo-zhan/volume/flex-volume","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernete","slug":"kubernete","permalink":"http://yoursite.com/tags/kubernete/"}]},{"title":"ETCD集群的备份与还原","slug":"etcd_backup","date":"2019-01-31T13:45:59.000Z","updated":"2019-01-31T13:45:59.000Z","comments":true,"path":"2019/01/31/etcd_backup/","permalink":"http://yoursite.com/2019/01/31/etcd_backup/","excerpt":"","text":"概述：etcd做为一个分布式键值数据库，在Kubernetes集群中是一个非常重要的组件，用于保存Kubernetes的网络配置信息和Kubernetes对象的状态信息，一旦etcd挂了，那么也就意味着整个集群就不可用了，所以在实际情况生产环境中我们会部署多个etcd节点，组成etcd高可用集群，在etcd高可用集群中可挂节点数为(n-1)&#x2F;2个，比如你有3个节点做etcd-ha那么最多可以挂掉(3-1)&#x2F;2个节点，虽然有HA集群可以保证一定的高可用，但在实际生产环境中还是建议将etcd数据进行备份，方便在出现问题时进行还原。 环境信息：OS：ubuntu16.04docker：17.03-2Kubernetes：v1.11.6etcd：v3.2.18Rancher：v2.1.5 节点信息与角色：172.31.164.58 control worker etcd172.31.164.59 control worker etcd172.31.164.139 control worker etcd 操作：以Rancher2.0部署的Kubernetes集群为例，其他部署工具部署的也是一样。 etcd备份备份步骤1、 如果是使用Rancher部署的Kubernetes集群可以直接使用rancher自动备份 Rancherv2.1.5 在部署集群的高级选项内是可以配置etcd的自动备份的，按照配置的备份策略会进行自动备份，默认的备份策略是每12小时备份一次，备份文件的轮转周期是72小时，避免因备份文件过多占用磁盘空间.也可以根据自己实际需求进行调整。 备份的文件放置在master节点宿主机的&#x2F;opt&#x2F;rke&#x2F;etcd-snapshots目录下以etcd结尾的就是etcd就是备份文件，pki.bundle.tar.gz文件是对应的集群证书的备份。 Rancher部署的Kubernetes集群etcd是以容器化的方式运行要使用etcdctl命令有两种方式1、etcd容器内执行etcdctl;docker exec -it etcd sh2、或直接在宿主机上安装一个etcd不用启动，只是使用它的etcdctl命令,那可以通过https://github.com/etcd-io/etcd/releases下载对应的etcd版本。 推荐使用方式二在宿主机上安装etcd使用etcdctl执行，因为在还原时需要将etcd关闭，还是需要准备etcdctl命令执行以下命令 12export ETCDCTL_API=3etcdctl --endpoints=https://172.31.164.58:2379 --cert=/etc/kubernetes/ssl/kube-etcd-172-31-164-58.pem --key=/etc/kubernetes/ssl/kube-etcd-172-31-164-58-key.pem --cacert=/etc/kubernetes/ssl/kube-ca.pem snapshot save /var/lib/rancher/etcd/etcd_2019_1_31.db 1、&#x2F;var&#x2F;lib&#x2F;rancher&#x2F;etcd这个目录是根宿主机的&#x2F;var&#x2F;lib&#x2F;etcd&#x2F;目录相映射的,所以备份在这个目录在对应的宿主机上也是能看见的。2、这些证书对应文件可以直接在etcd容器内通过ps aux|more看见 其中–cert-file对应–cert，–key对应–key-file –cacert对应–trusted-ca-file 集群方式备份根上述一样 etcd还原单节点还原1、将kube-api-server关闭2、将etcd关闭在宿主机上执行将现有的etcd目录移除 1mv /var/lib/etcd /var/lib/etcd.bak 执行还原 1234567891011export ETCDCTL_API=3etcdctl snapshot restore etcd_2019_1_31.db \\ --endpoints=172.31.164.58:2379 \\ --name=etcd-rke-node2 \\ --cert=/etc/kubernetes/ssl/kube-etcd-172-31-164-58.pem \\ --key=/etc/kubernetes/ssl/kube-etcd-172-31-164-58-key.pem \\ --cacert=/etc/kubernetes/ssl/kube-ca.pem \\ --initial-advertise-peer-urls=https://172.31.164.58:2380 \\ --initial-cluster-token=etcd-cluster-1 \\ --initial-cluster=etcd-rke-node2=https://172.31.164.58:2380 \\ --data-dir=/var/lib/etcd 3、启动etcd 1docker start etcd 4、启动kube-apiserver 1docker start kube-apiserver 5、检查数据是否还原使用kubectl查看资源对象是否已恢复或直接get all 1kubectl get all --all-namespace HA环境下还原（多个etcd节点)将一个节点上的备份数据可以拷贝到其他全部节点，所有节点可以使用同一份数据进行恢复。 1、查看etcd集群信息 1234567export ETCDCTL_API=3etcdctl --endpoints=https://172.31.164.58:2379 --cert=/etc/kubernetes/ssl/kube-etcd-172-31-164-58.pem --key=/etc/kubernetes/ssl/kube-etcd-172-31-164-58-key.pem --cacert=/etc/kubernetes/ssl/kube-ca.pem member list2019-01-31 17:34:55.080375 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecatedabfc00bfbb85856d, started, etcd-rke-node3, https://172.31.164.59:2380, https://172.31.164.59:2379,https://172.31.164.59:4001cb7997809d89fa30, started, etcd-rke-node2, https://172.31.164.58:2380, https://172.31.164.58:2379,https://172.31.164.58:4001d3f1ec5a741e4dd1, started, etcd-rke-node4, https://172.31.164.139:2380, https://172.31.164.139:2379,https://172.31.164.139:4001 2、关闭集群的全部api-server和etcd 1docker stop etcd &amp;&amp; docker stop kube-apiserver 3、将集群内宿主机现有的etcd目录移除 1mv /var/lib/etcd /var/lib/etcd.bak 4、恢复数据在172.31.164.58上执行 1234567891011export ETCDCTL_API=3etcdctl snapshot restore etcd_2019_1_31.db \\ --endpoints=172.31.164.58:2379 \\ --name=etcd-rke-node2 \\ --cert=/etc/kubernetes/ssl/kube-etcd-172-31-164-58.pem \\ --key=/etc/kubernetes/ssl/kube-etcd-172-31-164-58-key.pem \\ --cacert=/etc/kubernetes/ssl/kube-ca.pem \\ --initial-advertise-peer-urls=https://172.31.164.58:2380 \\ --initial-cluster-token=etcd-cluster-1 \\ --initial-cluster=etcd-rke-node4=https://172.31.164.139:2380,etcd-rke-node3=https://172.31.164.59:2380,etcd-rke-node2=https://172.31.164.58:2380 \\ --data-dir=/var/lib/etcd 在172.31.164.59上执行 1234567891011export ETCDCTL_API=3etcdctl snapshot restore etcd_2019_1_31.db \\ --endpoints=172.31.164.59:2379 \\ --name=etcd-rke-node3 \\ --cert=/etc/kubernetes/ssl/kube-etcd-172-31-164-58.pem \\ --key=/etc/kubernetes/ssl/kube-etcd-172-31-164-58-key.pem \\ --cacert=/etc/kubernetes/ssl/kube-ca.pem \\ --initial-advertise-peer-urls=https://172.31.164.59:2380 \\ --initial-cluster-token=etcd-cluster-1 \\ --initial-cluster=etcd-rke-node4=https://172.31.164.139:2380,etcd-rke-node3=https://172.31.164.59:2380,etcd-rke-node2=https://172.31.164.58:2380 \\ --data-dir=/var/lib/etcd 在172.31.164.139上执行 1234567891011export ETCDCTL_API=3etcdctl snapshot restore etcd_2019_1_31.db \\ --endpoints=172.31.164.139:2379 \\ --name=etcd-rke-node4 \\ --cert=/etc/kubernetes/ssl/kube-etcd-172-31-164-58.pem \\ --key=/etc/kubernetes/ssl/kube-etcd-172-31-164-58-key.pem \\ --cacert=/etc/kubernetes/ssl/kube-ca.pem \\ --initial-advertise-peer-urls=https://172.31.164.139:2380 \\ --initial-cluster-token=etcd-cluster-1 \\ --initial-cluster=etcd-rke-node4=https://172.31.164.139:2380,etcd-rke-node3=https://172.31.164.59:2380,etcd-rke-node2=https://172.31.164.58:2380 \\ --data-dir=/var/lib/etcd 5、检查数据是否恢复 使用kubectl查看资源对象是否已恢复或直接get all 1kubectl get all --all-namespace 以上操作也可以写成一个自动化备份的脚本，但若使用Rancher部署的集群有自动定时备份更方便。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"Kubernetes下CICD工具的选型","slug":"k8s_cicd","date":"2018-12-29T13:45:59.000Z","updated":"2018-12-29T13:45:59.000Z","comments":true,"path":"2018/12/29/k8s_cicd/","permalink":"http://yoursite.com/2018/12/29/k8s_cicd/","excerpt":"","text":"什么是CI?持续集成(Continuous integration)频繁的将代码提交然后集成到主干。 什么CD?持续交付（Continuous Delivery)持续交付是在CI的基础上，将集成到主干的代码，产出的可部署软件版本，部署到类生产环境进行测试验证，没问题在手动部署到生产环境。 持续部署（Continuous Deployment)在持续交付基础上，部署到生产环境是自动操作的。 最终 为什么要做CI&#x2F;CD? 提高效率，减少人工操作。 能够快速确定新代码和原有代码能否集成。 降低风险，快速发现错误。 加速软件发布周期。 需要什么？1、具有版本管理功能的代码仓库管理系统（svn，gitlab，github)2、CI工具（jenkins，gitlab-ci，drone)3、自动化测试4、自动化部署工具（ansible、puppet，saltstack） 基于容器实现CICD的优势 持续集成与持续交付的难点在于如何屏蔽不同语言、不同框架、不同系统之间的持续集成与持续交付流程的差异性。 标准化打包方式，解决应用部署依赖问题。 抹平环境差异 提升软件交付和迭代效率，还能避免交付内容不同导致的人为错误。 CICD工具常见： Drone GitlabCI Jenkins 工具选型考虑点： 支持的代码库 易用性 插件生态 Drone版本：1.0.0-rc2 GO语言开发的一个非常轻量级的开源CI工具，整个镜像大小只有60多M，原生支持docker并且完全基于docker的CI工具，甚至连插件都是容器形式，使用的CICD操作，编译，测试，构建全部是都在docker中进行。 部署方式： docker Kubernetes 项目地址https://github.com/drone/drone插件地址http://plugins.drone.io/文档地址https://docs.drone.io/user-guide/ 支持的代码仓库 易用性Everything as docker，所有组件都是容器，包括你使用的插件 CI的执行是通过在代码仓库内定义个.drone.yml的文件,里面编写好对应的pipeline然后drone会去读取这个文件并执行。让CI工作所需的步骤可归纳为 部署drone-server 添加.drone.yml到代码仓库的根目录 没有自己的用户系统及认证系统，通过OAuth2.0协议与对接的代码仓库进行用户登证及代码仓库同步。也就意味着你不需要在单独管理CI系统的用户。UI整体简洁、美观。显示的是当前这个用户的项目信息，包括最新 commit信息及CICD 能清晰看见当前项目所有执行过的pipeline，以及这个pipeline是谁触发的。 单个pipeline的完整执行流程，及每个流程完整日志及消耗时间 Pipeline as codepipeline编写，Pipeline as code 所有的操作都是通过代码实现，包括drone1.0版本pipeline语法更贴近Kubernetes yaml语法，使用起来更加高效。 1234567891011121314151617181920kind: pipelinename: teststeps:- name: build image: golang commands: - CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app . - ./app- name: publish image: plugins/docker settings: repo: 172.31.164.66/library/test registry: 172.31.164.66 insecure: true tags: $&#123;DRONE_COMMIT_MESSAGE&#125;trigger: branch: - dev event: - push 插件生态http://plugins.drone.io/ 插件比较丰富，函盖了目前主要的一些应用场景 当需要自定义插件时也非常简单，因为drone Everything as docker，插件也是通过以容器方式运行，然后将pipeline中自定义的环境变量传到这个容器中去执行。 如设计个slack插件确定好需要传输的参数确定好需要传输哪些参数比如Slack插件将需要webhook网址，频道和消息文本 12345678910kind: pipelinename: defaultsteps:- name: webhook image: janecitizen/slack settings: webhook: https://hooks.slack.com/services/... channel: general text: hello 输入参数作为环境变量传递给插件，前缀为PLUGIN_。上面示例中的输入参数将传递给插件，如下所示： 123PLUGIN_CHANNEL=generalPLUGIN_WEBHOOK=https://hooks.slack.com/services/...PLUGIN_TEXT=hello 创建脚本使用上一节中定义的输入参数替换 12345#!/bin/shcurl -X POST \\ -H &#x27;Content-type: application/json&#x27; \\ -d &#x27;&#123;&quot;text&quot;:&quot;$&#123;PLUGIN_TEXT&#125;&quot;,&quot;channel&quot;:&quot;$&#123;PLUGIN_CHANNEL&quot;&#125;&#x27; \\ $&#123;PLUGIN_WEBHOOK&#125; 构建插件插件作为Docker镜像打包。创建一个Dockerfile，将先前创建的shell脚本添加到镜像。 12345FROM alpineADD script.sh /bin/RUN chmod +x /bin/script.shRUN apk -Uuv add curl ca-certificatesENTRYPOINT /bin/script.sh 构建你的插件docker镜像 1docker build -t rancher/slack . 总结：优点: 非常轻量级，部署简单。 高效率（Pipeline as code） 原生支持docker（ Everything as docker） 自定义插件和社区插件使用比较简单 难够针对pipeline的单个步骤多事件触发，或分支触发 123456789101112131415161718kind: pipelinename: defaultsteps:- name: build image: golang commands: - go build - go test -short- name: integration image: golang commands: - go test -v when: event: - pull_request branch: - feature 通过使用插件能够触发其他CI服务 分布式缺点： 高度依赖社区，文档不完善 插件质量参差不齐 UI非常简陋 只支持对接一个代码仓库管理系统 GitlabCIGitLab CI &#x2F; CD是GitLab的一部分,gitlab 8.0版本开始新增的功能，是用Ruby和Go语言编写的。根我们通常的CI系统不一样通常的是一个master-slave架构，即使没有slave，master一样可以做CI，slave只是做为一个压力分担功能，gitlab是gitlab-server本身是不执行的，是通过api与GitLab Runner交互让gitlab-runner去执行CI。 GitLab Runner是一个go语言编写程序，它可以运行在任何可以运行go环境的平台上(二进制包、docker、k8s)支持的代码仓库易用性同样也是Everything as dockerGitlab CI 整个流程和 Drone 以及流行的 Travis CI 都是比较类似的，通过在项目中添加一个 .gitlab-ci.yml 的配置文件，配置文件中描述构建流水线来执行任务，对不同编程语言的编译通过不同的docker image实现。CI的执行是通过在代码仓库内定义个.drone.yml的文件,里面编写好对应的pipeline然后drone会去读取这个文件并执行。让CI工作所需的步骤可归纳为 添加.gitlab-ci.yml到存储库的根目录 配置一个Runner Runner分两种类型，Specific Runners：单个项目独享Shared Runners：整个gitlab共享 Runner也可以选择CI方式：docker, parallels, shell, kubernetes, docker-ssh, ssh, virtualbox, docker+machine, docker-ssh+machine。如果是容器，代码编译的容器镜像需要提前在runner配置文件里面定义好。 123456789101112131415161718192021concurrent = 1check_interval = 0[session_server] session_timeout = 1800[[runners]] name = &quot;49faa2f8e25b&quot; url = &quot;http://47.104.182.1:1080/&quot; token = &quot;6ca2241a2924bd8763137d3bebf1c4&quot; executor = &quot;docker&quot; [runners.docker] tls_verify = false image = &quot;golang:1.10&quot; privileged = false disable_entrypoint_overwrite = false oom_kill_disable = false disable_cache = false volumes = [&quot;/var/run/docker.sock:/var/run/docker.sock&quot;, &quot;/cache&quot;] shm_size = 0 [runners.cache] Insecure = false .gitlab-ci.yaml默认build， test和deploy三个stage.gitlab-ci.yaml pipeline as code 当前项目下CI的执行记录。 整个pipeline的流程。 pipeline中的单个job执行过程详细信息输出，同时可以针对但个job进行重复执行。 项目CICD信息统计，按年、月、周汇总，报表展示 能直接对接现有Kubernetes集群，runner部署及pipeline CD流程。 插件生态优点： 根gitlab集成度非常高 不需要部署有gitlab&gt;&#x3D;8.0 就能直接使用 runner支持Autoscale UI可视化，可操作性强，可针对但个流程进行重复执行及报表展示 CI完全属于你这个代码库 缺点： 没有插件，对接第三方系统需要自己实现 只能支持gitlab代码仓库 Jenkinshttps://jenkins.io/Jenkins是一款java开发的功能强大的CI工具,其前身为oracle的Hudson (软件)项目，2011年正式独立出来，Jenkins也是目前非常老牌和主流的CI工具，最早只能支持java语言，后续通过各类语言插件实现多种编程语言支持，Jenkins也是目前插件种类最丰富的CI工具。 易用性：万物基于插件提供功能强大的UI，大部分操作都可以通过UI完成，但UI样式不符合当前审美于是Jenkins又开发一个Blueocean皮肤插件 对不同编程语言、docker、k8s、代码仓库对接都是通过插件方式去实现支持，当然也可以直接在Jenkins宿主机上安装好，直接通过shell方式调用。对代码仓库对Jenkins的对接，需要手动配置token，触发需要手动配置webhook和对应的触发事件。支持master-slave的架构方式，同时也能通过对接Kubernetes实现一个slave的动态创建过程，当然此时你可以封装不同编程语言的编译环境镜像，或在一个镜像内实现全部的编程语言。 这样可以大节省资源和多语言环境下部署slave的难度。Jenkins根drone和gitlab CI不一样，它拥有自己的用户认证系统，当然也可以去对接LDAP或AD。这样带来的问题是，权限管理很麻烦。Jenkins可以通过UI去配置CI，也可以通过Jenkinsfile是Groovy语言语法以下为例 代码仓库支持支持常见代码仓库 插件生态非常丰富的插件，覆盖面非常全https://plugins.jenkins.io/ 自定义插件学习成本较大，需要掌握JAVA语言总结：优点： 既有功能完善的UI，也支持pipeline as code 老牌CI工具文档很全面 插件生态丰富，基本上想要对接的工具都能找到对应插件 支持同时对接多个不同代码仓库 缺点： 对容器、k8s，代码仓库对接配置比较烦索 自定义插件难度大 独立的用户权限管理系统，多个开发团队共享一个master，会导致权限配置很困难，但若每个团队用各自Jenkins，又容易导致很多重复性工作 总结： 没有最好的工具，只有最合适的应用场景。 小团队，用的代码管理软件是gitlab，容器编排工具是Kubernetes建议用Gitlab-CI或Drone，开箱即用，可以减少很多工作量。 对插件有强烈需求，并且喜欢UI操作流水线的建议用Jenkins。 12https://mp.weixin.qq.com/s?__biz=MzIzNjUxMzk2NQ==&amp;mid=2247489519&amp;idx=1&amp;sn=4b42ec3c14b7ba8e21c741413d7bd6e4&amp;chksm=e8d7e82ddfa0613b85cf828443311af52890571052c6b6dce911c1cfe4e814dde28f8f98e086&amp;scene=27#wechat_redirecthttps://www.itread01.com/content/1531239621.html","categories":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/categories/CI-CD/"}],"tags":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/tags/CI-CD/"}]},{"title":"kubernetes service模式分析","slug":"Kubernetes_service_mode","date":"2018-12-10T13:45:59.000Z","updated":"2018-12-10T13:45:59.000Z","comments":true,"path":"2018/12/10/Kubernetes_service_mode/","permalink":"http://yoursite.com/2018/12/10/Kubernetes_service_mode/","excerpt":"","text":"概述：Kubernetes service是为POD提供统一访问入口的，实现主要依靠kube-proxy实现，kube-proxy有三种模式userspace、iptables，ipvs，同时我们也知道service有三种类型cluster_ip、nodeport，loadblance和三种端口类型port，targetport，nodeport。 环境信息：OS：Ubuntu16.04Kubernetes：v1.11.0kubeadm：v1.11.0docker：17.03network：flannel kube-proxy模式分析userspaceuserspace为kube-proxy为早期的模式，Kubernetes1.2版本之前主要使用这个模式，转发原理参考 1234567891011121314151617181920212223https://kubernetes.io/docs/concepts/services-networking/service/```这个模式最大缺点就是，所以端口请求都需要先经过kube-proxy然后在通过iptables转发，这样带来一个问题就是需要在用户态和内核态不断进行切换，效率低。#### iptablesKubernetes在1.2版本开始将iptables做为kube-proxy的默认模式，iptables根之前userspace相比，完全工作在内核态而且不用在经过kube-proxy中转一次性能更强，下面介绍Kubernetes中iptables转发流程iptables有链和表的概念，链就相当于一道道关卡，表就是这个关卡上对应的规则总共有四个表和五条链，kube-proxy在这里就使用了两个表分别是filter和nat表，也自定义了五个链KUBE-SERVICES，KUBE-NODE-PORTS，KUBE-POSTROUTING，KUBE-MARK-MASQ和KUBE-MARK-DROP五个链目前kubernetes提供了两种负载分发策略：RoundRobin和SessionAffinityRoundRobin：轮询模式，即轮询将请求转发到后端的各个Pod上。SessionAffinity：基于客户端IP地址进行会话保持的模式，第一次客户端访问后端某个Pod，之后的请求都转发到这个Pod上默认是RoundRobin模式。![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/service_mode_1.png)iptables数据包转发流程1、首先一个数据包经过网卡进来，先经过PREROUTING链2、判定目的地址是否为主机，为本机就通过INPUT链转发3、若不为本机通过FORWARDl链转发到POSTROUTING出去以下展示一个实例展示kube-proxy是如何根据service不同类型生成对应规则我们创建名为test的deployment，镜像为nginx:latest，replicas为3个 kubectl run test –image&#x3D;nginx –replicas&#x3D;3 1234567```kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IPtest-679b667858-9h9hr 1/1 Running 0 17h 10.244.0.16test-679b667858-g827r 1/1 Running 0 17h 10.244.0.15test-679b667858-nnr28 1/1 Running 0 6m 10.244.0.18 在给这个deployment创建一个ClusterIP类型的service 1kubectl expose deployment/test --type=ClusterIP --port=80 123kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEtest ClusterIP 10.98.243.51 &lt;none&gt; 80/TCP 43s 接下来我们将iptables规则导出来观察，用iptables-save将iptables规则重定向到一个文件 1iptables-save &gt; /tmp/1 ClusterIP类型查看规则首先Kubernetes会指对每个service在创建一些名为KUBE-SEP-xxx，KUBE-SVC-xxx的链 以刚刚创建的类型为Cluster-ip名为test这个service为例，创建了以下规则 1234567891011121314151617181920211、-A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.98.243.51/32 -p tcp -m comment --comment &quot;default/test: cluster IP&quot; -m tcp --dport 80 -j KUBE-MARK-MASQ2、-A KUBE-SERVICES -d 10.98.243.51/32 -p tcp -m comment --comment &quot;default/test: cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-IOIC7CRUMQYLZ32S3、-A KUBE-SVC-IOIC7CRUMQYLZ32S -m comment --comment &quot;default/test:&quot; -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-DZSN6N54CDU2RTAQ4、-A KUBE-SEP-DZSN6N54CDU2RTAQ -s 10.244.0.15/32 -m comment --comment &quot;default/test:&quot; -j KUBE-MARK-MASQ5、-A KUBE-SEP-DZSN6N54CDU2RTAQ -p tcp -m comment --comment &quot;default/test:&quot; -m tcp -j DNAT --to-destination 10.244.0.15:806、-A KUBE-SVC-IOIC7CRUMQYLZ32S -m comment --comment &quot;default/test:&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-XY6DO3BIJML2V7B57、-A KUBE-SEP-XY6DO3BIJML2V7B5 -s 10.244.0.16/32 -m comment --comment &quot;default/test:&quot; -j KUBE-MARK-MASQ8、-A KUBE-SEP-XY6DO3BIJML2V7B5 -p tcp -m comment --comment &quot;default/test:&quot; -m tcp -j DNAT --to-destination 10.244.0.16:809、-A KUBE-SVC-IOIC7CRUMQYLZ32S -m comment --comment &quot;default/test:&quot; -j KUBE-SEP-SWCXAUIAGJXMWYFS10、-A KUBE-SEP-SWCXAUIAGJXMWYFS -s 10.244.0.18/32 -m comment --comment &quot;default/test:&quot; -j KUBE-MARK-MASQ11、-A KUBE-SEP-SWCXAUIAGJXMWYFS -p tcp -m comment --comment &quot;default/test:&quot; -m tcp -j DNAT --to-destination 10.244.0.18:80 1、对源IP非10.244.0.0&#x2F;16访问目的地址10.98.243.51的80端口，执行KUBE-MARK-MASQ ，KUBE-MARK-MASQ会给这个包打上0x4000标签，后续KUBE-POSTROUTING链会根据这个标签做SNAT出去。2、对于目的IP为10.98.243.51，目的端口为80，然后将请求丢给KUBE-SVC-IOIC7CRUMQYLZ32S链处理。3、规则链KUBE-SVC-IOIC7CRUMQYLZ32S实现了将报文按33%的比例匹配，转给KUBE-SEP-DZSN6N54CDU2RTAQ链。4、对源IP为10.244.0.24的包 丢给KUBE-MARK-MASQ链，就如我们上面说的这个链会给包打上tag然后做SNAT，让这个地址能访问外网。5、KUBE-SEP-DZSN6N54CDU2RTAQ链直接进行DNAT操作将cluster-ip的80端口映射到pod的80。6、规则链KUBE-SVC-IOIC7CRUMQYLZ32S实现了将报文按50%的比例匹配，转给KUBE-SEP-XY6DO3BIJML2V7B5链。7、对源IP为10.244.0.25的包丢给KUBE-MARK-MASQ链，就如我们上面说的这个链会给包打上tag然后做SNAT，让这个地址能访问外网。8、 KUBE-SEP-XY6DO3BIJML2V7B5链直接进行DNAT操作进行DNAT到10.244.0.16 80端口。9、将KUBE-SVC-IOIC7CRUMQYLZ32S链剩余请求转发给KUBE-SEP-SWCXAUIAGJXMWYFS链。10、对源ip为10.244.0.18的包镜像SNAT。11、KUBE-SEP-SWCXAUIAGJXMWYFS 链直接进行DNAT操作进行DNAT到10.244.0.18 80端口。 NodePort类型我们将service类型改为NodePort 1kubectl edit service/test 将type改为NodePort 123kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEtest NodePort 10.98.243.51 &lt;none&gt; 80:32734/TCP 21m 重新保存iptables规则查看 1iptables-save &gt;/tmp/1 NodePort类型根ClusterIP 相比就多了两条规则，其他都一致。 121、-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/test:&quot; -m tcp --dport 32734 -j KUBE-MARK-MASQ2、-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/test:&quot; -m tcp --dport 32734 -j KUBE-SVC-IOIC7CRUMQYLZ32S 两条规则，主要是允许数据包转发和对目的端口为32734的端口进行DNAT映射 sessionaffinity对于一些特殊应用，我们需要做会话保持，让会话连接始终连接到上一次接收会话的POD上编辑我们刚刚创建的service 1kubectl edit service/test 将sessionaffinity参数改为 sessionAffinity: ClientIP，保存再次保存iptables规则查看 12345-A KUBE-SEP-DZSN6N54CDU2RTAQ -p tcp -m comment --comment &quot;default/test:&quot; -m recent --set --name KUBE-SEP-DZSN6N54CDU2RTAQ --mask 255.255.255.255 --rsource -m tcp -j DNAT --to-destination 10.244.0.15:80-A KUBE-SEP-SWCXAUIAGJXMWYFS -p tcp -m comment --comment &quot;default/test:&quot; -m recent --set --name KUBE-SEP-SWCXAUIAGJXMWYFS --mask 255.255.255.255 --rsource -m tcp -j DNAT --to-destination 10.244.0.18:80-A KUBE-SEP-XY6DO3BIJML2V7B5 -p tcp -m comment --comment &quot;default/test:&quot; -m recent --set --name KUBE-SEP-XY6DO3BIJML2V7B5 --mask 255.255.255.255 --rsource -m tcp -j DNAT --to-destination 10.244.0.16:80 会多三条规则，用的iptables recent模块进行会话保持总结一下使用iptables模式后，所有的如端口转发，会话保持，负载均衡都是通过iptables对应的模块和对应的规则去实现的比如端口转发用的DNAT规则，会话保持用的recent模块，负载均衡用的statistic 模块。虽然iptables模式弥补了userspace模式的一些缺陷，但iptables模式本身也存在一些缺陷，主要是在存在大量service的场景下。问题如下 在大规模集群下，随着service的数量越来越多时iptables规则会成倍的增长，大量的规则同时也会产生一些问题： iptables规则匹配延时：因为iptables采用的是线性匹配即一个数据包过来以线性的方式遍历整个规则集，直到找到匹配的否则退出，这种带来的问题，就是当iptables规则量很大时，性能会急剧下降，因为对应的匹配延时会增加 iptables规则更新延时：在实际使用过程中需要不断创建service，修改service，删除service，这其实也转换成了对iptables的不断修改，因为iptables是非增量式更新的，也就意味着，你上述所有操作它都是把全部归则拷贝出来，然后在修改，修改完在拷贝回去而且这个修改过程还会锁表。附上网易云的iptables测试的更新延时性能测试表https://zhuanlan.zhihu.com/p/39909011 负载均衡性能问题：前面我们也提过iptables并不是专业的负载均衡器，目前使用RoundRobin和sessionaffinity都是通过iptables内部模块statistic和recent实现的，性能根真正的负载均衡器相比肯定有差距。 QPS抖动问题：kube-proxy会周期性的更新iptables规则，大量iptables规则更新会花费很长时间，期间又会锁表所以会造成QPS抖动。 IPVSKubernetes社区为了解决上述iptables问题，在1.8版本引入了ipvs模式，并在Kubernetes 1.11版本正式GA。熟悉LVS的知道，LVS是一个工作在传输层的四层负载均衡器,是章文嵩博士开源贡献给社区的，后被并入linux内核，IPVS正是LVS的一部分。LVS根iptables一样也是工作在Netfilter之上。 IPVS主要有三种模式 DR模式：调度器LB直接修改报文的目的MAC地址为后端真实服务器地址，服务器响应处理后的报文无需经过调度器LB，直接返回给客户端。这种模式也是性能最好的。TUN模式：LB接收到客户请求包，进行IP Tunnel封装。即在原有的包头加上IP Tunnel的包头。然后发给后端真实服务器，真实的服务器将响应处理后的数据直接返回给客户端。NAT模式：LB将客户端发过来的请求报文修改目的IP地址为后端真实服务器IP另外也修改后端真实服务器发过来的响应的报文的源IP为LB上的IP。 kube-proxy的IPVS模式用的上NAT模式，因为DR,TUN模式都不支持端口映射。 ipvs也支持多种算法rr：轮询lc：最少连接dh：目的地址哈希sh：源地址哈希sed：最少期望延迟nq：永远不排队 通过kube-proxy的–ipvs-scheduler进行配置，目前这个配置是一个全局性的，无法针对单个service做单独配置，后续会支持单个service转发算法配置。启用方法以kubeadm为例因为ipvs是需要使用ipvs内核模块，先保证有这些内核模块ip_vs_sh,ip_vs_wrr,ip_vs_rr,ip_vs,nf_conntrack没有的话手动加载 1for i in &#123;ip_vs_sh,ip_vs_wrr,ip_vs_rr,ip_vs,nf_conntrack&#125;;do modprobe $i;done 记得设置开机自动加载。安装ipset和ipvsadm管理工具 1apt-get install ipset ipvsadm 创建kubeadm部署配置文件,文件内容如下 12345678apiVersion: kubeadm.k8s.io/v1alpha2kind: MasterConfigurationkubernetesVersion: v1.11.0networking: podSubnet: 10.244.0.0/16kubeProxy: config: mode: ipvs 执行kubeadm init –config xxxx 部署Kubernetes集群，然后就像之前方法一样通过kubeadm join加节点。部署完执行ipvsadm可以看见创建的一些ipvs规则 kube-proxy也会在集群每个节点创建一个kube-ipvs0的网卡，将集群的cluster-ip挂在上面。 转发原理 ClusterIP创建应用 1kubectl run test --image=nginx --replicas=3 创建一个clusterip类型的service 1kubectl expose deployment/test --port=80 --type=ClusterIP 查看service的cluster-ip 1234kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 13htest ClusterIP 10.105.170.66 &lt;none&gt; 80/TCP 12m 查看pod的ip 12345kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODEtest-679b667858-kz4wd 1/1 Running 0 18m 10.244.1.8 wan-node2test-679b667858-npdlp 1/1 Running 0 18m 10.244.1.7 wan-node2test-679b667858-qgwnj 1/1 Running 0 18m 10.244.0.21 wan-node1 可以看见kube-proxy将刚刚创建的testservice的cluster-ip 10.105.170.66挂载到kube-ipvs0虚拟网卡上了。 123456789root@wan-node1:~# ip akube-ipvs0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default link/ether 4a:ec:28:5d:a0:24 brd ff:ff:ff:ff:ff:ff inet 10.96.0.10/32 brd 10.96.0.10 scope global kube-ipvs0 valid_lft forever preferred_lft forever inet 10.96.0.1/32 brd 10.96.0.1 scope global kube-ipvs0 valid_lft forever preferred_lft forever inet 10.105.170.66/32 brd 10.105.170.66 scope global kube-ipvs0 valid_lft forever preferred_lft forever 查看ipvs规则 123456789ipvsadm -lnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.105.170.66:80 rr -&gt; 10.244.0.21:80 Masq 1 0 0 -&gt; 10.244.1.7:80 Masq 1 0 0 -&gt; 10.244.1.8:80 Masq 1 0 0 可以看见ipvs生成了对应的规则，VIP为10.105.170.66端口为80端口转发模式为rr，后端服务器IP为10.244.0.21，10.244.1.7，10.244.1.8正是我们的POD的IP. NodePort修改service类型为NodePortkubectl edit svc&#x2F;test将type修改为NodePort 123kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEtest NodePort 10.105.170.66 &lt;none&gt; 80:31389/TCP 21m 在次查看ipvs规则 123456789101112131415161718192021222324ipvsadm -lnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 192.168.250.200:31389 rr -&gt; 10.244.0.21:80 Masq 1 0 0 -&gt; 10.244.1.7:80 Masq 1 0 0 -&gt; 10.244.1.8:80 Masq 1 0 0TCP 10.244.0.0:31389 rr -&gt; 10.244.0.21:80 Masq 1 0 0 -&gt; 10.244.1.7:80 Masq 1 0 0 -&gt; 10.244.1.8:80 Masq 1 0 0TCP 172.17.0.1:31389 rr -&gt; 10.244.0.21:80 Masq 1 0 0 -&gt; 10.244.1.7:80 Masq 1 0 0 -&gt; 10.244.1.8:80 Masq 1 0 0TCP 10.244.0.1:31389 rr -&gt; 10.244.0.21:80 Masq 1 0 0 -&gt; 10.244.1.7:80 Masq 1 0 0 -&gt; 10.244.1.8:80 Masq 1 0 0TCP 127.0.0.1:31389 rr -&gt; 10.244.0.21:80 Masq 1 0 0 -&gt; 10.244.1.7:80 Masq 1 0 0 -&gt; 10.244.1.8:80 Masq 1 0 0 当service为NodePort，ipvs会以宿主机上所有网卡的ip为vip生成对应的转发规则，端口为nodeport端口 SessionAffinity编辑我们刚刚创建的service 1kubectl edit service/test 将sessionaffinity参数改为 sessionAffinity: ClientIP，保存在此查看ipvs规则 123456root@wan-node1:~# ipvsadm -ln-&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 192.168.250.200:31389 rr persistent 10800 -&gt; 10.244.0.21:80 Masq 1 0 0 -&gt; 10.244.1.7:80 Masq 1 0 0 -&gt; 10.244.1.8:80 Masq 1 0 0 ipvs在虚拟服务器中设置了会话超时时间，默认为10800秒(180分钟) 总结：可以看见ipvs模式根之前iptables有很大区别，之前iptables都是通过生成对应的iptables规则来实现端口映射，负载均衡，会话保持，但ipvs模式是通过将cluster-ip绑在kube-ipvs0虚拟网卡上，然后通过创建对应的ipvs规则来实现端口映射，负载均衡，会话保持。 ipvs依赖iptables因为ipvs只能实现端口映射，负载均衡，会话保存，但像包过滤、SNAT、hairpin-masquerade tricks（地址伪装）这些还是需要通过iptables实现，但也并不是直接调用iptables生成规则实现的而是通过ipset。ipset是什么？ipset是iptables的扩展，它可以创建一个集合，这个集合内容可以是ip地址，ip网段，端口等，然后iptables可以直接添加规则对这个集合进行操作。这样的好处在于不用针对每个ip或每个端口添加单独的规则，可以减少大量iptables规则添加，减少性能损耗。比如我们要禁止上万个IP访问我们的服务器，用iptables的话，你需要添加一条条规则，这样会在iptables中生成大量规则造成性能损耗，但通过ipset，可以将地址直接加入到ipset集合中，然后iptables可以添加规则对这个ipset进行操作。为什么用ipset？因为单独操作iptables就回到iptables模式的问题了，一但Kubernetes集群中service过多，会产生大量iptables规则，造成性能损耗，但用ipset可以配置集合将对象添加进去，这样可以保证即使我有在多的service和pod，但iptables规则是固定不变的。 查看ipset集合 1ipset list 保存集合 1ipset save 集合名 -f /tmp/1 kube-proxy使用的ipset集合 Kubernetes哪些场景会用到ipsetkube-proxy配置–masquerade-all &#x3D; true参数在kube-proxy启动中指定集群CIDR使用Loadbalancer类型的service使用NodePort类型的service 注意点：在用户环境中使用发现一些需要长连接的应用使用ipvs模式经常出现”Connection reset by peer”的错误,后经过抓包分析发现链接是被IPVS清理掉了，随后通过以下命令发现IPVS默认tcp连接超时时间为900s(15分钟) 12ipvsadm -l --timeoutTimeout (tcp tcpfin udp): 900 120 300 而操作系统默认是7200s(2小时),这就产生了一个问题如果client的tcp链接空闲时间超过900s后会首先被IPVS强制断开，但操作系统认为该链接还没有超时会继续保活，所以就产生了上述问题。 12sysctl -a|grep net.ipv4.tcp_keepalive_timenet.ipv4.tcp_keepalive_time = 7200 解决办法将net.ipv4.tcp_keepalive_time &#x3D; 7200设置为小于ipvs的900s即可，比如设置为600s https://berlinsaint.github.io/blog/2018/11/01/Mysql_On_Kubernetes%E5%BC%95%E5%8F%91%E7%9A%84TCP%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/https://github.com/projectcalico/calico/issues/2165https://fixatom.com/block-ip-with-ipset/https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/ipvs/README.md","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"cgroup的简单使用","slug":"cgroup","date":"2018-11-18T03:50:44.000Z","updated":"2018-11-18T03:50:43.000Z","comments":true,"path":"2018/11/18/cgroup/","permalink":"http://yoursite.com/2018/11/18/cgroup/","excerpt":"","text":"概述在实际生产应用中，我们经常在一台宿主机上运行多个应用程序，这时就会产生一个问题就是，多个应用程序之间如何避免资源抢占，资源进行限制，这时我们通过Linux内核自带的cgroup实现。 cgroup介绍Cgroup全称Control groups，最早是由Google的Paul Menage和Rohit Seth在2006年发起，最早名称叫最早的名称为进程容器（process containers）。在2007年时，因为在Linux内核中，容器（container）这个名词有许多不同的意义，为避免混乱，被重命名为cgroup，并且被合并到2.6.24版的内核中，cgroup的主要作用是用来控制、限制、分离一个进程组群的资源（如CPU、内存、磁盘输入输出等）。 cgroup能做什么 资源限制设置进程或进程组最大能使用的cpu、内存、磁盘的资源 优先级设置进程使用资源的优先级 资源使用统计测量组的资源使用情况，例如，可用于计费目的[12] 进程控制冻结或挂起进程 参考：https://en.wikipedia.org/wiki/Cgroups 更多详细情况可以参考https://www.infoq.cn/article/docker-kernel-knowledge-cgroups-resource-isolation 什么是NUMA?早期的SMP模型，所有CPU共享一个内存块，造成内存访问冲突加剧，命中率低，造成性能瓶颈。NUMA（Non-Uniform Memory Access）就是这样的环境下引入的一个模型。NUMA尝试通过为每个处理器提供单独的内存来解决此问题，避免在多个处理器尝试寻址相同内存时的性能损失。比如一台机器是有2个处理器，有4个内存块。我们将1个处理器和两个内存块合起来，称为一个NUMA node，这样这个机器就会有两个NUMA node。在物理分布上，NUMA node的处理器和内存块的物理距离更小，因此访问也更快。比如这台机器会分左右两个处理器（cpu1, cpu2），在每个处理器两边放两个内存块(memory1.1, memory1.2, memory2.1,memory2.2)，这样NUMA node1的cpu1访问memory1.1和memory1.2就比访问memory2.1和memory2.2更快。所以使用NUMA的模式如果能尽量保证本node内的CPU只访问本node内的内存块，那这样的效率就是最高的。 查看本机CPU和NUMA信息查看物理CPU个数 1cat /proc/cpuinfo | grep &quot;physical id&quot; | sort | uniq |wc -l 查看每颗cpu核数 1cat /proc/cpuinfo |grep &quot;cores&quot; 查看线程数 1cat /proc/cpuinfo | grep &quot;physical id&quot; | sort | uniq |wc -l 所以一台物理服务器总的cpu核数为物理cpu个数x每颗物理cpu的核数x线程数查看NUMA分布 12345[root@node-4 ~]# lscpu |grep NUMANUMA 节点： 2NUMA 节点0 CPU： 0-11,24-35NUMA 节点1 CPU： 12-23,36-47 这里是有两个NUMA节点，分别是0,1，其他NUMA0上对应的CPU核为0-11，24-35核，NUMA1上对应的CPU核为12-23，36-47看这些信息的意义在于，对进程进行cpu限制时，最好把它限制在一个NUMA节点内，因为跨NUMA节点的服务会带来一定性能损耗。 功能演示环境信息ubuntu：16.04 cgroups管理进程cpu资源用stress进行CPU压力测试在cpuset控制创建一个控制组 1mkdir /sys/fs/cgroup/cpuset/test 先有stress进行压力测试，占满两个逻辑核 1stress -c 2 &amp; 查看top看见两个逻辑核的空闲率都为0 使用cgroup的cpuset将它限制在某个核上。切换目录到刚刚创建的cpuset下的目录 1cd /sys/fs/cgroup/cpuset/test 将要限制的逻辑核范围输入到cpuset.cpus文件内，我这里限制跑在0号逻辑核上 1echo &quot;0&quot; &gt;/sys/fs/cgroup/cpuset/test/cpuset.cpus 使用cgexec 可以用来直接在子系统中的指定控制组运行一个程序，会自动将进程的PID填入的tasks文件中啊，不用手动输入 1cgexec -g cpuset:/test stress -c 2 &amp; 即使stress运行在两个逻辑核上，因为cgroup的限制，并且限制在0号逻辑核上。查看tasks，自动将 PID写入了tasks文件 cgroups管理进程内存资源跑一个耗内存的脚本，内存不断增长 1234x=&quot;a&quot;while [ True ];do x=$x$xdone; 将脚本保存为memeory.shtop看内存占用稳步上升 下面用cgroups控制这个进程的内存资源 1mkdir -p /sys/fs/cgroup/memory/test 分配1G的内存给这个控制组 1echo 1G&gt; /cgroup/memory/foo/memory.limit_in_bytes 设置为-1表示不限制执行 1cgexec -g memory:/test sh /root/memory.sh 发现很快之前的脚本被kill掉可以试着把memory.limit_in_bytes值设的更小，会发现被kill的时间会更快。还有个memory.soft_limit_in_bytes参数，用于配置内存软限制，简单来说就是当系统检测到系统内存争用或内存不足时，cgroup会将其限制在软限制范围内，如果需要将memory.soft_limit_in_bytes和memory.limit_in_bytes同时配置需要将memory.soft_limit_in_bytes的值设置低于memory.limit_in_bytes。 cgroups管理进程io资源跑一个耗io的脚本 1dd if=/dev/vda of=/dev/null &amp; 通过iotop看io占用情况，磁盘速度到了284M&#x2F;s 130252 be/4 root 284.71 M/s 0.00 B/s 0.00 % 0.00 % dd if=/dev/vda of=/dev/null 下面用cgroups控制这个进程的io资源 1mkdir /sys/fs/cgroup/blkio/test 把vda下载读取速率不超过1M 1echo &#x27;253:0 1048576&#x27; &gt;/sys/fs/cgroup/blkio/test/blkio.throttle.read_bps_device 253:0对应主设备号和副设备号，可以通过ls -l &#x2F;dev&#x2F;vda查看 123ls -l /dev/vdabrw-rw---- 1 root disk 253, 0 Nov 9 18:35 /dev/vda 执行dd测试速率 1cgexec -g blkio:/test dd if=/dev/vda of=/dev/null 再通过iotop看，确实将读速度降到了1M&#x2F;s 125206 be/4 root 1002.27 K/s 0.00 B/s 0.00 % 97.75 % dd if=/dev/vda of=/dev/null 实际应用测试环境ubuntu16.04使docker容器运行在指定的CPU核上,并限制内存的使用配置前查看已经运行的容器运行在哪些cpu核上 安装cgroup包 1apt-get install cgroup-bin cgroup-lite cgroup-tools cgroupfs-mount libcgroup1 配置cgconfig创建文件&#x2F;etc&#x2F;cgconfig.conf 123456789group test &#123;cpuset &#123;cpuset.cpus = &quot;0&quot;;cpuset.mems = &quot;0&quot;;&#125;memory &#123; memory.limit_in_bytes = &quot;3G&quot;; &#125;&#125; 注：- Group test 这里表示这个组名叫test，当然也可以根据需要自由去定义。- Cpuset表示使用cpuset控制器 Cpuset.cpus表示设置运行在哪些核上用”-“表示范围，多个不连续范围用”,”隔开，例如控制进程运行在0到7核和11到19核，应该写为0-7,11-19. Cpuset.mems表示上面控制的核对应的numa节点，尽量都控制在一个NUMA节点。 如何查看cpu核和numa节点对应关系? 这里表示 NUMA node0节点对应的CPU核数为0,1 Memory表示使用memory控制器 memory.limit_in_bytes限制内存的使用 创建并配置 &#x2F;etc&#x2F;cgrules.conf 1*:docker-containerd-shim cpuset,memory test *表示所有用户，如果要控制具体的用户可以直接写用户名 docker-containerd-shim表示需要控制的进程名 cpuset,memory表示应用哪些控制器，就是我们在cgconfig.conf中定义的那些 test表示对应的组，对应的是在cgconfig.conf中定义的group 编写init启动脚本 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667文件名/etc/init.d/cgconf#!/bin/sh### BEGIN INIT INFO# Provides: cgconf# Required-Start: $remote_fs $syslog# Required-Stop: $remote_fs $syslog# Should-Start:# Should-Stop:# Default-Start: 2 3 4 5# Default-Stop: 0 1 6# Short-Description: Configures CGroups### END INIT INFOstart_service() &#123; if is_running; then echo &quot;cgrulesengd is running already!&quot; return 1 else echo &quot;Processing /etc/cgconfig.conf...&quot; cgconfigparser -l /etc/cgconfig.conf echo &quot;Processing /etc/cgrules.conf...&quot; cgrulesengd -vvv --logfile=/var/log/cgrulesengd.log return 0 fi&#125;stop_service() &#123; if is_running; then echo &quot;Stopping cgrulesengd...&quot; pkill cgrulesengd else echo &quot;cgrulesengd is not running!&quot; return 1 fi&#125;status() &#123; if pgrep cgrulesengd &gt; /dev/null; then echo &quot;cgrulesengd is running&quot; return 0 else echo &quot;cgrulesengd is not running!&quot; return 3 fi&#125;is_running() &#123; status &gt;/dev/null 2&gt;&amp;1&#125;case &quot;$&#123;1:-&#125;&quot; in start) start_service ;; stop) stop_service ;; status) status ;; *) echo &quot;Usage: /etc/init.d/cgconf &#123;start|stop|restart|status&#125;&quot; exit 2 ;;esacexit $? 修改权限 1chmod 755 /etc/init.d/cgconf 更新注册系统启动项脚本 1update-rc.d cgconf defaults 启动服务 1systemctl start cgconf 重器docker生效 1systemctl restart docker 在此查看docker容器是否生效全部运行到指定的核上（测试配的是线程0） 如果配置了内存限制的，需要修改grub启动参数编辑&#x2F;etc&#x2F;default&#x2F;grub，GRUB_CMDLINE_LINUX_DEFAULT行 1GRUB_CMDLINE_LINUX_DEFAULT=&quot;cgroup_enable=memory quiet&quot; 更新 1update-grub 重启操作系统 1reboot","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"自定义Jenkins-slave镜像","slug":"jenkins_slave","date":"2018-10-23T13:45:59.000Z","updated":"2018-10-23T13:45:59.000Z","comments":true,"path":"2018/10/23/jenkins_slave/","permalink":"http://yoursite.com/2018/10/23/jenkins_slave/","excerpt":"","text":"概述：在前面章节，我们提到过通过Jenkins对接Kubernetes能实现对Jenkins slave的弹性部署和回收，能非常有效的利用资源，在实际使用的过程中，我们会根据不同的编程语言去构建一个对应的jnlp，或在一个jnlp镜像内包含全部编程语言。下面讲解一下如何定制自己的jnlp镜像。 软件版本：os：ubuntu 16.04Kubernetes：1.12.0jenkins：2.89.4 制做方法：基于标准操作系统base镜像构建，我这里使用debian，当然也可以使用centos，ubuntu，alpine等其他镜像（以java语言环境为例，构建maven编译环境，其他开发语言类似) 1、安装配置JAVA环境和maven编译环境，下载jdk，mavenhttps://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html下载Linux x64 tar包，解压到Dockerfile目录 http://mirror.bit.edu.cn/apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz解压tar包，到Dockerfile目录 2、安装配置Jenkins-slavehttps://github.com/jenkinsci/docker-jnlp-slave/blob/master/jenkins-slave下载jenkins-slave启动脚本，并放置到 &#x2F;usr&#x2F;local&#x2F;bin&#x2F;目录 3、安装slave.jar包slave.jar包主要用于Jenkins-slave去连接Jenkins-server所用https://repo.jenkins-ci.org/public/org/jenkins-ci/main/remoting/3.16/remoting-3.16.jar下载slave.jar包并放置到&#x2F;usr&#x2F;share&#x2F;jenkins&#x2F;目录 4、安装配置docker-in-docker下载docker二进制包https://download.docker.com/linux/static/stable/x86_64/docker-17.03.2-ce.tgz解压后，放到Dockerfile目录 5、下载kubectl二进制包https://dl.k8s.io/v1.11.3/kubernetes-client-linux-amd64.tar.gz 解压后，放到Dockerfile目录 6、放置连接Kubernetes集群的kubeconfig文件拷贝连接集群的kubeconfig文件，重命名为config 1mv kubeconfig config 基于以下Dockerfile构建镜像 12345678910111213141516171819FROM debian:9USER rootARG AGENT_WORKDIR=/home/jenkins/agentRUN mkdir -p /home/jenkins/.jenkins &amp;&amp; mkdir -p $&#123;AGENT_WORKDIR&#125;COPY jdk1.8.0_181 /usr/local/jdk1.8.0_181COPY maven3.3.9 /usr/local/maven3.3.9COPY jenkins-slave /usr/local/bin/jenkins-slaveCOPY slave.jar /usr/share/jenkins/COPY docker /usr/local/dockerCOPY kubectl /usr/bin/COPY config /rootENV PATH=/usr/local/jdk1.8.0_181/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/maven3.3.9/bin:/usr/local/dockerRUN chmod +x /usr/local/bin/jenkins-slaveENV HOME /home/jenkinsRUN chmod 755 /usr/share/jenkins &amp;&amp; chmod 644 /usr/share/jenkins/slave.jarUSER rootENV AGENT_WORKDIR=$&#123;AGENT_WORKDIR&#125;WORKDIR /home/jenkinsENTRYPOINT [&quot;jenkins-slave&quot;] 测试用例https://github.com/wanshaoyuan/pipeline-exemple.git 测试的Jenkins的pipeline注意：deployment.yaml里面应用部署在sock-shop这个命名空间内，需要提前创建好。jenkins-pipeline 1234567891011121314151617181920212223242526272829podTemplate(label: &#x27;jnlp-slave&#x27;, cloud: &#x27;kubernetes&#x27;) &#123; node(&#x27;jnlp-slave&#x27;) &#123; stage(&#x27;git clone&#x27;)&#123; //clone CODE根据实际情况修改 git credentialsId: &#x27;ba80cc36-77b1-42d0-8838-f834de346fe1&#x27;, url: &#x27;https://github.com/wanshaoyuan/pipeline-exemple.git&#x27; &#125; stage(&#x27;code build&#x27;)&#123; sh &#x27;&#x27;&#x27;/usr/local/maven3.3.9/bin/mvn -DskipTests package -f /home/jenkins/workspace/test-jnlp2&#x27;&#x27;&#x27; &#125; stage(&#x27;image build&#x27;)&#123; sh &#x27;&#x27;&#x27;echo $BUILD_ID GROUP=172.31.164.66/library COMMIT=$BUILD_ID /home/jenkins/workspace/test-jnlp2/scripts/build.sh&#x27;&#x27;&#x27; &#125; stage(&#x27;upload&#x27;)&#123; //上传镜像 sh &#x27;&#x27;&#x27;docker login 172.31.164.66 -u=admin -p=123456 docker push 172.31.164.66/library/shipping:$BUILD_ID&#x27;&#x27;&#x27; &#125; stage(&#x27;deploy&#x27;)&#123; //执行部署脚本 sh &#x27;sed -i &quot;s/172.31.164.66\\\\/library\\\\/shipping:.*/172.31.164.66\\\\/library\\\\/shipping:$BUILD_ID/g&quot; /home/jenkins/workspace/test-jnlp2/deployment.yml&#x27; sh &#x27;kubectl apply -f /home/jenkins/workspace/test-jnlp2/deployment.yml --kubeconfig /root/config&#x27; &#125; &#125;&#125; 配置jnlp映射宿主机的docker.sock文件,不然无法实现容器镜像build 执行完毕 最后访问集群30001 123456789101112131415kubectl get svc -n sock-shopNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEcarts ClusterIP 10.43.23.102 &lt;none&gt; 80/TCP 1dcarts-db ClusterIP 10.43.229.245 &lt;none&gt; 27017/TCP 1dcatalogue ClusterIP 10.43.133.136 &lt;none&gt; 80/TCP 1dcatalogue-db ClusterIP 10.43.156.93 &lt;none&gt; 3306/TCP 1dfront-end NodePort 10.43.130.209 &lt;none&gt; 80:30001/TCP 1dorders ClusterIP 10.43.212.179 &lt;none&gt; 80/TCP 1dorders-db ClusterIP 10.43.178.3 &lt;none&gt; 27017/TCP 1dpayment ClusterIP 10.43.205.54 &lt;none&gt; 80/TCP 1dqueue-master ClusterIP 10.43.178.73 &lt;none&gt; 80/TCP 1drabbitmq ClusterIP 10.43.233.8 &lt;none&gt; 5672/TCP 1dshipping ClusterIP 10.43.198.185 &lt;none&gt; 80/TCP 1duser ClusterIP 10.43.213.139 &lt;none&gt; 80/TCP 1duser-db ClusterIP 10.43.94.66 &lt;none&gt; 27017/TCP 1d http://host_ip:30001","categories":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/categories/CI-CD/"}],"tags":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/tags/CI-CD/"}]},{"title":"常见网络分析工具使用（持续更新整理)","slug":"network_tools","date":"2018-09-24T07:19:59.000Z","updated":"2018-09-24T07:19:59.000Z","comments":true,"path":"2018/09/24/network_tools/","permalink":"http://yoursite.com/2018/09/24/network_tools/","excerpt":"","text":"概述：在日常工作中经常会遇到一些网络问题，如何去排查网络问题，需要针对性的运用一些工具，以下就是常用的一些网络工具的介绍和简单使用教程，涉及从网络端口探测到网络质量检测、网络数据包分析等。 nc工具nc是netcat的简写，非常小巧，功能丰富强大，经常用在各种网络分析中。 主要作用：1、实现任意TCP&#x2F;UDP端口的侦听，nc可以作为server以TCP或UDP方式侦听指定端口。2、端口的探测，nc可以作为client发起TCP或UDP连接。3、机器之间传输文件。 nc常用参数 -l：用于指定nc将处于侦听模式。指定该参数，则意味着nc被当作server，侦听并接受连接，而非向其它地址发起连接。 -s ：指定发送数据的源IP地址，适用于多网卡机 -u：指定nc使用UDP协议，默认为TCP -v：输出详细信息-w：超时时间单位为秒-z：表示zero，表示扫描时不发送任何数据 常见用法启动任意端口侦听 1nc -l port_number (tcp) 1nc -ul port_name （udp) 端口探测 1nc -vzw 2 47.104.162.0 443 (tcp) 1nc -vuzw 2 47.104.162.0 9000(udp) -v 显示详细过程-z只扫描不发送数据-w超时时间（单位为秒）-u表示udp默认不指定为TCP 传输文件 nc传输文件根传统scp和rsync相比比较方便不用输密码，只要对端有通过nc启动对应服务有暴露端口就可以，还可以通过压缩方式传输文件，提高传输效率。方式一：先接收端启动nc准备接收文件，比如我们传输1.py这个文件接收端执行命令，这里命令意思就是监听TCP 9000端口，将收取到的文件重定向到当前目录sendmail.py文件 1nc -l 9000 &gt; sendmail.py 发送端执行发送命令 1nc 47.104.162.0 9000 &lt; sendmai.py 将sendmai.py重定向发送到47.104.162.0 TCP 9000端口发送完毕后，接收端和发送端会自动退出监听。 方式二：前面那种方式是需要先将接受端启动侦听，然后发送端在将文件通过接受端侦听的端口发送过去，方式二是反过来的是发送端先启动发送指令，然后接受端在接收发送端执行 1nc -l 9000 &lt; sendmai.py 表示通过本机9000端口发送接收端 1nc 47.104.162.1 9000 &gt; sendmai.py 从47.104.162.1 9000端口或取文件，然后写入到sendmai.py文件方式三：压缩传输，可以将整个目录文件传输到对端接收端启动，管道后必须接的是-不能是其他 1nc -l 9000 |tar xfvz - 发送端将目录下文件全部打包压缩发送到47.104.162.0 9000端口 1tar cfz - * |nc 47.104.162.0 9000 例在发送发端创建test目录并创建文件 1mkdir /root/test 1touch ~/test &#123;1..100&#125; 接收端执行 1nc -l 9000 |tar xfvz - 发送端切换test目录，执行 1tar cfz - * |nc 47.104.162.0 9000 nmap工具全称Network Mapper是linux系统下网络扫描和嗅探工具。 主要作用1、端口扫描，并自动分析端口对应的服务和对应的软件版本2、主机存活检测 nmap常用参数-s：扫描模式-S：指定源IP-p：指定端口范围扫描-sV：暴露端口的软件的版本-n：不做dns解析-sn：不做端口扫描只检测主机存活-e ：指定网卡-Pn：跳过主机存活性检测-Su：仅扫描UDP端口-sS：半连接方式扫描，不需要完成完整的tcp三次握手简单来说就是一个TCP_SYN扫描，扫描器对客户端发送syn包，得到ack响应，扫描器发送一个RST包断开连接，这样就不用完成三次握手，建立正常的TCP连接。 常见用法：端口扫描 1nmap ip_addr 直接不接任何参数扫描，默认户将tcp，udp这些全部列出来 加了-sS以后就是嗅探tcp时效率高点，其他都一样可以对域名进行扫描也可以对IP进行扫描 1nmap -sS ip_addr 端口状态open：端口处于开放状态，可以建立正常的连接。closed：端口处于关闭状态，没有应用在监听这个端口。Filtered：发出去的探测报文被过滤了，返回的ICMP报文状态是目的主机不可达，nmap无法确定端口状态，同时端口无法被nmap访问。Unfiltered：未被过滤状态，nmap不确定端口状态，但端口能被nmap访问。open|filtered：无法确定端口是open还是filtered状态，比如常见的开放的端口不响应发出去的请求就是这种状态。 按照上述命令扫描端口默认是扫描1-1024+nmap-server数据库里面收录的一些已知应用端口，所以要扫描大范围需要使用-p参数如 1nmap ip_addr -p1-65535 软件版本检测列出扫描出的端口对应的应用的软件版本信息，这里就直接显示了openssh的版本和对应的操作系统信息 1nmap -sV 172.31.164.57 通过ping扫描指定范围内机器是否up经常要测试一个地址范围内ip的联通性就可以使用这个命令 1nmap -sn 172.31.164.57-60 伪装源ip检测在用些场景我们需要伪装我们的源IP去进行嗅探，或我们的源IP被防火墙拉入了黑名单此时可以通过伪装ip的方式进行检测， 1nmap -e eth0 10.0.1.161 -S 10.0.1.167 -Pn 使用-S伪装自己源地址进行扫描的话，你必须另外使用-e 指定网卡和-Pn参数才能伪装 iperf工具iperf是一款网络性能测试工具，主要用来测试网络之间的带宽和丢包率。 主要作用1、网络带宽性能测试2、网络丢包率测试（UDP关注点) iperf3常用参数iperf3常用参数-f, –format [bkmaBKMA]格式化带宽数输出。支持的格式有：‘b’ &#x3D; bits&#x2F;sec ‘B’ &#x3D; Bytes&#x2F;sec‘k’ &#x3D; Kbits&#x2F;sec ‘K’ &#x3D; KBytes&#x2F;sec‘m’ &#x3D; Mbits&#x2F;sec ‘M’ &#x3D; MBytes&#x2F;sec‘g’ &#x3D; Gbits&#x2F;sec ‘G’ &#x3D; GBytes&#x2F;sec‘a’ &#x3D; adaptive bits&#x2F;sec ‘A’ &#x3D; adaptive Bytes&#x2F;sec -i：设置每次报告之间的时间间隔，单位为秒。如果设置为非零值，就会按照此时间间隔输出测试报告。默认值为零。-l,[KM]：设置读写缓冲区的长度。TCP方式默认为8KB，UDP方式默认为1470字节。-m, –print_mss：输出TCP MSS值（通过TCP_MAXSEG支持）。MSS值一般比MTU值小40字节。-p：设置端口，与服务器端的监听端口一致。默认是5001端口，与tcp的一样。-u：使用UDP方式而不是TCP方式(iperf3不需要)-b: 指定带宽（测试UDP时使用）-B：绑定到主机的多个地址中的一个。对于客户端来说，这个参数设置了出站接口。对于服务器端来说，这个参数设置入栈接口。这个参数只用于具有多网络接口的主机。在Iperf的UDP模式下，此参数用于绑定和加入一个多播组。使用范围在224.0.0.0至239.255.255.255的多播地址。参考-T参数。-M, –mss #[KM}：通过TCP_MAXSEG选项尝试设置TCP最大信息段的值。MSS值的大小通常是TCP&#x2F;IP头减去40字节。在以太网中，MSS值 为1460字节（MTU1500字节）。许多操作系统不支持此选项。-s：Server模式运行-c：Client模式运行，后面接上连接server的ip-P：测试数据流并发数量 iper3常见用法测试性能服务器端 1iperf3 -s -i2 -p 5001 客户端 1iperf3 -i 1 -c x.x.x.x -t 60 -i多久显示一次，单位S-c后跟服务器IP-t持续多久，单位秒 需要在主机上打开5001端口 1iptables -I INPUT -p tcp --dport 5001 -j ACCEPT 测试，可以发现，每秒都会显示网络带宽，这个网络是个千兆网络,默认是以4个并发进行测试的，如果发现带宽根实际物理带宽差距太大可以调整并发数在测试。 tcpdump工具tcpdump是linux下最常用的抓包分析工具。 主要作用：1、抓取网卡数据包tcpdump参数-i：指定监听的端口-w：将结果保存到指定位置，可以保存成cap文件用wireshark做数据包分析-vnn：以ip和端口方式输出详细信息-c：抓取包的数量，默认不指定会一直不断的抓直到ctrl+c-nn：以ip和端口方式显示，不以主机名和服务名方式显示，可以看起来直观-v：输出详细信息-X 直接输出package data数据，默认不设置，只能通过-w指定文件进行输出 常见用法：监视指定网络接口的数据包 1tcpdump -i eth1 如果不指定网卡，默认tcpdump只会监视第一个网络接口，一般是eth0，下面的例子都没有指定网络接口。 监视指定主机的数据包(如rke-node1) 1tcpdump host rke-node1 输出test-1 与 test-2或者与 test-3 之间通信的数据包 1tcpdump host test-1 and \\( test-2 or test-3 \\) 打印test-1与任何其他主机之间通信的IP 数据包, 但不包括与test-2之间的数据包. 1tcpdump host test-1 and ! test-2 截获源ip为xxx.xxx.xxx.xxx发送的所有数据 1tcpdump -i eth0 src host xxx.xxx.xxx.xxx 监视所有送到目的主机的数据包 1tcpdump -i eth0 dst host ipaddr or hostname 抓取100个数据包信息保存为cap文件可以用wireshark分析 1tcpdump -i eth0 -c 100 -w /tmp/capture2.cap 抓取指定协议的数据包 1tcpdump -i eth0 -vnn icmp -w /tmp/1 抓取udp协议的123端口的数据包 1tcpdump udp port 123 抓取host为192.168.1.1并且udp端口为23的数据包 1tcpdump tcp port 23 and host 192.168.1.1 netstat工具netstat是一款本机网络状态分析工具 作用：分析本机端口监听状态，连接状态，连接数等 netstat常用参数-a :显示所有状态连接-n：显示ip地址-t：显示TCP协议连接-l：显示LISTEN状态连接-u：显示UDP状态连接-p：显示socket的PID和程序名-i：显示网卡信息 TCP连接的常见状态LISTENING：侦听状态ESTABLISHED：建立连接状态CLOSE_WAIT：等待关闭状态。TIME_WAIT：主动关闭连接的一方会进入TIME_WAIT状态。为了防止最终的ACK包未到达，活动的TCP连接等待2MSL时间释放连接。SYN_SENT：表示请求连接，发送SYN报文。 常见用法：列出本机所有tcp连接 1netstat -at 列出本机所有udp连接 1netstat -au 显示所有数据包的统计情况 1netstat -s 显示网络接口收发报情况 1netstat -i 列出本机LISTEN状态的TCP端口 1netstat -lnt 列出本机所有LISTEN状态的UDP端口 1netstat -lnu 统计产生TIME_WAIT最多的几个IP 1netstat -ptan | grep TIME_WAIT |awk &#x27;&#123;print $5&#125;&#x27; |awk -F : &#x27;&#123;print $1&#125;&#x27;|sort |uniq -c|sort -r https://my.oschina.net/u/1024767/blog/757465","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"docker in docker","slug":"docker-in-docker","date":"2018-09-22T13:45:59.000Z","updated":"2018-09-22T13:45:59.000Z","comments":true,"path":"2018/09/22/docker-in-docker/","permalink":"http://yoursite.com/2018/09/22/docker-in-docker/","excerpt":"","text":"概述：docker-in-docker简单来说，就是在docker里面运行docker,大部分应用场景在在CI系统容器化后，如何在CI系统内构建容器镜像，比如我们前面例子，将Jenkins对接kubernetes然后Jenkins-slave以kubernetes内的pod方式运行实现动态创建和删除，这时如果需要构建容器镜像的话，就需要Jenkins-slave能支持docker-in-docker的方式了。 环境信息：os：ubuntu16.04docker：17.03-2 实现方式：docker-in-docker的实现方式有两种方式一：通过docker官方镜像docker:dind，或自己以特权模式启动一个基础镜像（然后安装docker) 1docker run -itd --name test-docker --privileged docker:dind（docker:dind镜像默认是最新版docker版本的dind,如果需要指定docker版本可以docker:17.03-dind) 配置镜像加速器 12345tee /etc/docker/daemon.json &lt;&lt; EOF&#123; &quot;registry-mirrors&quot;: [&quot;https://vqgjby9l.mirror.aliyuncs.com&quot;]&#125;EOF 启动应用功能测试 1docker run -itd -p 80:80 nginx 测试应用 12curl 127.0.0.1wget 127.0.0.1 &amp;&amp; cat index.html 构建镜像功能测试 1vi ~/Dockerfile 123FROM nginxMAINTAINER Alex wan &quot;wanshaoyuan@gmail.com&quot;RUN echo test &gt; /usr/share/nginx/html/index.html 构建镜像 基于新镜像启动容器 1docker run -itd -p 80:80 test-nginx:v1.0 验证 可以看见这根我们平常用的docker一模一样了，能够运行容器，管理容器，构建容器镜像。方式二：将宿主机的docker.sock文件映射到容器内,然后在应用镜像内安装一个docker client端就可以了，docker官方也有内部就封装了docker命令的镜像，镜像名就叫docker 1docker run -itd -v /var/run/docker.sock:/var/run/docker.sock docker 这种方式很简单了就是直接用的宿主机的docker，只不过用的另外的客户端去连接而已，所以你在容器里面的所有操作比如创建、删除、构建这些操作都是直接反映到宿主机的docker上的。 两种方式的区别：第一种方式使用docker inserd docker，是直接在子容器容器里面启动一个单独的容器，这种方案优点在于1、它是完全独立的一个docker，根宿主机是完全隔离的，在上面做任何操作都不会影响宿主机上的docker，对应的子容器销毁，里面容器和镜像就没有了，不会残留在父容器上，同时缺点也很明显1、因为需要操作iptables和cgroup所以需要特权模式启动，带来一定的安全隐患。2、StoragDriver问题，因为外部docker运行在操作系统的文件系统之上如ext4，xfs等，子容器运行在StoragDriver之上，然后子容器中的容器又运行在子容器的StoragDriver之上，所以这就成了一个多级嵌套的一个关系了，对于一些StoragDriver是不允许的，比如你不能在aufs之上在运行aufs，但目前overlay2是可以运行在aufs之上的，但会有较强的性能损耗。 第二种方式使用docker outside of docker，这种方式子容器内封装一个docker命令然后将宿主机的docker.sock文件映射进去，这种方案优点在于：1、因为子容器只是运行一个docker客户端，所以不需要特端模式，可以保证一定的安全性。2、没有多层StorageDriver嵌套问题因为它是直接运行在宿主机上的。缺点：1、因为是直接控制宿主机容器，所以所有操作都会反应在宿主机上，隔离性没那么好。2、端口要保证唯一性。3、容器层面安全性问题，因为可以直接操作宿主机docker，所以可以任意修改和删除上面的容器。 推荐做法：如果是使用ci工具在容器中构建容器的话，建议直接使用第二种方式docker outside of docker，因为这种方式不需要那么完全隔离，只要编译镜像就可以，另外就是编译出来的镜像直接在宿主机上，测试时还可以直接run起来。","categories":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"Helm(一)(介绍+安装+简单使用)","slug":"helm_1","date":"2018-09-09T13:45:59.000Z","updated":"2018-09-09T13:45:59.000Z","comments":true,"path":"2018/09/09/helm_1/","permalink":"http://yoursite.com/2018/09/09/helm_1/","excerpt":"","text":"概述什么是helm？helm是kubernetes里面的包管理工具，那什么包管理工具呢？其实就类似于我们用的centos的yum，ubuntu的apt一样能够快速安装、卸载、更新软件，最重要的一点是能够解决软件安装的依赖问题。 helm能帮助我们解决什么问题？在kubernetes中，部署应用是一件非常麻烦的事情，首先你得学会kubernetes中的各种概念，deployment，service，pv，pvc……，其次你还得会写yaml文件，通过yaml文件把这些资源对象整合起来，所以这就产生了一个很大问题了，这些yaml文件的管理和执行的先后顺序怎么解决。举个简单例子，WordPress这个博客应用，主要用到k8s哪些资源对象，首先通过deployment部署应用，通过service提供统一入口，如果需要数据持久化的话还需要创建pv和pvc，还需要secret配置WordPress的帐号密码，mariadb的帐号密码，而且部署时还有依赖关系，WordPress数据是存储在mariadb里面的，所以需要mariadb先启动就绪后在启动WordPress，那helm能如何解决这些问题？首先helm可以把这些资源对象封装在一起做成一个软件包统一发包，这个软件包里面就可以解决掉服务与访问之间的依赖问题，其次helm能更加高效的共享和重用访问，比如这个mariadb，如果就通过yaml文件访问部署，那这里里面很多配置参数都写死了，如果另外一个应用通过这个yaml部署mariadb，就需要改里面的配置参数，通过helm能把这些参数做成变量的形式，通过传递变量的方式来达到复用的效果。总结如下 1、helm能通过软件包的方式将这些资源对象统一管理起来。2、helm降低了应用配置和部署的复杂性。3、helm时应用程序可以多次复用。 helm的概念helm：命令行工具也是helm的客户端，主要用本地chart开发，chart仓库管理，与Tiller连接，release信息仓库。chart：helm的软件包，类似于yum和apt里面的软件安装包，chart里面内含了kubernetes对象的配置模块，参数，依赖关系和说明文档。release：是chart运行的实例，比如你通过helm将memcache这个chart部署到了这个kubernetes集群，那这个部署好的memcache就是一个release。tiller：helm是一个c&#x2F;s架构程序，tiller是helm的服务端，用于接收helm客户端的请求它与kube-apiserver交互，负责基于chart创建release和删除和release状态追踪。Repoistory：软件仓库，就类似于yum源和apt源一样。chart就是存在这里面。 helm安装环境信息：os：ubuntu16.04docker：17.03kubernetes：1.11.2helm：2.9.1 不同操作系统有不同安装方法https://github.com/helm/helm/releases，我这里使用ubuntu操作系统，直接下载对应的包就好了 下载helm会发现下载不下来，因为在google那边 1wget https://kubernetes-helm.storage.googleapis.com/helm-v2.9.1-linux-amd64.tar.gz 翻墙下载 1https://kubernetes-helm.storage.googleapis.com/helm-v2.9.1-linux-amd64.tar.gz 解压 1tar -xvf helm-v2.9.1-linux-amd64.tar.gz 1cd linux-amd64/ 1mv helm /usr/bin/ 安装Tiller执行helm init 1234567891011helm initCreating /root/.helmCreating /root/.helm/repositoryCreating /root/.helm/repository/cacheCreating /root/.helm/repository/localCreating /root/.helm/pluginsCreating /root/.helm/startersCreating /root/.helm/cache/archiveCreating /root/.helm/repository/repositories.yamlAdding stable repo with URL: https://kubernetes-charts.storage.googleapis.comError: Looks like &quot;https://kubernetes-charts.storage.googleapis.com&quot; is not a valid chart repository or cannot be reached: Get https://kubernetes-charts.storage.googleapis.com/index.yaml: net/http: TLS handshake timeout helm init 在缺省配置下， Helm 会利用 “gcr.io&#x2F;kubernetes-helm&#x2F;tiller” 镜像在Kubernetes集群上安装配置 Tiller；并且利用 “https://kubernetes-charts.storage.googleapis.com“ 作为缺省的 stable repository 的地址。由于在国内可能无法访问 “gcr.io”, “storage.googleapis.com”所以这里我们换成国内阿里的repo 1helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.9.1 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts 该命令会在当前目录下创建helm文件夹即~&#x2F;.helm，并且通过Kubernetes Deployment 部署tiller. 检查Tiller是否成功安装： 授权如果k8s集群打开的RBAC的话需要给tiller用ServiceAccount运行并且给这个ServiceAccount配置clusterrolebinding，因为tiller需要直接根kube-apiserver交互需要权限 1kubectl create serviceaccount --namespace kube-system tiller 1kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller 1kubectl patch deploy --namespace kube-system tiller-deploy -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;template&quot;:&#123;&quot;spec&quot;:&#123;&quot;serviceAccount&quot;:&quot;tiller&quot;&#125;&#125;&#125;&#125;&#x27; 验证执行helm version 查看当前仓库可用的chart 1helm search 部署应用测试部署mariadb 执行 1helm install --name mariadb --set &quot;persistence.enabled=false&quot; stable/mariadb 这里配置persistence.enabled&#x3D;false是因为我环境里面没有pvc所以把持久化存储关了，将chart内的这个变量设置为false部署完成查看是否部署成功 删除release 1helm del release_name 直接通过helm del删除release，会把对应的kubernetes资源对象删除，但release记录还是会保留下来，这时如果要还原可以直接执行 1helm rollback release_name revision 虽然rollback能执行成功并且，资源也重启创建回来了，但是状态会变成PENDING_ROLLBACK,看github的issue是说rollback只能用于现有的部署，已经删除了release回滚状态会报错https://github.com/helm/helm/issues/3722如果需要删除时，彻底删干净，连同release记录也删除，需要添加额外参数–purge 1helm del --purge release_name helm原理图片来源cloudman《每天5分钟玩转kubernetes》 创建release1、helm客户端从本地tar文件或repo里面解析出chart信息。2、helm客户端将解析出来的信息通过grpc传给tiller。3、tiller根据传来的信息生成release，然后将install release请求直接传递给kube-apiserver。 删除release1、helm客户端从本地tar文件或repo里面解析出chart信息。2、helm客户端将解析出来的信息通过grpc传给tiller。3、tiller将delete release请求直接传递给kube-apiserver。 更新release1、helm客户端将需要更新的chart的release名称chart结构和value信息传给tiller。2、tiller将收到的信息生成新的release，并同时更新这个release的history。3、tiller将新的release传递给kubernetes-apiserver进行更新。 https://www.cnblogs.com/CloudMan6/p/8970314.htmlhttps://docs.helm.sh","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"kubernetes namespace quota","slug":"kubernetes_namespace_quota","date":"2018-09-06T13:45:59.000Z","updated":"2018-09-06T13:45:59.000Z","comments":true,"path":"2018/09/06/kubernetes_namespace_quota/","permalink":"http://yoursite.com/2018/09/06/kubernetes_namespace_quota/","excerpt":"","text":"概述：当多个用户共享一个集群时，会担心其中某个用户将资源全部抢占，影响其他用户，namespace quota就是帮助我们解决这个问题的。namespace quota，就像我们在Iaas中针对每个project设置quota一样，来达到租户使用资源控制。 环境：os：ubuntu 16.04docker：17.03kubernetes：11.02 使用：目前kubernetes提供两种资源配额控制策略LimitRange：用来给namespace内的pod设置一个默认的request和limit值ResourceQuota：用来限制namespace中资源的占用 ResourceQuota：注意点：1、当创建的资源超过配额所限制的数量时，请求将失败，会抛出403错误。2、在配置namespace quota之前创建的资源对象还会被算到used里面。3、使用ResourceQuota后创建pod时必须配置Resource的request和limit不然会创建失败，当然你可以直接结合LimitRange使用，因为配置了limirange后会自动给pod配置limit和request。4、在集群资源总容量小于namespace的配额的情况下，可能会存在资源占用情况，以先到先得方式处理。 ResourceQuota能控制三种类型的资源计算资源配额主要控制一些物理资源如CPU、内存这些 存储资源配额主要配置请求存储的总量，pvc的数量 对象数量配额主要配置kubernetes内的资源的数量，比如控制namespace内能启动多少pod 例先创建一个namespace 1kubectl create namespace quota-object-example 使用以下yaml文件创建一个namespace quota，应用到quota-object-example这个namespace 123456789apiVersion: v1kind: ResourceQuotametadata: name: object-quota-demospec: hard: persistentvolumeclaims: &quot;1&quot; services.loadbalancers: &quot;2&quot; services.nodeports: &quot;0&quot; 1kubectl apply -f 1.yaml --namespace=quota-object-example 简单分析一下，这里限制只能创建一个pvc、一个loadblance。不能创建nodeport 1kubectl get resourcequota object-quota-demo --namespace=quota-object-example --output=yaml 12345678910status: hard: persistentvolumeclaims: &quot;1&quot; services.loadbalancers: &quot;2&quot; services.nodeports: &quot;0&quot; used: persistentvolumeclaims: &quot;0&quot; services.loadbalancers: &quot;0&quot; services.nodeports: &quot;0&quot; 测试创建应用，并用nodeport方式暴露 1kubectl run test --image nginx 1kubectl expose deployment test --port=80 --type=NodePort 因为我们设置的namespace的quota services.nodeports:0所以这里创建失败直接抛403错误了。创建pvc使用下面文件创建一个pvc 123456789101112kind: PersistentVolumeClaimapiVersion: v1metadata: name: pvc-quota-demospec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 3Gi 1kubectl apply -f 1.yaml --namespace=quota-object-example 查看资源情况 1kubectl get resourcequota object-quota-demo --namespace=quota-object-example --output=yaml 12345678910111213hard: persistentvolumeclaims: &quot;1&quot; services.loadbalancers: &quot;2&quot; services.nodeports: &quot;0&quot;status: hard: persistentvolumeclaims: &quot;1&quot; services.loadbalancers: &quot;2&quot; services.nodeports: &quot;0&quot; used: persistentvolumeclaims: &quot;1&quot; services.loadbalancers: &quot;0&quot; services.nodeports: &quot;0&quot; 可以看见的是在used这栏persistentvolumeclaims使用已经是1了，我们设置的配额是1如果我们再在这个namespace里面创建个pvc就会抛403了。 LimitRange：前面已经说过了，LimitRange是用来给namespace内的pod设置一个默认的request和limit值，所以这里我们简单介绍几个场景 先创建namespace 1kubectl create namespace default-cpu-example 使用以下yaml文件创建limitRange，指定一个默认的cpu request和cpu limit 1234567891011apiVersion: v1kind: LimitRangemetadata: name: cpu-limit-rangespec: limits: - default: cpu: 1 defaultRequest: cpu: 0.5 type: Container 应用到default-cpu-example这个namespace 1kubectl apply -f 1.yaml --namespace=default-cpu-example 场景一：在此命名空间创建pod，不设置Resource的limit和request使用以下yaml文件创建pod 12345678apiVersion: v1kind: Podmetadata: name: default-cpu-demospec: containers: - name: default-cpu-demo-ctr image: nginx 部署pod 1kubectl apply -f 1.yaml --namespace=default-cpu-example 查看pod yaml配置 1kubectl get pod default-cpu-demo --output=yaml --namespace=default-cpu-example 可以看见Resource这里自动配置了limit和request，这个就是继承LimitRange 12345resources: limits: cpu: &quot;1&quot; requests: cpu: 500m 场景二：只配置pod Resource的limit不配置request使用以下yaml文件创建pod 1234567891011apiVersion: v1kind: Podmetadata: name: default-cpu-demo-2spec: containers: - name: default-cpu-demo-2-ctr image: nginx resources: limits: cpu: &quot;1&quot; 部署pod 1kubectl apply -f 1.yaml --namespace=default-cpu-example 查看pod yaml配置 1kubectl get pod default-cpu-demo-2 --output=yaml --namespace=default-cpu-example 需要注意的是这里request值没有继承LimitRange配置的值0.5而是直接根limit相等。 12345resources: limits: cpu: &quot;1&quot; requests: cpu: &quot;1&quot; 场景三：指定容器请求值，不指定容器限额值使用以下yaml文件创建pod 1234567891011apiVersion: v1kind: Podmetadata: name: default-cpu-demo-3spec: containers: - name: default-cpu-demo-3-ctr image: nginx resources: requests: cpu: &quot;0.75&quot; 部署pod 1kubectl apply -f 1.yaml --namespace=default-cpu-example 查看pod yaml配置 1kubectl get pod default-cpu-demo-2 --output=yaml --namespace=default-cpu-example 需要注意的是这里request值没有继承LimitRange配置的值，而是直接是我们在pod中配置的值，limit继承的是LimitRange的值 12345resources: limits: cpu: &quot;1&quot; requests: cpu: 750m 总结： 如果没有在pod内设置request和limit默认就继承在namespace中配置的LimitRange。 如果在pod只配置了Resource的limit没配置request，这时request值不会继承LimitRange配置的值而是直接根pod中配置limit相等。 如果在pod中配置了request没有配置limit，这时request值以pod中配置的为准，limit值以namespace中的LimitRange为主。 https://kubernetes.io/docs/tasks/administer-cluster/quota-api-object/https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"Jenkins对接kubernetes实现slave动态创建","slug":"jenkins_dymaic_slave","date":"2018-08-25T13:45:59.000Z","updated":"2018-08-25T13:45:59.000Z","comments":true,"path":"2018/08/25/jenkins_dymaic_slave/","permalink":"http://yoursite.com/2018/08/25/jenkins_dymaic_slave/","excerpt":"","text":"概述：单个Jenkins很难满足生产环境中多个项目同时编译的需求，容易造成队列阻塞，所以这个时候需要通过Jenkins-slave的方式将编译分担到slave上，但传统的直接通过几台服务器专门做slave，浪费资源，还容易造成编译环境的不一致导致编译失败。后面有Jenkins有推出过基于docker的Jenkins-slave，虽然可以解决上述问题，但所以在这里我们可以充分利用kubernetes的功能，将Jenkins-slave以pod的方式运行在kubernetes集群内，将任务分发给这些Jenkins-slave pod，然后执行任务，任务执行完以后，在自动释放掉这些pod，能够很好的利用资源。 版本：os：ubuntu16.04kubernetes：1.10.5Jenkins：2.89.4 （通过deb包方式安装) 配置方法Jenkins安装kubernetes插件系统管理—&gt;管理插件—&gt;搜索Kubernetes plugin安装 配置系统管理—&gt;系统设置—&gt;新增一个云 配置Jenkins URL，这里我们没有配置api-server地址和证书key，连接kubernetes，所以默认会去读取放在JENKINS_HOME的.kube&#x2F;目录的kubeconfig文件，用于连接集群。我这里是通过安装包的方式安装的Jenkins HOME在&#x2F;var&#x2F;lib&#x2F;jenkins&#x2F;目录，如果是通过容器方式启动，将kubeconfig文件直接放~&#x2F;.kube&#x2F;目录。 配置pod template 名字可以随便写，注意这个namespace和labels，需要规划好namespace默认不写就是default labels是用于后面编译的项目调用这个kubernetes pod template时用的。 配置container template 注意这个pod的名字叫jnlp，用来覆盖默认容器，如果改成其他名字的话，那这个pod会启动两个container，都去连接master配置Arguments to pass to the command的用处是定义多个Jenkins-agent container时通过${computer.jnlpmac} ${computer.name}，做为jnlp的启动参数，用来区分不同container。配置cpu、memory资源request，因为可使用资源太低的节点上运行jnlp容易出错。 保存配置打开Jenkins-slave连接master的端口，这里定义为50000系统管理—&gt;全局安全配置—&gt;agents 验证创建个freestyle 特别需要注意这个label Expression，要根我们刚刚定义的kubernetes-template配置的一样，这样做的意义是什么，因为在实际使用环境中，不同的项目由不同的开发语言构成，也需要不同的编译环境，所以会配置多种语言环境的jnlp镜像，通过label去调用合适的kubernetes-template 这里我们配置一个最简单的shell，输出一串字符。点击立刻构建在kubernetes集群内就看见这个jnlp-slave已经被创建出来了。 查看本次编译结果，编译成功 pipline用法 12345678podTemplate(label: &#x27;jnlp-slave&#x27;, cloud: &#x27;kubernetes&#x27;) &#123; node(&#x27;jnlp-slave&#x27;) &#123; stage(&#x27;Run shell&#x27;) &#123; sh &#x27;echo hello world&#x27; &#125; &#125;&#125; 总结：通过jenkins-kubernetes-plugin能实现对Jenkins slave的弹性部署和回收，能非常有效的利用资源，在实际使用的过程中，我们会根据不同的编程去构建一个对应的jnlp，或在一个jnlp镜像内包含全部编程语言。 参考链接https://github.com/jenkinsci/docker-jnlp-slavehttps://github.com/jenkinsci/kubernetes-plugin/blob/master/README.md","categories":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/categories/CI-CD/"}],"tags":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/tags/CI-CD/"}]},{"title":"使用p2p技术加速容器镜像分发","slug":"docker_image_p2p","date":"2018-08-18T13:45:59.000Z","updated":"2018-08-18T13:45:59.000Z","comments":true,"path":"2018/08/18/docker_image_p2p/","permalink":"http://yoursite.com/2018/08/18/docker_image_p2p/","excerpt":"","text":"概述首先p2p网络是一种分布式的去中心化的网络，在网络中每个节点的地位都是对等的，每个节点既能充当服务器，也同时能为其他节点提供服务，同时也享有其他节点提供的服务。 为什么需要引入p2p技术来加速镜像分发？在大规模的容器集群内，镜像分发，往往需要消耗大量时间，并且会给镜像仓库带来很大的压力负担，通过p2p技术将流量分担到集群的每个节点上，这样可以大大缩短下载镜像的时间，并且能非常有效的减轻镜像仓库的压力。 Dragonfly介绍这里我们使用的是阿里巴巴开源的基于P2P技术的PB级文件分发系统蜻蜓(dragonfly)。引用阿里巴巴双11技术解密内容：蜻蜓整体架构分三层:第一层是 Config Service，他管理所有的 Cluster Manager，Cluster Manager 又管理所有的 Host，Host 就是终端，dfget 就是类似 wget 的一个客户端程序。Config Service 主要负责 Cluster Manager 的管理、客户端节点路由、系统配 置管理以及预热服务等等。简单的说，就是负责告诉 Host，离他最近的一组 Cluster Manager 的地址列表，并定期维护和更新这份列表，使 Host 总能找到离他最近的 Cluster Manager。Cluster Manager 主要的职责有两个: 以被动 CDN 方式从文件源下载文件并生成一组种子分块数据; 构造 P2P 网络并调度每个 peer 之间互传指定的分块数据。Host 上就存放着 dfget，dfget 的语法跟 wget 非常类似。主要功能包括文件下 载和 P2P 共享等。 原理 两个 Host 和 CM 会组成一个 P2P 网络，首先 CM 会查看本地是否有缓存，如果没有，就会回源下载，文件当然会被分片，CM 会多线程下载这些分片，同时会将 下载的分片提供给 Host 们下载，Host 下载完一个分片后，同时会提供出来给 peer 下载，如此类推，直到所有的 Host 全部下载完。本地下载的时候会将下载分片的情况记录在 metadata 里，如果突然中断了下 载，再次执行 dfget 命令，会断点续传。下载结束后，还会比对 MD5，以确保下载的文件和源文件是完全一致的。蜻蜓通过 HTTP cache 协议来控制 CM 端对文件的缓存时长，CM 端当然也有自己定期 清理磁盘的能力，确保有足够的空间支撑长久的服务。需要注意的是开源版的dragonfly目前没有开源config-Service。 使用Dragonfly做docker镜像分发技术原理类似我们用的BT下载技术的bitorrent协议cluster-manager就类似于Tracker服务器。.meta类似于torrent文件通过torrent文件，获取其他正在下载该文件的网址名单，根据torrent文件的网址然后连接tracker服务器，从tracker服务器获取正在下载该文件的网址名单，然后与它们取得联系，从他们那里获取文件的片端，直到整个下载完成。原理（来源官方github) 首先，docker pull 命令，会被 dfget proxy 截获。然后，由 dfget proxy 向 CM 发送调度请求，CM 在收到请求后会检查对应的下载文件是否已经被缓存到本地，如果没有被缓存，则会从Registry 中下载对应的文件，并生成种子分块数据(种子分块数据一旦生成就可以立即被使用);如果已经被缓存，则直接生成分块任务，请求者解析相应的分块任务，并从其他 peer 或者 supernode 中下载分块数据，当某个Layer的所有分块下载完成后，一个Layer也就下载完毕了，同样，当所有的Layer下载完成后，整个镜像也就下载完成了。 文件分块的下载 注: 其中cluster manager即超级节点(supernode) 每个文件会被分成多个块在对等者(peer)间进行传输。一个peer就是一个P2P客户端。超级节点会判断文件是否存在本地，如果不存在，则会将其从文件服务器下载到本地。 配置方法主要参考官方的服务端和客户端的安装和使用https://github.com/alibaba/Dragonfly/tree/master/docs/zh软件版本：docker：17.03-2os：ubuntu16.04Dragonfly：0.2.0Harbor：1.4.0 环境信息 rke-node1：172.31.164.57rke-node2：172.31.164.58rke-node3：172.31.164.59Harbor：172.31.164.66cluster-manger：172.31.164.113 安装cluster-manger（https://github.com/alibaba/Dragonfly/blob/master/docs/zh/install_server.md）分为两种方式1、通过docker镜像方式安装，适于用快速部署测试的环境。2、源码编译安装，适用于生产环境部署我们这里主要介绍docker镜像方式安装，源码方式安装参考上述链接。clone 代码 1git clone https://github.com/alibaba/Dragonfly.git 进入项目目录 1cd Dragonfly 编译代码，打包镜像需要本地安装java环境和maven 1./build/build.sh supernode 获取镜像ID 1$&#123;superNodeDockerImageId&#125;=`docker image ls|grep &#x27;supernode&#x27; |awk &#x27;&#123;print $3&#125;&#x27; | head -n1` 启动cluster-manger 1docker run -d -p 8001:8001 -p 8002:8002 $&#123;superNodeDockerImageId&#125; 测试验证telnet 127.0.0.1 8001telent 127.0.0.1 8002 安装client端（https://github.com/alibaba/Dragonfly/blob/master/docs/zh/install_client.md）安装客户端 1wget https://github.com/alibaba/Dragonfly/raw/master/package/df-client.linux-amd64.tar.gz 创建文件夹存放 1mkdir /root/install 1tar xzvf df-client.linux-amd64.tar.gz -C /root/install 设置环境变量 1vim ~/.bashrc PATH&#x3D;$PATH:&#x2F;root&#x2F;install&#x2F;df-client 1source ~/.bashrc 验证执行命令 1234567891011121314151617181920212223242526272829df-daemon -hUsage of df-daemon: -callsystem string caller name (default &quot;com_ops_dragonfly&quot;) -certpem string cert.pem file path -dfpath string dfget path (default &quot;/root/install/df-client/dfget&quot;) -h help -keypem string key.pem file path -localrepo string temp output dir of daemon (default &quot;/root/.small-dragonfly/dfdaemon/data/&quot;) -notbs not try back source to download if throw exception (default true) -port uint daemon will listen the port (default 65001) -ratelimit string net speed limit,format:xxxM/K -registry string registry addr(https://abc.xx.x or http://abc.xx.x) and must exist if df-daemon is used to mirror mode -rule string download the url by P2P if url matches the specified pattern,format:reg1,reg2,reg3 -urlfilter string filter specified url fields (default &quot;Signature&amp;Expires&amp;OSSAccessKeyId&quot;) -v version -verbose verbose 使用方法配置客户端连接超级节点 1vi /etc/dragonfly.conf 123[node]address=172.31.164.113 #多台cluster-manger用逗号分隔 启动dfdaemon，指定镜像仓库地址，默认端口为65001 1df-daemon --registry 172.31.164.66 &amp; 配置docker-mirror和docker http-proxy 1vim /etc/docker/daemon.json 12345678910&#123; &quot;registry-mirrors&quot;: [ &quot;http://127.0.0.1:65001&quot;], &quot;insecure-registries&quot; : [ &quot;http://127.0.0.1:65001&quot;, &quot;172.31.164.66&quot;]&#125; 1vim /lib/systemd/system/docker.service 1Environment=&quot;HTTP_PROXY=http://127.0.0.1:65001&quot; 重启docker 1systemctl daemon-reload 1systemctl restart docker 验证查看 12345678docker infoInsecure Registries: http://127.0.0.1:65001 172.31.164.66 127.0.0.0/8Registry Mirrors: http://127.0.0.1:65001 验证直接将Harbor的地址改成 127.0.0.1:65001就可以拉取镜像，看看是否能拉取成功 1docker pull 127.0.0.1:65001/library/front-end:30 Harbor内公开的项目镜像拉取，不用输入镜像仓库地址拉取 1docker pull library/front-end:30 注意点：1、private registry不能提供mirror的方式将流量转发到df-daemon，只能通过给docker配置http proxy的方式,原因在于docker pull project_name&#x2F;image_name:tag方式下载镜像时不会在请求头里面携带docker login时输入的账号和密码而通过docker pull registry_address&#x2F;project_name&#x2F;image_name:tag方式会在请求头内通过authorization传递login时输入的账号和密码 。 2、蜻蜓默认是限速20M的，取消限速的方法https://github.com/alibaba/Dragonfly/issues/38 抓包分析 1tcpdump -i lo port 65001 #确实有大量数据包经过 数据查看查看到大量数据分片 1ls ~/.small-dragonfly/data/ 查看下载日志 1less ~/.small-dragonfly/logs/dfclient.log 因为性能测试需要大并发，大流量环境下才能对比出差异性，所以这里性能测试结果使用阿里巴巴官方测试数据 上图可以看出，随着下载规模的扩大，蜻蜓与 Native 模式耗时差异显著扩 大，最高可提速可以达 20 倍。在测试环境中源的带宽也至关重要，如果源的带宽是 2Gbps，提速可达 57 倍。 向 200 个节点分发 500M 的镜像，比 docker 原生模式使用更低的网络流量， 实验数据表明采用蜻蜓后，Registry 的出流量降低了 99.5% 以上;而在 1000 并发 规模下，Registry 的出流量更可以降低到 99.9% 左右。 https://github.com/alibaba/Dragonfly","categories":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"kubernetes亲和性调度","slug":"kubernetes_affinty","date":"2018-08-12T13:45:59.000Z","updated":"2018-08-12T13:45:59.000Z","comments":true,"path":"2018/08/12/kubernetes_affinty/","permalink":"http://yoursite.com/2018/08/12/kubernetes_affinty/","excerpt":"","text":"概述：默认情况下创建pod调度是根据kubernetes scheduler默认的调度规则去进行调度的，但有些时候，有些应用有一些特殊的需求比如指定部署到对应的节点、多个pod之间需要部署在不同一个节点，需要互斥、pod和pod间相互交流比较频繁需要跑在同一个节点，需要亲和。这时就需要灵活配置scheduler来实现了。 场景一：调度到一组具有相同特性的主机上（label+nodeSelector)比如在整个kubernetes集群内，有一组配置比较好的服务器，然后有一些特殊的应用需要对硬件配置有需求，需要被调度到这些服务器上，这时可以通过kubernetes的label+nodeSelector去实现，实际上在kubernetes里面label是一个非常灵活的概念，kubernetes内不同资源对象之间的关联就是通过label的方式去进行的。例我们给那些配置高的节点打上个label 给高配置的host打上一个label Configuration&#x3D;hight（这里以rke-node2为例) 1kubectl label node rke-node2 Configuration=hight 在show labels可以看见我们刚刚打上去的label 创建pod使用nodeSelector调度到指定label的节点上mysql-deployment.yaml 123456789101112131415161718192021222324apiVersion: apps/v1beta1kind: Deploymentmetadata: name: mysql-2spec: selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: containers: - image: mysql:5.6 name: mysql env: - name: MYSQL_ROOT_PASSWORD value: password ports: - containerPort: 3306 name: mysql nodeSelector: Configuration: hight 配置nodeSelector调度到Configuration：hight的host上创建deployment 1kubectl apply -f mysql-deployment.yaml 查看pod被调度到label为 Configuration: hight的host上了 场景二：部署的应用不想调度到某些节点上（nodeaffinity)比如在一个kubernetes集群内，需要将应用部署到指定标签的机器上，但如果没有的话这类机器的话，就按正常调度规则进行调度。这时通过nodeSelector就无法实现这个需求了，在kubernetes内可以通过nodeaffinity的方式的去实现。 nodeaffinity和nodeSelecotr非常类似，都是node层面的调度策略的配置，但区别是nodeSelect功能比较单一，只能根据label选择对应的主机，而nodeaffinity更加灵活，nodeaffinity有两种调度类型requiredDuringSchedulingIgnoredDuringExecution(硬要求)和preferredDuringSchedulingIgnoredDuringExecution(软要求)，两种类型区别在于，硬要求要求调度时必须满足设置的调度规则，否则抛异常调度失败，而软要求，只是调度时优先考虑设置的调度规则，当达不到设置的规则时，则按kube-scheduler默认的调度策略进行调度。nodeaffinity还支持多种规则匹配条件的配置如In：label 的值在列表内NotIn：label 的值不在列表内Gt：label 的值大于设置的值Lt：label 的值小于设置的值Exists：设置的label 存在DoesNotExist：设置的 label 不存在 例 12345678910111213141516171819202122232425262728293031323334353637383940apiVersion: apps/v1beta1kind: Deploymentmetadata: name: mysql-2spec: selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: containers: - image: mysql:5.6 name: mysql env: - name: MYSQL_ROOT_PASSWORD value: password ports: - containerPort: 3306 name: mysql affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: NotIn values: - rke-node3 - rke-node4 preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: disk_type operator: In values: - ssd 如上所例，要求pod不能被调度到rke-node3和rke-node4上，但如果有满足dis_type&#x3D;ssd的label的节点则优先选择。如果在 nodeAffinity 类型中指定了多个 nodeSelectorTerms，那么 pod 将会被调度到只要满足其中一个 nodeSelectorTerms 的 node 上。如果在 nodeSelectorTerms 中指定了多个 matchExpressions，那么 pod 将会被调度到 满足所有 matchExpressions 的 node 上。 场景三：部署的应用关联性很强，需要尽量在一个节点上比如在kubernetes集群，有些pod和pod之间的交互很频繁，这时就需要将它们尽可能的调度到一台主机上，通过pod与pod之间的关系来选择调度，这时候我们就可以使用podaffinity。根nodeaffinity一样，podaffinity也有两种调度类型requiredDuringSchedulingIgnoredDuringExecution(硬要求)和preferredDuringSchedulingIgnoredDuringExecution(软要求)，多种规则匹配条件配置。例 部署一个nginx和mysql因为nginx和mysql之间的交互很频繁，所以尽量将他们部署在一个host上。先启动一个nginx。 12345678910apiVersion: v1kind: Podmetadata: name: nginx labels: app: nginxspec: containers: - name: nginx image: nginx 启动mysql 1234567891011121314151617181920212223apiVersion: v1kind: Podmetadata: name: mysql labels: app: mysqlspec: containers: - name: mysql image: nginx env: - name: &quot;MYSQL_ROOT_PASSWORD&quot; value: &quot;123456&quot; affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - nginx topologyKey: kubernetes.io/hostname topologyKey可以设置成如下几种类型kubernetes.io&#x2F;hostname ＃Nodefailure-domain.beta.kubernetes.io&#x2F;zone ＃Zonefailure-domain.beta.kubernetes.io&#x2F;region #Region可以设置node上的label的值来表示node的name,zone,region等信息，pod的规则中指定topologykey的值表示指定topology范围内的node上运行的pod满足指定规则 这里我们配置podaffinity，mysql会调度到有app:nginx这个label的pod的host上，所以你部署出来的mysql会调度到有nginx的pod上，因为默认部署出来的nginx就自带app:nginx这个label。 场景四：部署应用需要互斥，不能同时运行在一台主机上，会冲突我们继续以我们刚刚的nginx+mysql这个应用组合为例，现在我们修改mysql的podaffinity为podAntiAffinity 1234567891011121314151617181920212223apiVersion: v1kind: Podmetadata: name: mysql labels: app: mysqlspec: containers: - name: mysql image: nginx env: - name: &quot;MYSQL_ROOT_PASSWORD&quot; value: &quot;123456&quot; affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - nginx topologyKey: kubernetes.io/hostname 将podaffinity改为podAntiAffinity这样调度mysql这个pod时会避开有label为app:nginx的pod的主机 高级特性Taints and tolerationsTaints其实根nodeaffinity正好相反，nodeaffinity是将设置策略的pod调度到期望的节点上，而taints正好相反，如果一个节点被标记为taints，除非pod配置了tolerations，否则是不会被允许调度过来在生产环境中我们一般会将master节点配置Taints，因为master只跑kubernetes 系统组件，如果跑了用户应用pod容易把资源耗尽，造成master节点崩溃，当然后期如果要添加额外的系统组件，这时就可以通过给对应的pod配置toleration。 设置rke-node2不能被调度 1kubectl taint nodes rke-node2 key=value:NoSchedule value可以配置多个值如NoSchedule：不能调度，当之前调度的不管。PreferNoSchedule：尽量不调度上去，其实就是Noschedule软策略版。NoExecute：不能调度，但之前已经调度上去的也会自动迁移走。 NoSchedule更像执行kubectl cordon xxxNoExecute更想执行kubectl cordon xxx+kubectl drain 取消taints 1kubectl taint nodes rke-node2 key- 如何让pod调度到配置了taints主机上?通过配置tolerations，可以让pod调度到配置了taints的机器上如 12345678910111213141516171819202122232425262728apiVersion: apps/v1beta1kind: Deploymentmetadata: name: mysql-2spec: selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: containers: - image: mysql:5.6 name: mysql env: - name: MYSQL_ROOT_PASSWORD value: password ports: - containerPort: 3306 name: mysql tolerations: - key: &quot;key&quot; operator: &quot;Equal&quot; value: &quot;value&quot; effect: &quot;NoSchedule&quot; https://blog.csdn.net/horsefoot/article/details/72827790","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"kubernetes pod的弹性伸缩(一)(基于CPU、memory的弹性伸缩)","slug":"kubernetes_hpa","date":"2018-07-20T13:45:59.000Z","updated":"2018-07-20T13:45:59.000Z","comments":true,"path":"2018/07/20/kubernetes_hpa/","permalink":"http://yoursite.com/2018/07/20/kubernetes_hpa/","excerpt":"","text":"概念HPA是kubernetes里面pod弹性伸缩的实现,它能根据设置的监控阀值进行pod的弹性扩缩容，目前默认HPA只能支持cpu和内存的阀值检测扩缩容，但也可以通过custom metric api 调用prometheus实现自定义metric 来更加灵活的监控指标实现弹性伸缩。但hpa不能用于伸缩一些无法进行缩放的控制器如DaemonSet。这里我们用的是resource metric api. 实现hpa的两大关键1、监控指标的获取早期kubernetes版本是使用hepster，在1.10后面版本更推荐使用metric-server hepster简单来说是api-server获取节点信息，然后通过kubelet获取监控信息，因为kubelet内置了cadvisor。 metric-server，简单来说是通过metric-api来获取节点信息和监控信息。https://github.com/kubernetes-incubator/metrics-server 2、伸缩判定算法HPA通过定期（定期轮询的时间通过–horizontal-pod-autoscaler-sync-period选项来设置，默认的时间为30秒）查询pod的状态，获得pod的监控数据。然后，通过现有pod的使用率的平均值跟目标使用率进行比较。pod的使用率的平均值：监控资源1分钟使用的平均值&#x2F;设定的每个Pod的request资源值 扩容的pod数计算公式 1TargetNumOfPods = ceil(sum(CurrentPodsCPUUtilization) / Target) celi函数作用：返回大于或者等于指定表达式的最小整数 在每次扩容和缩容时都有一个窗口时间，在执行伸缩操作后，在这个窗口时间内，不会在进行伸缩操作，可以理解为类似等一下放技能的冷却时间。默认扩容为3分钟(–horizontal-pod-autoscaler-upscale-delay)，缩容为5分钟(–horizontal-pod-autoscaler-downscale-delay)。另外还需要以下情况下才会进行任何缩放avg（CurrentPodsConsumption）&#x2F; Target下降9%，进行缩容，增加至10%进行扩容。以上两条件需要都满足。 这样做好处是：1、判断的精度高，不会频繁的扩缩pod，造成集群压力大。2、避免频繁的扩缩pod，防止应用访问不稳定。 实现hpa的条件：1、hpa不能autoscale daemonset类型control2、要实现autoscale，pod必须设置request 配置HPA这里以kubeadm 部署和的kubernetes 1.11和Rancher2.0部署的kubernetes 1.10为例环境信息 操作系统：ubuntu16.04kubernetes版本：1.11rancher：2.0.6metric-server：v0.3.1 kubeadm方式将metric-server从github拉取下来 1git clone https://github.com/kubernetes-incubator/metrics-server.git -b v0.3.1 早期kubelet的10255端口是开放，但后面由于10255是一个非安全的端口容易被入侵，所以被关闭了。metric-server默认是从kubelet的10255端口去拉取监控信息的，所以这里需要修改从10250去拉取 1edit metrics-server/deploy/1.8+/metrics-server-deployment.yaml 添加command的一些配置参数 12345678containers: - name: metrics-server image: k8s.gcr.io/metrics-server-amd64:v0.3.1 command: - /metrics-server - --kubelet-insecure-tls - --kubelet-preferred-address-types=InternalIP apply yaml文件 1kubectl apply -f metrics-server/deploy/1.8+/ 等待一分钟执行 1kubect top node 1kubectl top pods 查看pod和node监控信息 创建个一个deployment，配置hpa测试 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162apiVersion: v1kind: Servicemetadata: name: podinfo labels: app: podinfospec: type: NodePort ports: - port: 9898 targetPort: 9898 nodePort: 31198 protocol: TCP selector: app: podinfo---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: podinfospec: replicas: 2 template: metadata: labels: app: podinfo annotations: prometheus.io/scrape: &#x27;true&#x27; spec: containers: - name: podinfod image: stefanprodan/podinfo:0.0.1 imagePullPolicy: Always command: - ./podinfo - -port=9898 - -logtostderr=true - -v=2 volumeMounts: - name: metadata mountPath: /etc/podinfod/metadata readOnly: true ports: - containerPort: 9898 protocol: TCP resources: requests: memory: &quot;32Mi&quot; cpu: &quot;1m&quot; limits: memory: &quot;256Mi&quot; cpu: &quot;100m&quot; volumes: - name: metadata downwardAPI: items: - path: &quot;labels&quot; fieldRef: fieldPath: metadata.labels - path: &quot;annotations&quot; fieldRef: fieldPath: metadata.annotations apply yaml文件配置hpa 123456789101112131415161718192021apiVersion: autoscaling/v2beta1kind: HorizontalPodAutoscalermetadata: name: podinfospec: scaleTargetRef: apiVersion: extensions/v1beta1 kind: Deployment name: podinfo minReplicas: 2 maxReplicas: 10 metrics: - type: Resource resource: name: cpu targetAverageUtilization: 80 - type: Resource resource: name: memory targetAverageValue: 200Mi apply yaml文件 get hpa 测试使用webbench进行压力测试。 1wget http://home.tiscali.cz/~cz210552/distfiles/webbench-1.5.tar.gz 1tar -xvf webbench-1.5.tar.gz 12345cd webbench-1.5/make &amp;&amp; make installwebbench -c 1000 -t 12360 http://172.31.164.104:31198/ 可以看见随着cpu压力的增加，已经自动scale了，需要注意的是，scale up是一个阶段性的过程，并不是一次性就直接scale到max了，而是一个阶段性的过程，判定算法就是上文介绍的内容。隔断时间没操作压力下来后，自动缩减pod https://zhuanlan.zhihu.com/p/34555654https://github.com/kubernetes/community/blob/master/contributors/design-proposals/autoscaling/horizontal-pod-autoscaler.md#autoscaling-algorithmhttps://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-metrics-APIs","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"Jenkins对接sonarqube","slug":"jenkins&sonar","date":"2018-06-11T13:45:59.000Z","updated":"2018-06-11T13:45:59.000Z","comments":true,"path":"2018/06/11/jenkins&sonar/","permalink":"http://yoursite.com/2018/06/11/jenkins&sonar/","excerpt":"","text":"概述：sonar是一个开源的代码质量分析检测平台，通过插件机制，能支持多种开发语言的代码质量检测，如java、php、ruby等，支持自定义代码扫描规则，同时sonar也支持对接常见的CI平台如Jenkins等，支持CI平台将代码重代码仓库拉取下来后直接调用sonar进行代码扫描，并将扫描结果进行汇总分类，汇制表图。 环境信息：system：ubuntu:16.04Jenkins：2.89.4java：jdk1.8.161rancher：2.0.2kubernetes：1.10.1 sonar安装sonar安装这里直接使用ranche2.0内置的应用商店安装的，当然你也可以通过其他方式安装 默认部署出来的访问是通过ingress，为了方便直接改成NodePort 部署成功默认会部署4个pod另外两个test直接删了，留这两个sonarqube和sonar-postgresql就可以 通过hostip:nodeport方式访问sonar,比如我这里nodeport是31963 sonar配置访问(默认帐号密码是admin&#x2F;admin) 如果需要中文直接安装插件就好administrator—-&gt;Marketplace搜索Chinese—-安装 安装java语言插件 申请tokenadministrator—&gt;security—&gt;user—&gt;token保存生成token Jenkins配置安装插件系统设置—&gt;插件管理安装SonarQube Scanner for Jenkins 配置系统设置—&gt;配置—&gt;SonarQube servers 1234Name：SonarQubeServer URL ：sonar-serverserver authentication token ： 刚刚sonar生成的tokenVersion of sonar-maven-plugin :5.3 or higher 配置Sonar-Scanner sonar-Scanner用于Jenkins扫描可以选自动安装也可以手动选择应该安装好了的，将路径填上去。这里我们选自动安装 系统管理—&gt;全局工具配置 配置JAVA_HOME 测试freestyle sudoers java路径Jenkins创建freestyle 1234567sonar.projectKey=Test #sonar那显示project-keysonar.projectName=Test #sonar那显示project名字sonar.projectVersion=1.0 ##sonar那显示project版本sonar.sources=srcsonar.java.binaries=/var/lib/jenkins/workspace/piggymetrics/gateway/target/classessonar.language=javasonar.sourceEncoding=UTF-8 执行构建 pipline 123456789101112131415node&#123; stage(&#x27;git clone&#x27;)&#123; //check CODE git credentialsId: &#x27;ab35a358-9743-4a65-afd2-58a56a3e8f29&#x27;, url: &#x27;http://172.31.164.58:1080/wanshaoyuan/piggymetrics-account-service.git&#x27; &#125; stage(&#x27;SonarQube analysis&#x27;) &#123; def sonarqubeScannerHome = tool name: &#x27;SonarQube Scanner&#x27; withSonarQubeEnv(&#x27;SonarQube&#x27;) &#123; sh &quot;$&#123;sonarqubeScannerHome&#125;/bin/sonar-scanner&quot; &#125; &#125;&#125; pipline必须在对应的代码库的根目录创建sonar-project.properties添加以下内容 12345678910111213sonar.projectKey=piggymetrics-auth-service:0.0.1# this is the name and version displayed in the SonarQube UI. Was mandatory prior to SonarQube 6.1.sonar.projectName=piggymetrics-auth-servicesonar.projectVersion=0.0.1# Path is relative to the sonar-project.properties file. Replace &quot;\\&quot; by &quot;/&quot; on Windows.# This property is optional if sonar.modules is set.sonar.sources=srcsonar.java.binaries=/var/lib/jenkins/workspace/piggymetrics-auth-service/target/classessonar.java.source=1.8sonar.java.target=1.8sonar.language=java# Encoding of the source code. Default is default system encodingsonar.sourceEncoding=UTF-8 执行构建 最终在sonarQue看见 测试代码可以直接使用测试 1https://github.com/sqshq/PiggyMetrics","categories":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/categories/CI-CD/"}],"tags":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/tags/CI-CD/"}]},{"title":"容器云平台GPU资源的应用","slug":"gpu_user","date":"2018-05-26T13:45:59.000Z","updated":"2018-05-26T13:45:59.000Z","comments":true,"path":"2018/05/26/gpu_user/","permalink":"http://yoursite.com/2018/05/26/gpu_user/","excerpt":"","text":"环境准备阿里云创建GPU计算型，规格型号为GPU计算型GN5，对应NVIDA的GPU型号为Nvidia P100 GPU 先决条件 主机安装gcc bios禁用禁用secure boot，也就是设置为disable 如果没有禁用secure boot,会导致NVIDIA驱动安装失败，或者不正常。 禁用nouveau打开编辑配置文件：1234sudo gedit /etc/modprobe.d/blacklist.conf在最后一行添加：blacklist nouveau 安装使用docker-ce:19.03 安装NVIDIA驱动阿里云启动机器时也可以选择自动安装驱动，这里为了方便连接使用手动方式。 linux上安装NVIDIA驱动有两种方式 方式一：通过执行二进制脚本安装 方式二：通过安装cuda的rpm包的方式安装 查看GPU型号 12 lspci|grep NV00:08.0 3D controller: NVIDIA Corporation GP100GL [Tesla P100 PCIe 16GB] (rev a1) 选择对应的型号下载gpu驱动https://www.nvidia.cn/Download/index.aspx?lang=cn安装gcc 1yum install gcc* -y 下载驱动 1wget https://cn.download.nvidia.cn/tesla/440.64.00/nvidia-driver-local-repo-rhel7-440.64.00-1.0-1.x86_64.rpm 若下载慢，也可以通过以下链接下载 1https://v2.fangcloud.com/share/cec9fca23cb2fe56b2d9d0732f 1rpm -ihv nvidia-driver-local-repo-rhel7-440.64.00-1.0-1.x86_64.rpm 1yum install cuda-driver -y 重启节点 1reboot 查看gpu 123456789101112131415161718nvidia-smi Sat Apr 25 00:32:30 2020 +-----------------------------------------------------------------------------+| NVIDIA-SMI 440.64.00 Driver Version: 440.64.00 CUDA Version: 10.2 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 Tesla P100-PCIE... Off | 00000000:00:08.0 Off | 0 || N/A 24C P0 26W / 250W | 0MiB / 16280MiB | 4% Default |+-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+ 配置docker使用GPUdocker要使用GPU，需要将docker的runtime替换为NVIDIA的docker-runtime，替换方法如下 安装NVIDIA-docker2123distribution=$(. /etc/os-release;echo $ID$VERSION_ID)curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | \\sudo tee /etc/yum.repos.d/nvidia-docker.repo 更新yum源 123456789DIST=$(sed -n &#x27;s/releasever=//p&#x27; /etc/yum.conf)DIST=$&#123;DIST:-$(. /etc/os-release; echo $VERSION_ID)&#125;sudo rpm -e gpg-pubkey-f796ecb0sudo gpg --homedir /var/lib/yum/repos/$(uname -m)/$DIST/*/gpgdir --delete-key f796ecb0sudo gpg --homedir /var/lib/yum/repos/$(uname -m)/latest/nvidia-docker/gpgdir --delete-key f796ecb0sudo gpg --homedir /var/lib/yum/repos/$(uname -m)/latest/nvidia-container-runtime/gpgdir --delete-key f796ecb0sudo gpg --homedir /var/lib/yum/repos/$(uname -m)/latest/libnvidia-container/gpgdir --delete-key f796ecb0sudo yum update 安装NVIDIA-docker2 1sudo yum install nvidia-docker2 -y 修改默认RUNTIME安装后会自动替换原有的daemon.json文件，需要重新修改替换，将默认的runtime替换为NVIDIA-container-runtime 12345678910111213141516tee /etc/docker/daemon.json &lt;&lt; EOF&#123; &quot;default-runtime&quot;: &quot;nvidia&quot;, &quot;registry-mirrors&quot;: [&quot;https://vqgjby9l.mirror.aliyuncs.com&quot;], &quot;runtimes&quot;: &#123; &quot;nvidia&quot;: &#123; &quot;path&quot;: &quot;nvidia-container-runtime&quot;, &quot;runtimeArgs&quot;: [] &#125; &#125;, &quot;log-opts&quot;: &#123; &quot;max-size&quot;: &quot;100m&quot;, &quot;max-file&quot;: &quot;3&quot; &#125;&#125;EOF 重启docker 1systemctl daemon-reload &amp;&amp; systemctl restart docker 执行docker info检查 测试docker调用使用一个简单的训练任务容器测试是否能正常通过docker调用GPU 1docker run -itd wanshaoyuan/pytorch:v1.0 查看GPU状态，有将正常进程调度到GPU上 1nvida-smi k8s配置对接docker对接成功后就可以正常使用容器应用调用GPU资源了，但此时kubernetes还是无法发现GPU资源对象，需要在在kubernetes中安装k8s-device-plugin插件这样kubelet就能正常上报GPU资源信息安装k8s-device-plugin 1kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta6/nvidia-device-plugin.yml 安装完成后会在kube-system命名空间中以daemonset方式部署nvidia-device-plugin此时通过kubectl查看节点上报资源信息可以看见GPU资源对象了 1kubectl describe node/xxx 在kubernetes中运行训练任务进行测试 默认情况下采用独占模式，在资源配置中设置gpu资源对象 kubernetes会将workload自动调度到有GPU的节点上在此查看GPU资源使用信息，调度成功。 参考文档：https://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html#redhat-x86_64","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"kubernetes 持久化存储","slug":"kubernetes_storage","date":"2018-05-19T13:45:59.000Z","updated":"2019-08-25T13:45:59.000Z","comments":true,"path":"2018/05/19/kubernetes_storage/","permalink":"http://yoursite.com/2018/05/19/kubernetes_storage/","excerpt":"","text":"操作系统：ubuntu 16.04kubernetes版本：1.10.0ceph：10.2.10 kubernetes常见的三种应用1、无状态应用容器内的应用状态无需保持，kubernetes通过replicaset保证pod的数量，一旦pod挂掉或崩溃，会基于image重建pod，此时pod内数据丢失。2、有状态应用和无状态应用相比，有状态应用多了一个应用状态保存的需求.3、有状态集群应用和有状态应用相比，多了集群管理的需求，那么需要解决的问题有两个，一个是应用状态的保存，一个是集群的管理。 有状态应用和有状态集群应用需要持久化存储里面的数据，比如数据库我们需要存储里面的数据库文件，直接用docker，我们可以使用docker的bind-mont、docker-managed-volume 、volume-container的方式，在kubernetes中解决方案是kubernetes volume和kubernetes persistent volume 。volume独立于pod，pod被销毁了数据没有了，但volume还是存在的，可以给其他pod使用。本质上，volume就是一个目录根目录的映射，它后端可以对应各种各样的存储，但pod在使用时，并不需要关心这些，对于pod来说，它看见的只是一个目录，这点根docker volume基本一样，当一个volume mount到pod中时，这个pod中所有容器都可以访问这个volume，kuberne volume支持多种类型的后端存如awsElasticBlockStore、azureDisk、ceph-rbd 完整参考列表https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes kubernetes中volume分为两种类型静态供给的和动态供给的静态供给volume：emptyDir、hostpath，Persistent volume动态供给volume：storage-class 静态供给volume 缺点：pod直接访问volume可移值性和可扩展性、安全性较差 emptyDiremptydir：临时空目录，与pod紧密联接在创建pod是创建，在删除这个pod时，也会自动删除，pod迁移到其他节点数据会丢失。用途：用于存储一些临时数据，如cookie等。 1234567891011121314151617181920例：apiVersion: v1kind: Podmetadata: name: producer-consumerspec: containers: - image: busybox name: producer volumeMounts: - mountPath: /producer_dir name: shared-volume args: - /bin/sh - -c - echo &quot;hello world&quot; &gt;/producer_dir/hello; sleep 30000 volumes: - name: shared-volume emptyDir: &#123;&#125; 创建一个pod里面有两个container，生产者、消费者共享一个emptyDir，生产者向&#x2F;producer_dir&#x2F;hello写入hello world 通过docker inspect查看映射到宿主机哪个目录 12docker inspect 5681556dfd95|grep Source &quot;Source&quot;: &quot;/var/lib/kubelet/pods/a22e4253-8290-11e8-85ec-00163e04ede2/volumes/kubernetes.io~empty-dir/shared-volume&quot;, hostpath：bind-mount与宿主机目录1:1映射，这类存储卷，当数据迁移到其他节点后，就会造成数据丢失。用途：根DaemonSet配合使用如EFK，中fluentd 根容器日志目录映射，来达到收集日志效果。 例 123456789101112131415161718192021222324252627282930apiVersion: apps/v1beta1kind: Deploymentmetadata: name: mysqlspec: selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: containers: - image: mysql:5.6 name: mysql env: - name: MYSQL_ROOT_PASSWORD value: password ports: - containerPort: 3306 name: mysql volumeMounts: - name: vol1 mountPath: /var/lib/mysql volumes: - hostPath: path: /tmp/mysql name: vol1 将pod内的&#x2F;var&#x2F;lib&#x2F;mysql与宿主机的&#x2F;tmp&#x2F;mysql映射。 storage-private不使用host存储空间，使用公有云厂商对象存储或分布式 persisten volume volume虽然能提供很好的数据持久化，但在可管理性上，还是不足的，要使用volume，用户必须，知道当前的volume信息和提前创建好对应的volume，kubernetes推荐使用pv、pvc来解决存储持久化的问题。因此kubernetes给出的解决方案是pv(persistent volume)、pvc(persistent volume Claim)persisten volume(pv)是k8s里面的一个资源对象，它是直接和底层存储关联的，pv具有持久性，生命周期独立于pod。persisten volume claim(pvc)是对pv的具体实现，pod使用存储是直接使用pvc，然后pvc会根据pod需要的存储空间大小和访问模式在去寻找合适的pv然后绑定。 kubernetes支持的pv类型https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes例子这里以nfs做为pv后端存储为例讲解pv和pvc的使用搭建nfs 1apt install nfs-kernel-server nfs-common rpcbind 配置共享目录修改&#x2F;etc&#x2F;exports文件 1/nfsdata *(rw,sync,no_root_squash) 1234*：表示所有用户都可以访问rw：挂接此目录的客户端对该共享目录具有读写权限sync：资料同步写入内存和硬盘no_root_squash：root用户具有对根目录的完全管理访问权限 启动rpcbind 1systemctl status rpcbind 启动nfs 1systemctl start nfs-kernel-server 验证 1showmount -e nfs_server_ip 挂载 1mount -t nfs 172.31.164.57:/nfsdata /mnt/ 配置pv 1234567891011121314apiVersion: v1kind: PersistentVolumemetadata: name: mysql-pvspec: accessModes: - ReadWriteOnce #可读写模式支持单节点挂载 capacity: storage: 1Gi persistentVolumeReclaimPolicy: Retain storageClassName: nfs nfs: path: /nfsdata/mysql-pv server: 172.31.164.57 pv的回收模式 persistentVolumeReclaimPolicy 为当pvc删除后pv的回收策略。 Retain – pvc删除后pv和数据仍然保留但此时不可以在创建pvc了，需要管理员手工回收。 Recycle – pvc删除后回自动起一个pod将pv内的数据全部清空，可以创建新的pvc。 Delete – 删除 Storage Provider 上的对应存储资源，例如 AWS EBS、GCE PD、Azure Disk、OpenStack Cinder Volume 等。 pv的访问模式在pvc绑定pv时通常根据两个条件绑定，一个是存储的大小，另外一个是存储的模式 ReadWriteOnce(RWO)：可读写模式支持单节点挂载。 ReadOnlyMany(ROX)： 只读模式支持多节点挂载。 ReadWriteMany(RWX)：可读写模式支持多节点挂载，目前只有少数存储支持这种方式，像ceph-rbd目前只能当个节点挂载。 创建pvc 1234567891011kind: PersistentVolumeClaimapiVersion: v1metadata: name: mysql-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: nfs pvc只需要指定大小，访问模式和className就会根pv关联。 创建mysql使用pvc 123456789101112131415161718192021222324252627282930apiVersion: apps/v1beta1kind: Deploymentmetadata: name: mysqlspec: selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: containers: - image: mysql:5.6 name: mysql env: - name: MYSQL_ROOT_PASSWORD value: password ports: - containerPort: 3306 name: mysql volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-pvc 进入container创建些数据 将pod所在宿主机关闭了，过几分钟kubernetes会在另外宿主机上启动,并且还是使用这个pvc，因为nfs支持多个host同时挂载. 动态供给volume(storage_class) storage-class：直接使用pv方式都是静态供给，需要管理员提前将pv创建好，然后再与pvc绑定，在kubernetes中动态卷是通过storage-class去实现的。配好storage-class与backend对接，当没有满足pvc条件的pv时，storage-class会动态的去创建一个pv动态卷的优势 1、不需要提前创建好pv，提高效率和资源利用率 2、封装不同的存储类型给pvc使用，在StorageClass出现以前，PVC绑定一个PV只能根据两个条件，一个是存储的大小，另一个是访问模式。在StorageClass出现后，等于增加了一个绑定维度。 在PVC里除了常规的大小、访问模式的要求外，还通过annotation指定了Storage Class的名字为fast，这样这个PVC就会绑定一个SSD，而不会绑定一个普通的磁盘。 例这里我们以ceph-rbd为例配置一个storage-class因为操作系统加载rbd modulemodprobe rbd记得加入开机启动脚本，不然重启module又没加载创建一个ceph-rbd的storage-class获取admin的secret并用base64加密 1ceph auth get-key client.admin |base64 创建个secret对象 12345678apiVersion: v1kind: Secretmetadata: name: ceph-secret-admintype: &quot;kubernetes.io/rbd&quot;data: key: QVFEMDJ1VmE0L1pxSEJBQUtTUnFwS3JFVjErRjFNM1kwQ2lyWmc9PQ== 创建storage-class 12345678910111213kind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: rbdprovisioner: kubernetes.io/rbdparameters: monitors: 172.31.164.57:6789 adminId: admin adminSecretName: ceph-secret-admin adminSecretNamespace: default pool: rbd userId: admin userSecretName: ceph-secret-admin monitors: ceph-monitor地址+端口adminId: secret的用户名adminSecretName: 上面创建secret的名字pool：rbd 在ceph哪个pooluserSecretName：上面创建secret的名字 123456789101112kind: PersistentVolumeClaimapiVersion: v1metadata: name: mysql-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: rbd storageClassName指向我们刚刚创建storage-class可以看见已经Bound了 查看ceph的pool，发现块设备已经创建好了继续使用上面的yml创建mysql，会直接用这个pvc 到rke-node3上去可以看见kernel rbd module将pool里面的块映射出来了，挂载到pod里面了 如果非hyperkube部署的kubernetes集群，如kubeadm部署的，在创建pvc时会报错如下 12persistentvolume-controller Warning ProvisioningFailed Failed to provision volume with StorageClass &quot;rbd&quot;: failed to create rbd image: executable file not found in $PATH, command output: 因为我们的k8s集群是使用kubeadm创建的，k8s的几个服务也是跑在集群静态pod中，而kube-controller-manager组件会调用rbd的api，但是因为它的pod中没有安装rbd，所以会报错。解决办法如下使用kubernetes上的存储扩展卷来解决https://github.com/kubernetes-incubator/external-storage 1git clone https://github.com/kubernetes-incubator/external-storage 执行 1234kubectl apply -f ./kubectl get podsNAME READY STATUS RESTARTS AGErbd-provisioner-23232323-i6223 1/1 Running 0 13s 然后在重新创建storage-class 1cat ceph-storageclass.yaml 1234567891011121314kind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: rbd#provisionen: kubernetes.io/rbdprovisioner: ceph.com/rbdparameters: monitors: 172.31.164.57:6789 adminId: admin adminSecretName: ceph-secret-admin adminSecretNamespace: default pool: rbd userId: admin userSecretName: ceph-secret-admin 注意provisioner由provisionen: kubernetes.io&#x2F;rbd改成provisioner: ceph.com&#x2F;rbd然后在创建pvc就没有问题了。 使用rbd做为pv的backend或storagclass的backend整体原理如下：创建pvc—-对应在ceph pool内创建一个rbd对象。pod挂载这个pvc—-将对应的rbd块通过内核rbd模块map到宿主机上。然后在mount到宿主机 &#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;pods&#x2F;xxxx目录，在将这个目录根pod对bind mount。删除pod—-在对应的host上umount目录然后将rbd块与宿主机unmap。在kubernetes1.11之前的版本有个问题就是删除pod后，不会自动将rbd块从host上unmap。PR如下 1.11版本已经修复这个问题https://github.com/kubernetes/kubernetes/pull/63579 就是在使用pod如果挂载后端为rbd的pvc后，在底层实际上是会从ceph的pool里面将一个rbd块设备map的host上然后在mount到pod内，需要注意的是： 因为在k8s里面是直接用的是内核的rbd块，openstack用的是librbd接口，所以这里需要在每个host上内核要加载rbd模块，不然挂载不上去的，modprobe rbd。 ceph-rbd 在AccessMode为ReadWriteOnce情况下只支持单节点挂载，不能被多个pod同时使用这个pvc。也就是说一个host挂了，如果上面pod使用了ceph-rbd，哪里在另外一个host上重建这个pod时会报这个pvc已经被使用了，但在AccessMode为ReadWriteOnce+ReadOnlyMany的情况下支持一个pvc被多个pod挂载。 local-pvLocal-pv是让用户使用标准的k8s pv和pvc的接口使用node节点的本地存储，kubernetes1.7为alpha版本，1.14正式GA，这和我们上面说的host-path和empty-Volume有个共同点都是使用node节点的本地存储，但区别在于hostpath是单节点的本地存储，也就是说只有当pod调度到哪台节点便使用那台节点对应的存储，无法提供node亲和性和POD调度支持，但通过localpv可以在pv的定义中配置节点亲和性，这样对应的使用这个pvc的POD也会根据亲和性配置调度到对应的节点上，比如在local-pv的定义配置了主机亲和性为node-2，那么使用这个pvc的pod也会调度到node-2上。 使用场景：1、分布式数据库和分布式文件系统（redis、ElasticSearch、memcache、ceph)，这类应用在应用本身能实现高可用，只是依靠localpv实现高性能存储。2、需要临时缓存存储的应用，当应用删除后对应的数据也能随之清理，因为使用localpv可以灵活设置pv的回收策略。 测试版本：kubernetesv1.14.3部署storageclass 123456apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: local-storageprovisioner: kubernetes.io/no-provisionervolumeBindingMode: WaitForFirstConsumer volumeBindingMode: WaitForFirstConsumer ：配置延迟绑定，也就是pv control并不会立刻将pvc与pv关联，而是等待pod调度后才去bond，还有种默认的Immediate模式则是当pvc创建时立刻绑定。 创建pv 12345678910111213141516171819202122apiVersion: v1kind: PersistentVolumemetadata: name: example-pvspec: capacity: storage: 100Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: delete storageClassName: local-storage local: path: /mnt/ nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - rke-node2 注意点 ： volumeMode: Filesystem为文件系统模式，也就是对应的目录，这里也可以直接设置为BlockVolume模式直接对应主机上的块设备，当需要在kube-apiserver、kube-controlmanager、kubelet开启–feature-gates&#x3D;BlockVolume&#x3D;true参数，但这个参数在1.13版本已经默认是true了，参考：https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/ local path设置的就是对应宿主机的供挂载的目录，nodeselectorTerms设置的节点的亲和性。创建pvc 12345678910111213141516apiVersion: v1kind: PersistentVolumeClaimmetadata: labels: cattle.io/creator: norman name: example-local-claim namespace: defaultspec: accessModes: - ReadWriteOnce resources: requests: storage: 100Gi storageClassName: local-storage volumeMode: Filesystem volumeName: example-pv 创建POD使用 123456789101112131415161718kind: PodapiVersion: v1metadata: name: example-pv-podspec: volumes: - name: example-pv-storage persistentVolumeClaim: claimName: example-local-claim containers: - name: example-pv-container image: nginx ports: - containerPort: 80 name: &quot;http-server&quot; volumeMounts: - mountPath: &quot;/usr/share/nginx/html&quot; name: example-pv-storage 局限性1、目前local-pv不支持对空间申请管理，需要手动对空间进行配置和管理.2、默认local pv的StorageClass的provisioner是kubernetes.io&#x2F;no-provisioner , 这是因为local pv不支持Dynamic Provisioning, 所以它没有办法在创建出pvc的时候, 自动创建对应pv. static-provisioner配置 可以自己实现一个static-provisioner来创建和管理PV，可以直接使用社区已经写的static-provisioner：https://github.com/kubernetes-incubator/external-storage/tree/master/local-volume#option-1-using-the-local-volume-static-provisioner当然也可以自己实现，这里使用Rancher实现的static-provisionerhttps://github.com/rancher/local-path-provisioner部署 1kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml 默认pv的回收模式为delete模式，需要修改为recycle模式直接修改yaml即可。执行后会部署 123456namespace/local-path-storage createdserviceaccount/local-path-provisioner-service-account createdclusterrole.rbac.authorization.k8s.io/local-path-provisioner-role createdclusterrolebinding.rbac.authorization.k8s.io/local-path-provisioner-bind createddeployment.apps/local-path-provisioner createdstorageclass.storage.k8s.io/local-path created 查看POD 123kubectl get pod -n local-path-storageNAME READY STATUS RESTARTS AGElocal-path-provisioner-848fdcff-tqwc2 1/1 Running 0 35s 查看storage-class 123kubectl get storageclassNAME PROVISIONER AGElocal-path rancher.io/local-path 4m17s 创建PVC 1kubectl create -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/examples/pvc.yaml 在没创建pod使用此PVC前，默认模式为 WaitForFirstConsumer，所以pvc的状态会是pending状态。 创建POD 1kubectl create -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/examples/pod.yaml 默认目录为&#x2F;opt&#x2F;local-path-provisioner目录下，可以在POD所在节点上查看当我们需要进行细粒化配置时，如在node1节点上使用&#x2F;data目录，在node2节点使用&#x2F;data2目录，其他节点使用&#x2F;data3目录时可以修改 local-path-storage命名空间内的local-path-config这个configmap 12345678910111213141516171819kind: ConfigMapapiVersion: v1metadata: name: local-path-config namespace: local-path-storagedata: config.json: |- &#123; &quot;nodePathMap&quot;:[ &#123; &quot;node&quot;:&quot;DEFAULT_PATH_FOR_NON_LISTED_NODES&quot;, &quot;paths&quot;:[&quot;/opt/local-path-provisioner&quot;] &#125;, &#123; &quot;node&quot;:&quot;rke-node2&quot;, &quot;paths&quot;:[&quot;/data2&quot;] &#125; ] &#125; 更新成功后，查看 1kubectl logs pod/local-path-provisioner-848fdcff-lr2pj -n local-path-storage 可以看见其加载配置的信息。 参考链接https://www.kubernetes.org.cn/3462.htmlhttps://blog.csdn.net/liukuan73/article/details/60089305","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"kubernetes services","slug":"kubernetes_service","date":"2018-05-05T13:45:59.000Z","updated":"2017-05-05T13:45:59.000Z","comments":true,"path":"2018/05/05/kubernetes_service/","permalink":"http://yoursite.com/2018/05/05/kubernetes_service/","excerpt":"","text":"pod是随时变化的，动态的伸缩，所以应用程序直接连接pod是不可靠的，所以kubernetes引入service，service是为一组功能相同的pod提供统一入口。例 12345678910111213141516171819202122232425262728293031kind: ServiceapiVersion: v1metadata: name: httpd-deploymentspec: ports: - protocol: TCP port: 80 targetPort: 80 selector: app: http---apiVersion: apps/v1beta2kind: Deploymentmetadata: name: http-appspec: selector: matchLabels: app: http replicas: 2 template: metadata: labels: app: http spec: containers: - name: nginx image: httpd ports: - containerPort: 80 上面例子，我们创建一个名为http-app的deployment，然后创建了个Service，selector labels方式将Service与deployment绑定，service类型为 cluster-ip，也就是内部访问类型在kubernetes集群任意一个节点curl访问 端口类型portService暴露出来的内部访问端口targetPortpod暴露出来的端口NodePortNodePort是kubernetes提供外部访问service的暴露端口，它暴露在集群所有宿主机上，默认端口范围为30000~32767 服务类型ClusterIP（默认)使用clusterip类型的Service默认会使用一个集群内部ip，通过kube-proxy调用iptables创建规则，将流量转发到pod中，需要注意的是clusterip是一个virtual_ip没有真正的网络设备绑定，所以是ping不通它的，直接在集群内部的访问就好。 NodePort使用NodePort类型的Service时会在集群内部所有host上暴露一个端口用于外部访问 创建方法 123456789101112kind: ServiceapiVersion: v1metadata: name: httpd-deploymentspec: type: NodePort ports: - port: 80 targetPort: 80 nodePort: 30080 selector: app: http 不指定nodePort端口将从30000~32767里面随机选一个LoadBlancer 使用loadblance类型时，会向cloud provider申请映射到service本身的负载均衡，比如AWS的ELB、google cloud 的GCP、Azure的azure-load-balancer。 外部访问podNodePortloadblanceingress Service负载均衡kube-proxy 会检测 API Server 上对于 service 和 endpoint 的新增或者移除。对于每个新的 service，在每个 node 上，kube-proxy 都会设置相应的 iptables 的规则来记录应该转发的地址。当一个 service 被删除的时候，kube-proxy 会在所有的 pod 上移除这些 iptables 的规则。默认转发规则是round robin 或session affinitykube-proxy 模式：userspace、iptables、ipvs(beta) 当创建一个service后，service会后iptables添加如下规则，以上面创建的httpd-deployment service为例，它的cluster-ip为10.43.129.201 cluster-ip转发流程在集群任意一台节点上查看iptables规则 1iptables-save &gt;/tmp/1 123456-A KUBE-SERVICES -d 10.43.129.201/32 -p tcp -m comment --comment &quot;default/httpd-deployment: cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-KO6WMUDK3F2YFERC-A KUBE-SVC-KO6WMUDK3F2YFERC -m comment --comment &quot;default/httpd-deployment:&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-37DWITJLRCDPHWZD-A KUBE-SEP-37DWITJLRCDPHWZD -s 10.42.0.87/32 -m comment --comment &quot;default/httpd-deployment:&quot; -j KUBE-MARK-MASQ-A KUBE-SEP-37DWITJLRCDPHWZD -p tcp -m comment --comment &quot;default/httpd-deployment:&quot; -m tcp -j DNAT --to-destination 10.42.0.87:80-A KUBE-SEP-BWU5RQOROOHNX3YY -s 10.42.1.82/32 -m comment --comment &quot;default/httpd-deployment:&quot; -j KUBE-MARK-MASQ-A KUBE-SEP-BWU5RQOROOHNX3YY -p tcp -m comment --comment &quot;default/httpd-deployment:&quot; -m tcp -j DNAT --to-destination 10.42.1.82:80 第一条规则目的ip是cluster-ip，命中规则，然后将请求丢给KUBE-SVC-KO6WMUDK3F2YFERC处理第二条规则链KUBE-SVC-KO6WMUDK3F2YFERC 实现了将报文按50%的统计概率随机匹配，转给KUBE-SEP-37DWITJLRCDPHWZD第三条和第五条是指对pod的SNAT操作第四条和第六条KUBE-SEP-37DWITJLRCDPHWZD链直接进行DNAT操作将cluster-ip的80端口映射到pod的80 NodePort转发流程修改Service类型为NodePort 查看iptables 12-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/httpd-deployment:&quot; -m tcp --dport 30080 -j KUBE-SVC-KO6WMUDK3F2YFERC-A KUBE-SERVICES -d 10.43.129.201/32 -p tcp -m comment --comment &quot;default/httpd-deployment: cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-KO6WMUDK3F2YFERC 第一条规则目的端口味30080的请求转发给KUBE-SVC-KO6WMUDK3F2YFERC处理第二条规则将请求给cluster-ip10.43.129.201然后转发给KUBE-SVC-KO6WMUDK3F2YFERC后面就根上面的cluster-ip转发流程是一样的了，所以NodePort就是在cluster-ip转发基础上加了层DNAT到cluster-ip 服务发现两种方式环境变量DNS:1.10版之前是通过kube-dns实现服务发现，1.10版后可以用CoreDns替代kube-dns做服务发现创建一个Service后，kubernetes会自动将servername做为服务名，添加一条dns记录 比如上面那个例子Service名为httpd-deployment,那么其他pod要访问直接访问httpd-deployment就会解析到对应cluster-ip,跨namespace的只需要在域名后接上.namespace_name 如httpd-deployment.default http://www.dbsnake.net/how-kubernetes-use-iptables.html","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"CI(jenkins+gitlab)","slug":"CI","date":"2018-05-01T13:45:59.000Z","updated":"2018-05-01T13:45:59.000Z","comments":true,"path":"2018/05/01/CI/","permalink":"http://yoursite.com/2018/05/01/CI/","excerpt":"","text":"Jenkins是什么？Jenkins是一款java开发的功能强大的持续集成，持续发布工具，与之相对应的还有Drone这个工具，这里我们主要介绍Jenkins。 为什么使用Jenkins 安装和配置灵活方便，支持多种方式安装(docker、软件包、war包、源码） 配置 方便可视化操作 丰富的插件 可扩展，分布式架构(master-slave) 环境信息操作系统：ubuntu 16.04java：1.8.0_161-b12jenkins： 2.89.4 部署gitlab123456789docker run --detach \\ --hostname gitlab.example.com \\ --publish 443:443 --publish 80:80 --publish 22:22 \\ --name gitlab \\ --restart always \\ --volume /srv/gitlab/config:/etc/gitlab \\ --volume /srv/gitlab/logs:/var/log/gitlab \\ --volume /srv/gitlab/data:/var/opt/gitlab \\ gitlab/gitlab-ce:latest 访问http://gitlab_ip 安装Jenkinsjenkins的安装方式有多种，有通过容器方式安装的，有通过rpm包和deb包方式安装还有通过war包安装的，这里使用deb包方式安装。 因为Jenkins是基于java开发的所以我们先配置java环境先安装jdk8http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html 安装 12mkdir /usr/local/jdk1.8/tar -xvf jdk-8u16.tar.gz -C /usr/local/jdk1.8/ 配置环境变量编辑&#x2F;etc&#x2F;profile 123export JAVA_HOME=/usr/local/jdk1.8/jdk1.8.0_161export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jart 1source /etc/profile 测试 安装Jenkins1234wget -q -O - https://pkg.jenkins.io/debian/jenkins-ci.org.key | sudo apt-key add -sudo sh -c &#x27;echo deb http://pkg.jenkins.io/debian-stable binary/ &gt; /etc/apt/sources.list.d/jenkins.list&#x27;sudo apt-get updatesudo apt-get install jenkins 启动 1systemctl start jenkins 访问Jenkins默认是访问8080端口 从图上给的路径获取默认密码 安装方式标准安装和自定义安装，标准安装主要进行一些标准插件的安装，自定义安装就是自己可以选择插件进行安装。这里我们选择标准安装 等待安装完 安装完，设置用户名和密码 这边我们与gitlab做对接所以需要安装gitlab插件系统管理—&gt;管理插件—–&gt;可选插件—-&gt;gitlab plugin 安装完成后配置gitlab plugin系统管理—&gt;系统配置 输入gitlab的地址 还有填写gitlab的 access token先去gitlab申请 将token保存好 配置credentials kind那选gitlab token，然后add 给gitlab配置公钥，用于pull和push代码，哪台机器需要从gitlab上拉取和上传代码就需要配置，这里Jenkins节点需要，当然也可以直接在Jenkins上配置用户名和密码去连接在gitlab的SSH keys处贴上机器的公钥，注意区分机器 Jenkins上创建个job 点击确定 可以看见这里连接上了我们刚刚配置Jenkins gitlab-plugin的配置 源码管理 可以用ssh的连接也可以用http的连接，我这里用ssh的因为我改了默认gitlab的ssh端口所以链接需要改一下如果gitlab 的ssh用的不是默认的22端口需要改成如下ssh:&#x2F;&#x2F;git@ip:ssh_port&#x2F;wanshaoyuan&#x2F;wanshaoyuan.git1、增加了ssh:&#x2F;&#x2F; 2、增加了端口号 3、把用户名前面的冒号改成了斜杠(&#x2F;)因为我们还没有配置credentials所以会报错点击旁边add配置credentials 用户名填写你生成公钥写的用户名private key填写你的私钥 然后就不会报错了,同时我这里选择的也是master分支触发器这里有几种方式 分别解释下Build after other projects are built 当另一个构建任务完成之后触发Build periodically 周期性的触发Build when a change is pushed to GitLab. GitLab CI Service URL: http://191.8.2.112:12000/project/test-go-dev 当代码有更新的时候触发，通过GitLab CIGitHub hook trigger for GITScm polling 通过Github钩子触发Poll SCM 定期检查代码有无更新，有更新时触发 这里我们使用Build when a change is pushed to GitLab. GitLab CI Service URL的方式，因为使用定时检查的话对Jenkins压力会很大的，使用webhook的方式，当gitlab有push代码或其他方式时会通知Jenkins，所以这里还需要配置gitlab的webhook 复制Build when a change is pushed to GitLab. GitLab CI Service URL: 后面那串地址链接 打开gitlab 输入刚刚那串url 默认触发器是push事件时，触发webhook，需要注意的是因为我们这里的Jenkins配置了用户名和密码所以url需要在原来基础上添加用户名和密码其他不变格式为http://username:password@192.168.1.4:1080/project/dashboard-build 点击test按钮进行测试，返回状态码为200表示配置正确。 如果同网段Jenkins webhook 返回500，需要用root帐号登录进gitlabsetting–&gt;勾上Allow requests to the local network from hooks and services 构建 这里我们选执行shell 做个简单的操作比如说我git push一个test.txt的文件上去，然后用构建命令就去读取这个文件 应用并保存 然后我们push到test.txt到wanshaoyuan这个project写这样一段话 123git add .git commit -m &quot;test jenkins&quot;git push 可以看见Jenkins很快触发了构建 查看输出","categories":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/categories/CI-CD/"}],"tags":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/tags/CI-CD/"}]},{"title":"Jenkins介绍和使用","slug":"jenkins_1","date":"2018-05-01T13:45:59.000Z","updated":"2018-05-01T13:45:59.000Z","comments":true,"path":"2018/05/01/jenkins_1/","permalink":"http://yoursite.com/2018/05/01/jenkins_1/","excerpt":"","text":"Jenkins安装常见安装方式Jenkins安装可以直接参考官方文档https://www.jenkins.io/download/Jenkins软件主要分为两个大版本Stable版和Regular releases 版本，最大区别在于Stable版，每隔12周会从Stable版本里面选出LTS (长期支持) 版本作为该时间段的稳定版本。Regular releases 版本是每周更新。支持以下几种常见安装方式：1、对应操作系统发行版软件安装包方式安装2、War包安装3、容器化安装 安装Jenkins这里使用Docker安装，能够快速拉起，解决依赖问题在一台装有Docker的主机上执行以下命令 1docker run -d --name jenkins -u root -p 8080:8080 -p 50000:50000 -v /var/jenkins_home:/var/jenkins_home -v /usr/bin/docker:/usr/bin/docker -v /var/run/docker.sock:/var/run/docker.sock jenkins/jenkins:lts-jdk11 查看初始化密码 1docker logs jenkins --tail 100 访问主机的8080端口 安装方式标准安装和自定义安装，标准安装主要进行一些标准插件的安装，自定义安装就是自己可以选择插件进行安装。离线模式可以选自定义安装，这里我们选择标准安装。 设置管理员用户名和密码进入界面 插件管理在线安装插件这边我们以gitlab做对接所以需要安装gitlab插件为例，操作任何系统管理—&gt;管理插件—–&gt;可选插件—-&gt;gitlab plugin默认在线方式安装是通过境外的Jenkins插件服务器进行下载，网络质量会比较差，为了更好的下载速度，可以配置使用境内插件地址。Manager jenkins &gt; manage plugins &gt; advanced &gt; upgrade url重新填写 清华大学的Jenkins Plugin URL : https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/current/update-center.json 安装完成后可以在已安装插件这里看见安装完后的gitlab-plugin 离线安装插件对于无法联网环境，可以通过离线插件方式进行安装，Jenkins插件之间本身是有依赖的，需要先找到对应的插件所依赖的其他插件，一同下载导入即可。通过系统管理-插件管理-可选插件-过滤框输入Maven Integration-选择Maven Integration-点击安装 可以看见这个插件所需要的依赖插件 去https://plugins.jenkins.io/ 下载这些插件（.hpi） 将hpi文件导入到Jenkins系统管理-插件管理-高级-选择.hpi文件-上传 另外一种方式，在外网可通环境下使用容器方式部署，将插件安装完后，直接将此容器重新做为镜像，到离线环境中使用，这样新启动的Jenkins里面就包含了这些插件。 创建一个简单的编译任务创建构建任务 选择FreeStyle Project 构建方式选择执行shell 输入shell脚本 1echo ”Hello world“ 开始构建 构建完成后，在左下角可以看见对应的构建记录 点击控制台输出查看详细情况 这就是一个还没有对接任何代码仓库，简单的编译任务。 Jenkins权限管理通过权限控制，使项目组对应的成员只能看见本项目下构建任务，如前端项目组成员只能看见前端项目下的构建任务，后端项目组成员，只能看见后端项目下的构建任务 安装Role-based 插件 修改全局安全设置进入 系统管理-&gt;全局安全配置 中将 访问控制 里 授权策略 配置为 Role-Based Strategy： 创建用户Manager Jenkins——&gt;Manager users——&gt; 新建用户 在 Jenkins 中创建两个视图 backend、frontend 并且在两个视图中创建相应的任务： 创建角色下面将创建一个拥有全局可读权限的全局角色来保证全部用户拥有全部可读权限，然后再创建Item组角色，利用通配符方式匹配对应项目前缀，这样用户拥有“全局可读”+“匹配项目读写”权限。 进入 系统管理-&gt;Manage and Assign Roles 可以看到 角色管理 与 角色分配： 设置全局之读角色 添加item角色 在 Item roles 一栏添加组角色 group_backend、group_frontend 两个角色，并且设置： 角色 group_backend 只能看见以 backend.* 开头的视图与项目。角色 group_frontend 只能看见以 frontend.* 开头的视图与项目。设置完成后再设置两个角色都勾选全部权限，确保两个角色拥有该组的全部权限。 Pattern 里面输入通配符，一般是项目前缀，经常以 “xxxx.*” 命名，“xxxx” 为前缀，“.*”为匹配任意名称。 分配角色进入Manage and Assign Roles——&gt;Assign Roles选项卡 将frontend-user1分配到group_frontend角色，backend-user分配到group_backend角色 登录frontend-user1和backend-user查看分配结果","categories":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/categories/CI-CD/"}],"tags":[{"name":"CI/CD","slug":"CI-CD","permalink":"http://yoursite.com/tags/CI-CD/"}]},{"title":"rke部署kubernetes","slug":"rke部署kubernetes","date":"2018-03-26T13:45:59.000Z","updated":"2018-03-26T13:45:59.000Z","comments":true,"path":"2018/03/26/rke部署kubernetes/","permalink":"http://yoursite.com/2018/03/26/rke%E9%83%A8%E7%BD%B2kubernetes/","excerpt":"","text":"RKE（rancher kubernetes engine)是rancher 发布的一款轻量级的kubernetes部署工具，部署整个集群只需要一个命令和一个配置文件，就可以轻松部署kubernetes集群，还支持kubernetes的ha部署，各类网络plugin。环境:rke-node1 172.31.164.57 （部署节点)rke-node2 172.31.164.58 (master节点、etcd节点)rke-node3 172.31.164.59 (node节点) 软件版本rke 0.1.6docker 17.03 操作系统ubuntu 16.04 准备工作配置hosts172.31.164.57 rke-node1172.31.164.58 rke-node2172.31.164.59 rke-node3 配置免密码登录我们在操作系统是ubuntu 16.04，rke可以直接使用root，centos的话就不行，必须要普通用户，并且属组为dockerssh-copy-id rke-node1ssh-copy-id rke-node2ssh-copy-id rke-node3 关闭selinux关闭防火墙 配置路由转发&#x2F;etc&#x2F;sysctl.confnet.ipv4.ip_forward &#x3D; 1net.bridge.bridge-nf-call-ip6tables &#x3D; 1net.bridge.bridge-nf-call-iptables &#x3D; 1 安装docker（全部节点)因为目前rke部署的kubernetes还是1.8版本的，kubernetes 1.8只支持docker1.12.6、1.13.1、17.03 更新软件源 1apt-get update 安装软件包，允许apt使用https 1apt-get install apt-transport-https ca-certificates curl software-properties-common 导入docker官方key 1curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 添加软件源 1add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot; 在刷新下 1apt-get update 安装指定版本docker列出软件版本 1apt-cache policy docker-ce 安装docker-17.03 1apt-get install docker-ce=17.03.0~ce-0~ubuntu-xenial 启动 12systemctl enable docker &amp;&amp; systemctl start docker 安装rkehttps://github.com/rancher/rke/releases/ 下载rke_linux-amd64 1mv rke_linux-amd64.dms rke cluster.yml的来源有两种，一种是直接从github上下载完整的下载cluster.ymlrke主要依靠cluster.yml去对集群角色进行划分和配置下载 12https://github.com/rancher/rke/blob/master/cluster.ymlcluster.yml 修改 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091nodes:- address: 172.31.164.58 internal_address: &quot;&quot; role: - controlplane - etcd hostname_override: rke-node2 user: root docker_socket: /var/run/docker.sock ssh_key: &quot;&quot; ssh_key_path: ~/.ssh/id_rsa labels: &#123;&#125;- address: 172.31.164.59 internal_address: &quot;&quot; role: - worker hostname_override: rke-node3 user: root docker_socket: /var/run/docker.sock ssh_key: &quot;&quot; ssh_key_path: ~/.ssh/id_rsa labels: &#123;&#125;services: etcd: image: rancher/etcd:v3.0.17 extra_args: &#123;&#125; kube-api: image: rancher/k8s:v1.8.7-rancher1-1 extra_args: &#123;&#125; service_cluster_ip_range: 10.233.0.0/18 pod_security_policy: false kube-controller: image: rancher/k8s:v1.8.7-rancher1-1 extra_args: &#123;&#125; cluster_cidr: 10.233.64.0/18 service_cluster_ip_range: 10.233.0.0/18 scheduler: image: rancher/k8s:v1.8.7-rancher1-1 extra_args: &#123;&#125; kubelet: image: rancher/k8s:v1.8.7-rancher1-1 extra_args: &#123;&#125; cluster_domain: cluster.local infra_container_image: registry.cn-shenzhen.aliyuncs.com/rancher_cn/pause-amd64:3.0 cluster_dns_server: 10.233.0.3 fail_swap_on: false kubeproxy: image: rancher/k8s:v1.8.7-rancher1-1 extra_args: &#123;&#125;network: plugin: flannel options: &#123;&#125;authentication: strategy: x509 options: &#123;&#125;addons: &quot;&quot;system_images: etcd: &quot;&quot; alpine: &quot;&quot; nginx_proxy: &quot;&quot; cert_downloader: &quot;&quot; kubernetes_services_sidecar: &quot;&quot; kubedns: &quot;&quot; dnsmasq: &quot;&quot; kubedns_sidecar: &quot;&quot; kubedns_autoscaler: &quot;&quot; kubernetes: &quot;&quot; flannel: &quot;&quot; flannel_cni: &quot;&quot; calico_node: &quot;&quot; calico_cni: &quot;&quot; calico_controllers: &quot;&quot; calico_ctl: &quot;&quot; canal_node: &quot;&quot; canal_cni: &quot;&quot; canal_flannel: &quot;&quot; wave_node: &quot;&quot; weave_cni: &quot;&quot; pod_infra_container: &quot;&quot;ssh_key_path: ~/.ssh/id_rsaauthorization: mode: rbac options: &#123;&#125;ignore_docker_version: falsekubernetes_version: &quot;&quot;private_registries: []ingress: provider: &quot;&quot; options: &#123;&#125; node_selector: &#123;&#125; 注意几个点1、nodes内定义的就是节点的ip、角色、key位置，有以下几种角色controlplane：也就是kubernetes中的master节点部署kube-api、kube-controller-manger、kube-scheduler组件etcd：部署etcd，kubernetes集群存储后端数据信息worker：kubernetes的node节点，承载应用 2、默认是会直接部署ingress的要disable的将ingress的provider弄为node 1234ingress: provider: none options: &#123;&#125; node_selector: &#123;&#125; 还有种办法直接生成,按提示一步步输入。 .&#x2F;rke config #生成名为cluster.yml文件.&#x2F;rke config –name mycluster.yml #生成指定名字的配置文件 当配置文件和rke命令在一个文件夹下时可以直接使用直接部署命令，否则需要指定yml配置文件直接部署 1./rke up 指定配置文件部署 1rke up --config cluster.yml 部署成功 默认rke是没有安装kubectl的手动安装kubectl 1wget https://dl.k8s.io/v1.8.7/kubernetes-client-linux-amd64.tar.gz 1cp kubernetes/client/bin/kube* /usr/local/bin/ 1chmod a+x /usr/local/bin/kube* 1export PATH=/usr/local/bin:$PATH 默认在当前目录生成了kubeconfig文件创建kube文件夹 1mkdir /root/.kube/ 拷贝文件到&#x2F;root&#x2F;.kube内 1cp kube_config_cluster.yml /root/.kube/config 测试 部署应用 1kubectl run nginx --image=nginx:1.7.9 --replicas=3 验证 HA部署 rke部署ha的方式非传统的vip的架构，而是通过在每个host上部署个nginx-proxy的container，然后通过这个container做反向代理，本地的kube进程直接连接127.0.0.1:6443 注意事项 12https://github.com/rancher/rkehttp://blog.51cto.com/10321203/2071396?utm_source=oschina-app","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"kubernetes ingress","slug":"kubernetes_ingress","date":"2018-03-11T13:45:59.000Z","updated":"2018-03-11T13:45:59.000Z","comments":true,"path":"2018/03/11/kubernetes_ingress/","permalink":"http://yoursite.com/2018/03/11/kubernetes_ingress/","excerpt":"","text":"操作系统：centos7.3 软件版本:kubernetes 1.9nginx-ingress-controller:0.10.2defaultbackend:1.4haproxy-ingress:latestdefaultbackend:1.0目前kubernetes内置暴露访问的方案 介绍目前kubernetes内置暴露访问的方案1、nodeport指定Service的端口类型为nodeport，它将在的host节点上面暴露出一个端口，kube-proxy同时也会监听这个端口，目前是通过kube-proxy调用iptables实现将流量负载均衡到不同的pod上去，外部访问pod是先到host-ip:port—-&gt;pod_ip:port.为了提高可用性，可以在集群前面在加个负载均衡器这样就变成 loadbalance_ip:port—-&gt;host_ip:port—-&gt;port_ip:port。 2、loadbalance使用云平台本身提供的负载均衡器，分配访问pod的流量，这个目前只支持goole container engineand，目前也只支持tcp&#x2F;udp负载均衡。 3、ingressingress就是可以暴露内部访问工作在7层的负载均衡器，根据域名或服务名对后端Service进行端口转发，并且能根据后端Service的变化，动态刷新配置。 为什么需要ingress？因为如果Service使用nodeport暴露host的端口方式去访问应用的话，当Service有多个时，会造成端口管理上的混乱，在企业内部中还涉及到开防火墙问题。 这里我们介绍3种ingress类型，用户可以自由选择使用哪种类型： 1、nginx-ingress 2、haproxy-ingress 3、Traefik-ingress ingress包含两大组件ingress-controllerdefault-backend ingress-controller本身是一个是一个pod，这个pod里面的容器安装了反向代理软件，通过读取添加的Service，动态生成负载均衡器的反向代理配置，你添加对应的ingress服务后,里面规则包含了对应的规则，里面有域名和对应的Service-backend ingress-controller怎么去读取我们添加的ingress服务的配置然后动态刷新呢？ ingress-controller通过与kube-apiserver交互，去读取ingress服务规则的变化，通过读取ingress服务的规则，生成一段新的负载均衡器的配置然后重新reload，生配置生效。 default-backend用来将未知请求全部负载到这个默认后端上，这个默认后端会返回 404 页面。 ingress-control有nginx类型的还有haproxy类型的和traefik类型。 部署一个nginx的ingress需要这些yaml文件namespace.yaml—————创建名为ingress-nginx命名空间default-backend.yaml———–这是官方要求必须要给的默认后端，提供404页面的。它还提供了nginx-ingress-controll健康状态检查，通过每隔一定时间访问nginx-ingress-controll的&#x2F;healthz页面，如是没有响应就返回404之类的错误码。configmap.yaml ——- 创建名为nginx-configuration的configmaptcp-services-configmap.yaml——–创建名为 tcp-services的configmapudp-services-configmap.yaml——-创建名为udp-services的configmaprbac.yaml——创建对应的Serviceaccount并绑定对应的角色with-rbac.yaml—–创建带rbac的ingress-control 创建文件夹mkdir &#x2F;root&#x2F;ingress 下载 1wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/namespace.yaml -P /root/ingress 1wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/default-backend.yaml -P /root/ingress 1wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/configmap.yaml -P /root/ingress 1wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/tcp-services-configmap.yaml -P /root/ingress 1wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/udp-services-configmap.yaml -P /root/ingress 1wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/rbac.yaml -P /root/ingress 1wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/with-rbac.yaml -P /root/ingress 安装 1kubectl apply -f /root/ingress/ 检查对应的pod是否创建好 1kubectl get pod --namespace=ingress-nginx 因为新版本取消host模式的网络模式，所以需要我们创建server将ingress的端口映射出来下载 1wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/baremetal/service-nodeport.yaml -P /root/ingress 1kubectl apply -f /root/ingress/service-nodeport.yaml 查看对应的enpoint和service 1kubectl get endpoints,service --namespace=ingress-nginx 80被映射到宿主机的31159端口了443被映射到31030端口了。部署应用测试部署两个httpd 1cat httpd-app.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556apiVersion: apps/v1beta2 # for versions before 1.8.0 use apps/v1beta1kind: Deploymentmetadata: name: http-appspec: selector: matchLabels: app: http replicas: 1 # tells deployment to run 2 pods matching the template template: # create pods using pod definition in this template metadata: labels: app: http spec: containers: - name: nginx image: httpd ports: - containerPort: 80 volumeMounts: - name: httpd mountPath: /usr/local/apache2/htdocs volumes: - name: httpd hostPath: path: /root/httpd---apiVersion: apps/v1beta2 # for versions before 1.8.0 use apps/v1beta1kind: Deploymentmetadata: name: http-app2spec: selector: matchLabels: app: http2 replicas: 1 # tells deployment to run 2 pods matching the template template: # create pods using pod definition in this template metadata: labels: app: http2 spec: containers: - name: nginx2 image: httpd ports: - containerPort: 80 volumeMounts: - name: httpd mountPath: /usr/local/apache2/htdocs volumes: - name: httpd hostPath: path: /root/httpd2 创建两个目录替换httpd的网页目录 1mkdir /root/httpd 1mkdir /root/httpd2 简单写个测试页 1echo This is test1!! &gt; /root/httpd/index.html 1echo This is test2!! &gt; /root/httpd2/index.html 创建应用 1kubectl apply -f httpd-app.yaml 检查 1kubectl get deployment 创建对应的servic 1cat httpd_service.yaml 123456789101112131415161718192021222324kind: ServiceapiVersion: v1metadata: name: httpd-deploymentspec: ports: - protocol: TCP port: 80 targetPort: 80 selector: app: http---kind: ServiceapiVersion: v1metadata: name: httpd2-deploymentspec: ports: - protocol: TCP port: 80 targetPort: 80 selector: app: http2 创建service 1kubectl apply -f httpd_service.yaml 检查 1kubectl get service 创建ingress 1cat httpd-ingress.yaml 1234567891011121314151617181920apiVersion: extensions/v1beta1kind: Ingressmetadata: name: httpd-ingress namespace: default #服务在哪个空间内就写哪个空间spec: rules: - host: wanshaoyuan1.com #此service的访问域名 http: paths: - backend: serviceName: httpd-deployment #servicname servicePort: 80 #service暴露出来的port - host: wanshaoyuan2.com #此service的访问域名 http: paths: - backend: serviceName: httpd2-deployment #servicname servicePort: 80 #service暴露出来的port 应用 1kubectl apply -f httpd-ingress.yaml 检查 1kubectl get ingress --all-namespaces 测试服务修改hosts 打开浏览器访问http://wanshaoyuan1.com:31159 http://wanshaoyuan2.com:31159 查看ingress-control生成的nginx.conf配置文件 1kubectl --namespace=ingress-nginx exec -it nginx-ingress-controller-756ffbdb79-5tjs5 cat /etc/nginx/nginx.conf 可以看见ingress-control添加了这两个upstream 如果需要ingress-control不使用service暴露端口，网络模式使用主机模式的话需要修改with-rbac.yaml 部署一个haproxy的ingressdefault-backend.yaml ——-这是官方要求必须要给的默认后端，提供404页面的。它还提供了haproxy-ingress-controll健康状态检查，通过每隔一定时间访问haproxy-ingress-controll的&#x2F;healthz页面，如是没有响应就返回404之类的错误码。 haproxy-ingress.yaml—–部署haproxy-ingress-controller。 rbac.yaml—–创建rbac权限和Serviceaccount githubhttps://github.com/jcmoraisjr/haproxy-ingress/tree/master/examples 如果你kubernetes配置了RBAC的验证模式，你需要创建个servicaccount，然后绑定相对应的role和clusterrole，否则你的ingress-control将无法从kube-apiserver获取对应的数据。 1cat rbam.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133apiVersion: v1kind: Namespacemetadata: name: ingress-controller---apiVersion: v1kind: ServiceAccountmetadata: name: ingress-controller namespace: ingress-controller---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: name: ingress-controllerrules: - apiGroups: - &quot;&quot; resources: - configmaps - endpoints - nodes - pods - secrets verbs: - list - watch - apiGroups: - &quot;&quot; resources: - nodes verbs: - get - apiGroups: - &quot;&quot; resources: - services verbs: - get - list - watch - apiGroups: - &quot;extensions&quot; resources: - ingresses verbs: - get - list - watch - apiGroups: - &quot;&quot; resources: - events verbs: - create - patch - apiGroups: - &quot;extensions&quot; resources: - ingresses/status verbs: - update---apiVersion: rbac.authorization.k8s.io/v1beta1kind: Rolemetadata: name: ingress-controller namespace: ingress-controllerrules: - apiGroups: - &quot;&quot; resources: - configmaps - pods - secrets - namespaces verbs: - get - apiGroups: - &quot;&quot; resources: - configmaps verbs: - get - update - apiGroups: - &quot;&quot; resources: - configmaps verbs: - create - apiGroups: - &quot;&quot; resources: - endpoints verbs: - get - create - update---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: ingress-controllerroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ingress-controllersubjects: - kind: ServiceAccount name: ingress-controller namespace: ingress-controller - apiGroup: rbac.authorization.k8s.io kind: User name: ingress-controller---apiVersion: rbac.authorization.k8s.io/v1beta1kind: RoleBindingmetadata: name: ingress-controller namespace: ingress-controllerroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ingress-controllersubjects: - kind: ServiceAccount name: ingress-controller namespace: ingress-controller - apiGroup: rbac.authorization.k8s.io kind: User name: ingress-controller 1cat default-backend.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: ingress-default-backend labels: app: ingress-default-backend namespace: ingress-controllerspec: replicas: 1 template: metadata: labels: app: ingress-default-backend spec: terminationGracePeriodSeconds: 60 containers: - name: ingress-default-backend # Any image is permissable as long as: # 1. It serves a 404 page at / # 2. It serves 200 on a /healthz endpoint image: gcr.io/google_containers/defaultbackend:1.0 ports: - containerPort: 8080 resources: limits: cpu: 10m memory: 20Mi requests: cpu: 10m memory: 20Mi---apiVersion: v1kind: Servicemetadata: name: ingress-default-backend namespace: ingress-controller labels: app: ingress-default-backendspec: ports: - port: 8080 targetPort: 8080 selector: app: ingress-default-backend 1cat haproxy-ingress.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels: run: haproxy-ingress name: haproxy-ingress namespace: ingress-controllerspec: selector: matchLabels: run: haproxy-ingress template: metadata: labels: run: haproxy-ingress spec: serviceAccountName: ingress-controller hostNetwork: true containers: - name: haproxy-ingress image: quay.io/jcmoraisjr/haproxy-ingress args: - --default-backend-service=$(POD_NAMESPACE)/ingress-default-backend - --default-ssl-certificate=$(POD_NAMESPACE)/tls-secret - --configmap=$(POD_NAMESPACE)/haproxy-ingress ports: - name: http containerPort: 80 - name: https containerPort: 443 - name: stat containerPort: 1936 env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace 应用 123kubectl apply -f rbac.yamlkubectl apply -f default-backend.yamlkubectl apply -f haproxy-ingress.yaml 查看对应的pod 1kubectl get pod --namespace=ingress-controller 创建应用测试，这里还是用上面的http-app 123kubectl apply -f httpd-app.yamlkubectl apply -f httpd_service.yamlkubectl apply -f httpd-ingress.yaml 1kubectl get ing 配置hosts访问测试 查看ingress-control的haproxy配置文件 1kubectl -n ingress-controller exec -it haproxy-ingress-785cc87f6b-h7kxn cat /etc/haproxy/haproxy.cfg 我ingress-control-haproxy使用的是hostnetwork模式，当我不使用hostnetwork模式时，我将ingress-control-haproxy使用service暴露nodeport端口去连接时会出现下面情况。 但使用curl指定http报文host字段头部 通过判断haproxy配置文件判断应该是haproxy.cfg里面的acl的配置限制了报文头部host字段。 但这样使用的话还有个问题，因为ingress-control也是以pod的方式部署的，在实际应用中，pod会发生漂移，这样的话ip就会发生变化，但在企业中，允许通过的ip都需要在防火墙上放行。所以这里我们需要一个vip，使用haproxy+keepalive实现高可用，这里就不介绍具体操作方法了。 部署一个Traefik的ingresstracfik是一款开源的反向代理和负载均衡工具,traefik是为微服务而生的，它可以随时感知后端服务的变化，自动更改配置并热重新加载，在这期间服务是不会暂停和停止的。 为什么选择traefik(http://blog.csdn.net/aixiaoyang168/article/details/78557739) 速度快 不需要安装其他依赖，使用 GO 语言编译可执行文件 支持最小化官方 Docker 镜像支持多种后台，如 Docker, Swarm mode, Kubernetes, Marathon, Consul, Etcd, Rancher, Amazon ECS 等等 支持 REST API 配置文件热重载，不需要重启进程 支持自动熔断功能 支持轮训、负载均衡 提供简洁的 UI 界面 支持 Websocket, HTTP&#x2F;2, GRPC 自动更新 HTTPS 证书 支持高可用集群模式 根上面的nginx-ingress和haproxy-ingress相比有什么区别呢 ？1、使用nginx-ingress和haproxy-ingress需要一个ingress-controller去根kube-apiserver进行交互去读取后端的service、pod等的变化，然后在动态刷新nginx的配置，来达到服务自动发现的目的，而traefik本身设计就能直接和kube-apiserver进行交互，感知后端service、pod的变化，自动更新并热重载。 2、traefik快速和方便自带web页面和监控状态检查。 部署traefikgit clone https://github.com/containous/traefik.gittraefik&#x2F;examples&#x2F;k8s&#x2F;目录存放的就是kubernetes集群所需要的yml文件traefik-rbac.yaml对创建traefik对应的ClusterRole和绑定对应的servicaccount。执行kubectl apply -f traefik-rbac.yaml traefik-deployment.yaml 创建对应的serviceaccount，创建对应的pod和service执行 1kubectl apply -f traefik-deployment.yaml 查看 它同时启动了80和8080端口，80端口对应服务端口，8080对应ui端口 访问 部署ui主要采用ingress的方式暴露服务traefix很快发现我们刚刚添加的ingress 使用上面ingress-nginx和ingress-haproxy的测试用例进行测试创建对应的deployment 1kubectl apply -f httpd-app.yaml 创建对应的service 1kubectl apply -f httpd_service.yaml 创建对应的ingress 1kubectl apply -f httpd-ingress.yaml 测试访问 参考链接：http://dockone.io/article/1418http://blog.csdn.net/aixiaoyang168/article/details/78557739http://www.mamicode.com/info-detail-2109270.html","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"kubernetes RBAC","slug":"kubernetes_rbac","date":"2018-02-26T13:45:59.000Z","updated":"2018-02-26T13:45:59.000Z","comments":true,"path":"2018/02/26/kubernetes_rbac/","permalink":"http://yoursite.com/2018/02/26/kubernetes_rbac/","excerpt":"","text":"操作系统：centos7.3软件版本： kubernetes 1.8 docker-ce 17.03.02 概念：权限管理是指控制用户对某些特定资源的访问权限。设计理念ACL(访问控制列表):对资源进行权限控制，每个资源都有一个权限列表记录着哪些用户对这个资源能进行哪些操作，优点：实现非常简单，只需要将用户和资源连接起来就可以。缺点是：当用户和资源增多时会产生大量访问控制列表，管理起来非常麻烦。ABAC(基于属性的访问控制)：ABAC控制权限的粒度非常细，非常灵活；属性通常来说分为四类：用户属性、环境属性、操作属性、对象属性；ABAC通过动态的控制一组属性是否满足条件来进行授权判断。优点：权限控制的粒度非常细，非常灵活。缺点：太过于复杂。例如规则：“禁止所有学生在上课时间进出校门”这条规则，其中，“学生”是用户的角色属性，“上课时间”是环境属性，“进出”是操作属性，而“校门”就是对象属性了。RBAC(基于角色的访问控制)：引入角色概念，通过将权限和角色关联，来实现用户根权限的解耦，角色可以看做是权限的一个集合。通过权限与角色关联，角色与用户的关联可以实现用户和角色多对多的对应关系。优点 ；将权限与角色关联，用户关联角色，这样权限控制更加灵活 。缺点：相比ABAC控制的粒度没有那么细。RBAC的模型Who是否可以对What进行How的访问操作（Operator） kubernetes之前的版本都是使用ABAC做为权限管理控制，在kubernetes1.3发布alpha版本RBAC,在kubernetes1.6版本RBAC提升为Beta版，kubernetes1.8版本正式提升为GA版，为什么用RBAC取代ABAC是因为在kubernetes中ABAC过于复杂，不好管理，难以理解，RBAC相对功能和可管理性来说更加合适kubernetes。所以说没有更好，只有更合适。 在kubernetes中的RBAC定义了四类API资源分别为：Role：role是一组权限的集合，namespace范围的Role，role权限的有效范围为指定的单一namespace。ClusterRole：clusterrole根role一样也是一组权限的集合，cluster范围的Role，但clusterrole的有效范围是整个集群的权限和一些集群级别的资源如：集群的node节点、非资源类型endpoint、跨所有命名空间的命名空间范围资源（例如pod，需要运行命令kubectl get pods –all-namespaces来查询集群中所有的pod）。RoleBinding：将角色根用户或组或ServiceAccount绑定，授予它们对该命名空间资源的权限。ClusterRoleBinding：将ClusterRole与用户绑定，授予它们对cluster的资源访问权限。 例子首先我们创建个wanshaoyuan的普通用户确认kubernetes证书目录是否有以下文件若没有ca-config.json创建一个 123456789101112131415161718192021cat &gt; ca-config.json &lt;&lt;EOF&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;kubernetes&quot;: &#123; &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ], &quot;expiry&quot;: &quot;87600h&quot; &#125; &#125; &#125;&#125;EOF ca-config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile；1、创建证书签名请求 1234567891011121314151617181920cat &gt; wanshaoyuan-csr.json &lt;&lt;EOF&#123; &quot;CN&quot;: &quot;wanshaoyuan&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;EOF 生成证书和公钥 1cfssl gencert -ca=ca.crt -ca-key=ca.key -config=ca-config.json -profile=kubernetes wanshaoyuan-csr.json | cfssljson -bare wanshaoyuan 创建kubeconfig文件 1export KUBE_APISERVER=&quot;https://192.168.1.10:6443&quot; 1kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true --server=$&#123;KUBE_APISERVER&#125; --kubeconfig=wanshaoyuan.kubeconfig 1kubectl config set-credentials wanshaoyuan --client-certificate=/etc/kubernetes/pki/wanshaoyuan.pem --client-key=/etc/kubernetes/pki/wanshaoyuan-key.pem --embed-certs=true --kubeconfig=wanshaoyuan.kubeconfig 设置context 1kubectl config set-context kubernetes --cluster=kubernetes --user=wanshaoyuan --kubeconfig=wanshaoyuan.kubeconfig 设置默认context,将集群参数和用户参数关联起来，如果配置了多个集群，可以通过集群名来切换不同的环境 1kubectl config use-context kubernetes --kubeconfig=wanshaoyuan.kubeconfig 查看kubectl的context 用户目前还是kubernetes-admin，切换到wanshaoyuan 1KUBECONFIG=/etc/kubernetes/pki/wanshaoyuan.kubeconfig 或 1cp /etc/kubernetes/pki/wanshaoyuan.kubeconfig /root/.kube/config 再次查看 用户目前还是kubernetes-admin，切换到wanshaoyuan 1KUBECONFIG=/etc/kubernetes/pki/wanshaoyuan.kubeconfig 或 1cp /etc/kubernetes/pki/wanshaoyuan.kubeconfig /root/.kube/config 再次查看 创建角色定义这个角色只能对default这个namespace 执行get、watch、list权限定义角色 123456789101112cat role.yamlkind: RoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: namespace: default name: pod-readerrules:- apiGroups: [&quot;&quot;] # 空字符串&quot;&quot;表明使用core API group resources: [&quot;pods&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] 角色绑定 123456789101112131415161718cat role_bind.yamlkind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: read-pods namespace: defaultsubjects:- kind: User name: wanshaoyuan apiGroup: rbac.authorization.k8s.ioroleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io 应用 1kubectl apply -f role.yaml 1kubectl apply -f role_bind.yaml check一下 切到wanshaoyuan这个用户root用户将KUBECONFIG这个变量指向刚刚创建用户出来的kubeconfig 1KUBECONFIG=/root/RBAC/wanshaoyuan.kubeconfig 非root用户替换默认的config文件 1cp wanshaoyuan.kubeconfig /root/.kube/config 检查是否切换成功 验证 可以看见wanshaoyuan这个用户只能访问default这个namespace的pod资源，其他的如namespace都访问不了，同样namespace的其他资源也访问不了clusterrole定义的是集群级别的权限，也可以把它授予一些集群级别的资源 例如集群的node节点非资源类型endpoint跨所有命名空间的命名空间范围资源（例如pod，需要运行命令kubectl get pods –all-namespaces来查询集群中所有的pod） 例如定义一个clusterrole限制只能访问集群中的node节点，不能修改，其他资源都不能访问。cat cluster_role.yaml 123456789kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: secret-readerrules:- apiGroups: [&quot;&quot;] resources: [&quot;nodes&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] 再定义一个ClusterRoleBinding，将上面的clusterrole和用户wanshaoyuan绑定起来 123456789101112131415cat cluster_role_bind.yamlkind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: read-secrets-globalsubjects:- kind: User name: wanshaoyuan apiGroup: rbac.authorization.k8s.ioroleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io 应用 1kubectl apply -f cluster_role.yaml 1kubectl apply -f cluster_role_bind.yaml 切换用户 1KUBECONFIG=/root/RBAC/wanshaoyuan.kubeconfig 测试 只能访问node，不能做其他编辑操作，集群内其他资源也访问不了。例如定义一个clusterrole限制只能访问集群所有namespace的cesecret，其他资源都不能访问先定义一个clusterRole 12345678910cat /root/RBAC/cluster_role.yaml kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: secret-readerrules:- apiGroups: [&quot;&quot;] resources: [&quot;secrets&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] 再定义一个ClusterRoleBinding，将上面的clusterrole和用户wanshaoyuan绑定起来 1234567891011121314cat /root/RBAC/cluster_role_bind.yamlkind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: read-secrets-globalsubjects:- kind: User name: wanshaoyuan apiGroup: rbac.authorization.k8s.ioroleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io 应用 1kubectl apply -f cluster_role.yaml 1kubectl apply -f cluster_role_bind.yaml 切换用户 1KUBECONFIG=/root/RBAC/wanshaoyuan.kubeconfig 验证 可以看到，wanshaoyuan这个用户，可以get集群中任意namespace的secret，但其他资源，如pod，之类的都无法查看修改下，让wanshaoyuan这个帐户可以同时查看pod、secret、deployment 1KUBECONFIG=/root/RBAC/wanshaoyuan.kubeconfig 1KUBECONFIG=/etc/kubernetes/admin.conf 修改 1234567891011121314151617/root/RBAC/cluster_role.yamlkind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: secret-readerrules:- apiGroups: [&quot;&quot;] resources: [&quot;secrets&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]- apiGroups: [&quot;&quot;] resources: [&quot;pod&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]- apiGroups: [&quot;extensions&quot;, &quot;apps&quot;] resources: [&quot;deployments&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] 用kubectl apply去实时更新 1kubectl apply -f cluster_role.yaml 1kubectl apply -f cluster_role.yaml 切换用户测试 pod、secret、deployment都能正常获取 角色绑定包含一组相关主体即subject，subject可以是User(用户)也可以是Group(组)还可以是Service Account。这里要强调下Kubernetes 集群中包含两类用户：一类是由 Kubernetes 管理的 service account，另一类是普通用户。 普通用户被假定为由外部独立服务管理。管理员分发私钥，用户存储（如 Keystone 或 Google 帐户），甚至包含用户名和密码列表的文件。在这方面，Kubernetes 没有代表普通用户帐户的对象。无法通过 API 调用的方式向集群中添加普通用户。 相对的，service account 是由 Kubernetes API 管理的帐户。它们都绑定到了特定的 namespace，并由 API server 自动创建，或者通过 API 调用手动创建。Service account 关联了一套凭证，存储在 Secret，这些凭证同时被挂载到 pod 中，从而允许 pod 与 kubernetes API 之间的调用。 API 请求被绑定到普通用户或 serivce account 上，或者作为匿名请求对待。这意味着集群内部或外部的每个进程，输入 kubectl ，都必须在向 API Server 发出请求时进行身份验证，否则被视为匿名用户。来源：https://jimmysong.io/posts/user-authentication-in-kubernetes/ kubernetes在每个namespace内都有一个默认的service account,当在创建pod或其他资源时，没有手工指定Service account时会默认使用namespace里面这个default这个Service account 一般来说为了安全起见，建议一个应用使用一个独立的Service account运行，这样RoleBindings就不会无意中授予其他应用程序的权限，如果需要这样使用的话在部署yaml文件中添加 可以通过以下命令对namespace中的Service account进行默认权限配置授予my-namespace命名空间内的default这个Service account帐户只读权限(view是集群中的一组通用角色，它的默认权限的是只读，它归属于clusterrole 1234kubectl create rolebinding default-view \\ --clusterrole=view \\ --serviceaccount=my-namespace:default \\ --namespace=my-namespace 授予my-namespace命名空间内的my-sa这个Service account帐户只读权限 1234kubectl create rolebinding my-sa-view \\ --clusterrole=view \\ --serviceaccount=my-namespace:my-sa \\ --namespace=my-namespace 授予my-namespace命名空间内所有的Service account帐户只读权限 1234kubectl create rolebinding serviceaccounts-view \\ --clusterrole=view \\ --group=system:serviceaccounts:my-namespace \\ --namespace=my-namespace 默认的Role和ClusteRroleapi-service会默认创建一些role和rolebinding 和clusterrole和clusterrolebind，他们默认都是以system开头的，以system开头的都是为kubernetes系统使用而保留的 这里不多介绍主要讲讲clusterrole里面cluster-admin、admin、edit、view这几个clusterrole，直接参考https://v1-8.docs.kubernetes.io/docs/admin/authorization/rbac/cluster-admin、admin、edit、view这几个角色是不包含system前缀的，他们是面向用户的角色，可以直接使用rolebind直接使用具体权限参考下图 比如我们需要访问kubernetes-dashboard时，需要一个能够访问获取所有集群资源的用户此时就可以创建一个帐户关联cluster-admin这个角色。 1234567891011121314151617181920212223kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: admin annotations: rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot;roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.iosubjects:- kind: ServiceAccount name: admin namespace: kube-system---apiVersion: v1kind: ServiceAccountmetadata: name: admin namespace: kube-system labels: kubernetes.io/cluster-service: &quot;true&quot; addonmanager.kubernetes.io/mode: Reconcile 上面创建一个ServiceAccount帐户admin并将它与cluster-admin角色绑定，然后我们可以通过获取admin的token去访问dashboard。kubectl -n kube-system describe secret admin-token-nwphb apiGroup解释上面各类角色转换中的apiGroup的解释来源：https://segmentfault.com/a/1190000008700595 k8s里面有各种资源，如Pod、Service、RC、namespaces等资源，用户操作的其实也就是这一大堆资源。但这些资源并不是杂乱无章的，使用了GroupVersion的方式组织在一起。每一种资源都属于一个Group，而资源还有版本之分，如v1、v1beta1等。k8s目前正在使用的API groups： “core” group：它的REST path是api&#x2F;v1 “extensions” group：它的REST path是&#x2F;apis&#x2F;extensions&#x2F;v1beta1 “autoscaling”, “abac” … k8s现阶段，API一共分为13个Group：Core、apps、authentication、authorization、autoscaling、batch、certificates、componentconfig、extensions、imagepolicy、policy、rbac、storage。其中Core的Group Name为空，它包含的API是最核心的API,如Pod、Service等。 参考链接：http://blog.csdn.net/wangjingna/article/details/49226727http://blog.csdn.net/jettery/article/details/70138003http://blog.csdn.net/yan234280533/article/details/75808048http://blog.csdn.net/yan234280533/article/details/76359199http://blog.csdn.net/qqhappy8/article/details/78891200http://www.bijishequ.com/detail/570501http://www.sohu.com/a/205750642_610730https://v1-8.docs.kubernetes.io/docs/admin/authorization/rbac/https://jimmysong.io/kubernetes-handbook/https://www.cnblogs.com/charlieroro/p/8489515.html","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"kubernetes集群节点资源预留","slug":"k8s_resource_resver","date":"2018-01-26T13:45:59.000Z","updated":"2018-01-26T13:45:59.000Z","comments":true,"path":"2018/01/26/k8s_resource_resver/","permalink":"http://yoursite.com/2018/01/26/k8s_resource_resver/","excerpt":"","text":"概述默认kubelet没配置资源预留应用没做应用资源限制情况下，那host上所有资源都是可以给pod调配使用的，这样很容易引起集群雪崩效应，比如集群内有一台上跑的pod没做resource limt导致占用资源过大导致将宿主机压死了，此时这个节点在kubernetes内就是一个no ready的状态了，kubernetes会将这台host上所有的pod在其他节点上重建，也就意味着那个有问题的pod重新跑在其他正常的节点上，将另外正常的节点压跨。循怀下去直到集群内所有主机都挂了，这就是集群雪崩效应。 如何避免？在kubernetes中可以通过给kubelet配置参数预留资源给系统进程和kubernetes进程保证它们稳定运行。目前能实现到cpu、memory、ephemeral-storage层面的资源预留。重点提两点cpu：cpu是配置cpu shares实际上对应的是cpu的优先级，简单来说，这个在cpu繁忙时，它能有更高优先级获取更多cpu资源。 ephemeral-storage是kubernetes1.8开始引入的一个资源限制的对象，kubernetes 1.10版本中kubelet默认已经打开的了,到目前1.11还是beta阶段，主要是用于对本地临时存储使用空间大小的限制，如对pod的empty dir、&#x2F;var&#x2F;lib&#x2F;kubelet、日志、容器可读写层的使用大小的限制。 配置在讲配置之前我们先了解几个概念 Node capacity：节点总共的资源kube-reserved：给kubernetes进程预留的资源system-reserved：给操作系统预留的资源eviction-threshold：kubelet eviction的阀值allocatable：留给pod使用的资源 node_allocatable&#x3D;Node_capacity-(kube-reserved+system-reserved+hard-eviction) eviction-threshold分两类：1、kube-control-manager周期性的接收kubelet发送过来的心跳，检查所有节点的状态，当节点属于no ready时，驱逐重建上面的pod(默认超时5分钟)。2、kubelet周期性的检查host上的资源，与配置项里面的配置进行比对，达到阀值后，按照优先级驱逐pod。 eviction-threshold实际上是对pod limit_resource的补充，因为limit_resource只能针对单个pod做资源限制，当这个pod达到限制的阀值后，kubelet便会oom_killer掉这个container，而eviction-threshold根据事先设定的Eviction Thresholds来触发Eviction，调用算法筛选出合适的几个pod，kill掉一个或多个pod回收资源，被eviction掉的pod会被kube-scheduler在其他节点重新调度起来。eviction-threshold分两种类型Soft Eviction Thresholds：达到触发值后，并不是马上去驱逐pod，而是等待一个缓冲时间，这个配置参考https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/ Hard Eviction Thresholds：达到触发值后，直接筛选出对应的pod kill掉 两种配置方式绝对值：memory.available&lt;5Gi百分比：memory.available&lt;5% 配置kubelet资源预留环境kubernetes：v1.11kubeadm：v1.11os：ubuntu16.04 一台master+2台worker节点 机器配置内存：8175176KBCPU：2vcpu磁盘：40G resource-reserved依赖cgroup，所以需要提前将kubelet和docker的cgroup-driver对应好在修改配置前，先查看node节点目前可用资源 1kubectl describe node rke-node3 这里以cpu、ephemeral-storage、memory为参考，capacity为host本身资源，Allocatable为host实际可分配资源，这里可以看见在没配置资源预留的情况下，Allocatable基本上等于capacity，然后我们配置rke-node3的kubelet 1vim /var/lib/kubelet/kubeadm-flags.env 1234567--enforce-node-allocatable=pods,kube-reserved,system-reserved --kube-reserved-cgroup=/system.slice/kubelet.service --system-reserved-cgroup=/system.slice --kube-reserved=cpu=1,memory=2Gi, --system-reserved=cpu=500m,memory=1Gienforce-node-allocatable=pods,kube-reserved,system-reserved #默认为pod设置，但我们这里要给kube进程和system预留所以要加上。kube-reserved-cgroup=/system.slice/kubelet.service #kube组件对应cgroup目录system-reserved-cgroup=/system.slice #系统组件对应cgroup目录kube-reserved=cpu=1,memory=2Gi #kube组件资源预留大小system-reserved=cpu=500m,memory=1Gi #系统组件资源预留大小 cpuset和hugetlb subsystem是默认没有初始化system.slice手动创建 1mkdir -p /sys/fs/cgroup/cpuset/system.slice/kubelet.service/ 1mkdir -p /sys/fs/cgroup/hugetlb/system.slice/kubelet.service/ 配好后重启kubelet 1systemctl restart kubelet 然后我们在通过kubectl describe node rke-node3查看信息。 kube-reserved&#x3D;cpu&#x3D;1，system-reserved&#x3D;cpu&#x3D;500m 所以cpu就1.5个了，总共就2个，那么Allocatable就为2000-1500&#x3D;500m了kube-reserved&#x3D;memory&#x3D;2Gi，system-reserved&#x3D;memory&#x3D;1Gi 所以你才为3Gi，8Gi-3GI&#x3D;5Gi&#x3D;5242990Ki 上面也论证了kubelet配置资源预留，实际上是kubelet上报资源时，把预留的值减去了，所以scheduler调度时得到的值就是减后的值。 配置eviction生产环境尽量配置eviction-soft，少用eviction-hard，因为eviction-hard没有缓冲时间，马上去驱逐pod，而eviction-soft可以通过配置eviction-soft-grace-period(达到阀值后等待多久在进行eviction，默认是90s)eviction-hard（默认就配置了eviction-hard&#x3D;memory.available&#x3D;100Mi，所以我们前面没做任何配置Allocatable看见的memory会少100M内存) 1--enforce-node-allocatable=pods,kube-reserved,system-reserved --kube-reserved-cgroup=/system.slice/kubelet.service --system-reserved-cgroup=/system.slice --kube-reserved=cpu=1,memory=2Gi, --system-reserved=cpu=500m,memory=1Gi, --eviction-hard=memory.available&lt;10%,nodefs.available&lt;10%,imagefs.available&lt;10% 配置了eviction-hard的话， 1Allocatable=Node_capacity-(kube-reserved+system-reserved+eviction-threshold) 1--enforce-node-allocatable=pods,kube-reserved,system-reserved --kube-reserved-cgroup=/system.slice/kubelet.service --system-reserved-cgroup=/system.slice --kube-reserved=cpu=1,memory=2Gi, --system-reserved=cpu=500m,memory=1Gi, --eviction-soft=memory.available&lt;10%,nodefs.available&lt;10%,imagefs.available&lt;10% --eviction-soft-grace-period=memory.available=2m,nodefs.available=2m,imagefs.available=2m 配置了eviction-soft的话， 1Allocatable=Node_capacity-(kube-reserved+system-reserved) 最佳实践1、生产环境中，建议资源预留+应用资源限制结合使用，因为如果单纯只做资源预留的，没做应用资源限制或配置eviction，如果应用运行了一段时间的pod突然资源使用暴涨，就会触发host oomkill。2、在kubernetes中资源分为可压缩资源和不可压缩资源 ，如我们常见的cpu是可压缩资源、内存、磁盘资源是不可压缩资源。建议只针对不可压缩资源进行资源预留。3、建议给系统预留10%的内存，kubernetes组件预留3%~5%的内存，磁盘预留10%。 1234567https://www.kubernetes.org.cn/1150.htmlhttp://wsfdl.com/kubernetes/2017/12/18/k8s资源预留.htmlhttps://my.oschina.net/jxcdwangtao/blog/1629059http://www.sohu.com/a/231155920_268033https://www.kubernetes.org.cn/4022.htmlhttp://licyhust.com/容器技术/2017/10/24/eviction/","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"kubernetes入门","slug":"kubernetes_base","date":"2018-01-26T13:45:59.000Z","updated":"2018-01-26T13:45:59.000Z","comments":true,"path":"2018/01/26/kubernetes_base/","permalink":"http://yoursite.com/2018/01/26/kubernetes_base/","excerpt":"","text":"操作系统：centos7.3软件版本：kubernetesV1.9 初步认识kubernetesWhat is kubernetes?Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. 特点：Automating deployment:自动部署Scaling：伸缩Management of containerized applications：管理容器应用 1、google容器内部容器编排系统borg目前又称omega的开源版，该项目由google 2014年开始启动，kubernetes 1.0版本在2015年7月21日正式发布。2、2017年7月google联合 linux基金会(Linux Foundation)创办了CNCF基金会(Cloud Native Computing Foundation)并将kubernetes种子项目捐献给了CNCF基金会。3、2017年kubernetes总共发布了四个版本，主要是在多用户、多负载、安全性和易用性方面做了改进。分别是：安全性和易用性方面做了改进。分别是：Kubernetes 1.6：伸缩性SLO支持包含5000个节点（15万pod)的集群和动态存储。Kubernetes 1.7：network policy API加强网络安全性。Kubernetes 1.8：角色访问控制功能变为稳定版。Kubernetes 1.9：支持window系统。 kubernetes能做什么？1、容器的自动化部署和升级2、容器的自动伸缩3、以集群的方式部署容器，并提供容器间的负载均衡4、容器的自动修复和健康检查 为什么使用kubernetes?两个维度背景维度1、大量巨头加入CNCF基金会，人多力量大，出现问题也不怕。2、2017年下半年kubernetes已经完全赢得了容器大战。技术维度3、Kubernetes具备超强的横向扩展能力，只要架构设计的好，甚至可以在线的线性扩展。4、kubernetes采用传统的master-slave架构在container上层又封装了层pod，这样设计非常简单灵活，能更好的适应和管理微服务。 总结： kubernetes的基本概念clustercluster是计算、存储、网络的资源的集合，kubernetes利用这些资源运行各种基于容器的应用。 PODpod是kubernetes最小的调度单元，一个pod对应一个或多个container，通常会将紧密相连的一组应用放到一个pod中，同一个pod的container共享ip和namespace。 为什么使用pod?方便管理在kubernetes中pod是container的载体，一个pod里面拥有一个或多个container，做为一个逻辑单元，方便管理。 资源共享和通信同一个pod中的container共享一个网络栈和存储，相互之间可以直接通过localhost进行通信，同时也共享同一块存储卷。 灵活kubernetes直接管理对象是pod，而不是底层的docker，对于docker的操作，被封装在pod中，不会直接操作，这也意味着，pod包含也可以是其他公司的容器产品，比如coreos的rkt或阿里巴巴的pouch。 ontrollerkubernetes使用controller来创建和管理pod，controller中定义了pod的部署特性，比如部署几个副本，在什么样的node上运行。为满足不同的业务需求，kubernetes提供了如下controller。 Deployment是最常用的 Controller，比如前面就是通过创建 Deployment 来部署应用的。Deployment 可以管理 Pod 的多个副本，并确保 Pod 按照期望的状态运行。 Replicaset实现了 Pod 的多副本管理。使用 Deployment 时会自动创建 ReplicaSet，也就是说 Deployment 是通过 ReplicaSet 来管理 Pod 的多个副本，我们通常不需要直接使用 ReplicaSet。 Daemonset 用于每个 Node 最多只运行一个 Pod 副本的场景。正如其名称所揭示的，DaemonSet 通常用于运行 daemon。 StatefuleSet能够保证 Pod 的每个副本在整个生命周期中名称是不变的。而其他 Controller 不提供这个功能，当某个 Pod 发生故障需要删除并重新启动时，Pod 的名称会发生变化。同时 StatefuleSet 会保证副本按照固定的顺序启动、更新或者删除。 Job用于运行结束就删除的应用。而其他 Controller 中的 Pod 通常是长期持续运行。 service直接接通过pod的ip加端口去访问应用是不靠谱的，因为pod的生命周期是不断变化的，每次重新生成的pod ip地址都是都是不一样的，并且当pod有多个副本时，这样的访问就更痛苦了，所以kubernetes通过service来解决这些问题，简单来说，你可以把service理解为一个负载均衡器，也可以说是service是为一组功能相同的pod提供统一入口。Service默认有自己的ip和端口的叫cluster-ip和port，内部可以直接通过这个endpoint(clusterip+port)去访问应用。 不过有一点需要注意，这个cluster-ip是个virtual_IP，它是ping不通的，底层是通过node节点的kube-proxy调用iptables生成对应的转发规则，新版本的kube-proxy可以直接使用ipvs效率更高，不过目前还在测试阶段。namespace这里根linux的namespace不一样，这个更像是一个分组。通过namespace将用户创建的controller和pod等资源分开。namespace可以将一个物理的cluster划成多个虚拟的cluster，每个cluster就是一个namespace，不同的namespace里面资源是完全隔离的。便于不同分组共享使用集群物理资源的同时，还能被管理。使用Namespace来组织Kubernetes的各种对象，可以实现对用户的分组，即“多租户”的管理。对不同的租户还可以进行单独的资源配额设置和管理，使得整个集群的资源配置非常灵活、方便。一个集群中的资源总是有限的，当这个集群被多个租户的应用同时使用时，为了更好地使用这种有限的共享资源，需要将资源配额的管理单元提升到租户级别，通过在不同租户对应的Namespace上设置对应的ResourceQuota即可达到目的。kubernetes默认创建了三个namespace，default、kube-public、kube-system。 default：默认创建的资源将放到这个namespace里面。kube-system：kubernetes自己创建的系统资源将放这里。kube-public：kubernetes自己创建的命名空间，用于确保整个集群中公开的看到某些资源。创建pod时指定namespace 1kubectl --namespace=&lt;insert-namespace-name-here&gt; run nginx --image=nginx labelLabel以key&#x2F;value键值对的形式附加到各种对象上，如Pod、Node等。Service通过selector label建立service和pod的关联，简单来说，资源和资源之间的关联都是通过label，通过给一个资源绑定一个或多个不同的label，实现多维度的资源管理，selector label类似于sql语句的where。 总结： kubernetes的架构角色上：典型的master-slave架构master节点node节点或叫Minion节点功能上：master节点做为集群大脑node节点用于承载部署的容器应用 master节点master是cluster的大脑，类似openstack的控制节点，运行着apiserver、controller-manager、scheduler它的主要职责是对资源管理、调度，还有认证、弹性伸缩、安全认证。 master节点组件构成etcdkubernetes的后端数据存储系统，cluster中的所有资源对象数据都存储在这，用于配置共享和服务发现。 kube-apiserver1、整个集群管理的api接口，所有对集群的操作和查询都需要经过api-server。2、集群内各个模块通信的枢钮，集群内其他模块，互相之间并不能直接通信，都是通过api-server打交道，来完成自己那部分工作。3、集群安全控制，apiserver提供了集群的安全验证，和角色权限分配。4、连接etcd，集群内所有组件都不能直接连接etcd只能通过api-server去连接etcd。 kube-controller-manager1、负责集群的故障检测和修复。2、根据deployment的定义，维持正确的pod副本数。3、根据Service与Pod的管理关系，完成服务的Endpoints对象的创建和更新。4、为新的namespace创建帐户和api访问token。 kube-scheduler创建pod时，选择合适的node进行调度。kube-dnskubernetes中的域名解析服务器。flannelflannel的网络组件。kube-dashboardkubernetes的web控制端 node节点运行容器应用，由mater管理，接收master节点的各类请求，进行容器的创建和管理，并将运行在上面的容器应用上报到master节点，类似openstack的计算节点。 node节点组件介绍kubelet负责node节点容器的创建、删除监控等生命周期管理，上报node信息如cpu、内存、pod的ip等信息导master节点的api-serverkube-proxy用于实现端口映射和负载均衡。kube-dnskubernetes的域名解析服务器flannel网络flannel的组件 根openstack类比角色功能上kubernetes分为master节点和node节点，其中master节点是大脑，node节点上承载master调度过来的资源，接收master节点的各类请求。这点根openstack的control节点和compute节点基本一样。组件作用上master节点先从etcd说起，etcd做为kubernetes的后端数据存储系统用于配置共享和服务发现，和openstack中的控制节点的数据库和消息队列服务提供的服务相似。kube-api-server在kubernetes提供集群管理的api接口，角色的权限分配和安全验证，和openstack中的keystone和各类组件的api-server相似。kube-scheduler在kubernetes中为pod选择最合适的node节点。与openstack的nova-scheduler提供功能相似。node节点kubelet在node节点上负责pod的创建和生命周期的管理，同时对宿主机资源使用情况进行上报到master，与openstack的nova-compute所提供的功能非常一致。docker运行容器的引擎根openstack的hypervisor提供的功能一致。kubelet调用docker创建容器，nova-compute调用kvm去创建虚拟机。 总结： kubernetes的部署kubeadmkubeadm为kubernetes官方推荐的自动化部署工具，它将kubernetes的组件以pod的形式部署在master节点和node节点上并自动完成证书认证等操作。因为kubeadm默认要从google的镜像仓库下载镜像，所以需要翻墙或提前下好导入。 源码安装https://github.com/kubernetes/kubernetes/tree/release-1.9github上下载对应版本的源码包，编译，自己生成证书编写systemd启动文件。推荐先用kubeadm装一遍，熟悉下各个组件，在用源码安装了解组件之间的调用关系。 总结： kubernetes的基本操作集群操作查看集群信息 1kubectl cluster-info 查看组件健康状态 1kubectl get cs 集群操作 1kubectl get pod --all-namespaces 查看指定namespace的pod的状态以kube-system为例 1kubectl get pod --namespace=kube-system 查看pod详细信息 1kubectl describe pod kube-apiserver-master --namespace=kube-system 查看pod详细run信息 1kubectl get pods -o wide 删除service 1kubectl delete service service_name 设置label 1kubectl label node node-1 disktype=ssd 查看设置的label 1kubectl get node --show-labels 应用操作例子(参考cloudmanhttps://steemit.com/kubernetes/@cloudman6/k8s-kubernetes-3)部署应用（kubernetes-bootcamp) 1kubectl run kubernetes-bootcamp --image=docker.io/jocatalin/kubernetes-bootcamp:v1 --port=8080 查看部署的应用，这里deployment为kubernetes术语，理解为应用。 1kubectl get deployment docker镜像通过–image指定–port设置应用对外服务的端口。 流程:1、kubectl发送应用部署请求到kube-apiserver，kube-apiserver将请求写入etcd中。2、api-server通知kube-controller-manger，创建一个deployment对象。将结果通过api-server写入etcd。3、Scheduler发现后，执行调度流程，为这个新Pod选定一个落户的Node，然后通过API Server将结果写入到etcd中。4、目标节点kubelet进程通过api-server检测这个新生的pod，并按照定义创建pod。 查看当前pod 1kubectl get pods 查看 pod实际运行在哪个节点上 1kubectl get pod -o wide 访问 默认情况下使用pod只能在集群内部进行访问，为了能在外部访问容器，需要将容器应用的端口映射到node节点端口。修改kubernetes-bootcamp的端口类型为NodePort，容器内端口为8080，映射的端口会自动从30000~32767中挑选一个。services可以认为是端口映射。 1kubectl expose deployment/kubernetes-bootcamp --type=&quot;NodePort&quot; --port=8080 kubernetes-bootcamp分配到了node-2上，可以直接访问node-2的:31079 scale应用（弹性伸缩)当前kubernetes-bootcamp副本数还是为1 扩展应用为3 1kubectl scale deployment/kubernetes-bootcamp --replicas=3 pod数也增长为3了。 每次访问nodeip:31079，都是不同的pod给回的返回结果。可以发现每次请求发到了不同的pod上从而实现了负载均衡。 缩减pod数为2 1kubectl scale deployment/kubernetes-bootcamp --replicas=2 滚动更新当前应用image版本为v1，升级为v2 1kubectl set image deployment/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2 可以看见kubernetes是创建出两个v2版本的镜像，然后在缓慢用v2将v1替换掉在次访问，版本是v2了回退版本 1kubectl rollout undo deployment/kubernetes-bootcamp 验证结果 总结： 参考链接：http://blog.csdn.net/liukuan73/article/details/54971854https://steemit.com/kubernetes/@cloudman6/k8s-kubernetes-3","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"docker storage-driver","slug":"docker_storage_driver","date":"2018-01-25T13:45:59.000Z","updated":"2018-01-25T13:45:59.000Z","comments":true,"path":"2018/01/25/docker_storage_driver/","permalink":"http://yoursite.com/2018/01/25/docker_storage_driver/","excerpt":"","text":"作用： 容器镜像的存储和管理。 容器启动时的文件系统的准备。 概念：写时复制（copy-on-write)只有当快照出来的对象，它需要进行写操作时，它会从父镜像中将需要修改的块拷贝到自己的文件系统，然后在进行修改。所以通过cow，容器启动拷贝只是拷贝一些元数据，所以容器启动速度特别快。用时分配（allocate-on-daemand)thin-pool用的就是此技术当分配一个100G的空间时，并不会立刻占满这100，只是占用了一些文件的元数据，当写入数据时会根据实际的大小动态的分配,类似linux中的稀疏文件。 常见的storage-driveraufs：linux上较老牌的storage_driver,基于文件级的存储driver，将多层合并成文件系统的单层表示。底层都是只读层，只有最上层文件系统可写,当写和修改文件时也是使用cow技术。 优点：拥有较长的历史，在大量生产环境中验证过，稳定性、可靠性较高缺点：aufs对文件第一次进行修改操作时需要将整个文件复制到读写层，哪怕你只是修改文件的一小部分，这样会造成很大的性能开销。 overlay:与aufs设计非常类似，也是基于文件级的存储driver同时 也是 一个union filesystem，overlay主要分两层upperdir和lowerdir,其中lowerdir对应的docker中的镜像层，upperdir对应的 是容器层，merged对应的是有效的容器挂载点。 优点：overlay的速度比aufsh和devicemapper快，因为aufs有很多层，而overlay只有两层。overlay支持页缓存共享，多个容器访问同一个文件，可以共享一个或多个页缓存选项。 缺点：1、overlay是个新生儿，还不具备在生产中使用的条件，但overlay2在centos的3.10.0-514及以上内核版本可以使用。2、因为overlay也是基于文件级的存储，所以根aufs 一样对文件进行第一次修改操作时需要将整个文件拷贝到读写层但比aufs好，因为overlay只有两层，而aufs有多层。3、overlay只有两层，镜像中的每一层并不对应overlay中的层，对应的是&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay&#x2F;中一个文件夹，然后用硬连接的方式将下面层文件引用到上层。overlayfs的详细原理https://blog.csdn.net/luckyapple1028/article/details/78075358 overlay2：overlay2：overlay的改进版，只支持4.0以上内核因为在4.0的内核上添加了Multiple lower layers in overlayfs的特性，所以overlay2可以直接造成muitiple lower layers不用像overlay一样要通过硬链接的方式(最大128层) centos的话支持3.10.0-514及以上内核版本也有此特性，所以消耗更少的inode。 docker官方overlay2的PR:https://github.com/moby/moby/pull/22126 LINUX KERNERL 4.0 release说明：https://kernelnewbies.org/Linux_4.0 为什么overlay相比overlay2要更加占用inode？ 1、overlay只支持两层lowerdir和upperdir，并且只支持一个lowerdir，所以如果你的容器镜像有多层的话，层与层之前的数据共享是通过硬链接来实现的，我们也知道硬链接本身是同一个inode但不同的文件名而已，但为什么还是会大量消耗inode这里简单做的实验我们在一台配置了storage-driver的机器上PULL ubuntu:18.04镜像 12345678[root@master dir]# docker pull ubuntu:18.0418.04: Pulling from library/ubuntu32802c0cfa4d: Pull completeda1315cffa03: Pull completefa83472a3562: Pull completef85999a86bef: Pull completeDigest: sha256:48eb09a5c832172357f172593ce5e98e027814f758f6aeaef59a7fd658e50d49Status: Downloaded newer image for ubuntu:18.04 整个镜像是一个四层镜像层组成的镜像，在&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay&#x2F;下每层对应一个目录，通过tree命令可以看见每个目录内只包含一个root文件夹 12345678910111213tree -L 2 /var/lib/docker/overlay//var/lib/docker/overlay/|-- 0a9cd41c44a802a325bfc5bfdda757bced8eaf69a8d6004e5d6fcb730591ab31| `-- root|-- 1f9c95c6642a98930a14d914ac9bcfe9535b5a604dc27851cd76266326c76ed7| `-- root|-- 2d79f688e1bf505ef20100f7450ad7e5ea550500bd07a17b7b9b08513fc96988| `-- root`-- e8b1fcddec600628a75964619da3d0de7310fcbd6350b0c07ddf038d71c25d8b `-- root8 directories, 0 files 这个root目录内包含的是该层独有的文件和根lowdir共享的数据硬链接，这里看见共享的数据他们本身都是通过硬链接方式连接起来的。 1234567891011[root@master overlay]# ls -i /var/lib/docker/overlay/0a9cd41c44a802a325bfc5bfdda757bced8eaf69a8d6004e5d6fcb730591ab31/root/bin/ls70832997 /var/lib/docker/overlay/0a9cd41c44a802a325bfc5bfdda757bced8eaf69a8d6004e5d6fcb730591ab31/root/bin/ls[root@master overlay]# ls -i /var/lib/docker/overlay/1f9c95c6642a98930a14d914ac9bcfe9535b5a604dc27851cd76266326c76ed7/root/bin/ls70832997 /var/lib/docker/overlay/1f9c95c6642a98930a14d914ac9bcfe9535b5a604dc27851cd76266326c76ed7/root/bin/ls[root@master overlay]# ls -i /var/lib/docker/overlay/2d79f688e1bf505ef20100f7450ad7e5ea550500bd07a17b7b9b08513fc96988/root/bin/ls70832997 /var/lib/docker/overlay/2d79f688e1bf505ef20100f7450ad7e5ea550500bd07a17b7b9b08513fc96988/root/bin/ls[root@master overlay]# ls -i /var/lib/docker/overlay/e8b1fcddec600628a75964619da3d0de7310fcbd6350b0c07ddf038d71c25d8b/root/bin/ls70832997 /var/lib/docker/overlay/e8b1fcddec600628a75964619da3d0de7310fcbd6350b0c07ddf038d71c25d8b/root/bin/ls 但为什么overlay还是会占用大量inode呢？根本原因在于那些文件夹，每层的root目录内存放的都是完整的rootfs文件夹，但它们都是新建出来的，它们inode都不一样，所以在overlay下一个容器镜像层数越多，占用的inode就越多 我们看看overlay2的方式因为overlay2是支持linux kernel的Multiple lower layers in overlayfs的特性所以原生就支持多个lower所以不需要在通过硬链接方式同样拉取个ubuntu:18.04的镜像 1234567891011121314151617181920212223242526272829/var/lib/docker/overlay2/|-- 6e599f35a3366d95287390fc00183a80bfc9a34492aaf9694836ec7b5722d0b6| |-- diff| |-- link| |-- lower| |-- merged| `-- work|-- 6f66846fe6a29834193bf36380eb1664947f21435edd8ce8fbc9b79dea92c51b| |-- diff| |-- link| |-- lower| |-- merged| `-- work|-- a1869d02b056d66742372c4b6d31d64f98b477590cdd76a842a653424f46710b| |-- diff| |-- link| |-- lower| |-- merged| `-- work|-- backingFsBlockDev|-- da05539957d703ae81cf279c76059c947c96bd1f91fd24691b9e7630dcb13ff7| |-- diff| `-- link`-- l |-- 2LYVTSJQWPVOB46BCA74GFHJ7T -&gt; ../da05539957d703ae81cf279c76059c947c96bd1f91fd24691b9e7630dcb13ff7/diff |-- HEXAQW5O23XL6HHRVOMKWGAI4Z -&gt; ../6f66846fe6a29834193bf36380eb1664947f21435edd8ce8fbc9b79dea92c51b/diff |-- KXIOHK4OI6PPPFZ56I3ZORNHS7 -&gt; ../6e599f35a3366d95287390fc00183a80bfc9a34492aaf9694836ec7b5722d0b6/diff `-- PHQQ6RCEXR6BLZJGKBP6GYS55Q -&gt; ../a1869d02b056d66742372c4b6d31d64f98b477590cdd76a842a653424f46710b/diff 在overlay2每层的内容都是不一样的，diff是文件系统的统一挂载点,link文件描述的是该层的标识符，lower文件描述了层与层之间的组织关系，overlay2是将底层多个lowerdir和upperdir和workdir联合挂载，形成最终的merged挂载点。 devicemapper：在linux2.6内核版本中并入内核，devicemapper将所有的镜像和容器存储在自己的虚拟块设备上，devicemapper根上面aufs和overlay不一样的是devicemapper工作在块层次上而不是文件层次上。devicemapper驱动会从配置好的thin-pool中创建一个带一个带有文件系统的块设备，所有镜像都是创建的块设备的快照，容器层是镜像层的快照，都是采用copy-on-write策略。优点：1、devicemapper的copy-on-write策略以64KB为最小单元，相比aufs和overlay整个文件的复制，这个效率很高了。缺点：2、启动n个容器就复制n分文件在内存中，对内存影响大，不合适单台host容器密度高的业务场景。 从上图可以看出，镜像的每一层都是它下面一层的快照，镜像是thin-pool里面创建的块设备的快照。devicemapper分两种模式loop-lvm，direct-lvm，这两种模式的主要区别就是创建thin-pool的方式不一样。loop-lvm是在&#x2F;var&#x2F;lib&#x2F;docker&#x2F;devicemapper&#x2F;创建data和metada两个稀疏文件通过losetup将这两个文件映射成块设备，然后通过devicemapper映射成thin-pool。 direct-lvm是直接通过lvm的方式将设备创建成pv，组成两个vg分别为data和metadata，将这两个vg组成lvm的thin-pool。需要注意的docker官方推荐centos和rhel生产中使用devicmapper的direct-lvm模式，不推荐loop-lvm因为loop-lvm在挂载多个容器后性能会急剧下降。 适用场景：没有好坏分只有合适分，在不同业务场景下使用不同的storage-driver。这是docker官方给出的一个不同linux发行版的推荐表https://docs.docker.com/engine/userguide/storagedriver/selectadriver/#docker-ce 不同storage-driver所支持的文件系统 不同的存储驱动在不同的应用场景下性能不同。例如，AUFS、Overlay、Overlay2 操作在文件级别，内存使用相对更高效，但大文件读写时，容器层会变得很大；DeviceMapper、Btrfs、ZFS 操作在块级别，适合工作在io负载高的场景；容器层数多，且写小文件频繁时，devicemapper 效率比 Overlay 更高；Btrfs、ZFS 更耗内存。此图片来源七牛布道师 陈爱珍在docker.io的分享http://dockone.io/article/1513 性能对比http://dockone.io/article/1513 docker默认是使用操作系统本身默认的storage-driver，比如说你在ubuntu上安装docker那么默认就是aufs，在centos7或rhel7上部署docker默认就是overlay。将centos7默认的overlay改成生产可用的devicemapper的direct-lvm模式。安装device-mapper-persistent-data, lvm2包 1yum install device-mapper-persistent-data, lvm2 -y 创建pv 1pvcreate /dev/sdb 创建vg 1vgcreate docker /dev/sdb 创建lv，这里需要注意，我们分两个lv，一个是data，一个是metadata，最后要将这个两个lv组成一个thin-pool 12lvcreate --wipesignatures y -n thinpool docker -l 95%VG #使用95% VG剩余空间lvcreate --wipesignatures y -n thinpoolmeta docker -l 1%VG 将lv docker&#x2F;thinpool做为thinpool ，docker&#x2F;thinpoolmeta做为pool的metadata 1lvconvert -y --zero n -c 512K --thinpool docker/thinpool --poolmetadata docker/thinpoolmeta 配置lvm thinpool的自动扩容 vi &#x2F;etc&#x2F;lvm&#x2F;profile&#x2F;docker-thinpool.profile 1234activation &#123; thin_pool_autoextend_threshold=80 thin_pool_autoextend_percent=20&#125; thin_pool_autoextend_threshold:已用空间百分比 ，100为关闭thin_pool_autoextend_percent&#x3D;20：每次扩容thinpool的容量比例，0为关闭 当磁盘使用达到80%时，再扩展当前容量的20%。更新&#x2F;etc&#x2F;lvm&#x2F;profile&#x2F;docker-thinpool.profile 激活此规则 1lvchange --metadataprofile docker-thinpool docker/thinpool 对逻辑卷启用监视 1lvs -o+seg_monitor 如果之前&#x2F;var&#x2F;lib&#x2F;docker有数据，建议进行备份，没有的可以直接运行。 1234567891011编辑daemon.json文件/etc/docker/daemon.json&#123; &quot;storage-driver&quot;: &quot;devicemapper&quot;, &quot;storage-opts&quot;: [ &quot;dm.thinpooldev=/dev/mapper/docker-thinpool&quot;, &quot;dm.use_deferred_removal=true&quot;, &quot;dm.use_deferred_deletion=true&quot; ]&#125; 重启docker 1systemctl restart docker 验证 我们可以直接pull一个镜像看看逻辑卷使用会不会改变。 1docker pull centos 在次查看发现使用率提高了 扩容按照传统的lvm扩容的方式扩容就可以了，假设我添加一个100G的sdc设备先创建pv 1pvcreate /dev/sdc 扩容vg 1vgextend docker /dev/sdc 扩容thin-pool 1lvextend -l+100%FREE -n docker/thinpool 检查 当启动一个容器时 1docker run -itd centos 就生成了对应的块设备 进入对应目录可以看见运行的容器对应的rootfs 参考链接：http://blog.csdn.net/M2l0ZgSsVc7r69eFdTj/article/details/78153724http://blog.csdn.net/vchy_zhao/article/details/70238690http://blog.csdn.net/a85880819/article/details/52448654https://docs.docker.com/engine/userguide/storagedriver/selectadriver/#related-information","categories":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"修改docker容器默认空间大小","slug":"docker_default_storage","date":"2018-01-23T13:45:59.000Z","updated":"2017-01-23T13:45:59.000Z","comments":true,"path":"2018/01/23/docker_default_storage/","permalink":"http://yoursite.com/2018/01/23/docker_default_storage/","excerpt":"","text":"软件版本docker-ce 17.03storagedriver：device-mapper：direct-lvm docker 启动一个容器后默认根分区大小为10GB，通过docker info可以看见默认大小为10G,有时会不够用需要扩展。 两种修改方式静态扩展启动容器查看容器分区大小指定容器默认空间大小为20G修改&#x2F;etc&#x2F;docker&#x2F;daemon.json，添加dm.basesize&#x3D;xxx 需要注意的是只能扩容不能缩减。重启docker 1systemctl restart docker 此时还是不生效的，需要把镜像重新pull一遍 12docker rmi centosdocker pull centos 重新run一个缺点： 非动态更改，改完后还需要重新启动docker 更改后镜像需要重新pull下来。 只能扩容不能缩减。 动态修改以centos这个容器为例，动态修改这个容器默认空间为30G创建个centos容器 1docker run -itd --name centos centos 找到对应的块设备 1docker inspect centos|grep DeviceName 使用dmsetup table查看文件扇区信息结果的第二个数字(41943040)是设备的大小，表示有多少个512字节的扇区，计算方法是(2010241024*1024&#x2F;512)我们修改大小也只需要修改这个值。将新的扇区大小写入，只需要修改第二个值 1echo 0 62914560 thin 253:3 15|dmsetup load /dev/mapper/docker-253\\:0-1940029-7ad938b4cb54e70bed0028bb5705c3f7ecd832591d56b076c7c59e51f8cb1276 将修改后的容器存储文件激活 1dmsetup resume /dev/mapper/docker-253\\:0-1940029-7ad938b4cb54e70bed0028bb5705c3f7ecd832591d56b076c7c59e51f8cb1276再次查看 发现扇区大小改好了文件系统更新ext4、ext3、ext3文件系统使用 1resize2fs /dev/mapper/docker-253\\:0-1940029-7ad938b4cb54e70bed0028bb5705c3f7ecd832591d56b076c7c59e51f8cb1276 xfs文件系统使用 1xfs_growfs -d /dev/mapper/docker-253\\:0-1940029-7ad938b4cb54e70bed0028bb5705c3f7ecd832591d56b076c7c59e51f8cb1276 优点： 动态修改，不需要修改docker启动参数，重启docker。 不用重新pull镜像，针对单个容器，控制灵活。 缺点： 上面用的方法只能是devicemapper的storage-driver。 只能扩容不能缩减。 建议：实际使用中，数据还是尽量放数据卷中，放镜像中的话，会造成镜像太过于雍肿，就违背了容器本身设计的初仲了。","categories":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"kubernetes V1.9安装(附离线安装包和离线镜像）","slug":"kubernetes_install","date":"2018-01-02T13:45:59.000Z","updated":"2018-01-02T13:45:59.000Z","comments":true,"path":"2018/01/02/kubernetes_install/","permalink":"http://yoursite.com/2018/01/02/kubernetes_install/","excerpt":"","text":"简介环境信息（采用一个master节点+两个node节点)master 192.168.2.110node-1 192.168.2.112node-2 192.168.2.113 软件版本kubernetes v1.9docker：17.03kubeadm:v1.9.0kube-apiserver:v1.9.0kube-controller-manager:v1.9.0kube-scheduler:v1.9.0k8s-dns-sidecar:1.14.7k8s-dns-kube-dns:1.14.7k8s-dns-dnsmasq-nanny:1.14.7kube-proxy:v1.9.0etcd:3.1.10pause :3.0flannel:v0.9.1kubernetes-dashboard:v1.8.1 采用kubeadm安装 kubeadm为kubernetes官方推荐的自动化部署工具，他将kubernetes的组件以pod的形式部署在master和node节点上，并自动完成证书认证等操作。因为kubeadm默认要从google的镜像仓库下载镜像，但目前国内无法访问google镜像仓库，所以这里我提交将镜像下好了，只需要将离线包的镜像导入到节点中就可以了。 开始安装所有节点操作下载链接: https://pan.baidu.com/s/1c2O1gIW 密码: 9s92比对md5解压离线包 MD5 (k8s_images.tar.bz2) &#x3D; b60ad6a638eda472b8ddcfa9006315ee 解压下载下来的离线包 1tar -xjvf k8s_images.tar.bz2 安装docker-ce17.03(kubeadmv1.9最大支持docker-ce17.03) 12rpm -ihv docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpmrpm -ivh docker-ce-17.03.2.ce-1.el7.centos.x86_64.rpm 修改docker的镜像源为国内的daocloud的。 1curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://a58c8480.m.daocloud.io 启动docker-ce 1systemctl start docker &amp;&amp; systemctl enable docker 绑定hosts将master和node-1、node-2绑定hosts master节点与node节点做互信 123[root@master ~]# ssh-keygen[root@master ~]# ssh-copy-id node-1[root@master ~]# ssh-copy-id node-2 关闭防火墙和selinux 1systemctl stop firewalld &amp;&amp; systemctl disable firewalld vim &#x2F;etc&#x2F;selinux&#x2F;configSELINUX&#x3D;disabled 1setenforce 0 配置系统路由参数,防止kubeadm报路由警告 echo &quot; net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 &quot; &gt;&gt; /etc/sysctl.conf sysctl -p 123导入镜像 docker load &lt; /root/k8s_images/docker_images/etcd-amd64_v3.1.10.tar docker load &lt;/root/k8s_images/docker_images/flannel\\:v0.9.1-amd64.tar docker load &lt;/root/k8s_images/docker_images/k8s-dns-dnsmasq-nanny-amd64_v1.14.7.tar docker load &lt;/root/k8s_images/docker_images/k8s-dns-kube-dns-amd64_1.14.7.tar docker load &lt;/root/k8s_images/docker_images/k8s-dns-sidecar-amd64_1.14.7.tar docker load &lt;/root/k8s_images/docker_images/kube-apiserver-amd64_v1.9.0.tar docker load &lt;/root/k8s_images/docker_images/kube-controller-manager-amd64_v1.9.0.tar docker load &lt;/root/k8s_images/docker_images/kube-scheduler-amd64_v1.9.0.tar docker load &lt; /root/k8s_images/docker_images/kube-proxy-amd64_v1.9.0.tar docker load &lt;/root/k8s_images/docker_images/pause-amd64_3.0.tar docker load &lt; /root/k8s_images/docker_images/kubernetes-dashboard_v1.8.1.tar 12安装安装kubelet kubeadm kubectl包 rpm -ivh socat-1.7.3.2-2.el7.x86_64.rpm rpm -ivh kubernetes-cni-0.6.0-0.x86_64.rpm kubelet-1.9.9-9.x86_64.rpm kubectl-1.9.0-0.x86_64.rpm rpm -ivh kubectl-1.9.0-0.x86_64.rpm rpm -ivh kubeadm-1.9.0-0.x86_64.rpm 1234### master节点操作启动kubelete systemctl enable kubelet &amp;&amp; sudo systemctl start kubelet 1开始初始化master kubeadm init --kubernetes-version=v1.9.0 --pod-network-cidr=10.244.0.0/16 123456789101112kubernetes默认支持多重网络插件如flannel、weave、calico，这里使用flanne，就必须要设置--pod-network-cidr参数，10.244.0.0/16是kube-flannel.yml里面配置的默认网段，如果需要修改的话，需要把kubeadm init的--pod-network-cidr参数和后面的kube-flannel.yml里面修改成一样的网段就可以了。![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_1.png)![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_2.png)发现kubelet启动不了查看日志/var/log/message![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_4.png)发现原来是kubelet默认的cgroup的driver和docker的不一样，docker默认的cgroupfs，kubelet默认为systemd修改 vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 123![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_5.png)重启reload systemctl daemon-reload &amp;&amp; systemctl restart kubelet 1234567查看状态![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_6.png)此时记得将环境reset一下执行 kubeadm reset 1在重新执行 kubeadm init --kubernetes-version=v1.9.0 --pod-network-cidr=10.244.0.0/16 1234567891011![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_7.png)将kubeadm join xxx保存下来，等下node节点需要使用如果忘记了，可以在master上通过kubeadmin token list得到![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_8.png)![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_9.png)默认token 24小时就会过期，后续的机器要加入集群需要重新生成token kubeadm token create 1然后在执行 kubeadm join --token xxx master_ip:6443 12按照上面提示，此时root用户还不能使用kubelet控制集群需要，配置下环境变量 对于非root用户 mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 1对于root用户 export KUBECONFIG=/etc/kubernetes/admin.conf 1也可以直接放到~/.bash_profile echo &quot;export KUBECONFIG=/etc/kubernetes/admin.conf&quot; &gt;&gt; ~/.bash_profile 1source一下环境变量 source ~/.bash_profile 123456kubectl version测试![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_10.png)安装网络，可以使用flannel、calico、weave、macvlan这里我们用flannel。下载此文件 wget https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml 12345或直接使用离线包里面的若要修改网段，需要kubeadm --pod-network-cidr=和这里同步vim kube-flannel.yml修改network项 &quot;Network&quot;: &quot;10.244.0.0/16&quot;, 123执行 kubectl create -f kube-flannel.yml 123456### node节点操作修改kubelet配置文件根上面有一将cgroup的driver由systemd改为cgroupfs vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Environment=&quot;KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs&quot; systemctl daemon-reload 12```systemctl enable kubelet&amp;&amp;systemctl restart kubelet 使用刚刚执行kubeadm后的kubeadm join --xxx 1kubeadm join --token 361c68.fbafaa96a5381651 192.168.2.110:6443 --discovery-token-ca-cert-hash sha256:e5e392f4ce66117635431f76512d96824b88816dfdf0178dc497972cf8631a98 多次加入报错查看/var/log/message日志. 这个错是因为没有配置前面sysctl的router的环境变量 ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_26.png) 发现node节点启动不了flannel容器，查看容器log发现是host上没有默认路由，在网卡配置文件里面设置好默认路由。 ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_11.png) 加入成功 在master节点上check一下 ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_27.png) kubernetes会在每个node节点创建flannel和kube-proxy的pod ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_14.png) ### 测试集群 在master节点上发起个创建应用请求 这里我们创建个名为httpd-app的应用，镜像为httpd，有两个副本pod 1kubectl run httpd-app --image=httpd --replicas=2 ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_15.png) 检查pod 可以看见pod分布在node-1和node-2上 ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_16.png) 因为创建的资源不是service所以不会调用kube-proxy 直接访问测试 ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_17.png) 至此kubernetes基本集群安装完成。 ### 部署kubernetes-dashboard kubernetes-dashboard是可选组件，因为，实在不好用，功能太弱了。 建议在部署master时一起把kubernetes-dashboard一起部署了,不然在node节点加入集群后，kubernetes-dashboard会被kube-scheduler调度node节点上，这样根kube-apiserver通信需要额外配置。 直接使用离线包里面的kubernetes-dashboard.yaml 修改kubernetes-dashboard.yaml 如果需要让外面访问需要修改这个yaml文件端口类型为NodePort默认为clusterport外部访问不了， ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_18.png) nodeport默认端口范围30000-32767 32666就是我的映射端口，根docker run -d xxx:xxx差不多 创建kubernetes-dashboard 1kubectl create -f kubernetes-dashboard.yaml 访问 1https://master_ip:NodePort ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_19.png) 默认验证方式有kubeconfig和token，这里我们都不用。 这里我们使用basicauth的方式进行apiserver的验证 创建/etc/kubernetes/pki/basic_auth_file 用于存放用户名和密码 #user,password,userid admin,admin,2 给kube-apiserver添加basic_auth验证 1vim /etc/kubernetes/manifests/kube-apiserver.yaml ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_20.png) 加上这行 更新kube-apiserver容器 1kubectl apply -f kube-apiserver.yaml 授权 k8s1.6后版本都采用RBAC授权模型 给admin授权 默认cluster-admin是拥有全部权限的，将admin和cluster-admin bind这样admin就有cluster-admin的权限。 ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_21.png) 那我们将admin和cluster-admin bind在一起这样admin也拥用cluster-admin的权限了 1kubectl create clusterrolebinding login-on-dashboard-with-cluster-admin --clusterrole=cluster-admin --user=admin 查看 1kubectl get clusterrolebinding/login-on-dashboard-with-cluster-admin -o yaml ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_22.png) 在此访问https://master:32666 ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_28.png) 选基本，就可以通过用户名和密码访问了 ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_23.png) 创建个应用测试 ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_24.png) 部署成功 ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kubernetes_install_25.png) 参考链接: https://kubernetes.io/docs/setup/independent/install-kubeadm/ http://tonybai.com/2017/07/20/fix-cannot-access-dashboard-in-k8s-1-6-4/","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"Docker多主机网络(weave)","slug":"docker_weave","date":"2017-12-15T13:45:59.000Z","updated":"2017-12-15T13:45:59.000Z","comments":true,"path":"2017/12/15/docker_weave/","permalink":"http://yoursite.com/2017/12/15/docker_weave/","excerpt":"","text":"此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。weave软件版本docker：17.09weave：2.1.3项目文 档：https://www.weave.works/docs/net/latest/overview/项目地址：https://github.com/weaveworks/weave环境信息container-1 192.168.111.161container-2 192.168.111.162 概述weave是weaveworks开发的容器网络解决方案，weave会创建一个大的二层网络，来将各个host上的容器连接起来，容器与容器之间可以直接通信，无须nat和端口映射,weave还提供dns服务，使容器可以直接通过hostname访问。weave不需要依赖k\\v服务，而是在每个主机上运行一个weave容器路由器。来根据不同的主机交换路由信息。 注意点：1、一个Weave网络是由一定数量的Weave节点构成的。每个Weave路由器运行在不同的宿主机上。 2、Weave网络中的每个节点都有一个名字，默认是Weave网卡的地址。还有一个人类便于识别的名称，默认是主机名，也可以在Weave启动的时候指定一个名称。 3、Weave路由器节点之间会建立TCP连接，通过TCP连接，进行心跳检测和交换路由信息。通过配置可以让这条链接进行加密处理。 4、Weave路由器节点之间也会建立UDP连接，通过UDP连接，进行网络数据包的封装。同样也可以将UDP链接进行加密处理。 Weave路由器节点之间的连接(TCP连接或者UDP连接)可以穿透防火墙，意味着，Weave网络可用于跨数据中心的Docker通信。 5、Weave网络会在每个宿主机上创建一个网桥，每个容器通过veth pair连接到这个Weave 网桥。容器里面的veth网卡会获取到Weave网络分配给的IP地址和子网掩码。 6、Weave网络在不同宿主机之间路由数据的方法有两种：fast data path模式(完全工作在内核态)，sleeve模式，在这种模式中，发往非本地容器的数据会被内核捕获，然后交给用户态的Weave网络路由器处理，通过UDP发送到其他的Weave节点路由器，然后注入到内核空间，最后再转发到本地的容器中。 7、Weave网络路由器会学习其他节点特定的MAC地址，然后将已知的信息和拓扑结合起来。 8、Weave网络可以在partially connected路由数据，通过拓扑交换。比如下图中：节点1 直接连接着 节点2和节点3，如果节点1 先要发送数据到 节点4或者节点5 ，那么先必须发送到节点3。 Weave 网络如何了解网络拓扑？官网地址：https://www.weave.works/docs/net/latest/concepts/network-topology/ Peers之间的拓扑交流 连接到Weave网络的节点会捕获其他Peers节点的拓扑信息。Weave节点会将自己已知的拓扑，和改变的内容传递给其他节点，以便于所有的Weave Peers了解整过网络拓扑。 Weave节点之间的通信建立在TCP上的，有两种方法： 基于STP(生成树)的广播方法基于邻居Gossip方法(八卦算法)以下情况，拓扑信息会发送： 当一个连接新加入进来的时候，如果远程节点似乎是新加入到Weave网络中的，那么会发送整个网络拓扑，并且增量更新，广播节点两端的连接。当一个连接被标记为已经建立，则意味着远端可以从本端接受UDP报文，然后广播一个包含本端信息的数据包。当一个连接断开，一个包含本端信息的报文被广播。周期性的定时器，整个拓扑信息被gossip给邻居，这是为了拓扑敏感的随机分布系统。这是为了防止由于频繁的拓扑变化，造成广播路由表过时，而使前面提到的广播没有到达所有的peers。如果Peers更新拓扑信息之后，发现有一个Peer已经离线，那么就会清除掉这个Peer相关的所有的信息。 拓扑过期怎么办？ 将拓扑变化信息广播给所有peers不是立即发生的。这就意味着，很有可能一个节点有过期的网络拓扑视图。 如果目的peer的数据包仍然可达，那么过期的拓扑可能会导致一次低效的路由选择。 如果过期的拓扑中目的peer不可达，那么数据包会被丢弃，对于很多协议（如TCP)，数据发送会在稍后重新尝试，在这期间拓扑信息应当被正确更新。 weave安装container-1和container-2上安装weave 12curl -L git.io/weave -o /usr/local/bin/weavechmod a+x /usr/local/bin/weave 在container-1上启动weave在 host1 中执行 weave launch 命令，weave的组件都是以容器启动的，weave 会从 docker hub 下载最新的 image 并启动容器。 10.32.0.0&#x2F;12是weave网络使用的默认subnet，如果需要改变(所有host都要改) 1weave launch --ipalloc-range 10.2.0.0/16 weave 运行了三个容器：weave 是主程序，负责建立 weave 网络，收发数据 ，提供 DNS 服务等。weavevolumes容器提供卷存储weavedb容器提供数据存储 创建了一个linux bridge桥 和weave网络 container-2执行 1weave launch 192.168.111.161 这里指定的container-1的地址，这样container-2和container-1才会加入到同一个网络，也可以写多个地址，格式为 ip_addr ip_addr. 两个host要互通，首先要确认建立了集群 测试在container-1上执行特别说明：声明环境变量的作用是告诉后续的docker命令都发给weave proxy进行处理。声明环境变量 12eval $(weave env)docker run -itd --name bbox1 busybox 如果要恢复之前的环境，可执行 1eval $(weave env --restore) 在container-2上执行 12eval $(weave env)docker run -itd --name bbox2 busybox bbox1 ping bbox2 原理单台host架构图 bbox1和bbox2的地址为10.32.0.1&#x2F;12、10.44.0.0&#x2F;12, 子网掩码都是12位的，实际上这3个ip都位于 weave网络的10.32.0.0&#x2F;12，通过container-1和container-2上的vxlan的遂道连接在一起。 查看bbox1的网络接口 bbox1有两个网络接口eth0和ethwe，eth0桥接的是默认docker0网络。ethwe和宿主机weave网卡有关ethwe@if39表示ethwe对应编号39的interface。 在宿主机ip a看见了39的interface，vethwepl19152和ethwe是一对veth-pair，同时vethwep19152还挂载在weave 桥上。 同时挂载的还有vethwe-bridge，ip -d link查看 vethwe-bridge和vethwe-datapath是一对veth-pair，但vethwe-datapath的父设备是datapath，其中datapath是个openvswitch网桥，vxlan-6784是一个vxlan-interface，其父设备也是datapath，weave主机间是通过vxlan建立的遂道通信，等于说，veth-pair vethwe-bridge和vethwe-datapath将linux-bridge的weave桥和datapath连接在一起，其中，weave桥负责容器接入weave网络，datapath负责主机间建立vxlan隧道并收发数据。 多host时网络架构图 1、当在container-1上的bbox1 ping container-2上bbox2时 2、将数据包给ethwe 2、ethwe和vethwep是veth-pair，所以数据包到了宿主机的weave桥上，然后给vethwe-bridge，然后通过veph-pair的另外一端vethwe-datapath到datapath，通过vxlan发送给container-2。3、container-2的vxlan接口收到报文后会给datapath然后通过vethwe-datapath到weave桥收到报文后，根据目的IP地址将数据包转发给bbox2。 weave网络隔离默认weave使用一个大的subnet(10.32.0.0&#x2F;12)所有主机容器都从这个地址空间分配ip，因为是同一个subnet，容器可以直接通信，如果要隔离，可以通过环境变量WEAVE_CIDR&#x3D;net:xxx.xxx.xxx&#x2F;24指定网段,但需要注意的是这个网段也不是随便写的，要在10.32.0.0&#x2F;12这个大子网范围内。 1docker run -itd -e WEAVE_CIDR=net:xxx.xxx.xxx.xxx/24 image_name weave与外网连接宿主机访问weave网络的容器因为weave默认是一个私有的vxlan网络，默认与外部网络隔离，要想host访问，需要将host加入weave。 10.32.0.2会被分配到weave网桥上 此时就可以ping通了 如果其他主机想访问这台host上的容器，只需要添加一条默认路由将下一跳指向host的地址。container-2访问container-1上的bbox1只需要在container-2上添加 1ip route add 10.32.0.0/12 via 192.168.111.161 IPAM默认weave使用一个大的subnet(10.32.0.0&#x2F;12)所有主机容器都从这个地址空间分配ip一个大二层。 参考链接 http://ibash.cc/frontend/article/59/","categories":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"Docker多主机网络(flannel)","slug":"docker_flannel","date":"2017-12-14T13:45:59.000Z","updated":"2017-12-14T13:45:59.000Z","comments":true,"path":"2017/12/14/docker_flannel/","permalink":"http://yoursite.com/2017/12/14/docker_flannel/","excerpt":"","text":"此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。flannel软件版本docker：17.09etcd：etcd-3.2.7-1flannel：flannel-0.7.1-2项目地址：https://github.com/coreos/flannel项目地址：https://github.com/coreos/etcd 环境信息container-1 192.168.2.110 etcdcontainer-2 192.168.2.112 dockercontainer-3 192.168.2.113 docker flannel是core os开发出的docker多host的网络解决方案，flannel为每个宿主机分配一个subnet，每个宿主机上都有一个flannel的agent端，通过这个agent端可以进行根其他宿主机网络信息的共享，创建flannel网卡生成路由信息，建立vxlan遂道,各个宿主机的网络信息存储在etcd这个key-value软件中。 flannel的backend vlxan、host-gw、udp等。 安装安装etcd1yum install etcd -y 配置etcd 1cp /etc/etcd/etcd.conf /etc/etcd/etcd.conf.bak vim &#x2F;etc&#x2F;etcd&#x2F;etcd.conf 12345ETCD_NAME=default #节点名称ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot; #数据存放位置ETCD_LISTEN_CLIENT_URLS=&quot;http://0.0.0.0:2379,http://0.0.0.0:4001&quot; #监听客户端地址ETCD_ADVERTISE_CLIENT_URLS=&quot;http://etcd:2379,http://etcd:4001&quot; #通知客户端地址 启动服务 1systemctl start etcd 检查服务 导入网络配置key先将配置信息写到文件 flannel-config.json 中，内容为：[root@container-0 ~]# cat flannel-config.json 12345678910111213&#123; &quot;Network&quot;: &quot;10.2.0.0/16&quot;, &quot;SubnetLen&quot;: 24, &quot;Backend&quot;: &#123; &quot;Type&quot;: &quot;vxlan&quot; &#125;&#125; Network 定义该网络的 IP 池为 10.2.0.0&#x2F;16。 SubnetLen 指定每个主机分配到的 subnet 大小为 24 位，即10.2.X.0&#x2F;24。 Backend 为 vxlan，即主机间通过 vxlan 通信，后面我们还会讨论host-gw。 将配置存入etcd &#x2F;docker-test&#x2F;network&#x2F;config 是此 etcd 数据项的 key，其 value 为 flannel-config.json 的内容。key 可以任意指定，这个 key 后面会作为 flanneld 的一个启动参数。执行 etcdctl get 确保设置成功。 测试get value 在container-1和container-2上执行安装flannel 1yum install flannel -y 配置flannel 1cp /etc/sysconfig/flanneld /etc/sysconfig/flanneld.bak ip写etcd的ip 启动flannel 123systemctl enable flannelsystemctl start flannel 验证可以看见flannel网卡已经出来了 配置docker使用flannel 1cat /run/flannel/subnet.env 将flannel_subnet和flannel_mtu写入docker.service编辑docker.service 1vim /usr/lib/systemd/system/docker.service bip和mtu为上图cat &#x2F;run&#x2F;flannel&#x2F;subnet.env的配置项。重启docker 12systemctl daemon-reloadsystemctl restart docker docker会10.2.71.1配置到docker0上，同时生成一条路由 同主机的docker使用docker0进行通信，跨主机的使用flannel1.1转发 1ps -ef|grep docker 测试连通性flannel不会创建新的网络，会使用默认的bridge网络启动容器container-1 123docker run -itd --name bbox1 busybox ````container-2 docker run -itd –name bbox2 busybox 12345678910111213141516171819202122232425262728293031323334353637383940414243444546![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/flnnel_11.png)![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/flnnel_12.png)![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/flnnel_13.png)### 原理#### vxlan容器内网卡和物理机上一veth-xxx是一对veth-pair，同时物理机的veth-xxx挂载在docker0这个bridge上。 ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/flnnel_14.png)容器内默认路由是给10.2.71.1![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/flnnel_15.png)10.2.71.1是docker0 ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/flnnel_16.png) 查看host路由，是10.2.0.0/16这个subnet的包会交给flannel1.1,,flannel1.1封装成udp包通过ens3，发送出去。container-2 收到包解封装，发现数据包目的地址为 10.2.36.2，根据路由表将数据包发送给 flannel1..1，并通过 docker0 到达 bbox2。 ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/flnnel_17.png) ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/flnnel_18.png) ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/flnnel_19.png)flannel并不会创建什么网桥，同一主机通过docker0连接，不同主机通过flanel1.1建立vxlan遂道连接。 ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/flnnel_20.png)#### host-gw flannel支持的backend; VXLAN: 使用Linux的VxLan；默认的VNI 是 1 ；默认的UDP端口是 8472; host-gw: 创建IP路由的方式, 不会对数据进行封装; UDP: 使用UDP 8285 端口; AliVPC: 不能用于生产; Alloc: 不能用于生产; AWS VPC: 不能用于生产; GCE: 不能用于生产; 修改etcd ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/flnnel_21.png) 导入新配置 ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/flnnel_22.png)重启etcd systemctl restart etcd 12345678重启container-1和container-2上的flannel 查看路由表，发现生成了一条到container-2的明细路由 ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/flnnel_23.png)mtu变成了1500 ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/flnnel_24.png)重新修改docker.service vim &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.service 123修改mtu为1500 重启docker systemctl restart docker 测试连通性 ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/flnnel_25.png) 面对 host-gw 和 vxlan 这两种 backend 做个简单比较。 host-gw 把每个主机都配置成网关，主机知道其他主机的 subnet 和转发地址。vxlan 则在主机间建立隧道，不同主机的容器都在一个大的网段内（比如 10.2.0.0/16）。 虽然 vxlan 与 host-gw 使用不同的机制建立主机之间连接，但对于容器则无需任何改变，bbox1 仍然可以与 bbox2 通信。 由于 vxlan 需要对数据进行额外打包和拆包，性能会稍逊于 host-gw。 IPAM: flannel为每个主机自动分配独立的subnet，用户只需要指定一个大的地址段，每个host从这个地址段里面在细分。 网络隔离: flannel都用的vxlan的vni1没有实现网络隔离。 flannel 为每个主机分配了独立的 subnet，但 flannel.1 将这些 subnet 连接起来了，相互之间可以路由。本质上，flannel 将各主机上相互独立的 docker0 容器网络组成了一个互通的大网络，实现了容器跨主机通信。flannel 没有提供隔离。 http://ibash.cc/frontend/article/58/ http://www.cnblogs.com/CloudMan6/p/7270551.html","categories":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"Docker多主机网络(macvlan)","slug":"docker_macvlan","date":"2017-12-07T13:45:59.000Z","updated":"2017-12-07T13:45:59.000Z","comments":true,"path":"2017/12/07/docker_macvlan/","permalink":"http://yoursite.com/2017/12/07/docker_macvlan/","excerpt":"","text":"docker 版本：Docker version 17.10.0-ce, build f4ffd25操作系统版本：centos7.3 物理网卡虚拟化技术，将一张物理网卡虚拟化出多个网卡，每个都用独立的mac地址和ip地址docker用macvlan优点：1、性能好，相比其他的实现，macvlan不需要创建linux bridge、直接通过物理网卡出去 缺点：vlan子接口和需要自己提前划出来，ip地址使用需要自己手动去配置。 准备交换机：1、创建vlan 这里使用vlan 100和vlan 200。2、将两台container host连接的交换机口配置为trunk模式允许vlan 100和vlan200通过。 注意1、使用macvlan网络时因为不使用bridge了所有网卡都是一个真实的物理设备，能进行三层转发。 服务器1、使用container-1和container-2的eth0连接交换机； 操作container-1和container-2分别执行 1vconfig add eth0 100 1vconfig add eth0 200 1ip link set eth0.100 up 1ip link set eth0.200 up 创建macvlan网络在container-1和container-2上执行 1docker network create -d macvlan --subnet=192.168.100.0/24 --gateway=192.168.100.1 -o parent=eth0.100 mac_net100 1docker network create -d macvlan --subnet=192.168.200.0/24 --gateway=192.168.200.1 -o parent=eth0.200 mac_net200 mac_net100和mac_net200本质上是独立的，为了避免ip冲突，在run时手动指定ip ;container-1上运行容器 1docker run -itd --name mac_net1 --ip=192.168.100.10 --network mac_net100 busybox 1docker run -itd --name mac_net2 --ip=192.168.200.10 --network mac_net200 busybox container-2上运行容器 1docker run -itd --name mac_net3 --ip=192.168.100.20 --network mac_net100 busybox 1docker run -itd --name mac_net4 --ip=192.168.200.20 --network mac_net200 busybox 正常情况是container-1上的mac_net1能够container-2上的mac_net3通新，mac_net2能根mac_net4通信; 原理 1、mac_net100 ping mac_net300 2、因为macvlan的包是直接走子接口出去的，所以不用桥，也不用通过封包。 IPAMMACVLAN需要用户自己管理subnet，自己分配地址，不同subnet通信依赖外部网关。","categories":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"docker-compose","slug":"docker_compose","date":"2017-12-05T13:45:59.000Z","updated":"2017-12-05T13:45:59.000Z","comments":true,"path":"2017/12/05/docker_compose/","permalink":"http://yoursite.com/2017/12/05/docker_compose/","excerpt":"","text":"概念用来定义和管理多个容器的容器编排工具，docker-compose是docker-fig的升级版。能做什么？能够快速管理和创建多个容器，比如我直想起一个WordPress，在容器中也不需要自己去搭建lamp和然后安装WordPress了，只需要在docker hub找mysql和WordPress的镜像就可以，然后自己docker run，传对应的参数进去，映射端口，映射目录，很麻烦，同时非常容易出错，有了docker-compose，你只需要通过yaml语言，编写docker-compose脚本，然后就可以通过docker-compose一键是启动，关闭，删除这些容器了。脚本更加好维护和方便迁移。 安装docker-compose可以直接通过pip 安装，因为docker-compose是基于python写的 1pip install docker-compose 原理dockerc-compose使用不同的请求将会调用不同的python方法去处理，比如使用docker-compose run -d将会调用run方法处理，最终调用docker-daemon去完成操作，docker-compose借助docker-py来完任务，docker-py是一个使用python开发并调用docker-daemon api的docker client包。 docker-compose命令 1234567891011121314151617181920–-verbose：输出详细信息。-f：制定一个非docker-compose.yml命名的yaml文件。-p：设置一个项目名称，默认是当前目录的名称。-d：表示后台运行--project-directory ：设置一个项目路径默认build:构建镜像，docker-compose里面镜像来源有两种一种是通过Dockerfile构建，一种是通过镜像仓库pull下来，build在这里就是指调用Docker-compose的build参数，构建镜像config:用于检查docker-compose的yml文件语法是否有错误create:创建出来的容器状态为create状态。down：将创建出来的容器集群down掉。events:接收容器事件exec：对run的容器执行命令ps：输出正在运行容器pull:pull一个服务镜像下来 docker-compose pull server_namepush:上传一个容器镜像 docker-compose push server_namestop：停止容器集群start：启动容器集群up：创建和启动一个容器scale:设置运行容器的数量，可以进行集群伸缩，如docker-compose scale resource_name=container_num 例下面这个docker-compose.yml默认执行docker-compose up -d只会起一个web1一个db1，web1使用centos镜像，db1使用httpd镜像，并且db1依赖web1，必须等到web1安装完才能操作。 12345678910version: &quot;3&quot;services: web1: image: centos tty: true db1: image: httpd depends_on: - web1 tty: true 假设web1资源不够了需要进行扩容到5个执行 1docker-compose scale web1=5 迅速扩容到5个，此时高峰期以过，不需要这么多web1容器了需要缩减到正常执行 1docker-compose scale web1=1 docker-compose.yml语法解析详细参考 12https://docs.docker.com/compose/compose-file/compose-versioning/https://www.cnblogs.com/freefei/p/5311294.html 1234567891011以下面这个文件为例version: &quot;3&quot;services: web1: image: centos tty: true db1: image: centos depends_on: - web1 tty: true version ：版本，docker-compose的版本，不同的版本所支持的语法是不一样的，所对应的docker版本也是不一样的如,但新版本都是向下兼容的 。 services：表示定义服务，是以容器为颗粒度的。web1：表示服务名。image：才用哪个镜像，docker-compose支持两种镜像来源，一种是从docker-hub上，或本地镜像仓库，一种是通过dockerfile构建镜像那这里就应该是build了。tty：根docker run的-t一样，为容器重新分配一个伪输入终端。depends_on:依赖那个服务，这里表示等待哪个服务先完成，在进行本服务的操作。 其中参数 123links: - server_name - server_name:alias_name 链接到其他服务中的容器，使用服务名称，或别名,这样在容器中就可以通过服务名的方式联通。但在新版本docker 17.09版本中发现就算不link，docker默认也将他们信息添加到docker dns中了可以直接ping通。 12external_links - container_name 链接外部已经存在容器，非docker-compose管理的容器 12ports: - &quot;xxx:xxx&quot; 端口映射 123volumes： -/dxx/xxx:/xxx -/dxx/xxxx:xxx:ro 卷映射,加上ro表示只读模式。 12volume_from: - container_name 从另外一个服务或容器挂载他的卷。 123environment： - XXXX=XXX - XXXX=XXX 设置环境变量 12extra_hosts: - &quot;www.test.com: xxx.xxx.xxx.xx&quot; 添加此项到hosts文件 container_name: xxx 设置容器名 hostname： xxx 设置主机名 restart:always 根docker run参数一样，无论怎么样，自动拉起此容器 例子python flask框架网站以docker官网例子docker-compose构建一个简单的动态网站为例创建compose文件夹，并cd到目录 1mkdir /compose 1cd /compose 编写一个python文件，内容如下vim app.py 1234567891011121314from flask import Flaskfrom redis import Redisapp = Flask(__name__)redis = Redis(host=&#x27;redis&#x27;, port=6379)@app.route(&#x27;/&#x27;)def hello(): count = redis.incr(&#x27;hits&#x27;) return &#x27;Hello World! I have been seen &#123;&#125; times.\\n&#x27;.format(count)if __name__ == &quot;__main__&quot;: app.run(host=&quot;0.0.0.0&quot;, debug=True) 其中host的redis为docker-compose里面定义的redis的service-name，因为新版本docker默认会将service_name 添加到dns所以可以不用link直接就可以通过主机名进行通信。 创建requirements.txt文件用于pip程序安装 vim requirements.txt 12flaskredis 创建Dockerfile文件用于构建镜像vim Dockerfile 123456FROM python:3.4-alpineADD . /codeWORKDIR /codeRUN pip install -r requirements.txtCMD [&quot;python&quot;, &quot;app.py&quot;] 创建Docker-compose文件，用于构建应用集群vim docker-compose.yml 12345678910version: &#x27;3&#x27;services: web: build: . volumes: - .:/code ports: - &quot;5000:5000&quot; redis: image: &quot;redis:alpine&quot; 开始运行docker-compose up 访问浏览器http://localhost:5000,刷新一下times根着变一次。 查看容器 1docker ps -a 如果要后台运行 1docker-compose up -d 查看服务的环境变量 通过service_name控制单个服务的开关。 需要注意的是，如果没指定网络的话，默认docker-compose会创建一个以docker-compose.yml所在文件夹名为名的bridge网络。比如我的docker-compose.yml文件是放在zabbix这个目录，那么通过docker-compose构建集群后，会自动创建一个zabbix_default网络，类型为bridge。 如果需要使创建的应用集群同时创建网络，使用下面例子，创建docker集群时，同时构建指定网络名称和类型、网段的docker网络。 12345678910111213141516171819version: &#x27;3&#x27;services: web: build: . ports: - &quot;5000:5000&quot; networks: - test redis: networks: - test image: &quot;redis:alpine&quot;networks: test: driver: bridge ipam: driver: default config: - subnet: 172.28.0.0/16 web和reids服务都通过network参数加入到test网络中，下面networks资源开始创建test网络，类型为bridge，ipadm驱动为默认，网段为172.28.0.0&#x2F;16。 docker-compose默认没指定网络会创建一个自己的网络，然后将这些service加入进去。 执行docker-compose up -d 查看这个网络 如果需要加入一个已经存在的网络创建网络 12docker network create --driver bridge --subnet 192.168.1.0/24 --gateway 192.168.1.1 my_net 12345678910111213141516171819version: &quot;3&quot;services: web1: image: centos tty: true networks: - my_net db1: image: centos depends_on: - web1 tty: true networks: - my_netnetworks: my_net: external: true 1docker-compose up -d 在去镜像内查看就发现是192.168.1.0&#x2F;24的网段了。 构建WordPress应用使用两个镜像，一个mysql，一个WordPress，WordPress已经部署好了apache和php和WordPress。 1234567891011121314151617181920212223version: &#x27;3&#x27;services: db1: image: mysql:5.7 volumes: - /var/lib/mysql:/var/lib/mysql environment: - MYSQL_ROOT_PASSWORD=password - MYSQL_DATABASE=wordpress - MYSQL_USER=wordpress - MYSQL_PASSWORD=wordpress wordpress: image: wordpress:latest depends_on: - db1 ports: - 8000:80 environment: - WORDPRESS_DB_HOST=db1 - WORDPRESS_DB_USER=wordpress - WORDPRESS_DB_PASSWORD=wordpress - WORDPRESS_DB_NAME=wordpress 默认docker-compose出来的容器已经自动添加service_name和ip的映射了，所以这里DB_HOST直接写db1，不用links也是可以的互通的。这些镜像的环境变量可以在docker hub上找到 构建使用下面的命令，所有的服务将使用后台模式被启动 1docker-compose up -d 访问http://localhost:8000 构建zabbix监控zabbix-server分为两个部分，zabbix-server-mysql负责接收zabbix-agent的监控数据，然后将监控数据存储到mysql中，zabbix-web-nginx-mysql负责展示监控数据。 因为默认mysql数据库不是utf-8的编码所以需要重新构建下镜像vim Dockerfile 12FROM mysql:5.7CMD [&quot;mysqld&quot;, &quot;--character-set-server=utf8&quot;, &quot;--collation-server=utf8_bin&quot;]， 所以在mysql镜像这里没有直接用仓库的镜像，而是采用dockerfile重新构建。 123456789101112131415161718192021222324252627282930313233343536version: &#x27;3&#x27;services: db1: build: . volumes: - /var/lib/mysql:/var/lib/mysql environment: - MYSQL_ROOT_PASSWORD=password - MYSQL_USER=zabbix - MYSQL_PASSWORD=zabbix - MYSQL_DATABASE=zabbix zabbix-server: image: zabbix/zabbix-server-mysql:ubuntu-3.4.4 ports: - 10051:10051 volumes: - /var/lib/mysql:/var/lib/mysql environment: - DB_SERVER_HOST=db1 - MYSQL_USER=zabbix - MYSQL_PASSWORD=zabbix - MYSQL_DATABASE=zabbix zabbix-nginx-mysql: image: zabbix/zabbix-web-nginx-mysql:ubuntu-3.4.4 depends_on: - db1 ports: - 8000:80 environment: - DB_SERVER_HOST=db1 - MYSQL_USER=zabbix - MYSQL_PASSWORD=zabbix - ZBX_SERVER_HOST=zabbix-server - PHP_TZ:Asia/Shanghai 启动docker-compose up -d访问http://localhost:8000 参考链接http://wiki.jikexueyuan.com/project/docker-technology-and-combat/yaml_file.htmlhttps://www.hi-linux.com/posts/12554.htmlhttps://docs.docker.com/compose/install/#install-composehttp://www.ywnds.com/?p=7592","categories":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"docker镜像仓库管理软件(habor）","slug":"docker_habor","date":"2017-12-04T13:45:59.000Z","updated":"2017-12-04T13:45:59.000Z","comments":true,"path":"2017/12/04/docker_habor/","permalink":"http://yoursite.com/2017/12/04/docker_habor/","excerpt":"","text":"项目首页http://vmware.github.io/harbor/#gettingHarbor软件版本操作系统：centos7.2harbor：1.2.2docker：17.09-ce环境信息container-0 192.168.2.110container-1 192.168.2.112container-2 192.168.2.113 habor是什么？habor并不是镜像仓库，更确切的说是一个镜像仓库管理平台，原生的docker registry不支持用户权限管理，而habor镜像仓库也是用的原生的docker registry，只是又通过一些其他组件来进行用户和权限管理功能。habor默认镜像是存储在本地文件系统，可以的支持的第三方对象存储，如Amazon s3、openstack swift、ceph radosgw。 组件介绍habor安装默认会启动这些组件habor-administratorhabor的系统管理容器，可以进行habor-Server的一些系统信息的获取，如存储用量harbor-db负责存储habor的用户信息和项目信息harbor-jobservice负责habor与habor的之间项目的同步harbor-log负责统一管理habor日志habor-ui负责web端展示，token的验证和生成。nginx前端反向代理，registrydocker的镜像仓库 整体架构来源(http://blog.csdn.net/u010278923/article/details/77941995) 主要组件包括proxy，他是一个nginx前端代理，主要是分发前端页面ui访问和镜像上传和下载流量，上图中通过深蓝色先标识；ui提供了一个web管理页面，当然还包括了一个前端页面和后端API，底层使用mysql数据库；registry是镜像仓库，负责存储镜像文件，当镜像上传完毕后通过hook通知ui创建repository，上图通过红色线标识，当然registry的token认证也是通过ui组件完成；adminserver是系统的配置管理中心附带检查存储用量，ui和jobserver启动时候回需要加载adminserver的配置，通过灰色线标识；jobsevice是负责镜像复制工作的，他和registry通信，从一个registry pull镜像然后push到另一个registry，并记录job_log，上图通过紫色线标识；log是日志汇总组件，通过docker的log-driver把日志汇总到一起，通过浅蓝色线条标识。 docker login过程(http://www.sohu.com/a/67065472_115128) (a) 首先，这个请求会由监听80端口的proxy容器接收到。根据预先设置的匹配规则，容器中的Nginx会将请求转发给后端的registry 容器；(b) 在registry容器一方，由于配置了基于token的认证，registry会返回错误代码401，提示Docker客户端访问token服务绑定的URL。在Harbor中，这个URL指向Core Services；(c) Docker 客户端在接到这个错误代码后，会向token服务的URL发出请求，并根据HTTP协议的Basic Authentication规范，将用户名密码组合并编码，放在请求头部(header)；(d) 类似地，这个请求通过80端口发到proxy容器后，Nginx会根据规则把请求转发给ui容器，ui容器监听token服务网址的处理程序接收到请求后，会将请求头解码，得到用户名、密码；(e) 在得到用户名、密码后，ui容器中的代码会查询数据库，将用户名、密码与mysql容器中的数据进行比对（注：ui 容器还支持LDAP的认证方式，在那种情况下ui会试图和外部LDAP服务进行通信并校验用户名&#x2F;密码)。比对成功，ui容器会返回表示成功的状态码，并用密钥生成token，放在响应体中返回给Docker 客户端。至此，一次docker login 成功地完成了，Docker客户端会把步骤(c)中编码后的用户名密码保存在本地的隐藏文件中。 docker push的流程(http://www.sohu.com/a/67065472_115128) 用户登录成功后用docker push命令向Harbor 推送一个Docker image： 1# docker push 192.168.2.110/library/hello-world (a) 首先，docker 客户端会重复login的过程，首先发送请求到registry,之后得到token 服务的地址；(b) 之后，Docker 客户端在访问ui容器上的token服务时会提供额外信息，指明它要申请一个对imagelibrary&#x2F;hello-world进行push操作的token；(c) token 服务在经过Nginx转发得到这个请求后，会访问数据库核实当前用户是否有权限对该image进行push。如果有权限，它会把image的信息以及push动作进行编码，并用私钥签名，生成token返回给Docker客户端；(d) 得到token之后Docker客户端会把token放在请求头部，向registry发出请求，试图开始推送image。Registry 收到请求后会用公钥解码token并进行核对，一切成功后，image的传输就开始了。 安装haborcontainer-0上安装habor安装是写好的docker-compose.yml文件，所以需要先安装docker-compose 1easy_install pip 1pip install docker-compose 下载habor离线安装包 1wget http://harbor.orientsoft.cn/harbor-1.2.2/harbor-offline-installer-v1.2.2.tgz md5d2af810be0554319181969835a462807解压 1tar -xvf harbor-offline-installer-v1.2.2.tgz 修改配置文件1cd /root/habor vim &#x2F;root&#x2F;habor&#x2F;harbor.cfg 123hostname=192.168.2.110 //这里不能写localhost和127.0.0.1，ui_url_protocol = http //协议写http或httpsharbor_admin_password = 123456 //harbor admin的密码，默认为Harbor12345 执行安装1[root@container-0 harbor]# ./install.sh 它会将刚刚下载的镜像import进行，然后调用docker-compose，初始化，并启动。 查看集群状态 web访问和基本使用 我这里刚刚设置了用户名为admin密码为123456，如果默认密码为Harbor12345 用户管理创建用户 创建项目 将刚刚创建的wanshaoyuan添加成项目内，角色为项目管理员用户加入对应的项目分为三种角色项目管理员、开发人员、访客分别对应的权限为项目管理员：M(管理)、D(删除)、R(读取)、W(写入)、S(查询)开发人员：R(读取)、W(写入)、S(查询)访客：R(读取)、S(查询) 项目管理项目是一组镜像仓库的逻辑集合，habor对用户权限的控制，实上就是对这个用户在这个项目的权限控制，一个项目可以有多个项目管理员，下面的habor同步也是基于项目的。 验证docker-client配置去上传镜像啦，因为habor使用的是http协议，而默认docker push镜像默认是https的所以我们需要修改下docker-clientcontainer-1和container-2上操作vim &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.service 1ExecStart=/usr/bin/dockerd --insecure-registry=192.168.2.110 1systemctl daemon-reload 1systemctl restart docker 在container-1上，上传镜像导habor给镜像打taghabor格式为 1[registry_host]:[port]/[project_name]/[封装的应用]:[版本] 原生的registry，是用户名，habor是写刚刚创建的项目名 1[registry_host]:[port]/[username]/[封装的应用]:[版本] push镜像 在habor web端查看 日志可以看见刚刚的push操作 pull镜像在container-2上将刚刚push的镜像pull下来 多habor的同步有多个habor服务器时可以设置镜像的同步 在container-1上在启动一个habor 安装方法同上访问 配置项目复制点击进项目 添加复制规则container-0上查看日志，已经同步成功了 container-1上查看，项目已经创建好了 后面只要container-0上的镜像有更新，container-1也会自动同步 http://blog.csdn.net/u010278923/article/details/72468791http://www.sohu.com/a/67065472_115128https://github.com/vmware/harbor/blob/master/docs/installation_guide.mdhttp://www.cnblogs.com/huangjc/p/6270405.html","categories":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"Docker 管理平台(Rancher)","slug":"docker-rancher","date":"2017-12-01T13:45:59.000Z","updated":"2017-12-01T13:45:59.000Z","comments":true,"path":"2017/12/01/docker-rancher/","permalink":"http://yoursite.com/2017/12/01/docker-rancher/","excerpt":"","text":"环境信息3台服务器192.168.2.110 rancher192.168.2.112 docker0192.168.2.113 docker1 软件版本操作系统：centos7.3rancher：v1.6.11在rancher上安装rancher 概念：http://rancher.com/docs/rancher/v1.6/zh/来自rancher官方文档Rancher是一个开源的企业级容器管理平台。通过Rancher，企业再也不必自己使用一系列的开源软件去从头搭建容器服务平台。Rancher提供了在生产环境中使用的管理Docker和Kubernetes的全栈化容器部署与管理平台。 Rancher由以下四个部分组成： 基础设施编排Rancher可以使用任何公有云或者私有云的Linux主机资源。Linux主机可以是虚拟机，也可以是物理机。Rancher仅需要主机有CPU，内存，本地磁盘和网络资源。从Rancher的角度来说，一台云厂商提供的云主机和一台自己的物理机是一样的。 Rancher为运行容器化的应用实现了一层灵活的基础设施服务。Rancher的基础设施服务包括网络， 存储， 负载均衡， DNS和安全模块。Rancher的基础设施服务也是通过容器部署的，所以同样Rancher的基础设施服务可以运行在任何Linux主机上。 容器编排与调度很多用户都会选择使用容器编排调度框架来运行容器化应用。Rancher包含了当前全部主流的编排调度引擎，例如Docker Swarm， Kubernetes， 和Mesos。同一个用户可以创建Swarm或者Kubernetes集群。并且可以使用原生的Swarm或者Kubernetes工具管理应用。 除了Swarm，Kubernetes和Mesos之外，Rancher还支持自己的Cattle容器编排调度引擎。Cattle被广泛用于编排Rancher自己的基础设施服务以及用于Swarm集群，Kubernetes集群和Mesos集群的配置，管理与升级。 应用商店Rancher的用户可以在应用商店里一键部署由多个容器组成的应用。用户可以管理这个部署的应用，并且可以在这个应用有新的可用版本时进行自动化的升级。Rancher提供了一个由Rancher社区维护的应用商店，其中包括了一系列的流行应用。Rancher的用户也可以创建自己的私有应用商店。 企业级权限管理Rancher支持灵活的插件式的用户认证。支持Active Directory，LDAP， Github等 认证方式。 Rancher支持在环境级别的基于角色的访问控制 (RBAC)，可以通过角色来配置某个用户或者用户组对开发环境或者生产环境的访问权限。 rancher本身也是个master&#x2F;agent的架构模式，在server端安装的是rancher-server，被rancher纳管的机器，安装rancher-agent等其他rancher组件。 单节点安装container-0上通过容器的方式安装 1docker run -d --restart=unless-stopped -p 8080:8080 rancher/server HA方式安装 组件分配vip 192.168.2.120192.168.2.110 rancher-server、nginx、mariadb、keepalived 192.168.2.112 rancher-server、nginx、mariadb、keepalived 192.168.2.113 rancher-server、nginx、mariadb、keepalived 建议采用3台host3台host上面都部署keepalive+rancher+nginx+mysqlnginx做反向代理keepalive用于管理vip落到健康的节点mysql做galera集群，保证3个节点数据库一致 配置环境绑定hosts 12scp /etc/hosts 192.168.2.112:/etc/hosts scp /etc/hosts 192.168.2.113:/etc/hosts 在三台服务器上安装nginx和keepalived 1yum install nginx keepalived -y 三台服务器启动nginx 1systemctl start nginx &amp;&amp; systemctl enable nginx 编写测试网页每个host上都不同，以此来验证。 123[root@container-0 ~]# echo &quot;container-0&quot; &gt; /usr/share/nginx/html/index.html[root@container-1 ~]# echo &quot;container-1&quot; &gt; /usr/share/nginx/html/index.html[root@container-2 ~]# echo &quot;container-2&quot; &gt; /usr/share/nginx/html/index.html 重启nginx 1systemctl restart nginx 验证 配置keepalived修改配置前先备份配置文件 1cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak 编辑keepalived配置文件 123456789101112131415161718192021222324! Configuration File for keepalivedvrrp_script check_running &#123; script &quot;/opt/check.sh&quot; interval 10 #执行的间隔时间 weight -20 # script执行失败vrrp_instance的优先级会减少20，比如master为100 slave为90，master故障后优先级掉到80，slave比master高，则vip到slave，若，master恢复优先级又会大于backup节点mater重新接管&#125;vrrp_instance VI_1 &#123; state MASTER #设置为主服务器 interface ens3 #vip放置的网卡 virtual_router_id 51 #主、备必须一样 ,这样加入一个集群 priority 100 #(主、备机取不同的优先级，主机值较大，备份机值较小,值越大优先级越高) advert_int 1 authentication &#123; auth_type PASS #VRRP认证方式，主备必须一致 auth_pass 1111 ##(密码) &#125; virtual_ipaddress &#123; 192.168.2.120 #vip地址 &#125; track_script &#123; #执行服务检查 check_running &#125;&#125; scp一份到另外两个备节点 12scp /etc/keepalived/keepalived.conf container-1:/etc/keepalived/keepalived.confscp /etc/keepalived/keepalived.conf container-2:/etc/keepalived/keepalived.conf 两个备节点修改下配置文件 123456789101112131415161718192021222324! Configuration File for keepalivedvrrp_script check_running &#123; script &quot;/opt/check.sh&quot; #检查脚本 interval 10 weight -20&#125;vrrp_instance VI_1 &#123; state BACKUP #节点类型为backup interface ens3 virtual_router_id 51 priority 90 #优先级要比master节点低 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.2.120 &#125; track_script &#123; check_running &#125;&#125; 编写进程检测脚本为防止脑裂。(container-1和cotainer-2上的脚本需要修改ping那hostname，别ping自己。)vim &#x2F;opt&#x2F;check.sh 1234567891011121314151617181920212223242526#!/bin/bashfunction check_process &#123; ps -ef |grep nginx|grep -v grep &amp;&gt;/dev/null value=`echo $?` if [ $value -ne 0 ];then pkill keepalived fi&#125;#如果ping集群另外机器和网关都ping不通，则本身网络出问题，为防止脑裂，自杀。function check_network &#123; ping container-1 -c 5 &amp;&gt;/dev/null value1=`echo $?` ping container-2 -c 5 &amp;&gt;/dev/null value2=`echo $?` ping 192.168.2.1 -c 5 &amp;&gt;/dev/null value3=`echo $?` if [ $value1 -ne 0 -a $value2 -ne 0 -a $value3 -ne 0 ];then pkill keepalived fi&#125;main() &#123; check_process check_network&#125;main 添加执行权限 1chmod a+x /opt/check.sh 脚本scp到container-1、container-2的&#x2F;opt&#x2F;目录，添加执行权限 1234scp /opt/check.sh container-1:/opt/check.shscp /opt/check.sh container-2:/opt/check.shssh container-1 &quot;chmod a+x /opt/check.sh&quot;ssh container-2 &quot;chmod a+x /opt/check.sh&quot; container-1和container-2启动keepalived 1systemctl start keepalived &amp;&amp; systemctl enable keepalived 验证keepalived验证nginx进程被kill掉时 vip迁移目前vip在master节点ens3上 将master节点的nginx kill掉去把container-2的nginx kill掉验证网络出问题时vip的迁移动将container-0的 ens3网卡关闭 1ifdown ens3 配置mariadb galera集群三个节点操作配置repo文件vim &#x2F;etc&#x2F;yum.repo&#x2F;mariadb.repo 12345[mariadb]name = MariaDBbaseurl = https://mirrors.ustc.edu.cn/mariadb/yum/10.2/centos7-amd64gpgkey=https://mirrors.ustc.edu.cn/mariadb/yum/RPM-GPG-KEY-MariaDBgpgcheck=1 1yum install MariaDB-server MariaDB-client galera 1systemctl start mariadb 一些初始化安全配置 1/usr/bin/mysql_secure_installation 关闭数据库 1systemctl stop mariadb 修改container-0上的&#x2F;etc&#x2F;my.cnf.d&#x2F;server.cnf文件如下 123456789[galera]wsrep_provider = /usr/lib64/galera/libgalera_smm.sowsrep_cluster_address = &quot;gcomm://192.168.2.110,192.168.2.112,192.168.2.113&quot;wsrep_node_name = container-0wsrep_node_address=192.168.2.110wsrep_on=ONbinlog_format=ROWdefault_storage_engine=InnoDBinnodb_autoinc_lock_mode=2 将此文件复制到container-1、container-2，注意要把 wsrep_node_name 和 wsrep_node_address 改成相应节点的 hostname 和 ip。 查看是否启用galera插件 连接mariadb,查看是否启用galera插件 配置nginx反向代理三个节点备份配置文件 1cp /etc/nginx/nginx.conf /etc/nginx/nginx.conf.bak 修改配置文件 123456789101112131415161718192021222324252627282930313233343536373839vim /etc/nginx/nginx.confuser nginx;worker_processes auto;error_log /var/log/nginx/error.log;pid /run/nginx.pid;# Load dynamic modules. See /usr/share/nginx/README.dynamic.include /usr/share/nginx/modules/*.conf;include /etc/nginx/conf.d/*.conf;events &#123; worker_connections 1024;&#125;创建rancher配置文件vim /etc/nginx/conf.d/rancher.confhttp &#123;upstream rancher &#123; server 192.168.2.110:8080; server 192.168.2.112:8080; server 192.168.2.113:8080;&#125;server &#123; listen 80; server_name 192.168.2.120; location / &#123; proxy_set_header Host $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://rancher; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection &quot;Upgrade&quot;; proxy_read_timeout 900s; &#125; &#125;&#125; 同步这两个配置文件到另外两个主机重启nginx 1systemctl restart nginx 三个节点安装rancher-server在mariadb上创建库并授权用户 1234CREATE DATABASE IF NOT EXISTS cattle COLLATE = &#x27;utf8_general_ci&#x27; CHARACTER SET = &#x27;utf8&#x27;;GRANT ALL ON cattle.* TO &#x27;cattle&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;cattle&#x27;;GRANT ALL ON cattle.* TO &#x27;cattle&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;cattle&#x27;; 使用外部数据库在三个host上执行 1docker run -d --restart=unless-stopped -p 8080:8080 -p 9345:9345 rancher/server --db-host xxxx --db-user cattle --db-pass cattle --db-name cattle --advertise-address xxxx –advertise-address 为当前主机ip–db-host 指定MySQL服务器的连接地址(写本机ip)–db-port 连接端口–db-user 连接用户–db-pass 连接密码–db-name 连接库名 非HA打开浏览器访问http://localhost:8080 HA情况打开浏览器访问http://192.168.2.120在系统管理–&gt;高可用里面可以看见集群 测试高可用将container-0关机访问192.168.2.120 正常显示创建帐号系统配置—-&gt;访问控制—-&gt;本地帐号验证 以新帐号登录，并选择中文 添加主机进rancher管理将另外两台host给rancher管理，需要安装rancher-agent 复制图上第5步的命令在192.168.2.112上执行 1sudo docker run -e CATTLE_AGENT_IP=&quot;192.168.2.112&quot; --rm --privileged -v /var/run/docker.sock:/var/run/docker.sock -v /var/lib/rancher:/var/lib/rancher rancher/agent:v1.2.7 http://192.168.2.110:8080/v1/scripts/282E41C46DCA4A2041AA:1483142400000:CPSeNWQ19B9bCfFAULmsX4TKY 在rancher上可以看见刚刚添加的主机已经进来了 在主机上查看发现rancher会在host上起这些容器 123456789r-network-services-metadata-dns-1-ffdc1af6r-ipsec-ipsec-router-1-eed9ad34r-scheduler-scheduler-1-9a4c9847r-ipsec-cni-driver-1-e5fd917dr-healthcheck-healthcheck-1-0363e64br-ipsec-ipsec-1-f3612834r-network-services-network-manager-1-9d604f7er-network-services-metadata-1-198285aerancher-agent 其中rancher-net：rancher网络的核心，它的作用是用strongSwan 和 charon创建IPSec网络;rancher-scheduler：负责容器的调度，选择合适的宿主机;rancer-dns：负责容器的主机名和ip的映射;rancher-metadata：为容器提供元数据的管理;rancher-healthcheck: 为容器提供健康状态检查，原理是通过haproxy进行检查，只有使用了rancher的托管网络才的容器才能被检查，其他网络检测不到; 基本使用点进host名字可以查看这个host的硬件信息和资源负载情况 按上面方法添加第二个host对容器的基本操作 启动、停止、查看日志、执行命令 构建单主机容器应用玩个简单的添加个http的container在对应的host下点击添加容器 输入应用的基本信息 可以看见httpd容器已经起来了 访问ip为宿主机的ip地址 构建多主机的容器集群应用两种方式一种是通过rancher本身自带的应用商店另外一种通过自己编写的docker-compose或rancher-compose，来进行应用编排 方式一通过应用商店构建grafana在应用商店搜索grafana 查看详情里面选择版本和设置密码 预览这可以看见对应的docker-compose和rancher-compose 点击部署后默认会生成1个容器访问进行容器伸缩添加负载均衡器配置负载均衡器 访问负载均衡器的地址 容器的伸缩点击容器名 进容器的管理页，通过修改容器数量进行增减 这样就做成了一个简单的高可用 方式二通过docker-compose进行构建应用用以下docker-compose构建个WordPressWordPress主要由三部分构建web Server+php+database这里是WordPress镜像里面本身包含了php和apache，只要安装WordPress和mysql就可以了 1234567891011121314151617181920212223version: &#x27;2&#x27;services: db1: image: mysql:5.7 volumes: - /var/lib/mysql:/var/lib/mysql environment: - MYSQL_ROOT_PASSWORD=password - MYSQL_DATABASE=wordpress - MYSQL_USER=wordpress - MYSQL_PASSWORD=wordpress wordpress: image: wordpress:latest depends_on: - db1 ports: - 8000:80 environment: - WORDPRESS_DB_HOST=db1 - WORDPRESS_DB_USER=wordpress - WORDPRESS_DB_PASSWORD=wordpress - WORDPRESS_DB_NAME=wordpress 构建完成 WordPress的数据主要存储在数据库中，如果没做data volume数据就存放在容器中，一旦这个容器remove了则数据就丢失了，做了data volume数据存放在host上，mysql的话可以通过主从和galera集群来使得多个host上的mysql容器数据保持一致，为了数据存放更加可靠，我们通常将数据存放在高可靠性的分布式存储和nfs上，如glusfs、ceph、nfs等上面。 rancher可以使用的持久化存储rancher-nfs http://blog.csdn.net/rancherlabs/article/details/52774702rancher ceph http://geek.csdn.net/news/detail/229747rancher本身的分布式块存储longhorn http://blog.csdn.net/rancherlabs/article/details/71080450 rancher-ebs http://rancher.com/docs/rancher/v1.6/zh/rancher-services/storage-service/rancher-ebs/ rancher网络rancher默认创建容器使用的是rancher本身的rancher-network，与其他基于vxlan和基于gre的overalay网络不一样，rancher-network是一种基于ipsec的隧道网络，使用ipsec数据报文都是经过加密算法进行加密了的，相比于其他overlay网络更加安全，但同时加密、解密对cpu资源消耗也比较多。 这就是创建容器时的托管网络 分别介绍rancher 5种网络类型和rancher-networkbridge：将容器的网卡桥接到docker0，根docker的桥接模式一样;container：两个容器共享一个网络栈他，共享网卡核配置信息。包括ip;host: 连接到host网络的容器，共享docker 宿主机的网络，并且连hostname也是宿主机的;manager:使用的就是rancher-network;none：none网络就是什么都没有的网络; rancher-network10.42.0.0&#x2F;16IPAM相比V1.2版本，使用manger网络的容器都拥有两个ip地址，一个是docker0段的一个是manger段的，V1.6采用CNI接口，这样manger网络的容器只有一个manger段的ip了。 网络隔离性：目前使用manger网络的容器都在一个大二层，也就是没有网络的隔离。 rancher-agent将host的docker0也由之间的172.17.0.1改成了10.2.0.0&#x2F;16段的地址，并且每个host的docker0的ip都不一样了 实现原理container-0 上的test-2 ping container-1上的test 通过路由追踪发现包先给了 host上的ipsec容器，10.42.190.140为container-0上的ipsec容器，通过他将包封装，加密，通过ipsec遂道传到contain-1上的ipsec容器 10.42.208.216，然后ipsec容器在转发到目标容器。 源节点包从ipsec容器到宿主机网卡，目标节点包从宿主机网卡到ipsec容器，都是Iptables 规则管理则是由 容器network-manager 组件来完成的。 查看ipsec 服务的 rancher-compose.yml 可以看到，type 使用 rancher-bridge，ipam 使用 rancher-cni-ipam，bridge 网桥则复用了 docker0，有了这个配置我们甚至可以随意定义 ipsec 网络的 CIDR，如下图所示： 12http://blog.csdn.net/rancherlabs/article/details/53835564 http://baijiahao.baidu.com/s?id=1561101273463943&amp;wfr=spider&amp;for=pc","categories":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"openstack使用ISO镜像","slug":"openstack_iso","date":"2017-11-28T16:15:26.000Z","updated":"2018-11-07T12:47:28.507Z","comments":true,"path":"2017/11/29/openstack_iso/","permalink":"http://yoursite.com/2017/11/29/openstack_iso/","excerpt":"","text":"openstack默认使用ISO镜像创建虚机时，nova从glance那获取镜像，并将根磁盘设置的磁盘类型设置为cdrom类型，ide总线类型，导致到安装系统界面时，会发现找不到磁盘。 这种情况是正常的，因为你的根分区mout了这个系统的cdrom。当我们在直接使用kvm或vmware时，使用cdrom做为安装介质时，我们也是在选完安装介质后，还需要在创建硬盘进行安装，总好像pc的cdrom是 cd或dvd设备，创建的磁盘是pc的硬盘一样。 解决方法1、修改flavor，增加临时磁盘空间，这样安装时就可以看见硬盘了 弊端是openstack的快照功能只能针对根磁盘，所以想通过此方法创建主机，在做快照，通过快照做成镜像是不行的。可以通过修改libvirt代码将cdrom mount到临时磁盘，根分区用来安装系统就解决了这个问题。 2、给云主机挂载块云硬盘，将系统安装在云硬盘内 将此云硬盘做为镜像方法通过glance create创建一个空镜像，记录uuid占个坑glance image-create 在ceph底层将这个volume卷cp到glance读取image的pool rbd cp volume&#x2F;volume-xxxxx images&#x2F;image_uuid 创建snap打上protectrbd snap create images&#x2F;image_uuid@snaprbd snap protet image&#x2F;image_uuid@snap 修改刚刚创建的空的glance镜像 1ceph_id=`ceph -s | grep cluster | awk &#x27;&#123;print $2&#125;&#x27;` 设置后端存储URL 1glance location-add --url rbd://$&#123;ceph_id&#125;/images/$&#123;image_uuid&#125;/snap $image_uuid images为ceph中存放镜像的pool名，根据实际环境修改。更新镜像元数据 1glance image-update --name=&quot;test_image&quot; --disk-format=raw --container-format=bare image_uuid 然后就可以基于新的镜像创建云主机了。","categories":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/categories/openstack/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/tags/openstack/"}]},{"title":"Docker多主机网络(overlay)","slug":"docker_network_overlay","date":"2017-11-17T13:45:59.000Z","updated":"2017-11-17T13:45:59.000Z","comments":true,"path":"2017/11/17/docker_network_overlay/","permalink":"http://yoursite.com/2017/11/17/docker_network_overlay/","excerpt":"","text":"此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记docker 版本：Docker version 17.10.0-ce, build操作系统版本：centos7.3 Docker Libnetwork Container Network Model（CNM）阵营 Docker Swarm overlayMacvlan &amp; IP network driversCalicoContiv（from Cisco） Docker Libnetwork的优势就是原生，而且和Docker容器生命周期结合紧密；缺点也可以理解为是原生，被Docker“绑架”。Container Network Interface（CNI）阵营 KubernetesWeaveMacvlanFlannelCalicoContivMesos CNI 其中overlay和macvlan是docker原生就支持的。weave、flannel、calico是需要安装额外的组件才可以的。 overlay 结构图 docker官方文档中，overlay网络是在swarm集群中配置的，但实际上，overlay网络可以独立于swarm集群实现，只需要满足以下前提条件即可。1、有consul或者etcd，zookeeper的集群key-value存储服务；2、组成集群的所有主机的主机名不允许重复，因为docker守护进程与consul通信时，以主机名相互区分；3、所有主机都可以访问集群key-value的服务端口，按具体类型需要打开进行配置。例如docker daemon启动时增加参数–cluster-store&#x3D;consul:&#x2F;&#x2F;:8500 – -cluster-advertise&#x3D;eth0:2376overlay网络依赖宿主机三层网络的组播实现，需要在所有宿主机的防火墙上打开下列端口; | 协议 |端口 | 说明启动后去访问8500端口| —————— |:—————–:||udp | 4789 | 容器之间流量的vxlan端口|tcp&#x2F;udp | 7946 | docker守护进程的控制端口 安装consul安装consul，这里通过容器的方式安装 1docker run -d -p 8500:8500 -h consul --name consul progrium/consul -server -bootstrap 启动后去访问8500端口 修改container-1和container-2的docker-daemon的配置文件 vim &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service.d&#x2F;10-machine.conf 添加–cluster-store&#x3D;consul:&#x2F;&#x2F;192.168.111.159:8500–cluster-advertise&#x3D;eth0:2376其中–cluster-store：指定consul的地址–cluster-advertise：告知consul自己连接的地址打开consule可以看见已经连接进来了 创建overlay网络在container-1中创建ov_net1 docker network ls查看当前网络 默认是一个10.0.0.0&#x2F;24的网段 在container-2中发现这个网络同步过来了 这是因为创建ovnet1时，container-1将overlay网络信息存在consul，container-2从consul读取了新网络的数据，后面ov_net的任何变化都会同步到container-1和container-2 在overlay中运行容器container-1 1docker run -itd --name overlay_1 --network ov_net1 busybox container-2 1docker run -itd --name overlay_2 --network ov_net1 busybox container-1上地址为 container-2上地址为 contaner-1 ping container-2 container-2 ping container-1 发现container-1和container-2都有两块网卡 eth0连接的是overlay ,另外docker会创建个docker_gwbridge为使用overlay网络的容器提供上外网的能力 inspect这个网络 1docker network inspect 07a9d7 发现地址分配记录overlay隔离不同overlay网络是相互隔离的在创建个overlay网络 1docker network create -d overlay ov_net2 新创建的网络地址是10.0.1.0&#x2F;24 创建主机contain-1上 1docker run -itd --name over_lay3 --network ov_net2 busybox contain-2上 1docker run -itd --name over_lay4 --network ov_net2 busybox ping ov_net1的机器，不通 因为他们vni不同，vlxan就是通过vni起到网络隔离效果。 通过docker_gwbridge也是一样的 overlay原理 1、docker会为每个overlay网络创建个单独的命名空间，在这个命名空间里创建了个br0的bridge。2、在这个命名空间内创建两张网卡并挂载到br0上，创建一对veth pair端口 和vxlan设备。3、veth pair一端接在namespace的br0上一端接在container上。4、vxlan设备用于建立vxlan tunnel，vxlan端口的vni由docker-daemon在创建时分配，具有相同vni的设备才能通信。5、docker主机集群通过key&#x2F;value存储（我们这里用的是consul)共享数据，在7946端口上，相互之间通过gossip协议学习各个宿主机上运行了哪些容器。守护进程根据这些数据来在vxlan设备上生成静态MAC转发表。6、vxlan设备根据静态mac转发表，通过host上的4789端口将数据发到目标节点。7、根据流量包中的vxlan隧道ID，将流量转发到对端宿主机的overlay网络的网络命名空间中。8、对端宿主机的overlay网络的网络命名空间中br0网桥，起到虚拟交换机的作用，将流量根据MAC地址转发到对应容器内部。 查看namespace 由于容器和overlay的网络的网络命名空间文件不再操作系统默认的&#x2F;var&#x2F;run&#x2F;netns下，只能手动通过软连接的方式查看。 ln -s&#x2F;var&#x2F;run&#x2F;docker&#x2F;netns &#x2F;var&#x2F;run&#x2F;netns可以看见两个host上都有这个1-xx的namespace 查看vni 查看vxlan设备上生成的静态mac地址转发表 可以看见又192.168.111.162和192.168.111.163的mac地址 缺点1、由于vxlan网络与宿主机网络默认不再同一网络环境下，为了解决宿主机与容器的通信问题，docker为overlay网络中的容器额外增加了网卡eth1作为宿主机与容器通信的通道。这样在使用容器服务时，就必须根据访问性质的不同，选择不同的网卡地址，造成使用上的不便。2、容器对外暴露服务仍然只能使用端口绑定的方式，外界无法简单地直接使用容器IP访问容器服务。3、从上面的通信过程中来看，原生的overlay网络通信必须依赖docker守护进程及key&#x2F;value存储来实现网络通信，约束较多，容器在启动后的一段时间内可能无法跨主机通信，这对一些比较敏感的应用来说是不可靠的。 container的ip都是自动分配的，如果需要静态的固定ip，怎么办？在创建网络的过程中有区别 12345docker network create -d overlay --subnet=192.168.2.0/24 multihostdocker run -d --name host1 --net=multihost --ip=192.168.2.2 centos7docker run -d --name host2 --net=multihost --ip=192.168.2.3 centos7 IPAMoverlay网络中所有主机共享一个大的subnet，容器启动时会顺序分配ip，可以通过–subnet指定ip地址。","categories":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"Docker单主机网络","slug":"docker_network","date":"2017-11-03T13:45:59.000Z","updated":"2017-11-03T13:45:59.000Z","comments":true,"path":"2017/11/03/docker_network/","permalink":"http://yoursite.com/2017/11/03/docker_network/","excerpt":"","text":"此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。docker网络（单host上容器网络)显示docker网络docker network list docker run时通过–network指定网络 none网络1docker run -it --network=none busybox none网络就是什么都没有的网络，挂载在这个网络的容器除了loopback没有其他任何网卡。 使用场景：对于一些安全性要求高且不需要联网的应用可以使用。 host网络连接到host网络的容器，共享docker 宿主机的网络，并且连hostname也是宿主机的。 1docker run -itd --network=hhost busybox 使用场景：用host网络性能比较高，容器可以直接操作宿主机网络，缺点是牺牲了灵活性，需要考虑端口冲突问题。 bridge网络docker使用最广泛的方式，docker安装时会创建一个docker0的linux bridge，如果不指定容器网络默认都会挂到docker0上 启动个容器，查看docker0docker run -itd centos_10_23 docker 内的网卡eth0@if28和linux bridge上的vethfe788c9是一堆veth-pair 默认bridge配置的subnet就是172.17.0.3&#x2F;16网关为172.17.0.1配在docker0上，容器创建时会自动从172.17.0.3&#x2F;16里面抽出一个地址。 user-define(用户自定义网络)user-define网络支持：bridge，overlay和macvlan网络驱动，macvlan和overlay属于跨宿主机的容器的通信，这里先不讨论，这里讨论使用bridge驱动创建user-define网络。 创建一个bridge为driver的user-define网络 docker network create –driver bridge my_net 查看网络状态 inspect一下，发现新创建的网络默认subnet地址自动会从上一个开始， 创建好后，多了个网桥 指定网段,只需要加–subnet 和–gateway参数 1docker network create --driver bridge --subnet 192.168.1.0/24 --gateway 192.168.1.1 my_net 容器指定ip，run时加上–ip，只能是使用的–subnet的网络的容器才能使用指定ip功能 1docker run -itd --network=my_net --ip 192.168.1.100 centos_10_23 定义另外一个network网络 1docker network create --subnet 192.168.2.0/24 --gateway=192.168.2.1 my_net2 在mynet_2上创建容器 1docker run -itd --network=my_net2 centos_10_23 宿主机上查看路由 开启路由转发sysctl net.ipv4.ip_forward 在容器内测试192.168.2.2 ping 192.168.1.100，发现网关都能通就是ping不通192.168.1.100 默认宿主机上的iptables DROP掉了这两个网桥的连接 给my_net2的容器加入my_net网络 docker network connect my_net 60cae2aee142 在去ping 192.168.1.100 就ok了 容器和容器间的通信3种方式：IP、Docker DNS server 、joined IP方式使用docker network connect将现有容器加入到指定网络实现容器间的通信。 Docker DNS serverDocker daemon内嵌了一个DNS Server通过Docker run –name定义的容器名进行通信，只能是user-define网络可以使用，默认的bridge不行 。 12docker run -itd --name=wan_1 --network=mynet centos_10_23 docker run -itd --name=wan_2 --network=my_net centos_10_23 1docker exec -it 8549430966f7 bash ping wan_1 joined方式两个容器共享一个网络栈他，共享网卡核配置信息。joined容器之间通过127.0.0.1直接通信 12docker run -itd --name=wan1 centos_10_23docker run -itd --name=wan2 --network=container:wan1 centos_10_23 joined合适以下场景1，不同容器中的程序希望通过loopback高效快速通信，比如web-Server和app-serve。2，独立的网络监控容器用来监控网络流量。 容器访问外网容器访问外网(SNAT)通过宿主机的iptables的snat，在宿主机查看 外界访问容器通过Docker-proxy每一个映射端口，host都会启动一个Docker-proxy进程来处理访问容器的流量","categories":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"Docker基础","slug":"docker_base","date":"2017-10-30T13:45:59.000Z","updated":"2017-10-30T13:45:59.000Z","comments":true,"path":"2017/10/30/docker_base/","permalink":"http://yoursite.com/2017/10/30/docker_base/","excerpt":"","text":"此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。基础容器不关有docker还有core os的rkt、 容器runtime是真正运行容器的地方，runtime需要跟操作系统kernel紧密协作，为容器提供运行环境，docker曾经的runtime是linux上的lxc，后面docker自己开发了runc，目前runc是docker默认的runtime rkt是coreos开发的容器runtime. 容器管理工具lxd是lxc对应的管理工具runc的管理工具是docker engine，docker engine包含daemon和cli两个部分rkt的管理工具rkt cli 容器定义工具docker image是docker容器的模板,runtime依据docker image创建容器docker file是包含若干命令的文本文件，可以通过这些命令创建出docker image aci与docker image类似，它是coreos开发的rkt容器image格式。 容器仓库私有仓库：registry 公共仓库：docker hub，Quay.io 容器oscoreos 、atomic、ubuntu core 容器平台技术 容器编排引擎:docker swarm、kubernetes、mesos+mariathon 容器管理平台：rancher、containership、 基于容器的pass：deis、flynn、dokku 容器支持技术容器网络:docker network、flannel、weave、calico服务发现：etcd、consul、zookeeper数据管理：rex-ray日志管理：docker logs、logspout安全性：openscap docker镜像linux发行版都是由内核空间和用户空间组成，内核空间是kernel，用户空间的文件系统是rootfs，可以简单的说不同linux发行版的主要区别都是rootfs base镜像：1，不依赖其他镜像，其他应用镜像可以在base镜像的基础上进行扩展，base镜像通常是各linux发行版的docker镜像，如centos、ubuntu、Debian，大部分应用镜像都是在base镜像的基础上进行扩展的。 下载镜像docker pull centos 查看镜像docker images centos 正如上面所说base镜像这么小，因为base镜像只有一个rootfs，kernel它是和宿主机共享的，base镜像只是在用户空间和发行版是一致的，内核版本是共享宿主机的kernel的，ubuntu14.04 kernel版本是4.x，但centos7.3kernel版本是3.10，当在ubuntu14.04上跑centos7.3的docker镜像时，centos7.3的内核版本也会变成4.x因为它是共享宿主机内核。 容器只能使用宿主机kernel，不能更改，不能升级，如果应用要求高内核，建议使用虚拟机。 docker镜像采用分层的结构。基于docker file文件，你每做的操作其实都是在base镜像上加一层，每安装一个软件就是在现有镜像基础上增加一层，这样做的好处时，一个共享资源 比如：有多个镜像都从相同的base镜像构建出来，那宿主机只需要一个base_image,多个镜像共享一个base镜像当某个容器镜像修改基础镜像内容时，会将基础镜像的内容像copy一份到应用镜像内，然后在修改应用镜像内的数据，这根前面说的ceph的快照和clone是一样的，都是使用的cow(copy on write)技术 当容器启动时，会有一层可写层在容器顶部叫容器层，所有对容器的改动，无论添加、删除，修改文件都会发生的容器层中，容器层下面是镜像层，镜像层是只读的。 构建镜像docker commit 在对原镜像做修改后使用docker commit将生成一个新镜像。docker commit 原镜像名 新镜像名 Dockerfile Dockerfile是一个文本文件记录镜像构件的步骤 如Dockerfile为FROM centosRUN yum install vim -y docker build-t 为新镜像名字.为指定当前目录为build context，docker 会从build context中寻找Dockerfile-f可以手动指定Dockerfile的文件 如果Dockerfile不在本地 可以看见启动一个3d799f4的临时容器，然后在临时容器中安装vim，安装成功后将临时容器保存为镜像，最后删除临时容器。 查看容器分层 docker history 镜像名 镜像的缓存特性 docker会缓存已有的镜像的镜像层，如果镜像存在就直接使用，不用创建，如果不希望使用缓存镜像则在docker build时加上–no-cache参数 docker镜像的上层是依赖下层的，只要下层发生变化，上层也会根着变。 如果在build某一步中出错了,可以通过docker run -it 先进上一步生成临时镜像进行调试。 搭建docker私有镜像仓库 从docker hub上下载registryv2.3.1的私有仓库镜像docker pull registry:2.3.1 使用htpasswd配置docker私有仓库验证[root@wan_test &#x2F;]# mkdir &#x2F;root&#x2F;auth&#x2F; 生成验证admin：admin为用户名和密码 1docker run --entrypoint htpasswd registry:2.3.1 -Bbn admin admin &gt; /root/auth/htpasswd 启动私有仓库容器 1docker run -d -p 5000:5000 -v /myregistry/:/var/lib/registry -v /root/auth/:/auth -e &quot;REGISTRY_AUTH=htpasswd&quot; -e &quot;REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm&quot; -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd registry:2.3.1 测试登录先用个错误的帐号密码测试 在用对的 认证以后无法直接在服务器查看curl 127.0.0.1:5000/v2/_catalog 仓库的镜像，会出现报错，但是可以用浏览器输入帐号密码访问。 测试上传镜像命名要规范格式为 123[registry_host]:[port]/[username]/[封装的应用]:[版本]docker tag hello-world 10.211.55.5:5000/wan/hello-wolrd:v1 docker push上传镜像 默认client是打开ssl的，但Server端没有配置证书，所以先关闭&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.service 关闭ssl安全认证 1ExecStart=/usr/bin/dockerd --insecure-registry 10.211.55.5:5000 systemctl restart docker 测试下载将本地10.211.55.5:5000&#x2F;wan&#x2F;hello-wolrd:v1删除 重新从服务器上pull下来 查看镜像 1curl http://10.211.55.5:5000/v2/_catalog web页面版的私有镜像管理软件Portusharborhttps://www.linuxea.com/1557.html 两种进入容器的方法docker attach image_iddocker exec -it image_id区别是，attach进去的还是之前的终端，不会启用新的终端，exec进去是启动一个新的终端。 运行容器的最佳实践服务类型的容器通过-d以后台的方式启动这类容器 工具类型的直接docker run -it的方式，用完就exit掉 容器的操作 容器在docker host中实际就是一个进程，docker stop命令实际就是向中国进程发送一个SIGKILL信号 docker run –restart&#x3D;always 表示无论容器因何种原因退出（含正常退出)都立刻重启，–restart&#x3D;on-failure:3意思是如果启动进程退出代码非0，则重启容器，最多重启3次。 pause&#x2F;unpause 挂起容器&#x2F;解除挂起 查看所有exited状态的容器docker ps -aq -f status&#x3D;exited 容器资源的限制docker run时指定-m或–memory 设置内存的使用限额–memory-swap设置内存+swap的使用限额。 1docker run -m 200M --memory-swap=300M centos 其含义是该容器最多使用200M内存和100M swap，因为–memory-swap是memory+swap所以前面指定的-m后面swap等于300-200， 默认情况下这两组参数都是-1，不是不限制。只指定定了-m没指定–memory-swap时默认–memory-swap是-m的两倍。 使用progrium&#x2F;stress镜像测试，只要线分配给线程的内存大小在可分配范围(300M）内，镜像就会一直工作 1docker run -it -m 200M --memory-swap=300M progrium/stress --vm 1 --vm-bytes 180M 如果超过了，容器马上回报错退出 cpu限额cpu限额限制的不是实际cpu的使用量，资源的优先级，也就是，在cpu资源紧张时，优先级高的获的的cpu时间片要长一些 docker cpu限制参数–cpuset-cpus&#x3D;”” 允许使用的 CPU 集，值可以为 0-3,0,1-c,–cpu-shares&#x3D;0 CPU 共享权值（相对权重）cpu-period&#x3D;0 限制 CPU CFS 的周期，范围从 100ms~1s，即[1000, 1000000]–cpu-quota&#x3D;0 限制 CPU CFS 配额，必须不小于1ms，即 &gt;&#x3D; 1000–cpuset-mems&#x3D;”” 允许在上执行的内存节点（MEMs），只对 NUMA 系统有效 cpu-shares通过–cpu-shares设置容器cpu配额，默认为一个core为1024，二个为2048，依次类推，设置为0，系统将会按默认值配置，假定一个1core的运行3个container，其中一个cpu-shares为1024，另外两个为512则，当cpu忙碌时，1024的占用50%cpu时间，512的分别占用25%，当增加一个1024的contain时，占用比为1024+1024+512+512 &#x3D;3072 每个container占用cpu时间比为 1024&#x2F;3072&#x3D;33% 1024&#x2F;3072&#x3D;33% 512&#x2F;3027&#x3D;17% 512&#x2F;3072&#x3D;17% 比如在host上启用两个容器 1docker run --name container_a -it --cpu-shares 2048 progrium/stress --cpu 2 1docker run --name container_b -it --cpu-shares 1024 progrium/stress --cpu 2 –cpu 有多少核，就写多少 ，这里设置为2可以看见container_a的cpu资源消耗是container_b的两倍 paush container_a,此时container_b又会将cpu整格cpu占满 block io限制block io是用来限制容器的磁盘的读写，针对bps（每秒读写的数据量)和iops(每秒IO的次数),控制容器的读写磁盘的带宽。 限制block io的参数 目前block io限额只对dicectIO（不使用文件缓存)有效。 测试先在容器内使用dd测试bps run一个centos的镜像 docker run -itd centos 限制前进入centos镜像docker exec -it 15643e5ec7d9 &#x2F;bin&#x2F;bash 限制io为30M后 1docker run -itd --device-write-bps /dev/sda:300MB centos 实现容器的底层技术曾经docker的runtime是linux内核自带的lxc，后面docker重新开发了自己的runtime，runc，目前docker通过cgroup实现资源限制，如上面的限制cpu资源、限制memory资源、限制blockIO资源大都是通过linux底层的cgroup实现，namespace实现资源隔离，每个容器独立的文件系统，网卡资源等是通过linux底层的namespace，openstack neutron的 多租户隔离qroute和qdhcp也是通过namespace去实现的。 linux下6种namespace资源 docker存储容器镜像的分层结构容器镜像 1、镜像由一个base镜像+若干个只读镜像+一个可写的容器层，这种分层架构最大的特性建设copy-on-write。2、对数据的修改都是在容器层完成的，新修改的数据都会放在最上层容器层。3、修改数据时，如果容器层没有，会先从其他层将数据copy到容器层，然后在修改。此时镜像层数据保持不变。4、如果有多个层有命名相同的文件，用户只能看见最上层中的文件。 分层结构是由linux store driver提供，docker支持多种storage driver 如 AUFS、Device Mapper、Btrfs、OverlayFs、VFS、ZFS，ubuntu默认是AUFS、centos7默认是OverlayFs ， docker默认使用linux 发行版的storage driver docker info可以看见 有些容器创建和销毁，不依赖原有数据，有些容器创建时需要加载已有数据，销毁时希望保留之前产生的数据，这类容器就需要用到容器持久化存储 Data volumeData volume 本质是 Docker Host 文件系统中的目录或文件，能够直接被mount到容器的文件系统中 特点：1、Data volume是目录或文件，不是块设备2、容器可以读写volume中的数据3、volume的数据因为在host上，但容器被销毁时，不会影响数据，所以它是可以持久保存的。 在使用上，软件放在镜像层内，软件的数据如web Server的网页，数据库的数据这些放在Data volume上 因为data volume是宿主机的一部分，所以目前volume无法设置容量。 docker提供两种类型volumebind mount：宿主机上已经存在的目录或文件mount到容器。优点：使用起来直观、高效、可以控制目录的读写权限，支持单个文件的mount缺点：需要指定host 文件系统路径，限制了容器的可移值性，当需要将容器迁到其他host上，如果其他host上没有要mount的数据或数据不在相同目录时，操作会失败。 docker managed volume：与bind mount最大区别是不需要指定mount 源，只需要指定容器内mount的目录。优点：容器申请 mount docker manager volume时会自动在&#x2F;var&#x2F;lib&#x2F;docker&#x2F;volumes下生成一个目录，这样的话，容器移值性更好。缺点：不支持控制读写权限，不支持单个文件mount。 bind mount在宿主机上创建http目录 mkdir &#x2F;root&#x2F;httpecho “My test bind mount” &gt; &#x2F;root&#x2F;http&#x2F;index.html 1docker run -d -p 80:80 -v /root/http/:/usr/local/apache2/htdocs httpd 1-v &lt;host path&gt;:&lt;container path&gt; 1curl http://127.0.0.1 My test bind mount 将host的index.html更新看看，container内的是否也会根着改变 1echo &quot;My test bind mount test_2 &quot; &gt; /root/http/index.html 1curl http://127.0.0.1 My test bind mount test_2 设置只读权限，这样就不能在container内对该目录进行修改了。 1docker run -d -p 80:80 -v /root/http/:/usr/local/apache2/htdocs:ro httpd docker manged volumemanged volume最大特点是不需要指定源，只需要指定mount point就可以了查看挂载的源 1docker run -d -p 80:80 -v /usr/local/apache2/htdocs httpd 告诉docker需要一个volume并将其挂载到&#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs，这个但这个volume会从哪里来，inspect一下就知道了manged volume最大特点是不需要指定源，只需要指定mount point就可以了。 1docker inspect 79af2997ca68 关注mounts信息 1当容器申请 manged volume时docker会在/var/lib/docker/volumes/下生成一个目录xxx/_data这个就是mount源。mount point指向以有的目录，容器内原有的数据会被复制到这个目录中 我们可以直接更新宿主机上的数据。 docker managed volume创建过程 1、容器启动时，简单告诉docker 我需要一个volume存放数据，帮我mount到容器xxx目录。2、docker在&#x2F;var&#x2F;lib&#x2F;docker&#x2F;volumes&#x2F;中生成一个随机目录作为mount源。3、如果内mount的目录已经存在，则将数据复制到mount源。4、将host上生成的目录挂载的container中。 通过docker volume ls 查看 docker manged volume的容器 docker inspect 查看volume bind mount和docker managed volume比较 数据共享容器和宿主机共享数据1、两种类型的data volume它们均可以实现容器与host之间数据共享。2、docker cp 或linux的cp命令将host数据cp到容器中。 容器和容器间的数据共享方法一：将共享数据放到bind mount的源中然后mount到多个容器中。 如搭建web集群将宿主机目录共享到3个容器上，curl访问数据都一致 . 修改宿主机文件，3个容器会同时更新。方法二：volume container专门为其他容器提供volume的容器。专门create出一个容器用来提供volume，volume container的卷可以是bind mount 也可以是docker manged volume。如创建个容器 将&#x2F;root&#x2F;http通过bind mount或docker manged volume的方式挂上去，然后其他容器只需要挂这个容器即可。 创建 volume container 1docker create --name vc_data -v /root/http/:/usr/lolca/apache2/htdocs httpd 因为volume container的作用只是提供数据，它本身不需要处于运行状态 其他容器 volume 使用volume container 查看 [root@wan_test http]# docker inspect web1 12345678910&quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;/root/http&quot;, &quot;estination&quot;: &quot;/usr/local/apache2/htdocs&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125; ], volume container的特点：(1)与bind mount相比，不必为每一个容器指定host path，所有path都在volume contain 中定义好了，容器只需要与volume container关联，实现了容器与host解偶。(2)使用 volume container的容器，其mount point是一致的，有利于配置规范的和标准，但也带来一定局限行，使用使要综合考虑。 data-packed volume container volume container，数据还是在host上 data-packed volume container数据，将数据完全放到volume container中的称为data-packed volume container 。 原理先将文件ADD 或cp到镜像中，然后在创建docker manged volume 使用Dockerfile build镜像vim Dokerfile 123FROM centosADD http /usr/local/apache2/htdocsVOLUME /usr/local/apache2/htdocs 查看volume 在dockerfile中指定了读取volume的数据，所以这里就不需要指定volume的mount point了 启动http容器使用 data-packed volume container docker run -d -p 80:80 –volumes-from vc_data httpd 容器能正确读取volume中的数据，data-packed volume container是自包含的不依赖host提供数据，具有很强移值性，非常合适静态数据的场景，比如应用的配置信息，web Server的静态文件等. data volume 生命周期 备份1、备份host上对应的目录。2、使用一个临时容器挂载目标容器备份。如创建一个managerd-volume容器 1docker run -itd --name App-container1 -v /var/volume1 -v /var/volume2 ubuntu 编辑些数据 1docker exec -it data_volume bash 12cd /var/volume1/ &amp;&amp; touch &#123;1..100&#125;cd /var/volume2/ &amp;&amp; touch &#123;200..300&#125; 在宿主机上创建个目录用于存储备份的数据 123mkdir /backupdocker run -it --rm --volumes-from App-container1 -v /backup:/backup ubuntu tar -cvf /backup/data_volume_2017_10_01.tar /var/volume1 /var/volume2 这条命令启动一个临时容器，这个容器有挂载两个volume一个的宿主机的&#x2F;backup一个是映射App-container1的目录，这样这个临时容器就有了App-container1和宿主机backup目录，然后将App-container1目录的&#x2F;var&#x2F;volume1和&#x2F;var&#x2F;volume2目录备份到backup目录。因为backup目录和宿主机是映射的，这样宿主机就有了备份文件。 将本地backup目录和容器内backup目录进行映射然后将&#x2F;var&#x2F;volume1和&#x2F;var&#x2F;volume2打包备份到backup目录。可以查看宿主机上目录[root@wan_test &#x2F;]# ll &#x2F;backup&#x2F;总用量 112-rw-r–r– 1 root root 112640 10月 6 19:34 data_volume_2017_10_01.tar 恢复启动个App-container2将App-container-1的数据恢复 到App-container2中 1docker run -itd --name App-container2 -v /var/volume1 -v /var/volume2 ubuntu 此时App-container2中的&#x2F;var&#x2F;volume1和&#x2F;var&#x2F;volume2目录是空的开始恢复 1docker run -it --rm --volumes-from App-container2 -v /backup:/backup ubuntu tar -xvf /backup/data_volume_2017_10_01.tar -C / 查看App-container2的&#x2F;var&#x2F;volume1和&#x2F;var&#x2F;volume2，数据恢复。迁移1、stop原用容器2、将原目录mount到新容器 销毁销毁容器不会销毁bind mount data managed volume销毁需要docker rm -v 或docker volume rm xxx 批量删除docker volume rm $(docker volume ls -q) https://yeasy.gitbooks.io/docker_practice/content/network/linking.html","categories":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"Docker machine(docker 多主机部署)","slug":"docker_machine","date":"2017-10-30T13:45:59.000Z","updated":"2017-10-30T13:45:59.000Z","comments":true,"path":"2017/10/30/docker_machine/","permalink":"http://yoursite.com/2017/10/30/docker_machine/","excerpt":"","text":"此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。Docker machine概念Docker machine：在一个multi-host的环境中，手动去将安装和配置docker非常繁琐，用户能够使用docker machine能够快速的安装docker环境并配置安全配置。docker machine支持在不同的环境下安装配置docker host，同时还能对构建好的host进行一些简单的管理操作。(1)常规linux操作系统(2)虚拟化平台 ：virtualbox、vmware、kvm、hyperV等(3)公有云：aws、azure等 实际体验：可能因为网络原因，经常卡住不动，不如写shel脚本l、使用puppet、和ansible这些工具，可控程度高。 实验环境3个host 安装 docker machine1234curl -L https://github.com/docker/machine/releases/download/v0.13.0/docker-machine-`uname -s`-`uname -m` &gt;/tmp/docker-machine &amp;&amp;chmod +x /tmp/docker-machine &amp;&amp;sudo cp /tmp/docker-machine /usr/local/bin/docker-machine 执行docker-machine –version验证安装 安装 bash completion script，这样在 docker-manchine 管理对应的host时，命令提示符会变成对应host的name cd &#x2F;etc&#x2F;bash_completion.d执行以下命令 1scripts=( docker-machine-prompt.bash docker-machine-wrapper.bash docker-machine.bash ); for i in &quot;$&#123;scripts[@]&#125;&quot;; do sudo wget https://raw.githubusercontent.com/docker/machine/v0.13.0/contrib/completion/bash/$&#123;i&#125; -P /etc/bash_completion.d; done 修改bashrcvim ~&#x2F;.bashrc 12source /etc/bash_completion.d/docker-machine-prompt.bashPS1=&#x27;[\\u@\\h \\W$(__docker_machine_ps1)]\\$ &#x27; 创建machine创建machine就是在host上自动安装和配置dockerdocker-machine ls #显示当前通过docker-machine创建的host 创建前需要进行互信操作，实现免密码登录，docker-machine生成公钥 12ssh-copy-id root@192.168.111.162ssh-copy-id root@192.168.111.163 开始创建执行 1docker-machine create --driver generic --generic-ip-address=192.168.111.162 container-1 卡住的话 docker-machine rm xxx 然后做试几次create 123--driver 调用哪个driver，使用普通Linux就写generic ，--generic-ip-address 指定目标系统的ipcontainer-1表示命名为contaner-1 添加container-2 12docker-machine create --driver generic --generic-ip-address=192.168.111.163 container-2 执行过程 1、通过ssh登录到远程主机2、安装docker3、复制证书4、配置docker-daemon5、启动docker docker-machine ls 可以看见已经安装好的docker host了 docker-machine 执行的步奏docker-machine creat会做两步操作。关于docker-machine做的安全配置参考http://www.cnblogs.com/sparkdev/p/7066789.html1，安装docker，并进行配置。2，生成证书保证docker服务安全。我们手动按装的docker，docker daemon监听tcp端口，但没有做任何安全限制，也就是任何人想连都可以连接进来。使用docker-machine create创建的docker默认是做的tls证书加密验证的，只有安装了特定证书的client才能与docker-daemon交互。 &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service.d&#x2F;10-machine.conf 在 Docker daemon 的配置文件中看到四个以 –tls 开头的参数，分别是 –tlsverify、–tlscacert、–tlscert和 –tlskey。其中的 –tlsverify 告诉 Docker daemon 需要通过 TLS 来验证远程客户端。其它三个参数分别指定了一个 pem 格式文件的路径，按照它们指定的文件路径去查看一下： 回到安装docker-machine的机器上ls &#x2F;root&#x2F;.docker&#x2F;machine&#x2F;machines&#x2F;container-1&#x2F; 和docker-daemon上的是一样的 Docker Machine 在执行 create 命令的过程中，生成了一系列保证安全性的秘钥和数字证书*.pem文件。这些文件在本地和远程 Docker 主机上各存一份，本地的用于配置 Docker 客户端，远程主机上的用于配置 Docker daemon，让两边都设置 TLS 验证的标记，依此实现安全的通信。 管理machine显示container-1的环境变量 执行 1eval $(docker-machine env container-1) 可以发现命令提示符发生了改变，因为我们前面在bashrc配置了在此状态下执行所有的docker命令都相当于在container-1上执行 切换到container-2eval $(docker-machine env container-2) docker-machine 命令 upgrad ：更新machine的docker到最新版本 docker-machine upgrade container-1 container-2config：查看machine的docker daemon配置stop&#x2F;restart&#x2F;restart :对host操作系统进行操作scp：可以在不同machine中scp文件。 http://www.cnblogs.com/sparkdev/p/7066789.htmlhttp://www.cnblogs.com/CloudMan6/p/7237420.html","categories":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"Docker安装","slug":"install_docker","date":"2017-10-23T13:45:59.000Z","updated":"2017-10-23T13:45:59.000Z","comments":true,"path":"2017/10/23/install_docker/","permalink":"http://yoursite.com/2017/10/23/install_docker/","excerpt":"","text":"centos7安装docker官方推荐centos7.3系统安装依赖yum install -y yum-utils device-mapper-persistent-data lvm2 添加docker yum源 1yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 版本区别：docker-ce-edge：类似开发测试版，功能比较新，但不一定稳定，edge是每月更新一次。 docker-ce-stable：稳定版，每3个月更新一次。 开启docker-ce-edge更新(默认是关闭的)yum-config-manager –enable docker-ce-edgeyum-config-manager –disable docker-ce-edge 安装docker yum install docker-ce -y 启动docker systemctl start docker 测试docker是否安装成功,运行个httpd 镜像 docker run -d -p 80:80 httpd -d:以后台方式运行-p：端口映射，源端口:目标端口 docker hub在国外，速度太慢了，使用国内的镜像源daocloud 12curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://a58c8480.m.daocloud.io 重启docker 进程systemctl restart docker 运行mysql 1docker run --name first-mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=4435448 -d mysql -e设置环境变量这里使用-e设置环境变量MYSQL_ROOT_PASSWORD 连接方法mysql -h 10.211.55.5 -P3306 -uroot -p4435448","categories":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"从零开始搭建云平台监控(五)监控自动化部署","slug":"monitor_5","date":"2017-10-16T08:27:44.000Z","updated":"2017-10-16T15:27:43.000Z","comments":true,"path":"2017/10/16/monitor_5/","permalink":"http://yoursite.com/2017/10/16/monitor_5/","excerpt":"","text":"前面四篇文章讲的更多是说这套监控平台是如何一步搭建起来的，但在实际生产中，我们要给客户搭建很多套私有云环境，不可能每个环境手动去装一边，因为zabbix的监控项和grafana的dashboard都可以做成模板导入。剩下就是一些软件安装和zabbix-agent的配置问题了，目前我采用的做法是将monitor-server封装成一个镜像，镜像里面将软件都安装好了monitor-server上有写好的ansible的playbook，agent端通，过monitor-server上的ansible的playbook去推送安装。 ansible是一个python写的基于ssh的轻量级的自动化运维工具，它有以下优点优点：(1)、轻量级，无需在客户端安装agent，更新时，只需在操作机上进行一次更新即可；(2)、批量任务执行可以写成脚本，而且不用分发到远程就可以执行；(3)、使用python编写，维护更简单，ruby语法过于复杂；(4)、支持sudo；(5)、基于ssh无需agent端就可以实现自动化配置。 缺点：(1)、ansible默认才用轮询的方式，如果节点数量一多，撑不住，相比较基于c&#x2F;s架构的saltstack 中间还有消息队列辅助在大规模集群中性能确实弱。 下面主要介绍自己写的playbook，目录结构如下 1234567891011121314151617181920212223242526272829303132333435363738/etc/ansible/|-- ansible.cfg|-- hosts|-- readme|-- roles| |-- ceph| | |-- handlers| | | `-- main.yml| | |-- tasks| | | `-- main.yml| | |-- templates| | | `-- zabbix_agentd.conf.j2| | `-- vars| | `-- main.yml| |-- common| | |-- 1| | |-- handlers| | | `-- main.yml| | |-- tasks| | | |-- 1| | | `-- main.yml| | |-- templates| | | `-- zabbix_agentd.conf.j2| | `-- vars| | `-- main.yml| `-- telegraf| |-- handlers| | `-- main.yml| |-- tasks| | `-- main.yml| |-- templates| | `-- telegraf.conf.j2| `-- vars| `-- main.yml|-- server_ip.sh|-- site.retry|-- site.yml`-- zabbix_agent.tar 我这里分为3个roles ，ceph、common、telegrafceph主要做的操作有1、拷贝配置文件 common 主要做的操作有1、环境初始化2、拷贝软件包到对应节点3、解压tar包4、安装软件5、修改对应的配置文件6、修改执行权限7、修改防火墙开放端口8、启动服务、设置开机自启 telegraf主要做的操作有1、安装telegraf2、修改telegraf systemd的启动用户3、拷贝telegraf配置文件4、重启telegraf并设置开机启动 因为每个节点所对应的角色不一样，ansible所跑的脚本也是不一样的，所以定义这3个roles，服务器的角色通过hosts文件控制 定义了三组角色，控制节点、计算节点、存储节点、融合节点、每组角色设置了不同的metadata，这个metadata是通过templates&#x2F;zabbix_agentd.conf.j2传给对应节点zabbix-agent里面的HostMetadata参数，然后zabbix-server根据不同的metadata去调用不同的模板。定义group是为了在task里面调用不同的命令。 通过site.yml来控制不同的host调用不同的role 可以看见common的hosts是all表示在hosts定义的所有组都会调用common， telegraf roles是只有control的host会去调用 ceph roles是storage的 hosts去调用 每个roles下面分handlers、tasks、templates、vars 其中，handlers：也是task，但只有其关注的条件满足时，才会被触发执行 ， templates：定义的是配置文件模板，比如我们这里定义好了zabbix_agent、和telegraf的配置文件模板 vars：定义变量，我在templates里面定义了zabbix_agent变量，里面的server 和server_active我都是以变量方式存储的，然后在执行ansible前先把变量改好，这样在不同的环境配置文件里的ip也会根着变。 项目代码先以开源：https://github.com/wanshaoyuan/ansible_monitor","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"从零开始构建云平台监控(四)图形化展示监控效果","slug":"monitor_4","date":"2017-10-13T15:27:44.000Z","updated":"2017-10-13T15:27:43.000Z","comments":true,"path":"2017/10/13/monitor_4/","permalink":"http://yoursite.com/2017/10/13/monitor_4/","excerpt":"","text":"组件安装和配置zabbix本功能非常强大，自定义监控项，自动发现、自动注册等，但zabbix-server获取的zabbix数据看起来不是特别直观。 grafana的出现正好弥补了zabbix绘图能力上的不足，grafana是基于js开发的图形编辑器。telegraftelegraf是一个go语言编写的收集监控项的agentTelegraf内存占用小的特点，通过插件来实现不同的监控需求，这里我使用ceph插件。 influxdbInfluxDB 是一个开源分布式时序|、事件和指标数据库。使用 Go 语言编写，无需外部依赖。其设计目标是实现分布式和水平伸缩扩展。它有三大特性： Time Series （时间序列）：你可以使用与时间有关的相关函数（如最大，最小，求和等） Metrics（度量）：你可以实时对大量数据进行计算 Eevents（事件）：它支持任意的事件数据特点schemaless(无结构)，可以是任意数量的列Scalablemin, max, sum, count, mean, median 一系列函数，方便统计Native HTTP API, 内置http支持，使用http读写Powerful Query Language 类似sqlBuilt-in Explorer 自带管理工具 通过grafana调用zabbix的接口的实现，通过自定义模板和key去拉去数据，在grafana上进行展示，grafana只做统一的监控展示平台ceph的一些监控数据由telegraf收集存到influxdb，grafana去读取。 为什么选择telegraf而不直接通过zabbix写item获取这些数据？1，telegraf是一个go语言写的小程序，占用资源非常小，并且本身有监控ceph的插件2，telegraf监控ceph的插件，监控的数据非常全，osd数mon数，细致点，journal盘的速率、pgmap的数率，cluster的iops、pool池的使用趋势等等，如果用zabbix的话我们想获取这些数据要写非常多的item。 monitor-server端操作安装influxdbinfluxdb的安装配置yum源 123456789cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/influxdb.repo[influxdb]name = InfluxDB Repository - RHEL \\$releaseverbaseurl = https://repos.influxdata.com/rhel/\\$releasever/\\$basearch/stableenabled = 1gpgcheck = 1gpgkey = https://repos.influxdata.com/influxdb.keyEOF yum install influxdbsystemctl enable influxdbsystemctl restart influxdb 默认influxdb的web管理界面是关闭的开启方法 vim &#x2F;etc&#x2F;influxdb&#x2F;influxdb.conf 123[admin] enabled = true bind-address = &quot;:8083&quot;重启influxdb浏览器服务http://ip:8083 在ceph-mon节点上安装telegraftelegraf通过读取ceph的asok文件获取里面的信息来达到监控目的。 1wget https://dl.influxdata.com/telegraf/releases/telegraf-1.2.1.x86_64.rpm yum localinstall telegraf-1.2.1.x86_64.rpm 修改telegraf的启动用户，不然会读取ceph asok文件权限不足vim &#x2F;lib&#x2F;systemd&#x2F;system&#x2F;telegraf.service将User&#x3D;telegraf改为User&#x3D;rootsystemctl daemon-reloadsystemctl restart telegrafsystemctl enable telegraf 配置telegrafvim &#x2F;etc&#x2F;telegraf&#x2F;telegraf.conf 1234567891011121314[log]logfile = &quot;/var/log/telegraf/telegraf.log&quot;[[outputs.influxdb]]urls = [&quot;http://10.10.1.100:8086&quot;] # required #填写对应的influxdb的地址[[inputs.ceph]]interval = &#x27;1m&#x27;ceph_binary = &quot;/usr/bin/ceph&quot;socket_dir = &quot;/var/run/ceph&quot;mon_prefix = &quot;ceph-mon&quot;osd_prefix = &quot;ceph-osd&quot;ceph_user = &quot;client.admin&quot;ceph_config = &quot;/etc/ceph/ceph.conf&quot;gather_admin_socket_stats = truegather_cluster_stats = true 另外3个控制节点同步配置重启telegraf测试数据是否拿到 1telegraf -test -config /etc/telegraf/telegraf.conf -config-directory /etc/telegraf/telegraf.d -input-filter ceph 在monitor-server的influxdb上可以看见创建好的库 进入influxdbinfluxd列出全部库show databases;进入指定的库use telegraf列出库中全部表show measurements;查询表select * from ceph; 默认influxdb的数据存储时间为168小时也就是7天如果要修改方法为 123456create retention policy &quot;rp_name&quot; on &quot;db_name&quot; duration 3w replication 1 defaultrp_name：策略名db_name：具体的数据库名3w：保存3周，3周之前的数据将被删除，influxdb具有各种事件参数，比如：h（小时），d（天），w（星期）replication 1：副本个数，一般为1就可以了default：设置为默认策略 对已有的策略修改 1alter retention policy &quot;rp_name&quot; on &quot;db_name&quot; duration 30d default 删除已有策略 12drop retention policy &quot;rp_name&quot; 安装grafana配置安装grafana下载软件包 1wget https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana-4.1.2-1486989747.x86_64.rpm 安装软件 1yum localinstall /root/grafana-4.1.2-1486989747.x86_64.rpm 启动grafanasystemctl start grafana-server查看端口lsof -i:3000 安装pie chart插件 ceph这一块用饼图描述比较直观点，需要安装grafana Pie Chart插件 grafana-cli plugins install grafana-piechart-panel 在线安装方式grafana-cli plugins install grafana-piechart-panel 离线安装方式将下载好的plugins解压到&#x2F;var&#x2F;lib&#x2F;grafana&#x2F;pluginsgrafana-cli plugins install grafana-piechart-pane 重启grafanasystemctl restart grafana-server登录grafanahttp://localhost:3000默认帐号密码为admin&#x2F;admin将插件激活 enable zabbix 右侧可以看见已经安装的插件 添加Datasource选zabbix url输入http://192.168.122.100/zabbix/api_jsonrpc.php 测试api连接 然后就可以在dashboard里面看见两个新模板了 将Template Linux Server删除 查看zabbix-server模板 添加influxdb DataSource 创建以下dashboard 创建好后，我们从一个个dashboard里面添加模块 openstack物理节点性能监控配置template因为这是一个集群，有很多台机器，每台机器所对应的角色是不一样的，比如node-1到node-3是控制节点node-4到node6是存储节点 node-7到node-10是计算节点，不同角色的监控项也有略微不同。但我们想在一个dashboard展示多台机器的监控值 所以我们需要定义Templating 定义group的templating 然后在zabbix里面定义的group就出来了 selection options host可以选多个，然后在一个图上绘出，我建议，这里还是一一对应的显示吧定义host的templating 匹配完的显示 创建rowrow是一组panel的集合 这里面的uptime、memeory free、free disk space on &#x2F; 、system load(1min）这些panel就是属于主机情况这个row的。以创建主机情况row为例dashboard下面有一个add row的按钮 编辑row 编辑名称显示名字 按照上述方法创建另外3个row，资源使用情况，网卡信息，然后ctrl+s保存这个dashboard的改变。创建panel将panel创建到对应的row中创建uptime时间panel点击创建panel 类型为Singlestat panelgeneral配置 标题大小 metrics配置数据来源 配置字体颜色 ctrl+s保存，uptime 的panel就完成了。 定义memory free panel在主机情况row创建Singlestat类型的panel options定义背景色，和定义值范围，根据值的不同范围，背景色会自动调整 这里需要注意下Thresholds这个参数，这个参数定义值的范围，然后granafa会根据这个范围自动调整颜色，如7,9如果值小于7则显示红色，在79之间则显示橙色，大于9则显示绿色，但这个值不支持单位换算后的值，所以这也是为什么我这里是10737418240,21474836480，是10,20 G内存空闲小于10G变红色，10G20G之间橙色，大于20G绿色。 ctrl+s保存 定义根分区剩余空间panel在主机情况row创建Singlestat类型的panel 配置metrics options定义背景色，和定义值范围，根据值的不同范围，背景色会自动调整 这里也是定义小于10G时背景色变红，10G~20G范围内为橙色，大于20G为绿色。ctrl+s保存定义cpu负载panel不过这里首先需要修改我们zabbix的 item，zabbix item里面定义的 per cpu而不是all cpu，所以得出来的值会比操作系统里面直接uptime看起来小 ，需要修改zabbix 里面Template OpenStack Compute、和Template OpenStack Controller里面，找到这三个健值修改成all 测试zabbix_agentd -t “system.cpu.load[all,avg1]” 在主机情况row创建Singlestat类型的panel这里我选择15分钟负载情况 ctrl+s保存 定义cpu空闲率panel在资源使用情况row创建Graph类型的panel 单位为percent 保存ctrl+s 定义CPU时间片使用情况在资源使用情况rom创建Table panelGenterl配置panel 名字 配置Metrics 配置Options ctrl+s保存 定义可用内存panel在资源使用情况rom创建Graph panel 定义网桥流量监控panel这里需要注意，我们直接对ovs桥进行监控，因为通过ovs桥我们可以确认这部分流量的用途，直接通过物理网卡的话不直观，所以这里需要在zabbix里面将对应的item加上。 这里将Incoming和Outcoming拆分了开来用两张图显示，并且只监控traffic流量。在网卡信息row创建Graph panel先配置Incoming_Network_traffic 注意item为&quot;/Incoming network traffic on br-.*/&quot; 在配置Outcoming_Network_traffic item写这个 /Outgoing network traffic on br-.*/ ctrl+s 最终效果 添服务状态dashboard，这里我拆分为3个dashboard，分别是OpenStack控制节点服务状态、OpenStack计算节点服务状态、OpenStack储存节点服务状态 每个dashboard设置不同的Templating以OpenStack控制节点服务状态为例，后面都一样创建group，指定为controller这样就不会选别的了，计算节点就指定为computer、储存节点就指定为ceph 创建host ctrl+s保存 OpenStack控制节点服务状态定义nova-api panel 配置 value mapping，因为默认抓取的值为数字我们需要将数字映射为更好看的值，比如大于1映射为running，其他映射为down，这里测试发现down后值经常为0.0x所以这里设置映射为值的范围，比如0~0.9为down，1为up 效果如下我把nova-api关掉 其他控制节点和计算节点监控项目可以直接duplicate了，只需要修改下panel和metrics控制节点需要显示的panel 1234567891011121314151617181920212223nova-apinova-schedulernova-conductorNeutron-serverNeutron-l3-agentNeutron-ovs-agentNeutron-metadataNeutron-DHCPGlance-apiGlance-RegistryKeystoneRabbitmq_clusterRabbitmq_liste_portRabbitmq_BEAM_processRabbitmq_EPMD_listen_portRabbitmq_EPMD_processMysql_clusterCinder_apiCinder_schedulerCinder_volumerCeilometer_processCeilometer_API 计算节点需要显示的panel 1234openstack-computelibvirtdopenvswitch-agentceilometer-agent-compute zabbix-altert dashboardalter-dashboard主要显示整个集群的问题 创件panel，类型为zabbix-Triggers 添加后，它自动会将zabbix的Triggers同步过来 ceph dashboard创建template 这里需要注意把group中固定住控制节点 host配置 添加ceph健康状态监控panel在状态row中创建 Singlestat panel 添加ceph mon总数监控panel在状态row中创建一个singlestat的panel，General修改名字，metrics配置数据源 添加ceph mon up数panel在状态row中创建一个singlestat的panel，General修改名字，metrics配置数据源 thresholds要根据实际集群mon数写，ceph-mon不能挂掉总个数的一半，3个mon只能挂1个，所以这里写2,3小于2时就变红。 添加ceph osd总数监控panel在状态row中创建一个singlestat的panel，General修改名字，metrics配置数据源 添加ceph osd up数监控panel在状态row中创建一个singlestat的panel，General修改名字，metrics配置数据源 添加ceph总容量panel在容量row中创建一个singlestat的panel，General修改名字，metrics配置数据源 注意单位 添加ceph剩余容量panel在容量row中创建一个singlestat的panel，General修改名字，metrics配置数据源 添加ceph已用容量panel在容量row中创建一个singlestat的panel，General修改名字，metrics配置数据源 注意单位 添加ceph容量使用率(饼图)panel在容量row中创建一个 Pie Chart 的panel，General修改名字，metrics配置数据源已经使用和剩余可用百分比利用饼图展示已用空间占总空间的比例新建个panel，类型选择pie chartgeneral设置名字 添加pool容量使用率(已compute pool为例)panel在容量row中创建一个 Pie Chart 的panel，General修改名字，metrics配置数据源 option这里根刚才一样 另外几个pool根这个一样，需要注意的是 where name里面的pool name要更改。 添加pool池的使用趋势图(以compute pool为例)panel在pool池中的使用情况row中创建Graph panel 显示30天的数据剩下pool的使用情况图根上面有一，只需要修改metrics里面的pool名就可以了 添加性能panel在性能row中创将graph panel 注意这里的datasource是 zabbix 单位选择IOPS 添加ceph pg read rate panel在性能row中创将graph panel 单位为bites 显示一天的 添加ceph pg write rate panel在性能row中创将graph panel 最终效果","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"从零开始构建云平台监控(三)配置基础监控平台","slug":"monitor_3","date":"2017-10-11T15:27:44.000Z","updated":"2017-10-11T15:27:43.000Z","comments":true,"path":"2017/10/11/monitor_3/","permalink":"http://yoursite.com/2017/10/11/monitor_3/","excerpt":"","text":"环境：3台控制节点+2台融合节点 2个osd 给zabbix-server导入模板模板为分别对应为control、compute、ceph 导入这3个模板 配置自动注册自动注册就是zabbix-server根据zabbix-agent里面配置的HostMetadata参数的值去进行一系列的操作，和自动发现不一样的是，自动发现只能跟据zabbix-server配置的扫描的网段去添加机器，非常不灵活，并且这个操作是zabbix-server发起的机器规模一大，对zabbix-server有很大的负载压力。自动注册是agent端主动将HostMetadata参数的值给server，server根据管理员的配置在做出相应的操作，在这里比如我openstack的控制节点我就在agent端的HostMetadata配置openstack_controler、然后计算节点配置openstack_computer、存储节点openstack_storager、计算和存储融合节点为 openstack_computer&amp;storager,然后在zabbix-server端创建相应的动作如下图(以添加control为例)其他类似。 操作都填写这些 最后 配置zabbix-agent在agent端安装zabbix-agent，并导入脚本下载zabbix-agent，agent端就不用源码安装了，直接下载rpm包。 1wget http://repo.zabbix.com/zabbix/3.4/rhel/7/x86_64/zabbix-agent-3.4.2-1.el7.x86_64.rpm 安装 1234567891011[root@node-6 ~]# rpm -ivh zabbix-agent-3.4.2-1.el7.x86_64.rpmvim /etc/zabbix/zabbix.confPidFile=/var/run/zabbix/zabbix_agentd.pidLogFile=/var/log/zabbix/zabbix_agentd.logLogFileSize=0Server=10.10.1.100ListenPort=10050ServerActive=10.10.1.100HostMetadata=openstack_controlerTimeout=15Include=/etc/zabbix/zabbix_agentd.d/*.conf 解压 123tar -xvf controller.agent.tar.gz -C /etc/zabbix/[root@node-6 zabbix_agentd.d]# chown zabbix:zabbix *[root@node-6 zabbix_agentd.d]# chmod a+x * cd &#x2F;etc&#x2F;zabbix&#x2F;script[root@node-6 script]# chown zabbix:zabbix *其他几个节点一样 注意两个融合节点metadata配置HostMetadata&#x3D;openstack_computer&amp;storager重启zabbix-agent，等待自动注册。 12systemctl restart zabbix-agentsystemctl enable zabbix-agent 可以看见机器都自动注册进来了。 配置邮件告警脚本配置将发送邮件脚本放置到&#x2F;usr&#x2F;local&#x2F;etc&#x2F;zabbix&#x2F;alertscripts vim sendmail.py chmod a+x sendmail.py zabbix-server web端配置创建报警媒介类型 名字、类型、注意下面三个参数要传递到脚本里面。 配置用户 报警媒介 输入联系人更新 创建动作，zabbix在3.4.2版本中对创建动作这块有较大改动，多了确认操作，以前只有故障和恢复时发邮件，触发动作的条件可以自己配置。 添加新的动作 添加用户、添加组、选择刚刚创建的发送媒介 恢复操作通知。 测试将zabbix-agent关闭等待5分钟。动作日志。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"从零开始构建云平台监控(二)安装zabbix","slug":"monitor_2","date":"2017-10-10T15:27:44.000Z","updated":"2017-10-10T15:27:43.000Z","comments":true,"path":"2017/10/10/monitor_2/","permalink":"http://yoursite.com/2017/10/10/monitor_2/","excerpt":"","text":"安装zabbix-server这里我选择zabbix3.4.2正式版,zabbix3.4.2修复前期非常多bug，release文档如下:https://www.zabbix.com/rn3.2.4 1wget https://ncu.dl.sourceforge.net/project/zabbix/ZABBIX%20Latest%20Stable/3.4.2/zabbix-3.4.2.tar.gz 创建zabbix用户和组groupadd zabbixuseradd -g zabbix zabbix -s &#x2F;sbin&#x2F;nologin 创建库并授权12345678MariaDB [(none)]&gt; create database zabbix character set utf8 collate utf8_bin;Query OK, 1 row affected (0.00 sec)MariaDB [(none)]&gt; grant all privileges on zabbix.* to zabbix@localhost identified by &#x27;passw0rd&#x27;;Query OK, 0 rows affected (0.01 sec)MariaDB [(none)]&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) 解压zabbix tar包tar -xvf zabbix-3.4.2.tar.gz -C &#x2F;lnmp&#x2F; 导入数据库123456[root@localhost zabbix-3.4.2]# cd /lnmp/zabbix-3.4.2/database/mysql/注意顺序[root@localhost mysql]# mysql -uzabbix -ppassw0rd zabbix &lt; schema.sql[root@localhost mysql]# mysql -uzabbix -ppassw0rd zabbix &lt; images.sql[root@localhost mysql]# mysql -uzabbix -ppassw0rd zabbix &lt; data.sql 安装需要依赖1yum install -y net-snmp net-snmp-devel OpenIPMI OpenIPMI-devel libevent libevent-devel unixODBC-devel 编译注意这里不安装zabbix-java-gateway， zabbix-java-gateway是用来监控tomcat的需要本机有java环境，这里先不装，如果需要，安装java环境后重新编译加上–enable-java就可以了 123./configure --enable-server --enable-agent --with-mysql --with-net-snmp --with-libcurl --with-libxml2 --with-openipmi --with-unixodbc --with-opensslmake install 配置monitor-server创建zabbix日志目录 12[root@localhost yum.repos.d]# mkdir /var/log/zabbix[root@localhost yum.repos.d]# chown -R zabbix:zabbix /var/log/zabbix/ 创建自定义脚本目录 12[root@localhost tru64]# mkdir /usr/local/etc/zabbix/alertscripts -pv[root@localhost yum.repos.d]# chown -R zabbix:zabbix /usr/local/etc/zabbix/alertscripts 配置文件目录&#x2F;usr&#x2F;local&#x2F;etc&#x2F;编辑zabbix-server.conf 1234567891011LogFile=/var/log/zabbix/zabbix_server.logDBHost=localhostDBName=zabbixDBUser=zabbixDBPassword=passw0rdListenIP=0.0.0.0FpingLocation=/usr/sbin/fpingTimeout=20CacheSize=1024AlertScriptsPath=/usr/lib/zabbix/alertscriptsExternalScripts=/usr/lib/zabbix/externalscript 编辑zabbix-agent.conf 1234LogFile=/var/log/zabbix/zabbix_agentd.logServer=127.0.0.1ServerActive=127.0.0.1Hostname=Zabbix server 拷贝启动脚本 12[root@localhost tru64]# cp /lnmp/zabbix-3.4.2/misc/init.d/tru64/zabbix_server /etc/init.d/[root@localhost tru64]# cp /lnmp/zabbix-3.4.2/misc/init.d/tru64/zabbix_agentd /etc/init.d/ 添加执行权限 12[root@localhost tru64]# chmod a+x /etc/init.d/zabbix_server[root@localhost tru64]# chmod a+x /etc/init.d/zabbix_agentd 编辑启动脚本vim &#x2F;etc&#x2F;init.d&#x2F;zabbix_server 1234#!/bin/sh#chkconfig: 345 95 95#description: Zabbix_Server vim &#x2F;etc&#x2F;init.d&#x2F;zabbix_agent 123#!/bin/sh#chkconfig: 345 95 95#description: Zabbix_agentd 添加服务 12[root@localhost tru64]# /sbin/chkconfig --add zabbix_agentd[root@localhost tru64]# /sbin/chkconfig --add zabbix_server 开机自启 12[root@localhost tru64]# /sbin/chkconfig zabbix_server on[root@localhost tru64]# /sbin/chkconfig zabbix_agentd on 创建目录 1[root@localhost /]# mkdir /var/www/html/zabbix 拷贝安装页 1cp -rf /lnmp/zabbix-3.4.2/frontends/php/* /var/www/html/zabbix/ 重启nginx &#x2F;etc&#x2F;init.d&#x2F;nginx restart打开浏览器 按提示操作，下载文件放到指定目录设置中文安装完成","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"从零开始构建云平台监控(一)搭建LNMP","slug":"monitor_1","date":"2017-10-04T15:27:44.000Z","updated":"2017-10-04T15:27:43.000Z","comments":true,"path":"2017/10/04/monitor_1/","permalink":"http://yoursite.com/2017/10/04/monitor_1/","excerpt":"","text":"搭建LNMP先安装mysql—-&gt;安装php—–&gt;安装nginx版本:mysql 5.7.19php 7.1.7nginx 1.12.1 本次搭建除mysql外全部采用源码方式安装，因为采用源码的方式安装可以更好的对功能模块是否需要做定制。 安装依赖 1yum -y install gcc gcc-devel gcc-c++ gcc-c++-devel libaio-devel boost boost-devel autoconf* automake* zlib* libxml* ncurses-devel ncurses libgcrypt* libtool* cmake openssl openssl-devel bison bison-devel unzip numactl-devel 创建mysql软件源vim &#x2F;etc&#x2F;yum.repo.d&#x2F;mysql.repo 1234[mysql]name = mysqlbaseurl = https://mirrors.ustc.edu.cn/mysql-repo/yum/mysql-5.7-community/el/7/x86_64/gpgcheck=0 1yum install mysql-community-libs mysql-community-common mysql-community-client mysql-community-devel mysql-community-server 启动mysqlsystemctl start mysqld mysql初始化mysqld –initialize –user&#x3D;mysql 查找默认密码grep “temporary password” &#x2F;var&#x2F;log&#x2F;mysqld.log 默认密码策略设置密码需要大写、小写、特殊符号。关闭默认密码策略 编辑&#x2F;etc&#x2F;my.cnfvalidate_password&#x3D;off 重启mysqlsystemctl restart mysqld 用初始密码登录mysql重启密码 1alter user root@localhost identified by &#x27;passw0rd&#x27;; 修改默认编码由下图可见database和server的字符集使用了latin1编码方式，不支持中文，即存储中文时会出现乱码。以下是命令行修改为utf-8编码的过程，以支持中文 修改为utf-8打开，编辑&#x2F;etc&#x2F;my.cnf.d&#x2F;server.cnf在mysqld里面添加character_set_server &#x3D; utf8 重启mysqlsystemctl restart mysqld 再次查看 安装php7php7在性能上比php5.x有2倍多的提升，同时兼容性也特别好。同时php5.x很多openssl和openssh的漏洞 安装依赖包 1yum -y install libmcrypt-devel mcrypt mhash gd-devel ncurses-devel libxml2-devel bzip2-devel libcurl-devel curl-devel libjpeg-devel libpng-devel freetype-devel net-snmp-devel openssl-devel 安装libconv创建目录mkdir &#x2F;lnmp进入目录cd &#x2F;lnmpwget http://ftp.gnu.org/pub/gnu/libiconv/libiconv-1.14.tar.gz解压包 12345tar xvf libiconv-1.14.tar.gzcd libiconv-1.14./configure --prefix=/usr/local/libiconv1.14 make时包此错 解决方法 1234567vi libiconv-1.14/srclib/stdio.in.h将698行的代码：_GL_WARN_ON_USE (gets, &quot;gets is a security hole - use fgets instead&quot;);替换为：#if defined(__GLIBC__) &amp;&amp; !defined(__UCLIBC__) &amp;&amp; !__GLIBC_PREREQ(2, 16) _GL_WARN_ON_USE (gets, &quot;gets is a security hole - use fgets instead&quot;);#endif 重新make &amp;&amp; make install保存动态链接库 12echo &quot;/usr/local/lib64/&quot; &gt; /etc/ld.so.conf.d/lnmp.confecho &quot;/usr/local/lib/&quot; &gt;&gt; /etc/ld.so.conf.d/lnmp.conf 刷新ldconfig 下载php7.1.7源码包cd &#x2F;lnmpwget http://cn2.php.net/distributions/php-7.1.7.tar.gz 解压tar -xvf php-7.1.7.tar.gz 配置和检查依赖php7.1.7 1./configure --prefix=/usr/local/php7.1.7 --with-config-file-path=/usr/local/php7.1.7/etc --enable-mysqlnd --with-mysqli=mysqlnd --with-pdo-mysql=mysqlnd --with-iconv-dir=/usr/local/libiconv1.14 --with-pcre-regex --with-zlib --with-bz2 --enable-calendar --with-curl --enable-dba --with-libxml-dir --enable-ftp --with-gd --with-jpeg-dir --with-png-dir --with-zlib-dir --with-freetype-dir --enable-gd-native-ttf --with-mhash --enable-mbstring --with-mcrypt --enable-pcntl --enable-xml --disable-rpath --enable-shmop --enable-sockets --enable-zip --enable-bcmath --with-snmp --disable-ipv6 --with-gettext --enable-fpm --with-fpm-user=www --with-fpm-group=www --with-openssl make &amp;&amp; make install复制php配置文件 1cp /lnmp/php-7.1.7/php.ini-production /usr/local/php7.1.7/etc/php.ini 配置php修改时区 1sed -i &#x27;s#;date.timezone =#date.timezone = Asia/Shanghai#g&#x27; /usr/local/php7.1.7/etc/php.ini 隐藏php版本号 1sed -i &#x27;s#expose_php = On#expose_php = Off#g&#x27; /usr/local/php7.1.7/etc/php.ini 配置启动脚本 12[root@localhost php-7.1.7]# cp /lnmp/php-7.1.7/sapi/fpm/init.d.php-fpm /etc/init.d/php-fpm[root@localhost php-7.1.7]# chmod +x /etc/init.d/php-fpm 配置fast-cgi 1cp /usr/local/php7.1.7/etc/php-fpm.d/www.conf /usr/local/php7.1.7/etc/php-fpm.conf 修改fast-cgivim &#x2F;usr&#x2F;local&#x2F;php7.1.7&#x2F;etc&#x2F;php-fpm.conf修改已下参数rlimit_files &#x3D; 65535 创建www用户和组 1useradd www -s /sbin/nologin 启动php-fpm &#x2F;etc&#x2F;init.d&#x2F;php-fpm start设置开机自启&#x2F;sbin&#x2F;chkconfig –add php-fpm修改php.ini以满足zabbix要求vim &#x2F;usr&#x2F;local&#x2F;php7.1.7&#x2F;etc&#x2F;php.ini限制执行目录,nginx网页放在&#x2F;var&#x2F;www&#x2F;html下 12345open_basedir = &quot;/var/www/html/:/tmp&quot; max_execution_time = 300 max_input_time = 300 post_max_size = 24M upload_max_filesize = 4M 修改php连接mysqlpdo_mysql.default_socket&#x3D; &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql.sockmysqli.default_socket &#x3D; &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql.sock 安装nginxwget http://nginx.org/download/nginx-1.12.1.tar.gz解压 [root@localhost lnmp]# tar -xvf nginx-1.12.1.tar.gz [root@localhost lnmp]# cd nginx-1.12.1 1[root@localhost lnmp]#./configure --prefix=/usr/local/nginx1.12.1 --user=www --group=www --with-http_stub_status_module --with-http_gzip_static_module --with-http_ssl_module [root@localhost lnmp]#make &amp;&amp; make install 配置nginx 1cp /usr/local/nginx1.12.1/conf/nginx.conf /usr/local/nginx1.12.1/conf/nginx.conf.bak vim &#x2F;usr&#x2F;local&#x2F;nginx1.12.1&#x2F;conf&#x2F;nginx.conf 123456789101112131415161718192021222324252627282930313233343536373839404142434445user www www;worker_processes 4;error_log logs/error.log info;pid logs/nginx.pid;events &#123; worker_connections 65535; use epoll;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; tcp_nopush on; keepalive_timeout 65; gzip on; log_format main &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27; &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27; &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;; access_log logs/access.log main; server &#123; listen 80; server_name localhost; charset utf8; location / &#123; root /var/www/html; index index.php index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /var/www/html; &#125; location ~ \\.php$ &#123; root /var/www/html; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125; &#125;&#125; 修改ulimitulimit -n 65535编辑&#x2F;etc&#x2F;systemd&#x2F;system.conf设置DefaultLimitNOFILE&#x3D;65535 重启系统 启动nginx&#x2F;usr&#x2F;local&#x2F;nginx1.12.1&#x2F;sbin&#x2F;nginx重载nginx&#x2F;usr&#x2F;local&#x2F;nginx1.12.1&#x2F;sbin&#x2F;nginx -s reload关闭nginx&#x2F;usr&#x2F;local&#x2F;nginx1.12.1&#x2F;sbin&#x2F;nginx -s stop重启nginx&#x2F;usr&#x2F;local&#x2F;nginx1.12.1&#x2F;sbin&#x2F;nginx -s reopen 设置nginx启动脚本编辑&#x2F;etc&#x2F;init.d&#x2F;nginx注意PATH和NAME变量 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#!/bin/bash# chkconfig: - 85 15PATH=/usr/local/nginx1.12.1DESC=&quot;nginx daemon&quot;NAME=nginxDAEMON=$PATH/sbin/$NAMECONFIGFILE=$PATH/conf/$NAME.confPIDFILE=$PATH/logs/$NAME.pidSCRIPTNAME=/etc/init.d/$NAMEset -e[ -x &quot;$DAEMON&quot; ] || exit 0do_start() &#123;$DAEMON -c $CONFIGFILE || echo -n &quot;nginx already running&quot;&#125;do_stop() &#123;$DAEMON -s stop || echo -n &quot;nginx not running&quot;&#125;do_reload() &#123;$DAEMON -s reload || echo -n &quot;nginx can&#x27;t reload&quot;&#125;case &quot;$1&quot; instart)echo -n &quot;Starting $DESC: $NAME&quot;do_startecho &quot;.&quot;;;stop)echo -n &quot;Stopping $DESC: $NAME&quot;do_stopecho &quot;.&quot;;;reload|graceful)echo -n &quot;Reloading $DESC configuration...&quot;do_reloadecho &quot;.&quot;;;restart)echo -n &quot;Restarting $DESC: $NAME&quot;do_stopdo_startecho &quot;.&quot;;;*)echo &quot;Usage: $SCRIPTNAME &#123;start|stop|reload|restart&#125;&quot; &gt;&amp;2exit 3;;esacexit 0 添加执行权限chmod a+x &#x2F;etc&#x2F;init.d&#x2F;nginx注册成服务&#x2F;sbin&#x2F;chkconfig –add nginx 添加开机自启&#x2F;sbin&#x2F;chkconfig nginx on 查看创建目录mkdir -p &#x2F;var&#x2F;www&#x2F;html修改用户和属组chown www:www &#x2F;var&#x2F;www&#x2F;html重启测试启动nginxsystemctl start nginx重启systemctl restart nginx关闭systemctl stop nginx 测试php在&#x2F;var&#x2F;www&#x2F;html下编写index.php文件 123&lt;?php phpinfo();?&gt; 修改权限chown www:www &#x2F;var&#x2F;www&#x2F;html&#x2F;index.php重启nginx和php-fpm 打开浏览器输入地址 至此LNMP搭建完毕。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"从零开始构建云平台监控(写在前面)","slug":"monitor","date":"2017-10-03T15:27:44.000Z","updated":"2017-10-03T15:27:43.000Z","comments":true,"path":"2017/10/03/monitor/","permalink":"http://yoursite.com/2017/10/03/monitor/","excerpt":"","text":"监控的重要性在整个it系统中，监控是最为重要的一个环节，监控是发现集群问题的关键，没有监控的话，只能等到出了问题才能发现集群的问题，那时为时以晚。 监控的范围在一套完整的openstack私有云中，组件非常复杂因为它不仅仅有openstack的东西还有分布式存储ceph的东西。计算组件：nova-api、nova-scheduler、nova-condutor、nova-novncproxy、nova-compute……网络组件：neutron-server、neutron-dhcp-agent、neutron-metadata-agent、neutron-openvswitch-agent、openvswitch……存储组件：cinder-api、cinder-scheduler 、cinder-volume 其他：等还有认证组件keystone、消息队列、镜像组件、还有底层分布式存储ceph、一个这么庞大的系统可想而知如果没有一个比较完善的监控很多并且在不同的角色上所监控的组件也是不一样的，比如控制节点没有nova-compute服务，计算节点没有控制节点服务，所以监控项还得针对不同的角色做区分。 除了组件上的监控外，其实少不了还有硬件上的监控，比如说，在私有云生产环境中，网卡都是会做bond的，做了bond的好处是当其中一个网卡出现故障时并并不是影响到整个集群，但你也得通过监控发现这个挂了的网卡，然后准备更换，ceph的osd down了，有可能是osd进程被kill掉了，也有可能是硬盘坏了导致osd进程挂了，这也都通过监控去发现。 组件构成 监控节点：nginx、php、mysql、influxdb、grafana、zabbix-server、zabbix-agent控制节点：telegraf、zabbix-agent融合节点：zabbix-agent 组件作用nginx+php+mysql提供基础的lnmp平台让zabbix可以运行，mysql存储zabbix监控数据zabbix-server：用于接收各个机器agent端发过来的数据zabbix-agent：采集数据发给zabbix-servertelegraf：用于采集ceph的监控数据influxdb：存储telegraf的数据grafana：用于对采集数据图形化展示 其中需要注意的是因为教程所以监控节点组件都放一起了，并且还是单节点，在生产环境中监控节点要做高可用，根据监控的机器数决定是否将数据库能独立出来。 以下为最终效果。 效果展示ceph监控界面 openstack节点性能状态 控制节点服务状态展示 计算节点服务状态展示 存储节点osd监控 zabbix-server监控","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"Dockerfile","slug":"Dockerfile","date":"2017-10-03T13:45:59.000Z","updated":"2017-10-03T13:45:59.000Z","comments":true,"path":"2017/10/03/Dockerfile/","permalink":"http://yoursite.com/2017/10/03/Dockerfile/","excerpt":"","text":"本文为Dockerfile的语法简单介绍使用Dockerfile构建镜像比docker commit的优势 ：1、首先dockerfile是一个文本文件，可控性好，后面要进行翻看或check时更加简单。2、在使用CI&#x2F;CD等devops工具时，我们一般都是将写好的Dockerfile push到gitlab中去，然后Jenkins监测到对应的project有变动，然后执行自动构建，基于Dockerfile去构建docker image，然后去部署到kubernetes集群中。 Dockerfile常用指令FROM #指定base镜像MAINTAINER #设置镜像作者COPY #将文件从host上的build context拷贝到镜像目录，注意src文件必须是build context内的 支持的格式COPY src dest 和COPY [“src”,”dest”] ADD #与copy类似，从build context复制文件到镜像，不同的是如果src是归档类文件（tar,zip,tgz,xz等),文件会自动解压到dest ENV #设置环境变量，环境变量可被后面的指令使用。 EXPOSE#指定容器中进程会监听的端口，Docker可以将端口暴露出来。 VOLUME #将文件或目录声明为volume WORKDIR #build时会直接切换到此目录，为后面的RUN、CMD、ENTRYPOINT、ADD、COPY等指令设置镜像当前工作目录 RUN #容器中运行指定的命令 CMD #容器启动是运行指定的命令，可以由多个CMD命令，但只有最后一个会生效，CMD命令会被docker run之后根的参数替换掉。 ENTRYPOINT #设置容器启动时命令，Dockerfile中有多个ENTRYPOINT命令但只有最后一个生效，cmd或docker run之后的参数会被当做参数传递给ENTRYPOINT。 指令有两中格式shell和exec格式 shell格式 如RUN apt get install python3CMD echo “Hello world” exec格式 [“executable”,”param1”,”param2”,…]如RUN [“apt-get”, “install”,”python3”]CMD[“&#x2F;bin&#x2F;echo”, “hello world”] exec执行指令时，会直接调用[command]命令，不会被shell解析， 当有环境变量时使用exec格式不会被解析。如ENV name wan ENTRYPOINT [“&#x2F;bin&#x2F;echo”,”hello”,$name”]运行容器将输出hello $name 如果要输出变量ENV name wan ENTRYPOINT [“&#x2F;bin&#x2F;sh”,”-c”,”echo hello,$name”] 运行容器将输出hello ，wan cmd和entrypoint的区别如果docker run指定了其他命令，cmd指定的默认命令将被忽略，当entrypoint一定会执行。并将docker run后面的参数做为entrypoint后面的参数。 cmd可以做entrypoint的参数 ENTRYPOINT [“&#x2F;bin&#x2F;echo”,”hello”]CMD [“world”] 当容器通过docker run -it [image]启动时输出hello world entrypoint使用shell格式是，会忽略cmd或docker run提供的参数。","categories":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"Python socket模块","slug":"python_socket","date":"2017-10-01T03:27:44.000Z","updated":"2017-10-01T03:27:43.000Z","comments":true,"path":"2017/10/01/python_socket/","permalink":"http://yoursite.com/2017/10/01/python_socket/","excerpt":"","text":"socket俗称套接字，应用程序的进程和进程的之间的沟通是通过套接字来进行的。在python中，socket模块来创建套节字。 在同一台机器上，不同进程之间，通过进程号进行区分，但在不同的机器上，会存在相同的进程号，比如cenos7的PID为1的进程为systemd，在另外一个机器的centos7也是同样的，所以在网络环境下PID并不能唯一标识一个进程，比如主机A也有PID为1的进程，主机B也有PID为1的进程，但此问题，tcp&#x2F;ip协议族已经帮我们解决了，网络层的ip地址可以唯一的确定一台主机，传输层的协议和端口可以唯一确定这台主机上的进程。这样利用三元组(ip+协议+端口）可以与其进程进行交互。 python socket编程思路tcp服务端 12345678910111213141516创建socket [socket.socket(socket.AF_INET,socket.SOCK_STREAM)] | |将ip和端口与socket绑定 [socket.bind((host,port))] | |监听端口 [socket.listen()] | |接收连接，建立连接请求[socket.accept()] | |接收数据，并发送数据[socket.recv(),socket.send()] | |连接结束，关闭连接。[socket.close()] tcp客户端 12345678910创建socket | |建立连接[socket.socket(socket.AF_INET,socket.SOCK_STREAM)] | |接收数据，同时也发送数据。[socket.recv(),socket.send()] | |关闭连接[socket.close()] socket功能列表socket(family,type[,protocal]) 使用给定的地址族、套接字类型、协议编号（默认为0）来创建套接字。 socket类型 描述 socket.AF_UNIX 只能够用于单一的Unix系统进程间通信 socket.AF_INET 服务器之间网络通信 socket.AF_INET6 IPv6 socket.SOCK_STREAM 流式socket , for TCP socket.SOCK_DGRAM 数据报式socket , for UDP socket.SOCK_RAW 原始套接字，普通的套接字无法处理ICMP、IGMP等网络报文，而SOCK_RAW可以；其次，SOCK_RAW也可以处理特殊的IPv4报文；此外，利用原始套接字，可以通过IP_HDRINCL套接字选项由用户构造IP头 socket.SOCK_SEQPACKET 可靠的连续数据包服务 创建TCP Socket： s&#x3D;socket.socket(socket.AF_INET,socket.SOCK_STREAM) 创建UDP Socket： s&#x3D;socket.socket(socket.AF_INET,socket.SOCK_DGRAM) socket函数服务端函数 socket函数 描述 s.bind() 将套接字绑定到地址, 在AF_INET下,以元组（host,port）的形式表示地址 s.listen(backlog) 开始监听TCP传入连接。backlog指定在拒绝连接之前，操作系统可以挂起的最大连接数量。该值至少为1，大部分应用程序设为5就可以了。 s.accept() 接受TCP连接并返回（conn,address）,中conn是新的套接字对象，可以用来接收和发送数据。address是连接客户端的地址。 accept默认是阻塞，当有connect过来时才会打开 客户端socket函数 socket函数 描述 s.connect(address) 连接到address处的套接字。一般address的格式为元组（hostname,port），如果连接出错，返回socket.error错误。 s.connect_ex(adddress) 功能与connect(address)相同，但是成功返回0，失败返回errno的值。 公共socket函数 socket函数 描述 s.recv(bufsize[,flag]) 接受TCP套接字的数据。数据以字符串形式返回，bufsize指定要接收的最大数据量。flag提供有关消息的其他信息，通常可以忽略。 s.send(string[,flag]) 发送TCP数据。将string中的数据发送到连接的套接字。返回值是要发送的字节数量，该数量可能小于string的字节大小。 s.sendall(string[,flag]) 完整发送TCP数据。将string中的数据发送到连接的套接字，但在返回之前会尝试发送所有数据。成 功返回None，失败则抛出异常。 s.recvfrom(bufsize[.flag]) 接受UDP套接字的数据。与recv()类似，但返回值是（data,address）。其中data是包含接收数据的字符串，address是发送数据的套接字地址。 s.sendto(string[,flag],address) 发送UDP数据。将数据发送到套接字，address是形式为（ipaddr，port）的元组，指定远程地址。返回值是发送的字节数。 s.close() 关闭套接字。 s.getpeername() 返回连接套接字的远程地址。返回值通常是元组（ipaddr,port）。 s.getsockname() 返回套接字自己的地址。通常是一个元组(ipaddr,port) s.setsockopt(level,optname,value) 设置给定套接字选项的值。 s.getsockopt(level,optname[.buflen]) 返回套接字选项的值。 s.settimeout(timeout) 设置套接字操作的超时期，timeout是一个浮点数，单位是秒值为None表示没有超时期。一般，超时期应该在刚创建套接字时设置，因为它们可能用于连接的操作（如connect()） s.fileno() 返回套接字的文件描述符。 s.setblocking(flag) 如果flag为0，则将套接字设为非阻塞模式，否则将套接字设为阻塞模式（默认值）。非阻塞模式下，如果调用recv()没有发现任何数据，或send()调用无法立即发送数据，那么将引起socket.error异常。 s.makefile() 创建一个与该套接字相关连的文件 简单例子server端 1、使用一个死循环while True：将会使server端一直处于监听状态。2、children,addr&#x3D;s.accept 将创建一个新的socket，这样原先的socket继续侦听，而新的socket将接收client端的数据,addr返回的是客户端的ip。那么问题来了，当客户端发送数据关联时是与哪个socket进行连接呢，首先我们需要知道的是客户端发送的数据有两种，一种是请求建立连接的，一种是已经建立好连接后的数据传输，就如上所说tcp&#x2F;ip有接收缓存和发送缓存，当收到建立连接的请求时，则传给正在监听端口的socket调用accept，当收到连接好连接后的数据传输时，将输据放入接收缓冲区，这样当服务器需要读取数据时调用accept建立的新socket，的recv函数从接收缓冲区读取。3、将socket.accept写在循环里面client新的连接一次将重新创建一个新的socket。 client端 1，s.recv(1024)一次最大接收1024字节2, 当收到连接好连接后的数据传输时，将输据放入接收缓冲区，这样当服务器需要读取数据时调用accept建立的新socket，的recv函数从接收缓冲区读取。输出server端 client端socket.recv()tcp&#x2F;ip socket在内核中都有一个接收缓冲区和发送缓冲区，当socket接收到数据时，并不是马上调用socket.recv(),而是将数据拷贝到socket中的接收缓冲区中，调用socket.recv()后就是将接收缓冲区的数据，移动到应用层的buff中，并返回。当接收窗口满了后发生的操作是，收端通知发端，停止发送。socket.send()socket.send()是将应用层的buff拷贝到tcp-socket的发送缓冲区中 UDPserver端 client 输出server端 client端 socket实现tcp简单聊天server端 需要注意的是这里的socket.accept()是写在while循怀外的，因为写在死循怀里，脚本一直在执行，client调用一次accept后就进入阻塞状态了，而client端用的还是旧的connect所以，如果写在循怀里，就是执行client后建立一个连接后只能进行一次对话，因为server端的accept又重新回到阻塞状态了,重新执行client生成一个新的connec又可以一次对话。 client端 结果server端 client端 优化点：1，目前程序没有多线程，IO复用，还是半双工状态，一次只能一个用户说话，一个发时，另一个只能收，server在说话，client就不能说。 参考链接 123456http://www.oschina.net/question/12_76126?sort=default&amp;p=1https://segmentfault.com/q/1010000000591930http://www.cnblogs.com/aylin/p/5572104.htmlhttp://blog.csdn.net/rebelqsp/article/details/22109925http://blog.csdn.net/hguisu/article/details/7445768/http://blog.csdn.net/rebelqsp/article/details/22191409","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"Python logging模块","slug":"python_logging","date":"2017-09-27T15:27:44.000Z","updated":"2017-09-27T15:27:43.000Z","comments":true,"path":"2017/09/27/python_logging/","permalink":"http://yoursite.com/2017/09/27/python_logging/","excerpt":"","text":"默认logging函数输出的warning级别的日志。 日志级别日志级别大小关系为：CRITICAL &gt; ERROR &gt; WARNING &gt; INFO &gt; DEBUG &gt; NOTSET logging库提供了多个组件：Logger、Handler、Filter、Formatter： 1234Logger 对象提供应用程序可直接使用的接口，供应用代码使用；Handler 发送日志到适当的目的地，比如socket和文件等Filter 提供了过滤日志信息的方法，控制输出；Formatter 指定日志输出和显示的具体格式。 通过logging.basicConfig对日志的输出格式配置 cat test.log 需要注意的是只有等级大于等于basicConfig定义的level的log才会被输出，比如这里定义的等级为DEBUG、debug、info、warning、error日志等级都大于等于debug logging.basicConfig各参数level：日志等级format格式 123456789101112131415161718192021222324252627282930313233343536373839format: 指定输出的格式和内容，format可以输出很多有用信息，如上例所示: %(levelno)s: 打印日志级别的数值 %(levelname)s: 打印日志级别名称 %(pathname)s: 打印当前执行程序的路径，其实就是sys.argv[0] %(filename)s: 打印当前执行程序名 %(funcName)s: 打印日志的当前函数 %(lineno)d: 打印日志的当前行号 %(asctime)s: 打印日志的时间 %(thread)d: 打印线程ID %(threadName)s: 打印线程名称 %(process)d: 打印进程ID %(message)s: 打印日志信息 filename：输出文件名filemode：写入模式w为直接写入，a为追加datafmt：输出的时间格式 这里用%Y-python中时间日期格式化符号：%y 两位数的年份表示（00-99）%Y 四位数的年份表示（000-9999）%m 月份（01-12）%d 月内中的一天（0-31）%H 24小时制小时数（0-23）%I 12小时制小时数（01-12）%M 分钟数（00=59）%S 秒（00-59）%a 本地简化星期名称%A 本地完整星期名称%b 本地简化的月份名称%B 本地完整的月份名称%c 本地相应的日期表示和时间表示%j 年内的一天（001-366）%p 本地A.M.或P.M.的等价符%U 一年中的星期数（00-53）星期天为星期的开始%w 星期（0-6），星期天为星期的开始%W 一年中的星期数（00-53）星期一为星期的开始%x 本地相应的日期表示%X 本地相应的时间表示%Z 当前时区的名称 将日志同时输出到屏幕和文件 输出 logging 日志轮询使用TimedRotatingFileHandler设置日志轮转，轮转的方式有两种一种是基于时间的轮转，一种是基于日志文件大小的轮转TimedRotatingFileHandler函数参数说明 1logging.handlers.TimedRotatingFileHandler(file_name,when=时间单位, interval=&#x27;时间间隔&#x27;,backupCount=&quot;保留的文件个数&quot;) interval:表示等待多长时间文件重建，重建的文件名等于file_name+suffix 以下面例子说明 myadd.addHandler(filehandler)的意思是给myapp这个logger添加filehandler这个handler。 执行脚本可以看见每隔一秒会自动生成一个新的日志文件，到满3个时会自动进行一次新的轮转 TimedRotatingFileHandler设置的重建时间间隔后，suffix就需要按装下面的进行配置不然删除不了，比如设置的为S则suffix为%Y-%m-%d_%H-%M-%S RotatingFileHandler安文件大小切分 logger实例的父子关系通过前面几个例子对比你应该发现了前面我用logging.basicConfig()去设置format，后面我是通过getlogger创建一个logger后，通过setformat方法去给他对应的handler设置format。 root logger是处于最顶层的logger，同时也是默认的logger，如果不创建logger实例默认调用logger.info(),logger.debug(),logger.error()使用 如何得到root logger通过logging.getLogger()和logging.getLogger(“”)得到root logger实例。logger的父子关系logger以name命名方式来表达父子关系比如父logging.getLogger(foo)子logging.getLogger(foo.tar) effective level一个looger如果没有指定level，就继承父level，如果父logger也没有就直接继承root的level。handler同样，子没有就继承父的，父也没有的话就继承root的例 root logger这里没设置logger的setLevel默认是warning，但父logger设置了，所以父logger会将自己的logger setlevel传递给root logger 调用配置好的logging正常写程序中只要配置好一个logging，其他程序只要调用他就可以了一种是通过logging.config，一种是通过模块导入 介绍方法二：比如我将配置好的logging写在test.py里面在另外一个程序中调用它 12import testtest.myapp.info(&quot;test&quot;) 这样输出的就是按test.py里面myapp这个logger配置好的log了 12345http://blog.csdn.net/lizhe_dashuju/article/details/72579705http://blog.csdn.net/z_johnny/article/details/50812878http://kenby.iteye.com/blog/1162698http://blog.csdn.net/chosen0ne/article/details/7319306","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"kdump","slug":"kdump","date":"2017-09-21T03:50:44.000Z","updated":"2017-09-21T03:50:43.000Z","comments":true,"path":"2017/09/21/kdump/","permalink":"http://yoursite.com/2017/09/21/kdump/","excerpt":"","text":"什么是kdump?kdump 是一种的基于 kexec 的内核崩溃转储机制，类似飞机的黑匣子，系统一但崩溃，内核无法正常记录信息了，这时kdump将转入带第二个捕获内核，将第二个内核加载的内存中，对第一个内核的信息进行捕获。由于 kdump 利用 kexec 启动捕获内核，绕过了 BIOS，所以第一个内核的内存得以保留。这是内核崩溃转储的本质。kexec是一个快速启动机制，可以通过已经运行的内核，启动另外一个内核不需要经过bios kdump原理？kdump 需要两个不同目的的内核，生产内核和捕获内核。生产内核是捕获内核服务的对像。捕获内核会在生产内核崩溃时启动起来，与相应的 ramdisk 一起组建一个微环境，用以对生产内核下的内存进行收集和转存。 kdump配置和使用操作系统:centos7.2 安装配置分析工具crashyum install crash 安装kernel-debuginfo(需要根内核版本一一对应) 1wget http://debuginfo.centos.org/7/x86_64/kernel-debuginfo-common-x86_64-3.10.0-327.el7.x86_64.rpm 1234wget http://debuginfo.centos.org/7/x86_64/kernel-debuginfo-3.10.0-327.el7.x86_64.rpm````下载完成后安装 rpm -ivh kernel-debuginfo-common-x86_64-3.10.0-327.el7.x86_64.rpm kernel-debuginfo-3.10.0-327.el7.x86_64.rpm 1安装完后确认是否有 &#x2F;usr&#x2F;lib&#x2F;debug&#x2F;lib&#x2F;modules&#x2F;3.10.0-327.el7.x86_64&#x2F;vmlinux此文件 12345678910111213### 配置kdump可以配置内核崩溃后崩溃日志存到何地![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kdmup_1.png)默认是放在本地/var/crash 下图为配置scp，表示将log文件放到192.168.2.100/log 目录，同时key文件目录需要指定![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kdmup_2.png)### 测试kdump配置捕获占用的内存使用grubby修改grub2.cfg文件 grubby –update-kernel&#x3D;DEFAULT –args&#x3D;crashkernel&#x3D;128M 1234重启服务器 reboot 启动kdump systemctl start kdump &amp;&amp; systemctl enable kdump 1234ls /boot 会发现生成了一个kdump结尾的文件![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kdmup_3.png)执行以下命令让内核crash echo 1 &gt; &#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;sysrqecho c &gt; &#x2F;proc&#x2F;sysrq-trigger 1234567891011121314151617181920212223242526272829303132此时系统会立刻失去连接进入捕获内核开机后![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kdmup_4.png)![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kdmup_5.png)以下参考IBM文档链接见文尾 ### crash解析崩溃日志vmcore-dmesg.txt可以查看错误信息 (1) 错误类型 首先可以在vmcore-dmesg.txt中先查看错误类型，如： divide error: 0000 [#1] SMP，除数为0造成内核崩溃，由1号CPU触发。 BUG: unable to handle kernel NULL pointer dereference at 000000000000012c，引用空指针。这样一来就能知道引发内核崩溃的错误类型。 (2) 错误地点 RIP为造成内核崩溃的指令，Call Trace为函数调用栈，通过RIP和Call Trace可以确定函数的调用路径，以及在哪个函数中的哪条指令引发了错误。例如RIP为：[``&lt;ffffffff812cdb54&gt;``] ? tcp_enter_loss+0x1d3/0x23b[``&lt;ffffffff812cdb54&gt;``]是指令在内存中的虚拟地址。tcp_enter_loss是函数名(symbol)。0x1d3是这条指令相对于tcp_enter_loss入口的偏移，0x23b是函数编译成机器码后的长度。这样一来就能确定在哪个函数中引发了错误，以及错误的大概位置。Call Trace为函数的调用栈，是从下往上看的。可以用来分析函数的调用关系。### vmcore信息 crash &#x2F;usr&#x2F;lib&#x2F;debug&#x2F;lib&#x2F;modules&#x2F;3.10.0-327.el7.x86_64&#x2F;vmlinux &#x2F;var&#x2F;crash&#x2F;127.0.0.1-2017-09-20-13:09:30&#x2F;vmcore 12![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kdmup_6.png) KERNEL: 系统崩溃时运行的 kernel 文件DUMPFILE: 内核转储文件CPUS: 所在机器的 CPU 数量DATE: 系统崩溃的时间TASKS: 系统崩溃时内存中的任务数NODENAME: 崩溃的系统主机名RELEASE: 和 VERSION: 内核版本号MACHINE: CPU 架构MEMORY: 崩溃主机的物理内存PID:3507表示当崩溃时3507这个bash的进程在操作PANIC: 崩溃类型，常见的崩溃类型包括：SysRq (System Request)：通常是测试使用。通过 echo c &gt; &#x2F;proc&#x2F;sysrq-trigger，就可以触发系统崩溃。oops：可以看成是内核级的 Segmentation Fault。应用程序如果进行了非法内存访问或执行了非法指令，会得到 Segfault 信号，一般行为是 coredump，应用程序也可以自己截获 Segfault 信号，自行处理。如果内核自己犯了这样的错误，则会弹出 oops 信息。 1234567891011121314151617181920bt(backtrace) 显示内核堆栈信息![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kdmup_7.png)如上输出中，以“# 数字”开头的行为调用堆栈，即系统崩溃前内核依次调用的一系列函数，通过这个可以迅速推断内核在何处崩溃。log - dump system message buffer log 命令可以打印系统消息缓冲区，从而可能找到系统崩溃的线索。log 命令的截图如下（为节省篇幅，已将部分行省略）：![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kdmup_8.png)ps 命令用于显示进程的状态，（如图）带 &gt; 标识代表是活跃的进程。ps 命令的截图如下（省略部分行）：![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kdmup_9.png)dis - disassembling instructiondis 命令用于对给定地址的内容进行反汇编。dis 命令的截图如下：![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/kdmup_10.png) 参考链接http://www.361way.com/centos-kdump/3751.htmlhttp://blog.csdn.net/zhangskd/article/details/38084337https://www.ibm.com/developerworks/cn/linux/l-cn-kdump4/index.html","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"OpenStack vlan模式下网络流量走向","slug":"neutron_vlan","date":"2017-09-17T03:50:44.000Z","updated":"2017-09-17T03:50:43.000Z","comments":true,"path":"2017/09/17/neutron_vlan/","permalink":"http://yoursite.com/2017/09/17/neutron_vlan/","excerpt":"","text":"使用openvswitch+vlan的组网以生产网vlan为101和102为例在openstack上创建个私有网络，创建了 OVS Integration bridge br-int。它的四个 Access 端口中，两个打上了内部 Tag 1，连接接入 network 1 的两个网卡；另两个端口的 VLAN Tag 为 2。创建了一对 patch port，连接 br-int 和 br-eth0。设置 br-int 中的 flow rules。对从 access ports 进入的数据帧，加上相应的 VLAN Tag，转发到 patch port；从 patch port 进入的数据帧，将 VLAN ID 101 修改为 1, 102 修改为 2，再转发到相应的 Access ports。设置 br-eth0 中的 flow rules。从 patch port 进入的数据帧，将内部 VLAN ID 1 修改为 101，内部 VLAN ID 2 修改为 102，再从 eth1 端口发出。对从 eth1 进入的数据帧做相反的处理。 br-int上的local_id和vlan_id的转换是实现多租户的主要技术。 以下介绍在云平台三中情况数据流量的走向 相同物理机上的虚拟机vm1和vm2在同一个宿主机上同一个项目下同一个子网时，两个虚机之间的流量是不需要经过交换机的，直接通过ovs的br-int桥就可以做转发了,不同项目是不通的因为br-int上有local_id用于多租户隔离。 不同物理机上的虚拟机vm1和vm2在同一个宿主机上同一个项目下同一个子网时，两个虚拟机之间的流量是需要通过物理交换机进行转发的 vm的流量首先经过tap网卡到qbr桥上，qbr桥是linux-bridge桥，曾经的ovs不支持安全组的实现所以openstack只能加一个qbr桥在上面通过iptables来实现安全组，(mitaka版本中已经没有了因为ovs已经可以通过openflow来实现了),通过veth口将br-int和qbr桥连接起来，流量到br-int上，br-int上有对应的ovs规则转发到br-eth0上，，r-eth0 中的 flow rules。从 patch port 进入的数据帧，将内部 ocal_id 修改为vlan_id，再从 eth0 端口发出。通过交换机到达另外一个计算机节点。 不同租户的不同子网的通信或虚机根外部通信需要经过网络节点，由网络节点的qroute做三层转发，也可以直接使用物理交换机。不同子网的通信，虚机会将流量丢给默认网关然后到达网络节点的qrouter通过qrouter的三层转发通讯。 使用内部vlan号是为了实现多租户网络隔离，也就是说计算节点上将数据包隔离通过br-int的loca_id，假设没有这个local_id的话，租户a的192.168.1.0&#x2F;24和租户b的192.168.1.0&#x2F;24在同一个计算节点的同一个br-int的网桥上，如果没有local_id隔离，网络就串了。这种是openvswitch的实现方式， linuxbridge的实现方式比如租户a的私有网络是vlan100，租户b的私户网络是vlan200，然后linuxbridge会将在这个计算节点上创建两个网桥。然后用vconfig将对应的网卡创建出子接口，挂到刚刚创的桥上，在将对应的虚机tap网卡挂载到对应的桥上。","categories":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/categories/openstack/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/tags/openstack/"}]},{"title":"openstack添加多个不同物理出口的private网络","slug":"openstack_multi_private","date":"2017-09-13T03:50:44.000Z","updated":"2017-09-13T03:50:43.000Z","comments":true,"path":"2017/09/13/openstack_multi_private/","permalink":"http://yoursite.com/2017/09/13/openstack_multi_private/","excerpt":"","text":"在生产环境中我们有些虚拟机需要同时需要接入两个二层网络，一个生产网络，一个DMZ网络，通常情况下这两个网络也是对应两个不同的物理交换机的，所以对应的计算节点也应该是两个不同的物理出口。 以上图的环境为例compute-2上的虚拟机需要同时接入两个网络，生产网和DMZ网络，所以在对compute-2接完线后，还需要配置neutron，让br-int根对应的br-dmz桥通过ovs的patch口连接起来，同时生成对应的流表控制，修改openvswitch配置文件使得physnetX根br-dmz mapping。需要注意的是这里control-1接dmz的网络主要是为了dhcp 地址、metadata注入、和根三层网络的NAT。你可以通过neutron availability zone的方式把把dhcp和metadata和l3部署在网络能通的计算节点，这样就不需要控制节点接线了。 配置方法（组件为openvswitch)这里假设compute-2的br-dmz物理出口为br-ovs-bond2控制节点和计算节点都执行创建ovs桥 1ovs-vsctl add-br br-dmz 添加端口 1ovs-ovsctl add-port br-dmz br-dmz--br-ovs-bond2 设置端口类型为patch 1ovs-vsctl set interface br-dmz--br-ovs-bond2 type=patch 设置patch口的peer端 1ovs-vsctl set interface br-dmz--br-ovs-bond2 options:peer=br-ovs-bond2--br-dmz 在br-ovs-bond2上配置另外一个path口添加端口 1ovs-vsctl add-port br-ovs-bond2 br-ovs-bond2--br-dmz 设置端口类型为patch 1ovs-vsctl set interface br-ovs-bond2--br-dmz type=patch 设置patch口的peer端 1ovs-vsctl set interface br-ovs-bond2--br-dmz options:peer=br-dmz--br-ovs-bond2 控制节点配置vim &#x2F;etc&#x2F;neutron&#x2F;plugins&#x2F;ml2&#x2F;ml2_conf.ini 1network_vlan_ranges =physnet2:1000:2000,physnet3 #在原来基础上新增个physnet3为我新增的，没有明确vlan-id范围可以不写 vim &#x2F;etc&#x2F;neutron&#x2F;plugins&#x2F;ml2&#x2F;openvswitch_agent.ini 1bridge_mappings=physnet2:br-prv,physnet3:br-dmz #在原来基础上新增个physnet3:br-dmz 重启neutron-server和neutron-openvswitch-agent 1systemctl restart neutron-server 1systemctl restart neutron-openvswitch-agent 计算节点配置vim &#x2F;etc&#x2F;neutron&#x2F;plugins&#x2F;ml2&#x2F;openvswitch_agent.ini 1bridge_mappings=physnet2:br-prv,physnet3:br-dmz #在原来基础上新增个physnet3:br-dmz 重启neutron-openvswitch-agent 1systemctl restart neutron-openvswitch-agent 创建网络，输入对应的physnet 1neutron net-create dmz_net --router:external=true --provider:network_type=vlan --provider:physical_network=physnet3 --provider:segmentation_id=108 1neutron subnet-create --gateway 20.52.15.254 --allocation-pool start=20.52.15.30,end=20.52.15.40 --disable-dhcp dmz_net 20.52.15.0/24","categories":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/categories/openstack/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/tags/openstack/"}]},{"title":"OpenStack对接vmware","slug":"vmware&openstack","date":"2017-09-08T14:55:00.000Z","updated":"2017-09-08T14:55:00.000Z","comments":true,"path":"2017/09/08/vmware&openstack/","permalink":"http://yoursite.com/2017/09/08/vmware&openstack/","excerpt":"","text":"openstack版本：libertyvsphere版本：6.0 拓扑图 架构图 vmware对接openstack有两种驱动1，VCDriver，以nova driver的方式通过vcenter来控制计算和网络2，VMDKDriver，以cinder driver的方式通过vcenter来控制datastore。 需要注意的是本文档的对接都是对接vmware的标准交换机进行对接的，如果需要对接vmware的dvs的话需要安装另外的包并配置neutron。 这里采用vcdriver的方式这种方式目前有几个问题1，就是已经存在vmware集群的vm是不能被openstack管理了，只有新创建的机器可以被openstack管理。 2，vlan模式下，VCDriver要求vCenter里被使用的VLAN port group的名字必须和OpenStack里integration bridge（默认是br-int）的名字一样，而且vlan 号也得一样，这里我配置port group为0，表示不做限制。 3，首次创建虚机会花费很长时间，因为需要将镜像从glance通过vcenter 拷贝到cluster后端的datasotre，后面再创建速度就会很快。 要点：一台计算节点对接vsphere，需要这台计算节点能根vcenter通信。 Vsphere上的配置配置好对应的openstack计算节点对应的cluster如这个vsphere的cluster_name为UAT_Lenovo_6.0 cluster对应的storage名字 创建对应的port_group nova配置需要注意对应的物理适配器，待会需要写到配置项内。 12345678910openstack平台对接vcenter配置参考compute_driver #对应的驱动host_ip #连接vsphere的ip地址host_username #管理员帐户host_password #管理员密码datastore_rege #cluster对应的后端存储cluster_name #对应的cluster 名字vlan_interface #port group对应的物理网卡vmware.integration_bridge #对应的port_groupinsecure=True #关闭ssl连接不然报下面错 修改&#x2F;etc&#x2F;nova&#x2F;nova.conf配置文件： 123456789101112[default]compute_driver = vmwareapi.VMwareVCDriver[vmware]host_ip=10.3.1.11host_username=administrator@UAT.comhost_password=1qaz@WSXdatastore_regex=Huawei_S5500_01cluster_name=UAT_Lenovo_6.0api_retry_count=10vlan_interface=vmnic0vmware.integration_bridge=br-intinsecure=True 需要注意这些参数仔细核对。 启动openstack-nova-compute服务：systemctl restart openstack-nova-compute 此时应该可以看见vmware 对应的hypervisor 转换镜像将一个qcow2的镜像转换成vmware用的vmdk格式 glance配置转换镜像格式为vmdk：qemu-img convert -f raw -O vmdk centos_6.5.raw centos_6.5.vmdk 上传镜像： 1glance image-create --name=centos-6.5.vmdk --disk-format=vmdk --property hypervisor_type=&quot;vmware&quot; --property vmware_adaptertype=&quot;ide&quot; --property vmware_disktype=&quot;sparse&quot; --is-public=True --property vmware_ostype=&quot;otherLinuxGuest&quot; --container-format=bare &lt; centos_6.5.vmdk 创建虚拟机进行测试 第一次创建时间比较久因为要将镜像从glance 拷贝到vmware 的storage内 可以看见vsphere内已经有以uuid命名的vm了，表示创建成功 后面为可选配置，配置cinder对接vmware存储 cinder配置编辑&#x2F;etc&#x2F;cinder&#x2F;cinder.conf配置文件，修改如下参数： 12345volume_driver=cinder.volume.drivers.vmware.vmdk.VMwareVcVmdkDrivervolume_backend_name=vmwarevmware_host_ip=10.68.35.24vmware_host_username=administratorvmware_host_password=admin123 重启cinder-volume服务：/etc/init.d/openstack-cinder-volume restart创建多类型存储：cinder type-create vmwarecinder type-key vmware set volume_backend_name=vmwarecinder type-create cephcinder type-key ceph set volume_backend_name=DEFAULT 创建云硬盘，挂在云主机进行测试 mitaka版本对接vsphere6.5mitaka对接vsphere有个bug就是在https的连接情况下会连接失败https://review.openstack.org/#/c/341816/替换/usr/lib/python2.7/site-packages/nova/virt/vmwareapi/vmops.py 重启nova-computesystemctl restart openstack-nova-compute编辑nova.conf 1234567891011[vmware]host_port=443host_ip=vcenter.xjrccu.comhost_username=administrator@xjrccu.comhost_password=Rayking,135datastore_regex=Datastore2cluster_name=LenovoServer-Clusterapi_retry_count=10vlan_interface=vmnic4vmware.integration_bridge=br-intinsecure=True 重启nova-computesystemctl restart openstack-nova输入nova hypervisor-list可以看见vmware这个hypervisor cinder对接vmware datastore1cp /usr/lib/python2.7/site-packages/oslo_vmware/wsdl/6.0 /usr/lib/python2.7/site-packages/oslo_vmware/wsdl/6.5 2、修改 enabled_backends &#x3D; ceph为 enabled_backends &#x3D; ceph,vmware 3、创建type-key，其中ceph的也需要创建（web界面也可以做，在全局管理里——云硬盘类型） 1234567891011121314151617cinder type-create vmwarecinder type-key vmware set volume_backend_name=vmwarecinder type-create cephcinder type-key ceph set volume_backend_name=ceph````4、给vmware type-key添加扩展属性，vmware:storage_profile（可以在web页面里添加），但前提条件需要在vmware那边先创建好“虚机存储规则策略” 5、登录vsphere web client，先创建一个标签，在主页里，名字和类别随便起。![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/openstack&amp;vmware_10)6、给要使用datastore分配新建标签 ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/openstack&amp;vmware_11)7、回到主页，点击策略和配置文件，点击虚拟机存储策略，点击创建一个，按照向导做。名字很重要！！![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/openstack&amp;vmware_12)8、对到云平台 cinder type-key vmware set vmware:storage_profile&#x3D;cinder-storage-policy 1234即可。=号后是之前创建的策略名字！！另外，M版对接6.5，创建云硬盘，不会出现volume开头的虚机，存储里也看不到volume-id文件夹，只有挂载后才会出现，注意！！参考链接 https://docs.openstack.org/mitaka/config-reference/compute/hypervisor-vmware.htmlhttp://blog.csdn.net/jmilk/article/details/52102020http://blog.csdn.net/halcyonbaby/article/details/37818789http://www.cnblogs.com/zhoumingang/p/5514556.htmlhttp://www.iyunv.com/forum.php?mod=viewthread&amp;tid=86702","categories":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/categories/openstack/"}],"tags":[]},{"title":"openstack nova server_group","slug":"openstack_nova_server_group","date":"2017-08-12T05:15:26.000Z","updated":"2018-11-07T13:02:40.281Z","comments":true,"path":"2017/08/12/openstack_nova_server_group/","permalink":"http://yoursite.com/2017/08/12/openstack_nova_server_group/","excerpt":"","text":"前言：openstack在I版中增加了server group的概念，设置server group策略可以做为虚机调度的策略，目前支持的策略有”affinity”和”Anti-Affinity” 意思就是亲和性和非亲和性。 使用场景：我们知道nova创建虚机调度是通过nova-schedule的算法去筛选出对应的计算节点，然后调度到上面。在openstack上创建个app高可用集群，需要虚机分布在不同的本nova availability zone的不同的计算节点内，那么我们只需要创建个server group然后配置策略为anti-affinity，创建虚机时选择这个策略，这样创建出来的虚机就会落到不同的计算节点上。 虽然默认的nova-schedule也支持affinity和anti-affinity但nova-schedule并不能支持持久化，只在虚机创建时生效，如果后面执行migration、evacute、resize那高可用就被破坏了。 server group会将group的信息持久化，每次重新调度都能获得正确的虚机。 实现原理在nova库中增加了以下三张表instance_group_member、instance_group_policy instance_groups。 1，在创建云主机时如果加入了server-group这将instances加入到server_group_member中；2，conductor 从数据库中取得group信息，解析 group 的 policy 并设置调度参数，通过RPC让 scheduler 选择合适的宿主机；3，nova-scheduler将创建请求通过rpc传递给对应的nova-compute；4，nova-comput去访问nova-conductor去获取创建虚机的一些信息； 启用server_group修改控制节点nova.conf将scheduler_default_filters改成如下。 1scheduler_default_filters=RetryFilter,AvailabilityZoneFilter,RamFilter,CoreFilter,DiskFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter 目前dashboard没有Server group接口，只能通过cli的方式。CLI命令创建server_group 创建server-group 其中test-2为group的名字 affinity为policy 查看哪些云主机使用了此server_group 创建云主机时要指定server-group的方法nova boot时加上–hint group&#x3D;server_group_id如 1nova boot --image IMAGE_ID --flavor flavor_id --hint group=group-1 test3","categories":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/categories/openstack/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/tags/openstack/"}]},{"title":"openstack秒级创建云主级秒级创建快照原理","slug":"ceph_clone","date":"2017-08-05T07:19:59.000Z","updated":"2017-08-05T07:19:59.000Z","comments":true,"path":"2017/08/05/ceph_clone/","permalink":"http://yoursite.com/2017/08/05/ceph_clone/","excerpt":"","text":"概念thin-provisioned当分配一个100G的空间时，并不会立刻占满这100，只是占用了一些文件的元数据，当写入数据时会根据实际的大小动态的分配,类似linux中的稀疏文件。 cow （copy on write)写时复制，也就是做快照时，先圈好个位置但这里面是空的，只有当父镜像有数据有变化时，这时会先将变化前的数据cp到快照的空间，然后继续修改父镜象，这样做的好处是可以节省大量做快照的时间和减少存储空间，因为snapshot存储的都是发生改变前的区域，其它区域都是与父镜像共享的。 优点：cow快照，拷贝只是拷贝一些元数据，所以拷贝速度特别快，同时相比全量快照，占用的空间也要少很多 缺点：cow快照后的第一次数据更新时父镜像每次要写数据，要先将原始数据读出来然后在拷贝到快照卷中，然后在写父镜像，这样进行一次更新操作就需要一次读+两次写，会降低父镜像的写性能，如果父镜像链接更多的快照，那性能会更低。 ceph快照是基于cow（copy on write）ceph 使用 COW （copy on write）方式实现 snapshot：在写入object 之前，将其拷贝出来，作为 snapshot 的 data object，然后继续修改原始数据。rbd在创建快照时，并不会向pool中创建对象，也就是说并不会占用实际的存储空间,只是增加了一些对象信息 ceph cloneclone：clone是将一个snapshot变成一个image，它是基于snapshot 创建 Clone 是将 image 的某一个 Snapshot 的状态复制变成一个 image。如 imageA 有一个 Snapshot-1，clone 是根据 ImageA 的 Snapshot-1 克隆得到 imageB。imageB 此时的状态与Snapshot-1完全一致，并且拥有 image 的相应能力，其区别在于 ImageB 此时可写。 从用户角度来看，一个 clone 和别的 RBD image 完全一样。你可以对它做 snapshot、读&#x2F;写、改变大小 等等，总之从用户角度来说没什么限制。同时，创建速度很快，这是因为 Ceph 只允许从 snapshot 创建 clone，而 snapshot 需要是只读（protect）的。 向 clone 的 instance 的object 写数据ceph的克隆也是采用cow技术， 从本质上是 clone 的 RBD image 中读数据，对于不是它自己的 data objects，ceph 会从它的 parent snapshot 上读，如果它也没有，继续找它的parent image，直到一个 data object 存在。从这个过程也看得出来，该过程是缺乏效率的。 向 clone 的 instance object 写数据 Ceph 会首先检查该 clone image 上的 data object 是否存在。如果不存在，则从 parent snapshot 或者 image 上拷贝该 data object，然后执行数据写入操作。这时候，clone 就有自己的 data object 了。 flatten克隆操作本质上复制了一个 metadata object，而 data objects 是不存在的。因此在每次读操作时会先向本卷可能的 data object 访问。在返回对象不存在错误后会向父卷访问对应的对象最终决定这块数据是否存在。因此当存在多个层级的克隆链后，读操作需要更多的损耗去读上级卷的 data objects。只有当本卷的 data object 存在后(也就是写操作后)，才不需要访问上级卷。 为了防止父子层数过多，Ceph 提供了 flattern 函数将 clone 与 parent snapshot 共享的 data objects 复制到 clone，并删除父子关系。 flatten就是将 与父镜像共享的镜像都copy到clone中一层层递归，然后clone 与原来的父 snapshot 之间也不再有关系了，真正成为一个独立的 image，断绝父子关系，然后将那个snapshot删除但flatten是极其消耗网络IO的，非常耗时间 flatten有什么好处呢1，flatten后与父镜像和snapshot已经脱离关系了，可以任意删除。不然不能删除。2，clone的instance 当访问某个不存在data object，需要向上级一级查找，这是非常影响效率，而flatten后改instance这些data object都从上级copy过来，速率会更快。 使用ceph做后端存储，创建虚机如果没有使用ceph做后端存储，openstack创建虚机的流程是，先探测本地是否已经有镜像存在了，如果没有，则需要从glance仓库拷贝到对应的计算节点启动，如果有则直接启动，因此网络IO开销是非常大的如果使用Qcow2镜像格式，创建快照时需要commit当前镜像与base镜像合并并且上传到Glance中，这个过程也通常需要花费数分钟的时间。 当使用ceph做后端存储时，由于ceph是分布式存储，虚拟机镜像和根磁盘都是ceph的rbd image，所以就不需要copy到对应的计算节点，直接从原来的镜像中clone一个新镜像RBD image clone使用了COW技术，即写时拷贝，克隆操作并不会立即复制所有的对象，而只有当需要写入对象时才从parent image中拷贝对象到当前image中。因此，创建虚拟机几乎能够在秒级完成。 注意Glance使用Ceph RBD做存储后端时，镜像必须为raw格式，否则启动虚拟机时需要先在计算节点下载镜像到本地，并转为为raw格式，这开销非常大。 步骤如下1，先基于镜像 做个snapshot、并添加protect、因为clone操作只能针对snapshot、 2， 创建虚拟机根磁盘创建虚拟机时，直接从glance镜像的快照中clone一个新的RBD image作为虚拟机根磁盘: rbd clone 1b364055-e323-4785-8e94-ebef1553a33b@snap fe4c108a-7ba0-4238-9953-15a7b389e43a_disk 3，启动虚拟机启动虚拟机时指定刚刚创建的根磁盘，由于libvirt支持直接读写rbd镜像，因此不需要任何下载、导出工作。 openstack给云主机做快照openstack给云主机做快照，因为还原是基于快照重新创建个云主机，所以本质上是做一个clone操作 具体流程 1基于云主机创建个snapshot---&gt;给snapshot设置只读权限（protect）-----&gt;基于该snapshot clone一个image出来----&gt;flatten操作----&gt;删除snapshot 为了秒级快照，这导致的后果就是，1，做了快照的云主机在控制台删除后在ceph存储的pool中仍然还会存在，因为你的快照根主机的images还是存在父子关系，数据还是共享的，2，云主机 xxxx_disk 存在snapshot，因为做clone是基于protect的snapshot，没flatten的话，snapshot自然没删除。3，残留云主机和snapshot没清理的话会导致上传在glance的镜像也无法删除，因为你的云主机的xxx_disk是基于你glance的镜像clone的存在父子关系不能删除。。 解决办法，后台定期flatten然后rm那些客户已经删了云主机的残留数据。","categories":[{"name":"分布式存储","slug":"分布式存储","permalink":"http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"分布式存储","slug":"分布式存储","permalink":"http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"}]},{"title":"ceph crushmap","slug":"crush","date":"2017-08-05T07:19:59.000Z","updated":"2017-08-05T07:19:59.000Z","comments":true,"path":"2017/08/05/crush/","permalink":"http://yoursite.com/2017/08/05/crush/","excerpt":"","text":"前言通过crushmap划分性能型池和容量型池。在实际生产环境中，考虑到成本因素，很少土豪公司会将osd全部用ssd，但私有云上有部分业务需要高性能存储，部分业务只需要普通的sas盘做容量性存储，在公有云中也经常有，不同性能的存储卖不同的价格。在ceph中的解决方法就是通过修改crushmap，创建多种host，将osd加入到host中，在创建多个pool，每个pool对应不同的rule 本例中两个存储节点的前两个osd为ssd，后面两个osd为sas。需要划分ssd pool和sas pool，其中云主机和性能型存储用sas pool， 性能型存储为ssd pool。环境两台存储节点node-4、node-5，每个存储节点4个osd，将每个存储节点的前两个osd是ssd盘，后两个osd是sas盘 层级结构:中host bucket高于osd bucket，root bucket高于host bucket，划分步骤为：1、创建对应的host bucket 如node-4-sas、node-4-ssd、node-5-sas、node-5-ssd。2、将对应的osd划到对应的host中。3、创建root，如ssd、sas，将对应的host加到对应的root中。4、创建rules将root加入到对应的rule中。5、pool调用ruleset。 权重：修改crushmap时需要特别注意osd的权重问题,1TB OSD为1.00，500G为0.50，3TB位3.00 rules：pool所使用的规则，在crushmap中有一个对应的id，pool直接使用这个id表示这个pool的pg按这个规则进行分布。修改方法有两个 先修改ceph.conf禁止osd启动时自动修改crushmapecho ‘osd_crush_update_on_start &#x3D; false’ &gt;&gt; ceph.conf 第一直接直接使用ceph命令创建bucket，move bucket，在修改rules。第二通过将crushmap导出，修改crushmap的方式。 方法1(直接通过ceph命令)：12345678910111213141516171819202122231、创建对应的root ceph osd crush add-bucket ssd root ceph osd crush add-bucket sas root2、创建对应的host ceph osd crush add-bucket node-4-sata host ceph osd crush add-bucket node-5-sata host ceph osd crush add-bucket node-4-ssd host ceph osd crush add-bucket node-5-ssd host3、移动host到对应的root下 ceph osd crush move node-4-sas root=sas ceph osd crush move node-5-sas root=sas ceph osd crush move node-4-ssd root=ssd ceph osd crush move node-5-ssd root=ssd4、将osd移到host下 ceph osd crush move osd.3 0.88 host=node-4-sas ceph osd crush move osd.4 0.88 host=node-4-sas ceph osd crush move osd.6 0.88 host=node-5-sas ceph osd crush move osd.7 0.88 host=node-5-sas ceph osd crush move osd.1 0.97 host=node-4-ssd ceph osd crush move osd.2 0.88 host=node-4-ssd ceph osd crush move osd.0 0.97 host=node-5-ssd ceph osd crush move osd.5 0.88 host=node-5-ssd 导出crushceph osd getcrushmap -o crushmap.txt反编译crushtool -d crushmap.txt -o crushmap-decompile 打开反编译后的文件修改rule（修改ruleset、和step take) 123456789101112131415161718rule ssd &#123; ruleset 1 type replicated min_size 1 max_size 10 step take ssd step chooseleaf firstn 0 type host step emit&#125;rule sas &#123; ruleset 0 type replicated min_size 1 max_size 10 step take ssd step chooseleaf firstn 0 type host step emit&#125; 重新编译crushtool -c crushmap-decompile -o crushmap-compiled 应用到集群ceph osd setcrushmap -i crushmap-compiled 创建一个新的poolceph osd pool create ssd 1024 设置ssd pool使用rules 1ceph osd pool set ssd crush_ruleset 1 校验object的pg的散落方法参考方法2 方法2(直接修改crushmap)提取现集群中使用的crushmap保存到一个文件ceph osd getcrushmap -o crushmap.txt默认导出来的crushmap打开是乱码的，需要进行反编译才能修改crushtool -d crushmap.txt -o crushmap-decompile重新编译这个crushmapcrushtool -c crushmap-decompile -o crushmap-compiled 将新的CRUSH map 应用到ceph 集群中ceph osd setcrushmap -i crushmap-compiled修改crushmap，需要注意的是bucket ID不要重复了，还有osd的weigh，我这一个osd是90G所以为0.088 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768host node-5-sas &#123; id -2 # do not change unnecessarily # weight 0.176 alg straw hash 0 # rjenkins1 item osd.6 weight 0.088 item osd.7 weight 0.088&#125;host node-4-sas &#123; id -3 # do not change unnecessarily # weight 0.176 alg straw hash 0 # rjenkins1 item osd.3 weight 0.088 item osd.4 weight 0.088&#125;root sas &#123; id -1 # do not change unnecessarily # weight 0.720 alg straw hash 0 # rjenkins1 item node-5-sas weight 0.360 item node-4-sas weight 0.360&#125;host node-5-ssd &#123; id -6 # do not change unnecessarily # weight 0.185 alg straw hash 0 # rjenkins1 item osd.0 weight 0.097 item osd.5 weight 0.088&#125;host node-4-ssd &#123; id -5 # do not change unnecessarily # weight 0.185 alg straw hash 0 # rjenkins1 item osd.1 weight 0.097 item osd.2 weight 0.088&#125;root ssd &#123; id -4 # do not change unnecessarily # weight 0.720 alg straw hash 0 # rjenkins1 item node-5-ssd weight 0.360 item node-4-ssd weight 0.360&#125;# rulesrule ssd &#123; ruleset 1 type replicated min_size 1 max_size 10 step take ssd step chooseleaf firstn 0 type host step emit&#125;rule sas &#123; ruleset 0 type replicated min_size 1 max_size 10 step take sas step chooseleaf firstn 0 type host step emit&#125; 重新编译这个crushmapcrushtool -c crushmap-decompile -o crushmap-compiled将新的CRUSH map 应用到ceph 集群中ceph osd setcrushmap -i crushmap-compiled 创建一个新的poolceph osd pool create ssd 1024 设置ssd pool使用rules 1ceph osd pool set ssd crush_ruleset 1 检查下ceph osd pool get ssd crush_ruleset 测试在ssd中写入个数据是否都落到osd.0、osd.5、osd.1、osd.2rbd create ssd&#x2F;testimg -s 10240 #在ssd pool中创建块 查看pg的散落情况因为我这是两副本所以只会落到两个osd上，分别落在osd.0和osd.1上。 cinder多后端修改cinder.conf 1234567891011enabled_backends=sata,ssd[ssd]volume_backend_name=ssdvolume_driver=cinder.volume.drivers.rbd.RBDDriverrbd_pool=ssdrbd_user=volumesrbd_ceph_conf=/etc/ceph/ceph.confsrbd_secret_uuid=a5d0dd94-57c4-ae55-ffe0-7e3732a24455rbd_max_clone_depth=5 secret_uuid就是对接ceph时导入的secret 创建typecinder type-create ssdcinder type-key ssd set volume_backend_name&#x3D;ssd 重启cinder服务systemctl restart openstack-cinder-apisystemctl restart openstack-cinder-schedulersystemctl restart openstack-cinder-volume 通过crushmap隔离故障域，让pg分布在不同机柜上主机上但这样故域还是host，pg的分布还是比较散乱的，但集群规模大时，如果按照默认的host为故障域的话副本pg很有可能分布同一机架相邻的host的osd上，这样如果你一但此机架断电很有可能导致集群出现ERROR 但我们可以通过修改crushmap 让副本pg分布到不同机架的服务器上去，来达到隔离故障域的目的。 rack 底层的分支 下面方案如下有4个rack，每个rack有一个host ,通过修改crushmap将pg分布到不同列的rack的host上，比如可以指定compute pool的第一个副本放rackB01的node4上，第二个副本放rackC01的node-5上， 123456789node-5上，rackB01 node-4rackC01 node-5rackB02 node-6rackC02 node-7 这样做的好处就是将副本放在不同列的不同机柜上，来提高可靠性 12345rack方案node-4 ---B01柜node-5 ---B02柜node-6 ---C01柜node-7 ---C02柜 将compute pool的副本都放到B、C 01柜里面讲test_pool的副本都放到B、C 02柜里面 修改后 方法一(直接通过ceph命令)： 12345678910111213141516171819202122232425261、添加racks: ceph osd crush add-bucket rackB01 rack ceph osd crush add-bucket rackB02 rack ceph osd crush add-bucket rackC01 rack ceph osd crush add-bucket rackC02 rack3、把每一个host移动到相应的rack下面： ceph osd crush move node-4 rack=rackB01 ceph osd crush move node-5 rack=rackB02 ceph osd crush move node-6 rack=rackC01 ceph osd crush move node-7 rack=rackC024、添加root ceph osd crush add-bucket rackB_C01 root ceph osd crush add-bucket rackB_C02 root4、把所有rack移动到对应 root 下面： ceph osd crush move rackB01 root=rackB_C01 ceph osd crush move rackB02 root=rackB_C02 ceph osd crush move rackC01 root=rackB_C01 ceph osd crush move rackC02 root=rackB_C02 导出crushmap 添加rulesceph osd getcrushmap -o crushmap.txt 反编译crushtool -d crushmap.txt -o crushmap-decompile 修改 添加如下(注意ruleset、和step take) 123456789101112131415161718rule rackB_C01 &#123; ruleset 0 type replicated min_size 1 max_size 10 step take rackB_C01 step chooseleaf firstn 0 type rack step emit&#125;rule rackB_C02 &#123; ruleset 1 type replicated min_size 1 max_size 10 step take rackB_C02 step chooseleaf firstn 0 type rack step emit&#125; 编译crushtool -c crushmap-decompile -o crushmap-compiled 将新的CRUSH map 应用到ceph 集群中ceph osd setcrushmap -i crushmap-compiled 设置test_pool套用rule rackB_C02 ceph osd pool set test_pool crush_ruleset 1 查看是否应用成功 测试在test_pool里面创建个对象应该分到B02和C02柜的host上面的osd上,rbd create test_pool&#x2F;testimg -s 1024在test_pool里面创建个对象应该分到B01和C01柜的host上面的osd上,rbd create compute&#x2F;test_2.img -s 1024 查看 这样副本pg就会分布在不同机柜的不同host上的osd。","categories":[{"name":"分布式存储","slug":"分布式存储","permalink":"http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"分布式存储","slug":"分布式存储","permalink":"http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"}]},{"title":"neutron-高可用(1)--DVR","slug":"dvr","date":"2017-07-23T07:19:59.000Z","updated":"2017-07-23T07:19:59.000Z","comments":true,"path":"2017/07/23/dvr/","permalink":"http://yoursite.com/2017/07/23/dvr/","excerpt":"","text":"环境 fuel8.0 介绍DVR简称分布式路由，M版前的neutron网络结构。 无论是南北流量中的1:N还是1:1还是东西流量也好都经过网络节点，这就带来一个问题了，网络节点负载很大，往往成为整个架构的性能短板，同时还很容易出现单点故障。在Juno版本后可以配置DVR模式，每个compute节点上都配置了一个L3 agent，集群内的流量（东西流量）这里有个前提条件，同网段内主机在不同compute节点，在同一个compute节点就直接通过本机就能直接转发了。直接就通过tunnel到达另外一个compute节点，不在需要通过网络节点去做转发了，而配置了floatting ip的主机，直接通过本compute节点的L3 agent 通过br-ex连接外网。 进行深入前的一些基本概念的讲解：路由策略引用百科里的一句话路由策略是一种基于目标网络进行路由更加灵活的数据包路由转发机制。应用了路由策略，路由器将通过路由图决定如何对需要路由的数据包进行处理，路由图决定了一个数据包的下一跳转发路由器。 路由策略的种类 大体上分为两种：一种是根据路由的目的地址来进行的策略称为：目的地址路由； 另一种是根据路由源地址来进行策略实施的称为：源地址路由！ 随着路由策略的发展现在有了第三种路由方式：智能均衡的策略方式！ 使用 ip rule 操作策略路由 基于策略的路由比传统路由在功能上更强大，使用更灵活，它使网络管理员不仅能够根据目的地址而且能够根据报文大小、应用或IP源地址等属性来选择转发路径。ip rule 查看策略路由表 数字越小，优先级越高 规则0，它是优先级别最高的规则，规则规定，所有的包，都必须首先使用local表（254）进行路由。本规则不能被更改和删除。 规则32766，规定所有的包，使用表main进行路由。本规则可以被更改和删除。 规则32767，规定所有的包，使用表default进行路由。本规则可以被更改和删除。 在默认情况下进行路由时，首先会根据规则0在本地路由表里寻找路由，如果目的地址是本网络，或是广播地址的话，在这里就可以找到合适的路由；如果路由失败，就会匹配下一个不空的规则，在这里只有32766规则，在这里将会在主路由表里寻找路由;如果失败，就会匹配32767规则，即寻找默认路由表。如果失败，路由将失败。重这里可以看出，策略性路由是往前兼容的。 3232243201: from 192.168.30.1&#x2F;24 lookup 3232243201 #这条规则的含义是规则3232243201 源ip为192.168.30.1&#x2F;24的包，使用local表3232243201 进行路由 linux二层网络中一些基本名词 TAP设备：如kvm创建虚拟机后，宿主机上的vnet0、vnet1等，是操作系统内核中的虚拟网络设备。veth配对设备（veth pair）或ovs配对设备，它其实是一对网卡把一块叫veth，另外一块叫peer，当一个数据真被发送到其中一端中，veth的另外一端也会收到此帧。在虚拟化中常用veth pair来连接两个bridge，比如qbr网桥跟br-int网桥上的qvb-xxx和qvo-xxx就是一对veth pair。 启用DVR以下配置的前提是开启了l2_populationl2population原理介绍如下http://blog.csdn.net/cloudman6/article/details/53167522 1，安装compute节点(1)安装l3-agent(2)修改的内核参数 123456vi /etc/sysctl.confnet.ipv4.ip_forward = 1net.ipv4.conf.all.rp_filter = 0net.ipv4.conf.default.rp_filter = 0sysctl -p （3）增加一块能上外网的网卡，并创建个br-ex网桥把这块网卡桥接到br-ex上 12ovs-vsctl add-br br-exovs-vsctl add-port br-ex ethx (4)配置l3agent.ini 1234567vim /etc/neutron/l3_agent.iniinterface_driver =neutron.agent.linux.interface.OVSInterfaceDriveruse_namespaces = Trueagent_mode = dvr````vim /etc/neutron/plugins/ml2/ml2_conf.ini [agent]enable_distributed_routing&#x3D;True 12345重新启动 neutron-openvswitch-agent, netron-l3-agent ,openvswitch-switch 服务。#### 网络节点 vim &#x2F;etc&#x2F;neutron&#x2F;l3_agent.iniagent_mode &#x3D; dvr_snat 1234```vim /etc/neutron/plugins/ml2/ml2_conf.ini[agent]enable_distributed_routing=True 1重启neutron-server、netron-l3-agent ,openvswitch-switch 服务。 配置成功后可以neutron agent-list 看见compute的l3-agent。 原理解释1台control节点（server-4），2台compute（server-5、server-6）、外网为192.168.122.0&#x2F;24在control节点上在控制台创建个网络，创建个路由器，设置网关，在路由器上将网络端口添加进去。在基于此网络创建个主机。可以发现在control节点上创建路由器时，会同时在namespace里面创建qroute-xxx和snat-xxx两个命名空间，snat-xxx命名空间会根据创建的子网来添加接口，qg-xxx和sg-xxx，其中qg-xxx上面配置了外网ip，sg-xxx配置的是本子网可用的第一个ip（前两个为网关和dhcp） 当该网络内其他主机需要上外网时，源ip会已postrouting形式，snat成路由器的网关，这步操作会在snat-xx命名空间内做 在传统网络架构下，snat操作本来是应该在qroute下进行，而在DVR模式下qroute-xxx完全是连接内网的网关。 当创建一个实例时，会自动在实例所在的compute节点创建一个qroute-xxx 此qrouter-xxx的ip和mac都与control节点的一样。 当给实例分配一个floating_ip时，在自动在实例所在的计算节点namespace里面生成fip-xxx命名空间 需要注意的是fip-xxx这个id是与你的public网络id是一致 也就是说一个public网络对应一个fip-xxx namespace,在下面会解释为什么这样做节点fg-xxx对应的是外网接口，fpr接口与qrouter接口一对veth pair DVR模式下流量的走向南北流量（1：N）所谓南北流量（1：N）实际上就是指SNAT就是子网内的主机通过路由器的floatingIP去上外网1，首先需要确认router已经发到compute节点，我这router叫test2可以看见已经在server-6上有了 我在vm（192.168.50.4）上面ping百度 因为是跨网段的，所以发送给默认路由，就是网关。在网关上路由策略可以看见发现，发给默认路由192.168.50.1的数据，最终会发给192.168.50.3 那192.168.50.3在哪呢，通过前面分析，我们知道，192.168.50.3在control节点snat-xx命名空间里面 然后在iptables里面做一次snat 就出外网了，因为我这是KVM虚拟机里面所以还要nat一层。 ping public网络 192.168.122.1 然后抓包分析 所以SNAT流程 123 （compute节点） vxlan （network节点）vm---&gt;qr-xxx---&gt;br-int----&gt;br-tun---&gt; br-tun-----&gt;br-int-----&gt;snat-xxx----br-ex 南北流量（1：1）给主机192.168.50.4绑定一个floating_ip 观察control节点和compute节点的namespace发现control节点没有发生变化，而vm所在的compute的节点的namespace发现多了一个fip命名空间 注意看这个fip的ip是我们创建的外网网络id，这个在上面已经说过了。查看一个这个fip-xxx的内容 vm绑定floating_ip后如何跟外网进行通信 1，vm ping外网，首先数据包丢给默认路由，qroute，2，查找路由表 命中table 16 走rfp端口 3，然后进行snat 4，然后发送给fip-xxx命名空间 因为已经进行snat了地址变成外网的，所以这里都default route 直接给外网网关，完成 如下图来源http://www.cnblogs.com/sammyliu/p/4713562.html vm绑定vip后外网DNAT进入内网主机1，arp-proxy外网ip要ping通内网，首先需要知道内网ip的mac，fip，并没有直接绑定在一个特定的端口是怎么知道mac的呢 进行路由追踪发现 arping发现mac地址为 尾数为C7:C3的 那这个mac地址为谁的 竟然是fip-xxx的fg端口 原来是fip-xxx在fg这个端口上做了arp-proxy，这样，fg既可以响应发给它自己的arp请求，也可以响应发给经过它路由的端的arp请求 可以看见内核参数里面已经开启了 2，将包丢给路由器 我们从fip-xxx ip route 可以看见包被丢给169.254.93.254这个接口了，这个接口是qroute-xxx的一个接口 3，qroute–xxx做DNAT 12345流程如下 arp欺骗 通过fpr DNATextra-----&gt;fip--xxx----&gt;qroute-----&gt; VM 为什么要独立出一个snat namespace来做SNAT，而不继续之前的qrouter多SNAT方案？ 因为 当创建子网的第一台主机时，会同时在compute节点上也创建个qrouter-xxx的namespace，但computer节点上的qrouter并不做snat功能，只做当有绑定float_ip时 连接FIPnamespace中转，所以需要将SNAT功能独立出来，于是就有了SNATnamespace当然这个也只会在网络节点的namespace里面创建。 当虚拟机绑定fip时，为什么compute要多出一个fip namespace，不直接在qrouter上的qg直接连接br-ex？是为了减少暴露在外部网络上的mac和ip地址，所以才需要有fip的出现。由于传统router只在网络节点上，数量很少，直接在传统router的qg上配floating ip和用其mac地址应答arp请求，所以没这个问题。但是dvr router由于分布到大量的compute node（每个私有网络都可能有一个router），如果再这么做，会导致有大量router的qg接口暴露在外部网络上，因此分离出fip，而fip是按照（compute node，external network）分配的，数量少很多。一个external_network就一个另外fip也可以做集中的arp proxy，不需要把floating ip真正绑到fip的接口上。同时也可以减少交换机MAC地址表的占用，减轻交换机的负担。","categories":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/categories/openstack/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/tags/openstack/"}]},{"title":"openssl证书生成","slug":"openssl_ca","date":"2017-07-20T13:45:59.000Z","updated":"2017-07-20T13:45:59.000Z","comments":true,"path":"2017/07/20/openssl_ca/","permalink":"http://yoursite.com/2017/07/20/openssl_ca/","excerpt":"","text":"X.509是一种非常通用的证书格式。所有的证书都符合ITU-T X.509国际标准，因此(理论上)为一种应用创建的证书可以用于任何其他符合X.509标准的应用。x.5.09证书通常包含三类文件key文件，密钥文件用于对发送和接收到的数据进行加密和解密csr是签名请求文件，用于提交给CA进行签名crt是ca签名后颁发的证书pem是 用于导出和导入证书时的证书格式，通常是crt+key的结合 在内部使用时通常使用自签名的证书，简单来说是，就是自己通过一些工具如openssl或cfssl生成CA然后去进行签名证书，如果要对外提供公共服务，就需要去购买正规的证书颁发机构的CA进行签名。这里我们讨论的是自签名证书 首先我们需要生成CA根证书1、生成CA 私钥，这里使用的是RSA算法 1openssl genrsa -out ca.key 2048 2、生成根证书请求文件csr 1openssl req -new -key ca.key -out ca.csr 需要依次输入国家，地区，城市，组织，组织单位，Common Name和Email。其中Common Name，可以写自己的名字或者域名，如果要支持https，Common Name应该与域名保持一致，否则会引起浏览器警告。 3、自签名得到ca证书 1openssl x509 -req -days 36500 -in ca.csr -signkey ca.key -out ca.crt 得到自签名的CA证书以后我们要用CA证书对用户的证书进行签名，这样才能正常使用生成私钥 key文件 1openssl genrsa -out domain.rancher.com.key 1024 生成证书请求csr文件 1openssl req -new -key domain.rancher.com.key -out domain.rancher.com.csr 需要依次输入国家，地区，城市，组织，组织单位，Common Name和Email。其中Common Name，可以写自己的名字或者域名，如果要支持https，Common Name应该与域名保持一致，否则会引起浏览器警告。 用CA证书签名用户证书 1openssl ca -in domain.rancher.com.csr -out domain.rancher.com.crt -cert ca.crt -keyfile ca.key 注意: 此时会出错：Using configuration from &#x2F;usr&#x2F;share&#x2F;ssl&#x2F;openssl.cfg I am unable to access the .&#x2F;demoCA&#x2F;newcerts directory .&#x2F;demoCA&#x2F;newcerts: No such file or directory 解决方法： 1234mkdir -p ./demoCA/newcertstouch demoCA/index.txt #空文件，生成证书时会将数据记录写入touch demoCA/serialecho 01 &gt; demoCA/serial #写入01，然后生成证书会以此递增 然后会对应的ca证书和用户证书放到对应的web服务器的目录就可以了。 如果需要pem证书文件 ，就把证书文件(crt)和私钥文件(key)文件合并即可 1cat domain.rancher.com.crt domain.rancher.com.key &gt; domain.rancher.com.pem","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"Linux下单网卡配置属于不同vlan的ip(vlan子接口)","slug":"linux_vconfig","date":"2017-05-23T07:19:59.000Z","updated":"2017-05-23T07:19:59.000Z","comments":true,"path":"2017/05/23/linux_vconfig/","permalink":"http://yoursite.com/2017/05/23/linux_vconfig/","excerpt":"","text":"一台服务器上一块网卡想同时划到两个不同vlan，并且让他们之间可以互相通信。 1、首先确认Linux系统内核是否已经支持VLAN功能，加载了8021q模块，lsmod |grep 8021q。2、关于网卡的解释，好多人不知道网卡接口上的冒号和点号的区别，以下是一些解释（我也是从网上查的，仅供参考） a、物理网卡：物理网卡指的是服务器上实际的网络接口设备，如在系统中看到的2个物理网卡分别对应是eth0和eth1这两个网络接口。 b、子网卡：子网卡并不是实际上的网络接口设备，但是可以作为网络接口在系统中出现，如eth0:1、eth1:2这种网络接口。它们必须要依赖 于物理网卡，虽然可以与物理网卡的网络接口同时在系统中存在并使用不同的IP地址，而且也拥有它们自己的网络接口配置文件。但是当所依赖的物理网卡不启用 时（Down状态）这些子网卡也将一同不能工作。 c、虚拟VLAN网卡：这些虚拟VLAN网卡也不是实际上的网络接口设备，也可以作为网络接口在系统中出现，但是与子网卡不同的是，他们没有自己 的配置文件。他们只是通过将物理网加入不同的VLAN而生成的VLAN虚拟网卡。如果将一个物理网卡添加到多个VLAN当中去的话，就会有多个VLAN虚 拟网卡出现，他们的信息以及相关的VLAN信息都是保存在&#x2F;proc&#x2F;net &#x2F;vlan&#x2F;config这个临时文件中的，而没有独自的配置文件。它们的网络接口名是eth0.1、eth1.2这种名字。 下面为实际操作，我的环境是一台H3C S5120三层交换机，服务器有4个网卡。 交换机配置配置trunk，并且配置允许vlan id，我这里端口是g1&#x2F;0&#x2F;11 到g1&#x2F;0&#x2F;15 设置该端口为trunk模式 123[H3C-Git1/0/11]port link-type trunk //vlan端口默认为access模式``` 设置允许vlan 10和20的tag通过该端口 [H3c-Git1&#x2F;0&#x2F;11]port trunk permit vlan 40 50 &#x2F;&#x2F;替换后面的数字为all则是允许所有vlan tag通过该端口 12345其他12，13，14，15口一样配置接下来给vlan配置ip(配置网关) [H3C]interface Vlan-interface 40[H3C-Vlan-interface1]ip address 192.168.4.254 255.255.255.0 [H3C]interface Vlan-interface 50[H3C-Vlan-interface1]ip address 192.168.5.254 255.255.255.0 123456789101112交换机配置到此结束，接下来是Linux网卡配置Linux要配置多vlan口。注：当需要启用VLAN虚拟网卡工作的时候，关联的物理网卡网络接口上必须没有IP地址的配置信息。首先确认是否安装vconfig然后查看核心是否提供VLAN 功能，执行 dmesg | grep -i 802 [root@test]# dmesg | grep -i 802 802.1Q VLAN Support v1.8 Ben Greear&#x67;&#114;&#101;&#101;&#97;&#x72;&#x62;&#64;&#x63;&#97;&#x6e;&#x64;&#x65;&#x6c;&#97;&#x74;&#x65;&#x63;&#x68;&#46;&#99;&#x6f;&#109; [root@test]# modprobe 8021q [root@test~]#lsmod |grep 8021q &#x2F;&#x2F;查看系统内核是否支持802.1q协议 8021q 18633 0 12 vconfig add eno3 300 &#x2F;&#x2F; eno3为物理网络接口名称，300为 802.1q tag id 也即 vlan IDvconfig add eno3 500 ifconfig eno3.300 192.168.4.5&#x2F;24 upifconfig eno3.500 172.31.5.5&#x2F;24 up 1234567891011最终可以在可以创建成功的vlan接口 root@node-78:~# ls /proc/net/vlan/然后把这些写进配置文件不然重启就没有了vim /etc/network/interfaces auto lo eno3iface lo inet loopbackiface eno3.300 inet static address 192.168.4.5 netmask 255.255.255.0iface eno3.500 inet static address 172.31.5.5 netmask 255.255.255.0 1234567891011测试在服务器上分别ping 192.168.4.254和172.31.5.254可以ping通就行。不能通的话， router -n 查看路由，是不是还是以前的老路由，是的话要删除ip route del xxxx via xxx.xxx.xxx.xxx dev 接口5,删除VLAN命令 [root@test0001]#vconfig rem eth0.100Removed VLAN -:eth0.100:-[root@test0001]#vconfig rem eth0.200Removed VLAN -:eth0.200:- 在openstack虚拟机里不能做操作，不过newton版开发了这个plugin叫VLAN-aware-VMs http://blog.csdn.net/bc_vnetwork/article/details/53927687","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"boot from volume的两种方式","slug":"boot_from_volume","date":"2017-05-08T07:19:59.000Z","updated":"2017-05-08T07:19:59.000Z","comments":true,"path":"2017/05/08/boot_from_volume/","permalink":"http://yoursite.com/2017/05/08/boot_from_volume/","excerpt":"","text":"boot for volume的两种方式简介： openstack对接商业存储一般直接用cinder直接对接商业存储，但想将nova创建的云主机放商业存储上，此时需要使用boot from volume 方式一直接选择镜像，dest选择云硬盘，需要注意的是云硬盘的大小必须大于等于镜像大小。 方式二创建空云硬盘，创建主机时将选择的镜像，加载到这个空硬盘。（需要注意的是，创建的空盘大小必续要大于你选择的镜像的大小）。 方式一 1nova boot --flavor 1 --nic net-id=48baa60d-b785-45c1-8f90-8467f56abb5b --block-device source=image,id=d3a0512b-8cfc-4ad4-9fd4-7b38c9a44a32,dest=volume,size=10,shutdown=preserve,bootindex=0 test 流程：先是基于image创建block volume，然后从这个volume中boot instance 。shutdown选项选为preserve, 而不要选为remove， 这样在instance关闭时， volume会被save下来；其中的size选项要求大于等于flavor中的disk大小，同时要求我们的后端存储要有大于此size大小的空间。 ![](https://image-1251900790.cos.ap-chengdu.myqcloud.com/image/boot_from_volume_1.png 方法二openstack volume create –image IMAGE_ID –size SIZE_IN_GB –name CINDER_NAME 获取云硬盘id 基于云硬盘创建云主机 1nova boot --flavor 1 --nic net-id=48baa60d-b785-45c1-8f90-8467f56abb5b --boot-volume f9af8677-28cf-4dfb-8dbc-b722025f9fa0 --security-group default --admin-pass test test","categories":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/categories/openstack/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/tags/openstack/"}]},{"title":"ironic-简介","slug":"ironic","date":"2017-05-08T07:19:59.000Z","updated":"2017-05-08T07:19:59.000Z","comments":true,"path":"2017/05/08/ironic/","permalink":"http://yoursite.com/2017/05/08/ironic/","excerpt":"","text":"ironic简介介绍openstack中对物理机管理的组件，通过ironic可以给物理机，上电、下电、重启，自动安装操作系统，根据设定的规则模板，进行自动化配置。ironic是I版开始进入孵化，j版与nova进行集成、K版正式release,ironic原本是mirantis开发的，后面贡献到社区。部署虚机和部署物理机对底层nova调用来创建虚机，但对nova-compute的底层的hypevisor是不一样的调用虚机底层调用的驱动可能是libvirtd、vmwareapi.VMwareVCDriver等，而物理机底层驱动是ironic。 用途目前有些服务在虚机上运行达不到理想的性能，仅管openstack拥有trove、sahara、magnum等组件用于支持数据库平台、大数据平台、容器平台、但毕竟是在虚拟机里面部署服务，性能远远不够。但直接部署在物理机上又不好管理，此时ironic应运而生了。ironic可以解决物理机的添加、删除、电源管理、安装部署、未来可能能支持自动部署数据库、大数据平台、容器平台。 项目构成ironic： 包含ironic-api 和ironic-conductor进程python-ironicclinet: python clinet and CLIironic-python-agent: 一个运行在deployment ramdisk中的Python程序，用于执行一系列部署动作pyghmi: 一个python的IPMI库，可以代替IPMItoolironic-inspector: 硬件自检工具ironic-lib： ironic的通用库函数ironic-webclinet ：web consoleironic-ui：ironic的horizon插件bifrost：一套只运行Ironic的Ansible脚本 ironic组件ironic-api：负责接收请求，并且将请求传递给ironic-conductorironic-conductor：ironic中唯一一个能根数据库进行交互的组件，负责接收ironic-api的请求，然后根据请求执行相应的，创建、开机、关机、删除操作ironic-python-agent：部署裸机时，pxe启动进入的一个bootstrap镜像，此镜像是用社区diskimage-builder工具制做，镜像内安装了ironic-python-agent，进入系统后与ironic-conductor进行交互， 将本地磁盘以iscsi方式挂载到控制节点，控制节点将系统dd到裸机磁盘上 ironic用到的技术PXE+tftp+dhcp IPMI iscsi 裸机部署原理1，nova boot启动一个实例，nova-api将请求通过rabbitmq，传给nova-conductor，在传给nova-scheduler。2，nova-scheduler根据传递进来的flavor和image筛选出合适的计算节点，然后将请求传递到对应的计算节点。3，nova-compute与ironic通信，调用ironic-api。4，ironic-api将请求下发给ironic-conductor，ironic-ductor调用glance下载bootstrap镜像。在tftp-serer目录生成pxelinux配置文件，并将bootstrap镜像放到tftp目录。5，ironic-conductor通过调用ipmi driver将物理机开机，并设置以pxe的方式启动。6，物理服务器启动后，通过pxe加载了bootstrap镜像启动后，镜像内ironic-python-agent 启动，与ironic-ductor进行交互，将本地磁盘通过iscsi方式挂载到控制节点，控制节点通过dd的方式将用户选择的系统写入到磁盘中。7，写入完毕后ironic将调用ipmi driver对物理服务器进行重启操作。8，重新启动后主机，初次进入系统时，运行镜像内的cloud-init进行初始化，配置hostname和密码。这里需要注意，使用的镜像，需要提前在 &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;里面将网卡的配置文件提交准备好不然开机，网卡会up不起来。centos7需要修改grub参数，将网卡名以ethx显示，不然以设备名显示，配置文件不好写。 流程图如下 需要注意的是镜像ironic有两种镜像一种是部署时的bootstrap镜像，这种镜像是内含了，ironic-python-agent，启动后会主动与ironic-conductor进行交互。 另外一种是用户需要安装到裸机的镜像，这种镜像是通过bootstrap，通过iscsi将磁盘挂载到控制节点dd写入的。 网络在Newton版前，ironic都是不支持多租户，都是在一个flat网络上，互相之间是没有隔离关系的。 https://www.ibm.com/developerworks/cn/cloud/library/cl-cn-virtualboxironic/index.htmlhttp://www.cnblogs.com/menkeyi/p/6063551.htmlhttps://docs.openstack.org/developer/ironic/liberty/deploy/user-guide.htmlhttps://wiki.openstack.org/wiki/Ironic","categories":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/categories/openstack/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/tags/openstack/"}]},{"title":"trove原理","slug":"trove","date":"2017-04-17T15:27:44.000Z","updated":"2017-04-17T15:27:43.000Z","comments":true,"path":"2017/04/17/trove/","permalink":"http://yoursite.com/2017/04/17/trove/","excerpt":"","text":"版本：trove4.0openstack版本：liberty DBaaS是什么？字面上理解数据库即是服务，简单来说就是以服务的形式为用户提供数据库服务 在云平台上使用trove有什么优势？1，简化IT操作流程，降低使用数据库使用门槛举个例子，曾经我搭建一个LAMP网站，数据库要自己安装，创建，授权，必要的话，还要自己做主从很繁琐，而且不是专业人员也搞不定，有了Dbaas后，我只需要在控制台点几下就弄好。 2，自动化操作，自动的增、删、改、备。 3，更好的资源利用，你可以根据业务量，自由的对数据库实例进行伸缩。 架构解析 trove根其他一些openstak组件一样，它暴露一个public-api，通过这个api访问trove-service，同时也保存着一些数据库实例状态到数据库中。 组件功能trove-api用于操作请求的接收和分发操作提供REST风格的API，同时与trove-conductor和trove-taskmanager通信，一些轻量级的请求比如获取实例状态，实例数量等操作都是自身直接处理或访问conductor和trove-taskmanager处理，比较重量级的操作比如创建数据库，创建备份等操作都是通过rpc传递给trove-taskmanager，taskmanager，然后在通过调用nova、swift、neutron、cinder等组件来完成操作。 trove-conductor将vm内trove-guestagent发送的状态信息保存到数据库，与trove-guestagent的通信是通过rpc来实现的，trove-conductor这个组件的目的是为了避免创建的数据库的实例直接访问数据库，它是做为一个trove-guestagent将昨天写入数据库的中间件。 trove-taskmanager执行trove中大部分复杂的操作，请求者发送消息到task manager，task manager在请求者的上下文中调用相应的程序执行这些请求。task manager处理一些操作，包括实例的创建、删除，与其他服务如Nova、Cinder、Swift等的交互，一些更复杂的Trove操作如复制和集群，以及对实例的整个生命周期的管理。trov-taskmanager就像是其他openstak服务的客户端，如nova，swift，cinder等，当要创建数据库实例时就将请求发送给nova，让nova去创建个实例，要备份的话就调用swift接口上传备份。 trove-guestagenttrove-guestagent集成在vm镜像里面，通过监听rpc里面task manager发过来的指令，并在本地执行代码完成数据库任务，taskmanager将消息发送到guest agent，guest agent通过调用相应的程序执行这些请求。 功能原理介绍(这里只介绍对mysql数据库的功能实现，因为trove对mysql支持比较成熟）这里分别介绍三个功能的原理1，创建数据库实例2，创建数据库备份3，mysql的集群 创建数据库实例 创建数据库实例时，实际上就是通过trove-taskmanager create_instance()方法去调用nova-api 然后调用 _get_injected_files方法将guet_info和trove-guetagent.conf信息注入到 数据库实例&#x2F;etc&#x2F;trove&#x2F;conf.d&#x2F;里面，提供给guest-agent进行后续的操作。 所以4.0版本的trove并不需要一开始就将trove-guestagent.conf这个配置文件封装在镜像里面，这个配置文件是通过nova注入的，所以镜像只需要配置好guest-agent从哪里读这个配置文件。剩下的就交给trove-guestagent guest_info_file这个配置文件 1234[DEFAULT]guest_id=7ec35639-5139-4ae4-8388-8101e41cc0f7 #这个ID是trove 分配给这个实例的IDdatastore_manager=mysql #采用的是哪个datastoretenant_id=f2f0e038ff0342a3bc99d8971f829ac2 #是哪个租户的 当你在控制台输入需要创建的云硬盘的大小时，实际上是通过调用taskmanager里面的_create_volume方法 收集齐上面那些信息后，然后调用nova来创建数据库实例 然后数据库实例里面的guest-agent会去读取通过nova注入的trove-guestagent.conf 去连接rpc读取taskmanager发送过来的操作请求。 剩下的一些操作比如创建数据库、创建用户这些都是taskmanager调用数据库实例里面的guest-agent去实现 guest-agent对mysql的一些操作实现是在 123/usr/lib/python2.7/dist-packages/trove/guestagent/datastore/mysql/service_base.py 他里面包含了def _get_actual_db_status() #获取数据库实例状态方法 主要是通过调用&#x2F;usr&#x2F;bin&#x2F;mysqladmin ping” 和ps -C mysqld h 去获取数据库实例状态 通过判断pid文件是否存在来判断mysql是否shutdown。 def create_database #创建数据库实例方法 def create_user #创建用户并且授权方法 后面还有删除数据库，删除用户，获取binlog，开始slave、关闭slave等方法同时需要注意的是trove创建数据库实例时，会默认为每个数据库实例同时创建一个 SecGroup-xxx xxx为主机ID的安全组。trove默认是不启动root用户的所以在控制台用户选项卡里面用户名称是不能填root的。 需要注意的是上述所有操作都是由trove用户来执行，所以必须要确认的是trove用户拥有sudo权限，否则会失败。 在执行完上述操作前此时数据库状态还是building状态的， 那就是 vm 正在启动，创建数据库，创建用户，对用户授权，同步my.cnf配置文件到数据库实例内，重启mysql ，trove-guestagent 发送 rpc 给trove-taskmanager，最后检测数据库成功运行后发送 Active 状态消息给 rpc，trove-taskmanager 收到 Active 消息后，不再发送创建数据库消息，而 trove-conductor 同时收到 trove-guestagent Active 消息后，去数据库里更新 trove instance 的状态，在trove list 就可以看见instance Active 的状态了。 备份还原 目前trove-guestagent只支持mysql的三种备份方式，一种是传统的mysql Dump方式一种是InnoBackupEx 还有增是InnoBackup的增量备份方式InnoBackupExIncremental。 备份的程序放在 1/usr/lib/python2.7/dist-packages/trove/guestagent/strategies/backup 其调用方式也比较简单，就是trove-guestagent.conf里面配置了什么备份方式就调用指定类执行里面的方法，方法内也都是一些软件的命令。 需要注意的是默认不配置是调用Innobackup备份的日志会存在tmp目录下 备份完成后默认是会存储到swift内 默认备份在swift内的备份文件夹为database_backups 、开启压缩、ssl加密，分片等调用SwiftStorage类里面的save方法上传到Swift中 其中会进行文件的校验是备份上去的实际上有两个文件，第一个enc文件主要是用来分片使用，第二个文件才是主要的备份文件。 mysql主从 trove-master端先将当前数据备份到Swift—&gt;然后taskmanager重新创建个数据库实例——&gt;新创建的数据库实例将刚刚的备份从Swift拉下来根据里面的bin-log里面的GTID进行还原—-&gt;建立主从关系—检测创建成功taskmanager删除上传到Swift的备份。 备份前会做个检测，发现以前有备份就调用增量备份的方法节省空间，检测到没有就调用全备的方法这里先做个变量定义，定义好增量备份和全备的变量 if判断调用全备还是增量备份。 目前trove只支持mysql的主从不支持主主并且还是异步的主从。创建主从时，创建从同样是调用create_instance()方法 只是这里做了个判断，如果传过来了slave_of_id就调用__create_replication_slave()方法 __create_replication_slave()方法会去获去备份的ID ,然后继续调用nova创建主机 接下来操作会交给数据库实例里面的guest-agent进行操作。 guest-agent会先将备份文件从来Swift 下载下来。然后还原。接下来建立主从关系，这里要说明的是trove建立主从关系的方式有两种一种是传统的bin-log的形式，一种的用GTID的形式。 1在/usr/lib/python2.7/dist-packages/trove/common/cfg.py 这个是定义的两个不同的策略 同时也会调用不同的方法去执行当你的配置文件为 12replication_strategy = MysqlBinlogReplicationreplication_namespace = trove.guestagent.strategies.replication.mysql_binlog 调用的是 1/usr/lib/python2.7/dist-packages/trove/guestagent/strategies/replication/mysql_binlog.py 当你的配置文件为 12replication_strategy = MysqlGTIDReplicationreplication_namespace = trove.guestagent.strategies.replication.mysql_gtid 调用的是 1/usr/lib/python2.7/dist-packages/trove/guestagent/strategies/replication/mysql_gtid.py 这两个文件有何不同，方法内定义的命令不同 GTID的概述： 全局事物标识：global transaction identifieds。 GTID事物是全局唯一性的，且一个事务对应一个GTID。 一个GTID在一个服务器上只执行一次，避免重复执行导致数据混乱或者主从不一致。 GTID用来代替classic的复制方法，不在使用binlog+pos开启复制。而是使用master_auto_postion&#x3D;1的方式自动匹配GTID断点进行复制。 MySQL-5.6.5开始支持的，MySQL-5.6.10后开始完善。 在传统的slave端，binlog是不用开启的，但是在GTID中，slave端的binlog是必须开启的，目的是记录执行过的GTID（强制）。 下面介绍一下mysql GTID（忘记了从哪个网上摘录的) GTID的组成部分： 前面是server_uuid：后面是一个序列号 例如：server_uuid：sequence number 7800a22c-95ae-11e4-983d-080027de205a:10 UUID：每个mysql实例的唯一ID，由于会传递到slave，所以也可以理解为源ID。 Sequence number：在每台MySQL服务器上都是从1开始自增长的序列，一个数值对应一个事务。 GTID比传统复制的优势： 更简单的实现failover，不用以前那样在需要找log_file和log_Pos。 更简单的搭建主从复制。 比传统复制更加安全。 GTID是连续没有空洞的，因此主从库出现数据冲突时，可以用添加空事物的方式进行跳过。 GTID的工作原理： master更新数据时，会在事务前产生GTID，一同记录到binlog日志中。 slave端的i&#x2F;o 线程将变更的binlog，写入到本地的relay log中。 sql线程从relay log中获取GTID，然后对比slave端的binlog是否有记录。 如果有记录，说明该GTID的事务已经执行，slave会忽略。 如果没有记录，slave就会从relay log中执行该GTID的事务，并记录到binlog。 在解析过程中会判断是否有主键，如果没有就用二级索引，如果没有就用全部扫描。 要点： 1、slave在接受master的binlog时，会校验master的GTID是否已经执行过（一个服务器只能执行一次）。, 2、为了保证主从数据的一致性，多线程只能同时执行一个GTID。","categories":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/categories/openstack/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/tags/openstack/"}]},{"title":"vxlan","slug":"vxlan","date":"2017-02-24T06:24:44.000Z","updated":"2017-02-24T06:24:44.000Z","comments":true,"path":"2017/02/24/vxlan/","permalink":"http://yoursite.com/2017/02/24/vxlan/","excerpt":"","text":"什么是vxlan？VXLAN（Virtual Extensible LAN）虚拟可扩展局域网是目前NVO3(Network Virtualization Over Layer 3 基于三层IP overlay网络构建虚拟网络技术统称NVO3)，它是目前NVO3中影响力最为广泛的一种，它通过L2 over L4 (MAC in UDP)的报文封装方式，实现基于IP overlay的虚拟局域网。 传统网络存在的一些问题1，传统的二层网络，交换机通过mac地址进行数据转发，mac地址一多会造成交换机的转发速率变慢，同时交换机的mac地址表的大小是有限制的，在云计算中，mac表容量限制了虚拟机的数量。2，VLAN的vlan id是一个12bit，最大只支持4096个vlan，这在云计算中是远远不够的。 vxlan的提出很好的解决了上述问题1，vxlan采用MAC in UDP的方式将vm主机的数据报文封装在UDP中，并使用物理网络VTEP的IP和MAC地址在外层包头封装进行数据传输，对外表现为VTEP之间的mac地址，极大的降低了交换机mac地址使用。 2，VXLAN报文中拥有一个24bit vni段，vxlan隔离不同租户就是通过vni来进行，一个vni表示一个租户，不同vni之间二层不能直接通信，但可以通过vxlan三层网关进行通信。即使多个终端用户属于同一个vni也属于同一个租户。 VXLAN的优点1，基于ip的overlay网络，仅需要边界VTEP设备间可通信2，ip overlay TTL避免环路。3，overlay+vni构建虚拟网络支持多达16M的虚拟网络，充分满足多租户的需求4，接入交换机只需要学习物理服务器的mac地址，不需要学习每台VM的mac地址减轻了交换机的负担 概念NVE(Network virtrualization Edge网络虚拟边缘节点）是实现网络虚拟化功能的实体。VTEP(vxlan tunnel end point vxlan 隧道端点）vxlan网络中的NVE以VTEP为标识；每个NVE至少得有一个VTEP；VTEP使用NVE的IP地址表示；两个VTEP之间可以确立一条VXLAN隧道，VTEP间的这条VXLAN隧道将两个NVE之间的所有的VNI所公用。VTEP可以由专有硬件来实现，也可以使用纯软件实现，硬件的实现是通过一些SDN交换机，软件的实现主要有 1，带vxlan内核模块的Linux2，openvswitch vxlan报文格式 端口默认使用4789端口报文中源IP为报文的虚拟机所属的VTEP的IP地址，目的IP为目的虚拟机所属的VTEP的IP，目标ip地址可以为单播地址也可以为组播地址 vxlan网络架构图 vxlan使用 MAC IN UDP的方式来延伸二层网络，是一种扩展大二层网络的隧道封装技术。 NVE负责将vm的报文封装，建立隧道网络。如服务器上的openvswitch就是一个NVE。 VXLAN报文的转发一种是BUM(broadcast&amp;unknown-unicast&amp;multicast)报文转发，一种是已知的单播报文转发 BUM报文转发：采用头端复制的方式(vm上层VTEP接口收到BUM报文后本地VTEP通过控制平面获取同一VNI的VTEP列表，将收到的BUM报文根据VTEP列表进行复制并发送给属于同一VNI的所有VTEP)并进行报文封装。 1，switch_1收到来终端A发出的报文后，提取报文信息，并判断报文的目的MAC地址是否为BUM MAC. 是，在对应的二层广播域内广播，并跳转2。 不是，走已知单播报文转发流程。2，switch_1根据报文中的VNI信息获取同一VNI的VTEP列表获取对应的二层广播域，进行vxlan封装，基于出端口和vxlan封装信息封装vxlan头和外层IP信息，进行二层广播。3，switch_2和switch_3上VTEP收到VXLAN报文后，根据UDP目的端口号、源和目的IP地址VNI判断VXLAN报文的合法有效性，依据VNI获取对应的二层广播域，然后进行VXLAN解封装，获取内层二层报文，判断报文的目的MAC是否为BUM MAC 是，在对应的二层广播域内非VXLAN侧进行广播处理。 不是，再判断是否是本机的MAC地址。 是，本机MAC，上送主机处理。 不是，在对应的二层广播域内查找出接口和封装信息并跳转4.4，switch_2和switch_3根据查找到的出接口和封装信息，为报文添加VLAN tag，转发给对应的终端B&#x2F;C. 已知单播报文转发 1，switch_1收到来自终端A的报文，根据报文信息接入端口和VLAN信息获取对应的二层广播域，并判断报文的目的MAC是否为已知的单播MAC 是，在判断是否为本机MAC. 是，上送主机处理。 不是，在对应的二层广播域内查找出接口和封装信息，并跳装到2. 2， switch_1上VTEP根据查找到的出接口和封装信息进行VXLAN封装和报文转发。3，switch_2上VTEP收到VXLAN报文后，根据UDP目的端口号，源&#x2F;目的IP地址，VNI判断VXLAN报文的合法有效性，依据VNI获取对应的二层广播域，然后进行VXLAN解封装、获取内层二层报文，判断报文的目的MAC是否为已知单播报文MAC。 是，在对应的二层广播域内查找出接口和封装信息，并跳转到4. 不是，在判断是否是本机的mac。 是，上送主机处理。 不是，走BUM报文转发流程。4，switch_2根据查找的出接口和封装信息，为报文添加VLAN tag，转发给对应的终端B。 为了防止不同vm之间通信发送arp广播引发广播风暴，vtep有一个arp proxy的功能也就是说在请求这个VTEP节点上的虚拟机的mac地址都是由VTEP的mac地址应答。也就是l2population。 vxlan网关vxlan下vm之间的通信方式有3种，同VNI下不同VM，不同VNI下跨网访问，vxlan和非vxlan之间访问。 vxlan网关分为二层网关、三层网关。二层网关：主要用于解决同VNI下不同VM之间通信，一般为vetp IP三层网关：用于解决不同VNI下跨网访问，和vxlan和非vxlan之间访问。 通俗理解就是在一台实体服务器上可以虚拟出一个交换机来，这个交换机就是VSwitch，而这个VSwitch下挂的不再是实体服务器，而是一个个VM，一个VM其实就是一个租户租用的服务器，不同租户之间肯定是不能互访的，要不然租户数据的安全性如何保障，这个隔离就是靠的VNI这个ID，其实这个你可以向下VLAN是如何隔离的，目的就是为了隔离租户。我一个租户有2个VM的话，那么我这2个之间应该可以互访吧。所以说基于VNI定义的租户，而非基于VM。内部的结构说清楚了再来说上行如何访问，在一个L2交换机你要跨网访问必然要经过网关，这个网关的IP地址就是VTEP IP，在网络上有个概念叫arp-proxy，一般用途是为了保护内部私有网络，外界的所有应答都有网关来代替回答（可以理解为门卫）。在这里外界只需要你的VTEP IP即可，对端报文到达VTEP这个网关后自己在内部走L2进行转发。因此VXLAN报文中的目的IP就是对端的网关（VTEP IP），而源地址自然也是自己的网关（VTEP IP）。而对于不同leaf上的同一VNI的VM来说，他们的VTEP IP肯定要配置相同，想下同一vlan下的服务器的网关是如何配置的就明白了 http://www.cnblogs.com/hbgzy/p/5279269.htmlhttp://blog.csdn.net/zztflyer/article/details/51883523 vxlan实验环境操作系统：centos7.2内核版本：3.10openvswitch版本：2.5 在两台虚拟机上安装好ovs，并启动这个实验目的，就是HOST 1的 br1，没有根物理网卡绑定，但可以通过建利vxlan隧道与HOST2的br1通信。 配置host1创建两个网桥br0、br1ovs-vsctl add-br br0ovs-vsctl add-br br1将eth0挂载到br0上ovs-vsctl add-port br0 eth0 将eth0的ip分配到br0上ifconfig eth0 0 &amp;&amp; ifconfig br0 192.168.1.2&#x2F;24 给br1分配一个ipifconfig br1 192.168.2.2&#x2F;24 配置host2创建两个网桥br0、br1ovs-vsctl add-br br0ovs-vsctl add-br br1将eth0挂载到br0上 ovs-vsctl add-port br0 eth0 将eth0的ip分配到br0上ifconfig eth0 0 &amp;&amp; ifconfig br0 192.168.1.3&#x2F;24 给br1分配一个ipifconfig br1 192.168.2.3&#x2F;24 此时在hots1上ping host2上的br0的ip是可以通的 ping br1的ip是pint不通的 通过建立vxlan隧道来实现br1之间通信在host1上执行给br1上增加一个vxlan0接口类型为vxlan，远程ip为 host2上的br0 ip， vni为100在host2上执行远端ip为host1上br0的ip 此时在host1上ping host2的br1的ip 就可以ping通了 在host2上的 eth0上抓包 有两层报文封装第一层源ip为192.168.2.2 目标ip为192.168.2.3，然后被udp封装，走vxlan隧道，第二层源ip为192.168.1.2 目标ip为192.168.1.3 网络包经过 vxlan interface 到达 eth1 的过程中，Linux vxlan 内核模块会将网络包二层帧封装成 UDP 包，因此，vxlan interface 必须设置适当的 MTU 来限制通过它的网络包的大小（vxlan interface 的 MTU 需要比它所绑定的物理网卡的 MTU 小 50），否则，封装后的包会被 eth1 丢弃。 VTEP (vxlan 随道端点）vswitch生成br-tun连接各个节点 像这些 为什么vlan只支持4096个因为vid只有12bit 2的12次方&#x3D;4096 为什么用vxlan不用vlan？因为vlan最大只有4096个虚拟化技术的话用vlan运行vm一多，交换机上的mac地址会很多，影响交换机性能。stp算法会产生大量多路路径冗余 vxlan：建立在物理网络上的虚拟以太网vlxan是一种将二层报文用三层协议进行封装的技术，它进行传输的标识是通过VNI vni包含24bit所以vxlan最大支持2的24次方约16m个，会将二层数据包封装成udp报文通过隧道组播传输，一般配置的组播地址224.0.0.1发送arp，udp端口是4789，共50字节封装报头 UDP校验和：一般为0，非0则此包将会被丢弃。 数据包都是通过vtep进行封装传输在 OVS 中, 有几个非常重要的概念： Bridge: Bridge 代表一个以太网交换机（Switch），一个主机中可以创建一个或者多个 Bridge 设备。 Port: 端口与物理交换机的端口概念类似，每个 Port 都隶属于一个 Bridge。 Interface: 连接到 Port 的网络接口设备。在通常情况下，Port 和 Interface 是一对一的关系, 只有在配置 Port 为 bond 模式后，Port 和 Interface 是一对多的关系。 Controller: OpenFlow 控制器。OVS 可以同时接受一个或者多个 OpenFlow 控制器的管理。 datapath: 在 OVS 中，datapath 负责执行数据交换，也就是把从接收端口收到的数据包在流表中进行匹配，并执行匹配到的动作。 Flow table: 每个 datapath 都和一个“flow table”关联，当 datapath 接收到数据之后， OVS 会在flow table 中查找可以匹配的 flow，执行对应的操作, 例如转发数据到另外的端口。 veth-pair：一对接口，一个接口收另外一个接口同时也能收到，用于连接两个Brigge 设置网络接口设备的类型为“internal”。对于 internal 类型的的网络接口，OVS 会同时在 Linux 系统中创建一个可以用来收发数据的模拟网络设备。我们可以为这个网络设备配置 IP 地址、进行数据监听等等。 参考链接http://www.360doc.com/content/16/0227/12/3038654_537760576.shtmlhttp://blog.csdn.net/xjtuse2014/article/details/51376123?locationNum=7http://www.cnblogs.com/hbgzy/p/5279269.htmlhttp://blog.csdn.net/zztflyer/article/details/51883523","categories":[{"name":"Network","slug":"Network","permalink":"http://yoursite.com/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://yoursite.com/tags/Network/"}]},{"title":"kolla 3.0.3部署openstack newton","slug":"killo_newton","date":"2017-02-09T13:45:59.000Z","updated":"2017-02-09T13:45:59.000Z","comments":true,"path":"2017/02/09/killo_newton/","permalink":"http://yoursite.com/2017/02/09/killo_newton/","excerpt":"","text":"kolla是什么？容器化部署openstack，简而言之就是openstack里面每个组件都是用docker封装好了，对应docker的一个image。 容器化好处是什么?通过docker，降低openstack升级的难度，使升级对openstack影响最小，一旦升级失败，直接回滚旧的docker image，升级只需要三步：Pull新版本的容器镜像，停止老版本的容器服务，然后启动新版本容器。回滚也不需要重新安装包了，直接启动老版本容器服务就行，非常方便。对比过目前生产环境主流的部署方式，fuel、puppet、ROD， 我个人认为容器化部署将是未来的趋势。kolla底层通过ansoble去启动配置 已经封装好了的docker image。 缺点：要熟悉kolla，不仅仅是要熟悉openstack那么简单了，还要熟悉docker、和ansiable。 环境cpu：8核内存：16G根分区大小：100G至少需要2个网络接口（一个管理口，一个外网口）管理地址 192.168.122.77haproxy vip：192.168.122.76 安装时注意审查软件版本，我这里是安装newton版对应的是koll 3 ，2月24日发布的ocata版对应的是kolla 4 搭建docker本地镜像库 安装epel源和python-pip 安装dockercurl -sSL https://get.docker.io | bash 查看docker版本，确认docker是否安装成功。 修改docker参数，如果没有修改的话，会造成部署 neutron-dhcp-agent container 和访问APIError&#x2F;HTTPError mkdir -p &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service.d vi &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service.d&#x2F;koll.conf 12345[Service]MountFlags=sharedEnvironmentFile=/etc/sysconfig/dockerExecStart=ExecStart=/usr/bin/docker daemon $other_args 保存退出修改&#x2F;etc&#x2F;sysconfig&#x2F;docker参数并添加下面一行，目的是为了配置本地镜像仓库。other_args=&quot;--insecure-registry 192.168.122.77:4000&quot; 重启docker进程systemctl daemon-reloadsystemctl enable dockersystemctl restart docker 制做docker本地镜像仓库链接：http://pan.baidu.com/s/1gf5LTV9 密码：aysu下载解压将已经build好的openstack镜像解压到本地tar -xvf kolla-image-newton-latest.tgz 加载下载好的docker registry，docker搭建私有镜像仓库使用registry这个软件docker load &lt; .&#x2F;registry-server.tar将镜像文件放到&#x2F;home&#x2F;下 docker run -d -p 4000:5000 --restart=always -e REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY=/tmp/registry -v /home/tmp/registry/:/tmp/registry --name registry registry:2 测试是否搭建本地镜像仓库成功curl -XGET http://127.0.0.1:4000/v2/_catalog #正常情况会输出很多image name。仓库里面存在的镜像查看该镜像的tagcurl -XGET http://127.0.0.1:4000/v2/kolla/centos-binary-nova-compute/tags/list 3.0.3是版本号查看仓库数据 安装ansibleyum install ansibleclone newton分支git clone -b stable/newton https://github.com/openstack/kollacd kollacp -r etc&#x2F;kolla &#x2F;etc&#x2F; 安装kollapip install kolla vim &#x2F;etc&#x2F;kolla&#x2F;globals.yml 12345678openstack_release: &quot;3.0.3&quot; #上面搭建的本地仓库里面curl出来的tag号，写错了的话会导致找不到image。network_interface: &quot;ens3&quot;kolla_internal_vip_address: &quot;192.168.123.76&quot;#配置给高可用的vipneutron_external_interface: &quot;eth0&quot;docker_registry: &quot;192.168.122.77:4000&quot; #本地仓库地址docker_namespace: &quot;kolla&quot; 生成密码文件 kolla-genpwd路径&#x2F;etc&#x2F;kolla&#x2F;password.yml 执行部署cd &#x2F;root&#x2F;kolla&#x2F;tools&#x2F;.&#x2F;kolla-ansible deploy -i &#x2F;root&#x2F;kolla&#x2F;ansible&#x2F;inventory&#x2F;all-in-one这里需要注意的是，我是部署all-in-one，如果需要部署多节点的用multinode 修改一下里面的hostname部署完成查看容器pip install python-openstackclientpip install python-neutronclient 输入以下命令生成一个环境变量文件kolla-ansible post-deploy 文件路径为&#x2F;etc&#x2F;kolla&#x2F;admin-openrc.sh cp &#x2F;etc&#x2F;kolla&#x2F;admin-openrc.sh &#x2F;root&#x2F; source &#x2F;root&#x2F;admin-openrc.sh 查看nova service查看neutron agent 打开控制台访问 帐户admin，密码通过刚刚生成的admin-openrc.sh获取 执行cd &#x2F;usr&#x2F;share&#x2F;kolla .&#x2F;init-runonce #一个测试脚本，自动下载镜像，上传，创建网络，创建路由器…… 最后创建虚拟机 需要注意的是如果是在虚拟机中测试kolla需要在宿主机上修改nova-compute的配置文件 为virt_type&#x3D;qemu不然默认用的是kvm，会造成创建云主机失败。vim &#x2F;etc&#x2F;kolla&#x2F;nova-compute&#x2F;nova.conf 重启这个容器。docker restart nova_compute创建云主机测试 最后 Docker使用heka来展现收集到的日志信息。这些openstack容器的log都在heka 容器内展现默认是没有安装cinder和其他一些软件的，如果需要安装在部署时可以修改&#x2F;etc&#x2F;kolla&#x2F;globals.yml 参考链接http://blog.csdn.net/u011211976/article/details/52085891http://docs.openstack.org/developer/kolla-ansible/quickstart.htmlhttp://geek.csdn.net/news/detail/60805?utm_source=tuicool&amp;utm_medium=referral","categories":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"openstack N版对接ceph jewel版","slug":"openstac&ceph","date":"2017-02-06T06:24:44.000Z","updated":"2017-02-06T06:24:44.000Z","comments":true,"path":"2017/02/06/openstac&ceph/","permalink":"http://yoursite.com/2017/02/06/openstac&ceph/","excerpt":"","text":"环境网络配置： public_network cluster_network 控制节点&#x2F;ceph-mon 192.168.4.6 192.168.3.5 ceph-osd1 192.168.4.12 192.168.3.9 ceph-osd2 192.168.4.8 192.168.3.7 硬件配置：ceph-moncpu：4核内存：4G ceph-osd1cpu：4核内存：4G硬盘：3块100G磁盘 ceph-osd2cpu：4核内存：4G硬盘：3块100G磁盘将selinux和firewalld关闭，或配置防火墙规则 配置软件源：163 yum源wget -O /etc/yum.repo/ CentOS7-Base-163.repo http://mirrors.163.com/.help/CentOS7-Base-163.repo 配置epel源wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo 增加ceph源vim &#x2F;etc&#x2F;yum.repos.d&#x2F;ceph.repo 123456789[ceph]name=cephbaseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/x86_64/gpgcheck=0[ceph-noarch]name=cephnoarchbaseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/noarch/gpgcheck=0 清理yum缓存yum clean all创建缓存yum makecache所有节点安装cephyum install ceph –y 开始部署在部署节点安装我这里的是openstack的controller节点安装ceph-deploy，手动部署参考上一篇文章 http://www.bladewan.com/2017/01/01/manual_ceph/#more yum install ceph-deploy –y 在部署节点创建部署目录mkdir &#x2F;etc&#x2F;cephcd &#x2F;etc&#x2F;ceph&#x2F;ceph-deploy new control-node1没有erro继续向下 此时目录下有ceph.conf、ceph-deploy-ceph.log、ceph.mon.keyring 修改ceph.conf添加public_network和cluster_network，同时增加允许时钟偏移vim &#x2F;etc&#x2F;ceph&#x2F;ceph.conf 开始monitor在controller上执行ceph-deploy mon create-initial …… 部署目录多了以下文件 查看ceph状态ceph -s此时ceph状态应该是ERROR的health HEALTH_ERRno osdsMonitor clock skew detected 部署osd ceph-deploy --overwrite-conf osd prepare ceph-osd1:/dev/vdb /dev/vdc /dev/vdd ceph-osd2:/dev/vdb /dev/vdbc /dev/vdd --zap-disk部署完后查看ceph状态 查看osd tree 推送配置ceph-deploy --overwrite-conf config push ceph-osd1 ceph-osd2 重启ceph进程mon节点systemctl restart &#x63;&#101;&#112;&#x68;&#x2d;&#x6d;&#x6f;&#110;&#64;&#99;&#111;&#110;&#116;&#114;&#x6f;&#x6c;&#45;&#110;&#111;&#x64;&#x65;&#x31;&#x2e;&#x73;&#x65;&#x72;&#118;&#x69;&#x63;&#x65; osd节点重启systemctl restart ceph-osd@x 查看public_network和cluster_network配置是否生效 根openstack对接Ceph创建pool根据公式计算出每个pool的合适pg数 PG NumberPG和PGP数量一定要根据OSD的数量进行调整，计算公式如下，但是最后算出的结果一定要接近或者等于一个2的指数。Total PGs = (Total_number_of_OSD * 100) / max_replication_count 例：有6个osd，2副本，3个pool Total PGs &#x3D;6*100&#x2F;2&#x3D;300每个pool 的PG&#x3D;300&#x2F;3&#x3D;100，那么创建pool的时候就指定pg为128ceph osd pool create pool_name 128ceph osd pool create pool_name 128 创建3个poolceph osd pool create volumes 128ceph osd pool create images 128ceph osd pool create vms 128 创建nova、cinder、glance、backup用户并授权 12345ceph auth get-or-create client.cinder mon &#x27;allow r&#x27; osd &#x27;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rwx pool=images&#x27;ceph auth get-or-create client.glance mon &#x27;allow r&#x27; osd &#x27;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rwx pool=images&#x27;ceph auth get-or-create client.nova mon &#x27;allow r&#x27; osd &#x27;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rwx pool=images&#x27; 生成keyring文件控制节点 ceph auth get-or-create client.cinder | tee &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyringceph auth get-or-create client.glance | tee &#x2F;etc&#x2F;ceph&#x2F;ceph.client.glance.keyring 修改文件属组chown cinder:cinder &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyringchown glance:glance &#x2F;etc&#x2F;ceph&#x2F;ceph.client.glance.keyring 计算节点ceph auth get-or-create client.cinder |tee &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyringceph auth get-or-create client.nova |tee &#x2F;etc&#x2F;ceph&#x2F;ceph.client.nova.keyringceph auth get-or-create client.glance |tee &#x2F;etc&#x2F;ceph&#x2F;ceph.client.glance.keyring 修改文件属组chown cinder:cinder &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyringchown nova:nova &#x2F;etc&#x2F;ceph&#x2F;ceph.client.nova.keyring 在计算节点上生成uuidgen（所有计算节点用一个就可以）uuidgenf77169a0-7d56-4fc3-a436-35298081f9f9 创建secret.xmlvim secret.xml 123456&lt;secret ephemeral=&#x27;no&#x27; private=&#x27;no&#x27;&gt; &lt;uuid&gt;f77169a0-7d56-4fc3-a436-35298081f9f9&lt;/uuid&gt; &lt;usage type=&#x27;ceph&#x27;&gt; &lt;name&gt;client.nova secret&lt;/name&gt; &lt;/usage&gt;&lt;/secret&gt; 导出nova的keyring ceph auth get-key client.nova | tee client.nova.key virsh secret-define –file secret.xmlvirsh secret-set-value –secret f77169a0-7d56-4fc3-a436-35298081f9f9 –base64 $(cat client.nova.key ) 查看secret-value 另外一台计算节点一样 修改openstack组件配置 glancecp &#x2F;etc&#x2F;glance&#x2F;glance-api.conf &#x2F;etc&#x2F;glance&#x2F;glance-api.conf.bakvim &#x2F;etc&#x2F;glance&#x2F;glance-api.conf 1234567891011[DEFAULT]...show_image_direct_url = True...[glance_store]stores=glance.store.rbd.Storedefault_store = rbdrbd_store_pool = imagesrbd_store_user = glancerbd_store_ceph_conf = /etc/ceph/ceph.confrbd_store_chunk_size = 8 重启glance-api和glance-registrysystemctl restart openstack-glance-apisystemctl restart openstack-glance-registry cindercp &#x2F;etc&#x2F;cinder&#x2F;cinder.conf &#x2F;etc&#x2F;cinder&#x2F;cinder.conf.bakvim &#x2F;etc&#x2F;cinder&#x2F;cinder.conf 12345678910111213enabled_backends = rbd[rbd]volume_driver = cinder.volume.drivers.rbd.RBDDriverrbd_pool = volumesrbd_ceph_conf = /etc/ceph/ceph.confrbd_flatten_volume_from_snapshot = falserbd_max_clone_depth = 5rbd_store_chunk_size = 4rados_connect_timeout = -1glance_api_version = 1rbd_user = cinderrbd_secret_uuid = f77169a0-7d56-4fc3-a436-35298081f9f9volume_backend_name = rbd 重启cinder-api、cinder-schedule、cinder-volume systemctl restart openstack-cinder-apisystemctl restart openstack-cinder-volumesystemctl restart openstack-cinder-scheduler nova修改nova-computecp &#x2F;etc&#x2F;nova&#x2F;nova.conf &#x2F;etc&#x2F;nova&#x2F;nova.conf.bak修改nova.conf添加如下配置 12345678[libvirt]virt_type = qemuimages_type = rbdimages_rbd_pool = vmsimages_rbd_ceph_conf = /etc/ceph/ceph.confrbd_user = novarbd_secret_uuid = f77169a0-7d56-4fc3-a436-35298081f9f9live_migration_flag=&quot;VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE,VIR_MIGRATE_PERSIST_DEST,VIR_MIGRATE_TUNNELLED 重启nova-computesystemctl restart openstack-nova-compute 测试glance上传镜像，在ceph pool中查看是否存在openstack image create &quot;cirros&quot; --file cirros-0.3.4-x86_64-disk.img --disk-format qcow2 --container-format bare –public 存在说明对接正常 cinder在控制台创建个云硬盘，创建成功后在ceph的volumes pool池中可以看见刚刚创建的云硬盘说明创建成功 nova在控制台创建个云主记，创建成功后在ceph的vm pool池中可以看见刚刚创建的云主机说明创建成功","categories":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/categories/openstack/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/tags/openstack/"}]},{"title":"手动安装ceph","slug":"manual_ceph","date":"2017-01-01T01:50:44.000Z","updated":"2017-01-01T11:50:43.000Z","comments":true,"path":"2017/01/01/manual_ceph/","permalink":"http://yoursite.com/2017/01/01/manual_ceph/","excerpt":"","text":"环镜操作系统centos6.5网络规划Cluster_net：192.168.20.0&#x2F;24Public_net：192.168.2.0&#x2F;24 角色 Public_net Cluster_net Ceph-mon 192.168.2.4 192.168.20.2 Ceph-mon2 192.168.2.5 192.168.20.5 Ceph-mon3 192.168.2.6 192.168.20.6 Ceph-osd1 192.168.2.7 192.168.20.7 Ceph-osd2 192.168.2.8 192.168.20.8 Ceph-osd3 192.168.2.9 192.168.20.9 绑定hosts 123456192.168.2.4 ceph-mon.novalocal mon192.168.2.5 ceph-mon2.novalocal mon2192.168.2.6 ceph-mon3.novalocal mon3192.168.2.7 ceph-osd1.novalocal osd1192.168.2.8 ceph-osd2.novalocal osd2192.168.2.9 ceph-osd3.novalocal osd3 osd数规划每台osd节点挂载3块SSD硬盘 配置yum源根据操作系统版本任意调整(所有节点)vim &#x2F;etc&#x2F;yum.repos.d&#x2F;ceph.repo 12345678910[ceph]name=cephbaseurl=http://mirrors.aliyun.com/ceph/rpm-hammer/el6/x86_64/gpgcheck=0enablde=1[ceph-noarch]name=cephnoarchbaseurl=http://mirrors.aliyun.com/ceph/rpm-hammer/el6/x86_64/gpgcheck=0enablde=1 yum cleanyum makecache 在第一台mon节点安装pssh批量执行操作yum install pssh创建hosts（会被批量执行的主机）vim &#x2F;root&#x2F;hosts.txt 12345192.168.2.5192.168.2.6192.168.2.7192.168.2.8192.168.2.9 先进行测试 安装软件（所有节点）pssh -P -h hosts.txt yum install ceph –y同步hostspscp.pssh -h &#x2F;root&#x2F;hosts.txt &#x2F;etc&#x2F;hosts &#x2F;etc&#x2F; 都success进行下一步 配置ceph-mon创建集群fsiduuidgenfdc3d06b-7e05-44a8-b982-e8e04e4156db创建&#x2F;etc&#x2F;ceph&#x2F;ceph.conf将fsid写入配置文件[global]fsid &#x3D; fdc3d06b-7e05-44a8-b982-e8e04e4156db 将ceph-mon写入配置文件(有多个mon，用逗号隔开)mon_initial_members &#x3D; mon,mon2,mon3将mon节点ip写入ceph配置文件mon_host &#x3D; 192.168.2.4,192.168.2.5,192.168.2.6为集群创建mon密钥ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon &#39;allow *&#39; 生成管理员密钥ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --set-uid=0 --cap mon &#39;allow *&#39; --cap osd &#39;allow *&#39; --cap mds &#39;allow&#39; 将client.admin密钥加入到ceph.mon.keyringceph-authtool/tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring 生成mon mapmonmaptool --create --add mon 192.168.2.4 --fsid fdc3d06b-7e05-44a8-b982-e8e04e4156db /tmp/monmap在每个mon节点数据目录分别在mon、mon2、mon3上执行格式为(默认cluster-name为ceph)mkdir /var/lib/ceph/mon/&#123;cluster-name&#125;-&#123;hostname&#125;如mon为mkdir /var/lib/ceph/mon/ceph-monmon初始化(-i后接hostname)ceph-mon --mkfs -i mon --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring ceph.conf增加如下配置 1234public network = 192.168.2.0/24auth cluster required = cephxauth service required = cephxauth client required = cephx 创建两个空文件touch /var/lib/ceph/mon/ceph-mon/donetouch /var/lib/ceph/mon/ceph-mon/sysvinit启动第一个ceph-mon/etc/init.d/ceph status mon.mon 部署第二个mon将keyring复制到mon2scp &#x2F;tmp&#x2F;ceph.mon.keyring mon2:&#x2F;tmp&#x2F;在mon2节点上建立一个&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-mon2目录mkdir –p &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-mon2&#x2F;在mon2节点上初始化mon节点ceph-mon --mkfs -i mon2 --keyring /tmp/ceph.mon.keyring为了防止重新被安装，初始化一个空的done文件touch /var/lib/ceph/mon/ceph-mon2/donetouch /var/lib/ceph/mon/ceph-mon2/sysvinit&#x2F;etc&#x2F;init.d&#x2F;ceph start mon.mon2检查进程 第三个mon同上完后检查 发现有时种偏移问题，默认ceph是0.05s，为了方便同步直接把时钟偏移设置成0.5s修改ceph配置文件增加两条配置 1234[global]mon osd down out interval = 900 #设置osd节点down后900s，把此osd节点逐出ceph集群，把之前映射到此节点的数据映射到其他节点。 [mon] mon clock drift allowed = .50 同步配置 pscp.pssh -h hosts.txt &#x2F;etc&#x2F;ceph&#x2F;ceph.conf &#x2F;etc&#x2F;ceph&#x2F;重启进程pssh -h &#x2F;root&#x2F;hosts.txt &#x2F;etc&#x2F;init.d&#x2F;ceph restart 配置osd节点将keyring同步到osd节点pscp.pssh -h &#x2F;root&#x2F;hosts.txt &#x2F;etc&#x2F;ceph&#x2F;ceph.client.admin.keyring &#x2F;etc&#x2F;ceph&#x2F; 为osd分配uuid(我每台osd节点有3个osd所以创建3个uuid)uuidgen19ebc47d-9b29-4cf3-9720-b62896ce6f33uuidgen1721ce0b-7f65-43ef-9dfc-c49e6210d375uuidgenf5e0f54b-2ee3-41df-bf25-ad37371ab6ce 创建3个osd 123ceph osd create 19ebc47d-9b29-4cf3-9720-b62896ce6f33ceph osd create 1721ce0b-7f65-43ef-9dfc-c49e6210d375ceph osd create f5e0f54b-2ee3-41df-bf25-ad37371ab6ce 创建数据存储目录 1234567mkdir /var/lib/ceph/osd/&#123;cluster-name&#125;-&#123;osd-number&#125;mkdir /var/lib/ceph/osd/ceph-0mkdir /var/lib/ceph/osd/ceph-1mkdir /var/lib/ceph/osd/ceph-2mkfs.xfs /dev/vdbmkfs.xfs /dev/vdcmkfs.xfs /dev/vdd 挂载 123mount -o defaults,_netdev /dev/vdb /var/lib/ceph/osd/ceph-0mount -o defaults,_netdev /dev/vdc /var/lib/ceph/osd/ceph-1mount -o defaults,_netdev /dev/vdd /var/lib/ceph/osd/ceph-2 修改fstable 初始化osd目录 123ceph-osd -i 0 --mkfs --mkkey --osd-uuid 19ebc47d-9b29-4cf3-9720-b62896ce6f33ceph-osd -i 1 --mkfs --mkkey --osd-uuid 1721ce0b-7f65-43ef-9dfc-c49e6210d375ceph-osd -i 2 --mkfs --mkkey --osd-uuid f5e0f54b-2ee3-41df-bf25-ad37371ab6ce 注册此osd密钥 123ceph auth add osd.0 osd &#x27;allow *&#x27; mon &#x27;allow profile osd&#x27; -i /var/lib/ceph/osd/ceph-0/keyringceph auth add osd.1 osd &#x27;allow *&#x27; mon &#x27;allow profile osd&#x27; -i /var/lib/ceph/osd/ceph-1/keyringceph auth add osd.2 osd &#x27;allow *&#x27; mon &#x27;allow profile osd&#x27; -i /var/lib/ceph/osd/ceph-2/keyring 加入crush map ceph osd crush add-bucket ceph-osd1 hostceph osd crush move ceph-osd1 root&#x3D;default 设置权重 123ceph osd crush add osd.0 1.0 host=ceph-osd1 ceph osd crush add osd.1 1.0 host=ceph-osd1ceph osd crush add osd.2 1.0 host=ceph-osd1 要已守护进程开机启动，须创建一个空文件 123touch /var/lib/ceph/osd/ceph-0/sysvinittouch /var/lib/ceph/osd/ceph-1/sysvinittouch /var/lib/ceph/osd/ceph-2/sysvinit 启动osd进程 123/etc/init.d/ceph start osd.0/etc/init.d/ceph start osd.1/etc/init.d/ceph start osd.2 查看osd 树按上述方法配置osd2、osd33台节点添加完毕 这里有个warn，是pool的pg问题，我们重新计算，修改。查看我们现在拥有的poolceph osd lspools0 rbd, 查看默认的rbd pool的pg 1234567ceph osd pool get rbd pg_numpg_num: 64ceph osd pool get rbd pg_numpg_num: 64修改为128ceph osd pool set rbd pg_num 128ceph osd pool set rbd pgp_num 128 再查看 修改ceph.conf默认数据存两份，默认pg为128同步配置重启ceph 配置ceph-radosgw直接将第一个mon节点当radosgw，ceph在0.80及以上版本可以直接使用civeweb来构建对象网关，可以不需要使用apache或nginx+fastcgi了，所以我这里用civetweb。 安装软件yum install ceph-radosgw没有www-data用户创建useradd –r –s &#x2F;sbin&#x2F;nologin www-data创建gateway keyring并授权 ceph auth get-or-create client.radosgw.gateway osd &#39;allow rwx&#39; mon &#39;allow rwx&#39; -o /etc/ceph/keyring.radosgw.gateway 编辑ceph.conf文件增加下面内容 1234567891011[client.radosgw.gateway]keyring = /etc/ceph/keyring.radosgw.gatewayrgw_socket_path = /tmp/radosgw.sockrgw_frontends= &quot;civetweb port=7480&quot;host = ceph-monrgw_dns_name = *.domain.tldrgw_print_continue = Truergw_data = /var/lib/ceph/radosgwuser = www-datargw s3 auth use keystone = truelog file =/var/log/radosgw/client.radosgw.gateway.log 重启进程&#x2F;etc&#x2F;init.d&#x2F;ceph-radosgw restart查看是否启动成功 新建用户radosgw-admin user create –uid&#x3D;”testuser” –display-name&#x3D;”First User”得到如下结果","categories":[{"name":"分布式存储","slug":"分布式存储","permalink":"http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"分布式存储","slug":"分布式存储","permalink":"http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"}]},{"title":"Mariadb galera","slug":"mariadb-base","date":"2016-12-25T11:50:44.000Z","updated":"2016-12-25T11:50:43.000Z","comments":true,"path":"2016/12/25/mariadb-base/","permalink":"http://yoursite.com/2016/12/25/mariadb-base/","excerpt":"","text":"传统的数据同步方式传统的mysql数据库同步是使用二进制日志进行主从同步也就是semi-sync，这种同步也只是一种半同步，并不是完全的实时同步，在mysql 5.7.17中推出了MySQL Group Replication，这种实现方式与Galera cluster基本完全一样，不过MGR比Galera的优势在于 Mysql官方出品，能得到后续技术支持，Galera是芬兰的一家叫codership的公司开发的。 MGR使用Paxos协议，性能比Galera好，并且性能稳定，Galera实际只能使用三个节点，有性能和稳定性问题。 Galera目前只支持linux，MGR支持多个平台。 mysql 5.6下semi-sync数据同步方式master的dump进程需要发送binlog日志到slave，master要等待至少一个slave通知，slave将已经接收到传过来的events并写入relay log，salve发送ack信息到master，这个事务才能提交。 5.6版semi-sync的缺陷，dump thread要承但两份任务，传送binlog给slave还要等slave的返回，并且这两个任务是串行的，也就是说，dump thread要先传送binlog给slave，并还要slave返回才能传送下一个events事务，这样的dump thread成为性能瓶径。 mysql 5.7下semi-sync数据同步方式5.7.4为解决上述的问题，在master端增加了ack进程。这样事务写发送binlog与接收ack可以并行进行，提高semi-sync的效率。 GaleraGalera cluster是可以实现mariadb多主集群的软件，它目前只能用于linux和只支持XtraDB和Innodb存储引擎 ，mariadb和perconna提供了原生的Galera cluster的支持，所以可以直接使用Galera cluster，mysql要使用Galera cluster需要使用Galera cluster提供的插件。传统主从只能有一个数据库进行写服务，Galera集群，每个节点都可读可写，在Galera上层部署负载均衡软件如lvs和haproxy进行流量分担是常用做法。 原理各个节点的数据同步由wsrep接口实现。client发起一个commit命令时，所有本事务内对数据库的操作和primary_key都会打包写入到write-set，write-set随后会复制到其他节点，各个节点接收后，会根据write-set传送的primary key进行检验，检查是否与本地事务种read-write或write-write锁冲突，冲突则回滚，没有冲突就执行，完成后返回success。如果其他节点没有执行成功则存放在队列种，稍后会重新尝式。 Galera集群的优点 支持多个节点的数据写入，能保证数据的强一致性。 同步复制，各个节点无延迟，不会因为一台宕机导致数据丢失。 故障节点将自动从集群中移除 基于行级的并行复制 缺点 只支持Innodb存储引擎 只支持linux平台， 集群写入的tps由最弱节点限制，如果一个节点变慢，整的集群就是缓慢的，所以一般情况下部署，要求统一的硬件配置。 会因为网络抖动造成性能和稳定性问题。 参考链接:http://www.oschina.net/news/79983/galera-will-die-mysql-group-replication-realeasehttp://www.itbaofeng.com/?p=236","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"Mariadb galera集群搭建","slug":"mariadb-galera-2","date":"2016-12-22T15:35:44.000Z","updated":"2016-12-22T15:50:43.000Z","comments":true,"path":"2016/12/22/mariadb-galera-2/","permalink":"http://yoursite.com/2016/12/22/mariadb-galera-2/","excerpt":"","text":"环境配置准备3台服务器 192.168.1.16 mariadb-1.novalocal mariadb-1192.168.1.19 mariadb-2.novalocal mariadb-2192.168.1.18 mariadb-3.novalocal mariadb-3 配置repo文件 vim &#x2F;etc&#x2F;yum.repos.d&#x2F;MariaDB.repo 12345[mariadb]name = MariaDBbaseurl = http://yum.mariadb.org/10.1/centos7-amd64gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDBgpgcheck=1 yum makecache 禁用防火墙和selinux 如果要使用防火墙添加允许3306、和4567 端口规则。 安装Mariadb和galera cluster（三个节点都执行） yum install MariaDB-server MariaDB-client galera 启动mariadb systemctl start mariadb 一些初始化安全配置 &#x2F;usr&#x2F;bin&#x2F;mysql_secure_installation 关闭数据库systemctl stop mariadb 修改mariadb-1上的&#x2F;etc&#x2F;my.cnf.d&#x2F;server.cnf文件如下 1234567891011121314[galera]wsrep_provider = /usr/lib64/galera/libgalera_smm.sowsrep_cluster_address = &quot;gcomm://192.168.1.16,192.168.1.18,192.168.1.19&quot;wsrep_node_name = mariadb-1wsrep_node_address=192.168.1.16wsrep_on=ONbinlog_format=ROWdefault_storage_engine=InnoDBinnodb_autoinc_lock_mode=2#bind-address=0.0.0.0wsrep_slave_threads=1innodb_flush_log_at_trx_commit=0innodb_buffer_pool_size=122Mwsrep_sst_method=rsync 将此文件复制到mariadb-2、mariadb-3，注意要把 wsrep_node_name 和 wsrep_node_address 改成相应节点的 hostname 和 ip。 启动 MariaDB Galera Cluster 服务 &#x2F;usr&#x2F;sbin&#x2F;mysqld –wsrep-new-cluster –user&#x3D;root &amp;–wsrep-new-cluster 这个参数只能在初始化集群使用，且只能在一个节点使用。 观察日志： [root@node4 ~]# tail -f &#x2F;var&#x2F;log&#x2F;message 123150701 19:54:17 [Note] WSREP: wsrep_load(): loading provider library &#x27;none&#x27;150701 19:54:17 [Note] /usr/libexec/mysqld: ready for connections.Version: &#x27;5.5.40-MariaDB-wsrep&#x27; socket: &#x27;/var/lib/mysql/mysql.sock&#x27; port: 3306 MariaDB Server, wsrep_25.11.r4026 出现 ready for connections ,证明我们启动成功 查看是否启用galera插件连接mariadb,查看是否启用galera插件 目前集群机器数 查看集群状态show status like ‘wsrep%’; 查看连接的主机 另外两个节点mariadb会自动加入集群systemctl start mariadb这时查看galera集群机器数量 已经连接机器的ip 测试在mariadb-1上创建数据库，创建表，插入数据 12345678910111213141516171819202122MariaDB [(none)]&gt; create database test1；MariaDB [test1]&gt; insert into test values(1);Query OK, 1 row affected (0.01 sec)MariaDB [test1]&gt; insert into test values(2);Query OK, 1 row affected (0.01 sec)MariaDB [test1]&gt; insert into test values(3);Query OK, 1 row affected (0.00 sec)MariaDB [test1]&gt; insert into test values(4);Query OK, 1 row affected (0.00 sec)MariaDB [test1]&gt; insert into test values(5);Query OK, 1 row affected (0.03 sec)MariaDB [test1]&gt; insert into test values(6);Query OK, 1 row affected (0.01 sec)MariaDB [test1]&gt; insert into test values(7);Query OK, 1 row affected (0.01 sec) 在另外两台mariadb-2、mariadb-3上可以看见刚刚插入的数据，说明数据同步了。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"win10启用ubuntu子shell","slug":"win10_shell","date":"2016-12-22T15:35:44.000Z","updated":"2016-12-22T15:50:43.000Z","comments":true,"path":"2016/12/22/win10_shell/","permalink":"http://yoursite.com/2016/12/22/win10_shell/","excerpt":"","text":"以前想用linux的shell都得装第三方软件实现，比如git bash等，在win10周年更新版14393直接集成了bash on ubuntu了集成新的 Windows Subsystem for Linux 子系统，这样能直接在 Bash on Ubuntu 环境里编译运行 Linux 程序非常爽。 开启win10开发者模式win10设置 打开控制面板，启用或关闭windows功能 点确定，然后重启然后打开powershell或cmd直接在里面输入bash命令 它会询问你是否安装canonical 分发的ubuntu，输入y，然后等待。这里推荐使用vpn，然后安装完后，设置用户名，输入root用root权限吧。安装完后，然后你打开powershell和cmd输入bash就直接进入ubuntu子系统了 可以看出子bash还是非常给力的是ubuntu 14.04 3.4的内核。以后可以直接在上面跑python脚本了。不需要专门虚拟机。 默认字体蓝色不好看清楚，修改powershell或cmd的背景色即可。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://yoursite.com/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"http://yoursite.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"openstack 本地yum源搭建","slug":"openstack-localyumbuild","date":"2016-12-12T05:15:26.000Z","updated":"2018-11-07T13:05:26.387Z","comments":true,"path":"2016/12/12/openstack-localyumbuild/","permalink":"http://yoursite.com/2016/12/12/openstack-localyumbuild/","excerpt":"","text":"我们在部署openstack时用国外yum源的快，经常会很慢导致等待时间太久，所以建议使用本地yum源安装 这里以newton版centos7平台为例 首先下载官方repoyum install https://rdoproject.org/repos/rdo-release.rpm这时侯&#x2F;etc&#x2F;yum.repos.d里面会产生3个文件 12[root@test yum.repos.d]# ls rdo-qemu-ev.repo rdo-release.repo rdo-testing.repo 我这里打算用http搭建我的本地yum服务器 先安装httpdyum install httpd mkdir &#x2F;var&#x2F;www&#x2F;html&#x2F;newton 待会将同步下来的包放这个目录cd &#x2F;vaw&#x2F;www&#x2F;html&#x2F;newton yum repolist –列出你所有的仓库 前面是repo id不包含x86_64 这里我只需要openstack-newton、和rdo-qemu-ev这两个软件库 先同步openstack-newtonreposync --repoid=openstack-newton 指定要下载的仓库id，会通过网络全部下载到当前目录下载下来。 同步完第一个继续同步第二个 reposync --repoid=rdo-qemu-ev 同步完后这时查看 &#x2F;vaw&#x2F;www&#x2F;html&#x2F;newton里面已经有很多包了，只有软件包，没有repodate清单，所以需要自己重新createrepo来创建清单没有createrepo自己安装，创建软件清单 createrepo /var/www/html/newton/ 然后启动httpd服务，其他机器通过httpd服务来访问yum源 例如控制节点yum源配置vim &#x2F;etc&#x2F;yum.repos.d&#x2F;openstack.repo 123456[openstack]name=openstackbaseurl=http://192.168.4.3/newtonenabled=1gpgcheck=0~ yum makecache 其他节点一样。","categories":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/categories/openstack/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/tags/openstack/"}]},{"title":"openstack newton 安装","slug":"openstack-newton-install","date":"2016-12-12T03:35:44.000Z","updated":"2017-11-14T11:00:22.000Z","comments":true,"path":"2016/12/12/openstack-newton-install/","permalink":"http://yoursite.com/2016/12/12/openstack-newton-install/","excerpt":"","text":"环境配置机器配置：3台8v8G的虚拟机，1台做控制节点2台做融合节点。 网络划分：192.168.122.0&#x2F;24 public网络192.168.3.0&#x2F;24 存储网络192.168.4.0&#x2F;24管理网络、sdn隧道网络 我这里配置了本地源，就不用在手动配置官网源本地源的搭建和配置会在另外一个文档说明。节点网络信息 ： 管理网络和随道网络 存储网络 公网 控制节点 192.168.4.6 192.168.3.5 192.168.122.2 计算节点 192.168.4.7 192.168.3.6 192.168.125.5 计算节点 192.168.4.8 192.168.3.7 192.168.122.6 网络拓扑 安装chrony控制节点向外同步时间，其他节点如计算节点都直接同步控制节点yum install chrony 修改配置文件vim &#x2F;etc&#x2F;chrony.conf添加下面这两条server cn.ntp.org.cn iburstallow 192.168.4.0&#x2F;24 设置开机启动systemctl enable chronysystemctl start chrony 其他节点：yum install chrony 修改配置文件vim &#x2F;etc&#x2F;chrony.conf 添加下面这两条server 192.168.4.6 iburst 设置开机启动systemctl enable chrony 启动进程systemctl start chrony 安装openstack客户端yum install python-openstackclient 安装Mariadb（数据库服务）vim &#x2F;etc&#x2F;my.cnf.d&#x2F;openstack.cnf 1234567[mysqld] bind-address = 192.168.4.6 #填写管理网段ip default-storage-engine = innodb innodb_file_per_table max_connections = 4096collation-server = utf8_general_cicharacter-set-server = utf8 设置开机启动 systemctl enable mariadb 启动Mariadb systemctl start mariadb 安装rabbitmq（用于消息队列）yum install rabbitmq-server 设置开机启动 systemctl enable rabbitmq-server 开启rabbitmq systemctl start rabbitmq-server 创建openstack用户和配置密码 rabbitmqctl add_user openstack 123456 给openstack用户配置读和写权限 rabbitmqctl set_permissions openstack &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; 安装memcache（缓存token）yum install memcached phython-memcached systemctl enable memcached systemctl start memcached 安装keystone（认证服务）连接数据库 [root@control-node1 yum.repos.d]# mysql 创建keystone数据库 create database keystone; 数据库授权 密码自己设置，这里为了方便设置123456grant all privileges on keystone.* to &#39;keystone&#39;@&#39;localhost&#39; identified by &#39;123456&#39;; grant all privileges on keystone.* to &#39;keystone&#39;@&#39;%&#39; identified by &#39;123456&#39;; keystone使用httpd的mod_wsgi运行在端口5000和35357处理认证服务请求。默认情况下，keystone服务依然监听在5000和35357端口。 安装 keystone和wsgi yum install openstack-keystone httpd mod_wsgi 修改keystone配置文件 vim &#x2F;etc&#x2F;keystone&#x2F;keystone.conf connection &#x3D; mysql+pymysql:&#x2F;&#x2F;keystone:&#x31;&#x32;&#51;&#x34;&#53;&#54;&#64;&#49;&#57;&#x32;&#x2e;&#49;&#54;&#x38;&#x2e;&#52;&#46;&#54;&#x2F;keystone #加入连接数据库配置 配置使用哪种产生token方式目前keystone支持4种(UUID、PKI、PKIZ、Fernet)这里我们配置fernethttp://www.tuicool.com/articles/jQJNFrn 这篇文章有几种模式的详细介绍。 [token]provider &#x3D; fernet 同步数据库 su -s &#x2F;bin&#x2F;sh -c “keystone-manage db_sync” keystone #发现同步数据库就是错了也没有反应，需要检查keystone的日志文件查看是否正确 初始化fernet keykeystone-manage fernet_setup –keystone-user keystone –keystone-group keystone keystone-manage credential_setup –keystone-user keystone –keystone-group keystoen 创建Bootstrap the Identity service(就是创建admin用户的帐号信息) 12345keystone-manage bootstrap --bootstrap-password 123456 \\ --bootstrap-admin-url http://192.168.4.6:35357/v3/ \\ --bootstrap-internal-url http://192.168.4.6:35357/v3/ \\ --bootstrap-public-url http://192.168.122.2:5000/v3/ \\ --bootstrap-region-id RegionOne 配置apache服务器vim &#x2F;etc&#x2F;httpd&#x2F;conf&#x2F;httpd.conf 配置成管理网段的ipServerName 192.168.4.6 将keystone的配置文件软链接到apache的配置文件ln -s &#x2F;usr&#x2F;share&#x2F;keystone&#x2F;wsgi-keystone.conf &#x2F;etc&#x2F;httpd&#x2F;conf.d&#x2F; 设置开机启动systemctl enable httpd 启动httpdsystemctl start httpd检查端口 123456789lsof -i:5000COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEhttpd 18883 root 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18894 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18895 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18896 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18897 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18898 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN) 到root下创建环境变量文件 123456789vim /root/openrc#!/bin/bashexport OS_USERNAME=adminexport OS_PASSWORD=123456 #这个密码是上面Bootstrap the Identity service填的密码export OS_PROJECT_NAME=adminexport OS_USER_DOMAIN_NAME=defaultexport OS_PROJECT_DOMAIN_NAME=defaultexport OS_AUTH_URL=http://192.168.4.6:35357/v3export OS_IDENTITY_API_VERSION=3 创建域、项目、用户创建service projectopenstack project create --domain default --description &quot;Service Project&quot; service 创建user角色openstack role create user这里不创建普通用户了测试admin用户获取tokenopenstack --os-auth-url http://192.168.4.6:35357/v3 token issue 安装glance镜像服务连接Mariadb创建数据库 create database glance; 授权 123grant all privileges on glance.* to &#x27;glance&#x27;@&#x27;localhost&#x27; identified by &#x27;123456&#x27;; grant all privileges on glance.* to &#x27;glance&#x27;@&#x27;%&#x27; identified by &#x27;123456&#x27;; grant all privileges on glance.* to &#x27;glance&#x27;@&#x27;control-node1.novalocal&#x27; identified by &#x27;123456&#x27;; control-xxx换成主机名，我这里就算api.conf里面配置的ip默认还是去连接host是主机名，所以只能在加个主机名授权。 创建glance用户并设置密码openstack user create –domain default –password-prompt glance 给glance用户添加admin角色权限openstack role add –project service –user glance admin 创建glance serviceopenstack service create –name glance –description “OpenStack Image” image 创建glance endpoint 123openstack endpoint create --region RegionOne image public http://192.168.122.2:9292openstack endpoint create --region RegionOne image internal http://192.168.4.6:9292openstack endpoint create --region RegionOne image admin http://192.168.4.6:9292 安装软件包yum install openstack-glance 配置glancevim &#x2F;etc&#x2F;glance&#x2F;glance-api.conf 配置数据库 12[database]connection = mysql+pymysql://glance:123456@192.168.4.6/glance 配置glancevim &#x2F;etc&#x2F;glance&#x2F;glance-api.conf配置数据库 12345678910111213141516[database]connection = mysql+pymysql://glance:123456@192.168.4.6/glance配置keystone[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = glancepassword = 123456[paste_deploy]flavor = keystone 配置镜像存储 1234[glance_store]stores = file,httpdefault_store = filefilesystem_store_datadir = /var/lib/glance/images/ vim &#x2F;etc&#x2F;glance&#x2F;glance-registry.conf 12345678910111213141516[database]connection = mysql+pymysql://glance:123456@192.168.4.6/glance配置keystone[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = glancepassword = 123456[paste_deploy]flavor = keystone 同步数据库su -s &#x2F;bin&#x2F;sh -c “glance-manage db_sync” glance 设置开机启动systemctl enable openstack-glance-api 启动服务systemctl start openstack-glance-api 这些做完后最好在查看下日志，看看是否有错误，每部署完一个组件都这样，这样出错的可以很快定位。 下载cirros测试一下 wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.imgopenstack image create &quot;cirros&quot; --file cirros-0.3.4-x86_64-disk.img --disk-format qcow2 --container-format bare --public glance image-list 检查一下镜像是否上传成功 安装nova组件控制节点安装创建数据库create database nova_api;create database nova; 授权 12345678GRANT ALL PRIVILEGES ON nova_api.* TO &#x27;nova&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;123456&#x27;; GRANT ALL PRIVILEGES ON nova_api.* TO &#x27;nova&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;123456&#x27;; GRANT ALL PRIVILEGES ON nova_api.* TO &#x27;nova&#x27;@&#x27;control-node1.novalocal&#x27; IDENTIFIED BY &#x27;123456&#x27;; GRANT ALL PRIVILEGES ON nova.* TO &#x27;nova&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;123456&#x27;; GRANT ALL PRIVILEGES ON nova.* TO &#x27;nova&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;123456&#x27;; GRANT ALL PRIVILEGES ON nova.* TO &#x27;nova&#x27;@&#x27;control-node1.novalocal&#x27; IDENTIFIED BY &#x27;123456&#x27;; 创建为nova组件创建用户、service、endpointopenstack user create --domain default --password-prompt nova 给nova用户添加admin角色权限openstack role add --project service --user nova admin 创建serviceopenstack service create --name nova --description &quot;OpenStack Compute&quot; compute 创建endpoint 12345openstack endpoint create --region RegionOne compute public http://192.168.122.2:8774/v2.1/%\\(tenant_id\\)s openstack endpoint create --region RegionOne compute internal http://192.168.4.6:8774/v2.1/%\\(tenant_id\\)s openstack endpoint create --region RegionOne compute admin http://192.168.4.6:8774/v2.1/%\\(tenant_id\\)s 安装nova-api组件yum install openstack-nova-api openstack-nova-conductor openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler 配置novavim &#x2F;etc&#x2F;nova&#x2F;nova.conf 12345678910111213141516171819202122232425262728293031323334[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6 #配置rabbitmq帐户和密码my_ip = 192.168.4.6use_neutron = Truefirewall_drive = nova.virt.firewall.NoopFirewallDriverenabled_apis=osapi_compute,metadataauth_strategy=keystone[api_database]connection = mysql+pymysql://nova:123456@192.168.4.6/nova_api #配置nova连接数据库[database]connection = mysql+pymysql://nova:123456@192.168.4.6/nova[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = novapassword = 123456配置novncnovncproxy_port=6080novncproxy_base_url=http://211.156.182.144:6080/vnc_auto.htmlvncserver_listen=192.168.4.6[glance]api_servers = http://192.168.4.6:9292[oslo_concurrency]lock_path = /var/lib/nova/tmp 同步数据库su -s &#x2F;bin&#x2F;sh -c “nova-manage api_db sync” novasu -s &#x2F;bin&#x2F;sh -c “nova-manage db sync” nova 123456789101112systemctl enable openstack-nova-api.service \\openstack-nova-consoleauth.service \\openstack-nova-scheduler.service \\openstack-nova-conductor.service \\openstack-nova-novncproxy.servicesystemctl start openstack-nova-api.service \\openstack-nova-consoleauth.service \\openstack-nova-scheduler.service \\openstack-nova-conductor.service \\openstack-nova-novncproxy.service 计算机节点安装安装nova-computeyum install openstack-nova-compute 配置nova-computevim &#x2F;etc&#x2F;nova&#x2F;nova.conf 123456789101112131415161718192021222324252627282930313233[DEFAULT]enabled_apis = osapi_compute,metadatatransport_url = rabbit://openstack:123456@192.168.4.6 #配置rabbitmq帐号和密码auth_strategy = keystonemy_ip = 192.168.4.7use_neutron = Truefirewall_driver = nova.virt.firewall.NoopFirewallDriver[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = novapassword = 123456[vnc]enabled=truevncserver_listen=0.0.0.0vncserver_proxyclient_address=192.168.4.7 #填写本机ipnovncproxy_base_url=http://211.156.182.144:6080/vnc_auto.html #这个填你要用控制节点的public ip[glance]api_servers = http://192.168.4.6:9292配置锁路径[oslo_concurrency]lock_path = /var/lib/nova/tmp[libvirt]virt_type = qemu #物理服务器就配置kvm虚拟机就配置qemu 设置开机启动systemctl enable libvirtd.service openstack-nova-compute.service 启动nova-computesystemctl start libvirtd.service openstack-nova-compute.service 在控制节点查看检查一下compute进程根控制节点连接 配置neutron控制节点安装我这里使用openvswitch不使用linux bridge，因为openvswitch功能比linux Brige功能强太多了。但配置稍微复杂点。 创建数据库create database neutron; 授权 123GRANT ALL PRIVILEGES ON neutron.* TO &#x27;neutron&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;123456&#x27;; GRANT ALL PRIVILEGES ON neutron.* TO &#x27;neutron&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;123456&#x27;; GRANT ALL PRIVILEGES ON neutron.* TO &#x27;neutron&#x27;@&#x27;control-node1.novalocal&#x27; IDENTIFIED BY &#x27;123456&#x27;; 创建neutron用户并设置密码openstack user create --domain default --password-prompt neutron 给neutron用户添加admin角色权限openstack role add --project service --user neutron admin 创建neutron serviceopenstack service create --name neutron --description &quot;OpenStack Networking&quot; network 创建neutron endpoint 123openstack endpoint create --region RegionOne network public http://192.168.122.2:9696 openstack endpoint create --region RegionOne network admin http://192.168.4.6:9696 openstack endpoint create --region RegionOne network internal http://192.168.4.6:9696 安装neutron组件yum -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch vim &#x2F;etc&#x2F;neutron&#x2F;neutron.conf 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[DEFAULT]service_plugins = routertransport_url = rabbit://openstack:123456@192.168.4.6auth_strategy = keystonenotify_nova_on_port_status_changes = Truenotify_nova_on_port_data_changes = Truestate_path = /var/lib/neutronuse_syslog = Truesyslog_log_facility = LOG_LOCAL4log_dir =/var/log/neutroncore_plugin = neutron.plugins.ml2.plugin.Ml2Pluginbase_mac = fa:16:3e:00:00:00mac_generation_retries = 32dhcp_lease_duration = 600dhcp_agent_notification = Trueallow_bulk = Trueallow_pagination = Falseallow_sorting = Falseallow_overlapping_ips = Trueadvertise_mtu = Trueagent_down_time = 30router_scheduler_driver = neutron.scheduler.l3_agent_scheduler.ChanceSchedulerallow_automatic_l3agent_failover = Truedhcp_agents_per_network = 2api_workers = 9rpc_workers = 9network_device_mtu=1450[database]connection = mysql+pymysql://neutron:123456@192.168.4.6/neutron[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = neutronpassword = 123456[nova]auth_url = http://192.168.4.6:35357auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = novapassword = 123456[oslo_concurrency]lock_path = /var/lib/neutron/tmp 配置modular layer 2（ml2）插件 vim &#x2F;etc&#x2F;neutron&#x2F;plugins&#x2F;ml2&#x2F; ml2_conf.ini 1234567891011121314151617181920212223242526272829[DEFAULT]type_drivers = flat,vxlantenant_network_types = vxlanmechanism_drivers = openvswitch,l2populationextension_drivers = port_security[ml2]path_mtu = 1450type_drivers = flat,vxlantenant_network_types = vxlanphysical_network_mtus =physnet1:1500[ml2_type_flat]flat_networks =*[ml2_type_vxlan]vni_ranges =2:65535vxlan_group =224.0.0.1[securitygroup]enable_security_group = Truefirewall_driver=neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver[ovs]local_ip=192.168.4.6tunnel_bridge=br-tunenable_tunneling=Trueintegration_bridge=br-intbridge_mappings=physnet1:br-ex 配置l3-agentvim &#x2F;etc&#x2F;neutron&#x2F;l3_agent.ini 12345678910[DEFAULT]debug = Falseinterface_driver =neutron.agent.linux.interface.OVSInterfaceDriverhandle_internal_only_routers = Truemetadata_port = 8775send_arp_for_ha = 3periodic_interval = 40periodic_fuzzy_delay = 5enable_metadata_proxy = Truerouter_delete_namespaces = True 配置dhcp_agentvim &#x2F;etc&#x2F;neutron&#x2F;dhcp_agent.ini 12345678910[DEFAULT]resync_interval = 30interface_driver =neutron.agent.linux.interface.OVSInterfaceDriverenable_isolated_metadata = Trueenable_metadata_network = Falsedhcp_domain = openstacklocaldhcp_broadcast_reply = Falsedhcp_delete_namespaces = Trueroot_helper=sudo neutron-rootwrap /etc/neutron/rootwrap.confstate_path=/var/lib/neutron vim &#x2F;etc&#x2F;neutron&#x2F;plugins&#x2F;ml2&#x2F;openvswitch_agent.ini 123456789101112131415161718[agent]polling_interval = 2tunnel_types = vxlanvxlan_udp_port = 4789l2_population = Trueprevent_arp_spoofing = Falseextensions =[ovs]local_ip=192.168.4.6tunnel_bridge=br-tunenable_tunneling=Trueintegration_bridge=br-intbridge_mappings=physnet1:br-ex[securitygroup]firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriverenable_security_group = true 设置openvswitch开机启动systemctl enable openvswitch.service 启动openvswitchsystemctl start openvswitch 创建br-ex br-tun br-intovs-vsctl add-br br-intovs-vsctl add-br br-exovs-vsctl add-br br-tun 将上外网网卡挂载到br-ex上ovs-vsctl add-port br-ex eth2 设置开机启动项systemctl enable neutron-openvswitch-agent.service启动进程systemctl start neutron-openvswitch-agent.service 配置计算节点neutron配置一下内核参数修改配置文件 &#x2F;etc&#x2F;sysctl.conf 12net.ipv4.conf.all.rp_filter=0net.ipv4.conf.default.rp_filter=0 sysctl –pyum -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch vim &#x2F;etc&#x2F;neutron&#x2F;neutron.conf 1234567891011121314151617[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6auth_strategy = keystone[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = neutronpassword = 123456[oslo_concurrency]lock_path = /var/lib/neutron/tmp vim &#x2F;etc&#x2F;neutron&#x2F;plugins&#x2F;ml2&#x2F;ml2_conf.ini 1234567891011[ml2]path_mtu = 1450type_drivers = flat,vxlantenant_network_types = vxlanphysical_network_mtus =physnet1:1500mechanism_drivers = openvswitch,l2populationextension_drivers = port_security[securitygroup]enable_ipset = truefirewall_driver=neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver vim &#x2F;etc&#x2F;neutron&#x2F;plugins&#x2F;ml2&#x2F;openvswitch_agent.ini 123456789101112131415161718192021[ovs]local_ip=192.168.4.7tunnel_bridge=br-tunenable_tunneling=Trueintegration_bridge=br-intbridge_mappings=physnet1:br-ex[agent]enable_distributed_routing=Trueprevent_arp_spoofing=Truearp_responder=Truepolling_interval=2drop_flows_on_start=Falsevxlan_udp_port=4789l2_population=Truetunnel_types=vxlan[securitygroup]firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriverenable_security_group = true systemctl enable openvswitch.servicesystemctl start openvswitch.service 创建br-ex、br-int、br-tunovs-vsctl add-br br-intovs-vsctl add-br br-exovs-vsctl add-br br-tun vim &#x2F;etc&#x2F;nova&#x2F;nova.conf 123456789101112131415[DEFAULT]network_api_class = nova.network.neutronv2.api.APIsecurity_group_api = neutronlinuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver[neutron]url = http://192.168.4.6:9696auth_url = http://192.168.4.6:35357auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = neutronpassword = 123456 systemctl restart neutron-openvswitch-agent.servicesystemctl restart openstack-nova-compute 安装完后可以在在控制节点检查是否安装成功 安装控制台yum install openstack-dashboardvim &#x2F;etc&#x2F;openstack-dashboard&#x2F;local_settings 1234567891011121314151617181920212223242526272829这里配置控制节点ipOPENSTACK_HOST = &quot;192.168.4.6&quot;配置允许所有节点访问ALLOWED_HOSTS = [&#x27;*&#x27;, ]配置memcacheSESSION_ENGINE = &#x27;django.contrib.sessions.backends.cache&#x27;CACHES = &#123; &#x27;default&#x27;: &#123; &#x27;BACKEND&#x27;: &#x27;django.core.cache.backends.memcached.MemcachedCache&#x27;, &#x27;LOCATION&#x27;: &#x27;192.168.4.6:11211&#x27;, &#125;&#125;配置keystone v3验证OPENSTACK_KEYSTONE_URL = &quot;http://%s:5000/v3&quot; % OPENSTACK_HOST配置域OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = &#x27;default&#x27;配置api版本OPENSTACK_API_VERSIONS = &#123; &quot;identity&quot;: 3, &quot;image&quot;: 2, &quot;volume&quot;: 2,&#125;设置通过控制台默认创建用户的角色是userOPENSTACK_KEYSTONE_DEFAULT_ROLE = &quot;user&quot; 重启服务systemctl restart httpd.service memcached.service 通过http://control_ip/dashboard可以访问 Admin 登录，密码是你通过keystone创建的，如果不记得查看openrc 创建flat网络做float_ip池管理员—&gt;网络——&gt;创建网络 Phynet1是在ml2.ini里面bridge_mappings定义的br-ex对应的名字，创建完后增加子网，然后在创建个普通网络，创建个路由器，路由器绑定普通子网，创建个主机配置，然后创建vm加入到你创建的普通网络 这时在vm所在的计算节点或控制节点 ovs-vsctl show 可以看见计算节点根网络节道隧道已经建立。 Cinder配置配置控制节点创建数据库create database cinder;用户授权 123GRANT ALL PRIVILEGES ON cinder.* TO &#x27;cinder&#x27;@&#x27;localhost&#x27; identified by &#x27;123456&#x27;; GRANT ALL PRIVILEGES ON cinder.* TO &#x27;cinder&#x27;@&#x27;%&#x27; identified by &#x27;123456&#x27;; GRANT ALL PRIVILEGES ON cinder.* TO &#x27;cinder&#x27;@&#x27;control-node1.novalocal&#x27; identified by &#x27;123456&#x27;; 创建用户openstack user create --domain default --password-prompt cinder 给cinder用户赋予admin权限openstack role add --project service --user cinder admin openstack service create --name cinder --description &quot;OpenStack Block Storage&quot; volume openstack service create --name cinderv2 --description &quot;OpenStack Block Storage&quot; volumev2 创建endpoint 1234567891011121314151617openstack endpoint create --region RegionOne \\volume public http://192.168.122.2:8776/v1/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne \\volume internal http://192.168.4.6:8776/v1/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne \\volume admin http://192.168.4.6:8776/v1/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne \\volumev2 public http://192.168.122.2:8776/v2/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne \\volumev2 internal http://192.168.4.6:8776/v2/%\\(tenant_id\\)sopenstack endpoint create --region RegionOne \\volumev2 admin http://192.168.4.6:8776/v2/%\\(tenant_id\\)s 安装cinderyum install openstack-cinder vim &#x2F;etc&#x2F;cinder&#x2F;cinder.conf 1234567891011121314151617181920[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6auth_strategy = keystone[database]connection = mysql+pymysql://cinder:123456@192.168.4.6/cinder[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = cinderpassword = 123456[oslo_concurrency]lock_path = /var/lib/cinder/tmp 同步数据库su -s &#x2F;bin&#x2F;sh -c “cinder-manage db sync” cinder 配置计算机节点使用cindervim &#x2F;etc&#x2F;nova&#x2F;nova.conf [cinder]os_region_name &#x3D; RegionOne 重启服务systemctl restart openstack-nova-api.service 设置开机自启cindersystemctl enable openstack-cinder-api.service openstack-cinder-scheduler.service systemctl start openstack-cinder-api.service openstack-cinder-scheduler.service 配置一个存储节点安装lvmyum install lvm2 systemctl enable lvm2-lvmetad.servicesystemctl start lvm2-lvmetad.service 创建个lvm卷pvcreate &#x2F;dev&#x2F;vdb 创建vgvgcreate cinder-volumes &#x2F;dev&#x2F;vdb vim &#x2F;etc&#x2F;lvm&#x2F;lvm.conf 123devices &#123;filter = [ &quot;a/vdb/&quot;, &quot;r/.*/&quot;]&#125; 安装软件yum install openstack-cinder targetcli python-keystone 修改cinder配置文件vim &#x2F;etc&#x2F;cinder&#x2F;cinder.conf 1234567891011121314151617181920212223242526272829[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6verbose = Trueauth_strategy = keystoneenabled_backends = lvmglance_api_servers = http://192.168.4.6:9292[database]connection = mysql+pymysql://cinder:123456@192.168.4.6/cinder[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = cinderpassword = 123456[lvm]volume_driver = cinder.volume.drivers.lvm.LVMVolumeDrivervolume_group = cinder-volumesiscsi_protocol = iscsiiscsi_helper = lioadm[oslo_concurrency]lock_path = /var/lib/cinder/tmpsystemctl enable openstack-cinder-volume.service target.service systemctl start openstack-cinder-volume.service target.service 到控制台创建个卷，并挂载到云主机。 Ceilometer配置Ceilometer使用Mongdb存储meter数据，所以需要先在控制节点安装Mongdb 在控制节点安装Mongdbyum install mongodb-server mongodb 配置Mongdbvim &#x2F;etc&#x2F;mongod.confsmallfiles &#x3D; true #限制日志大小创建Mongdb数据库和帐户授权替换123456为你自己设置的密码创建用户openstack user create –domain default –password-prompt ceilometer 给ceilometer用户添加admin角色权限openstack role add –project service –user ceilometer admin创建ceilometer serviceopenstack service create –name ceilometer –description “Telemetry” metering 创建ceilometer endpoint 123openstack endpoint create --region RegionOne metering public http://192.168.122.2:8777openstack endpoint create --region RegionOne metering admin http://192.168.4.6:8777openstack endpoint create --region RegionOne metering internal http://192.168.4.6:8777 安装包yum install openstack-ceilometer-api \\openstack-ceilometer-collector \\openstack-ceilometer-notification \\openstack-ceilometer-central \\python-ceilometerclient 配置ceilometervim &#x2F;etc&#x2F;ceilometer&#x2F;ceilometer.conf 12345678910111213141516171819202122232425262728293031[DEFAULT]rpc_backend = rabbitauth_strategy = keystone[oslo_messaging_rabbit]rabbit_host = 192.168.4.6rabbit_userid = openstackrabbit_password = 123456[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = ceilometerpassword = 123456[service_credentials]auth_type = passwordauth_url = http://192.168.4.6:5000/v3project_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = ceilometerpassword = 123456 ##密码替换成在keystone创建ceilometer时设置的密码interface = internalURLregion_name = RegionOne 创建ceilometer的vhost vim &#x2F;etc&#x2F;httpd&#x2F;conf.d&#x2F;wsgi-ceilometer.conf 123456789101112Listen 8777&lt;VirtualHost *:8777&gt; WSGIDaemonProcess ceilometer-api processes=2 threads=10 user=ceilometer group=ceilometer display-name=%&#123;GROUP&#125; WSGIProcessGroup ceilometer-api WSGIScriptAlias / /usr/lib/python2.7/site-packages/ceilometer/api/app.wsgi WSGIApplicationGroup %&#123;GLOBAL&#125; ErrorLog /var/log/httpd/ceilometer_error.log CustomLog /var/log/httpd/ceilometer_access.log combined&lt;/VirtualHost&gt;WSGISocketPrefix /var/run/httpd 重启httpdsystemctl reload httpd.service设置服务开机启动systemctl enable openstack-ceilometer-notification.service \\openstack-ceilometer-central.service \\openstack-ceilometer-collector.service 启动进程systemctl start openstack-ceilometer-notification.service \\openstack-ceilometer-central.service \\openstack-ceilometer-collector.service 配置glance的ceilometer统计vim &#x2F;etc&#x2F;glance&#x2F;glance-api.conf 1234567891011[DEFAULT]rpc_backend = rabbit[oslo_messaging_amqp]driver = messagingv2[oslo_messaging_rabbit]rabbit_host = 192.168.4.6rabbit_userid = openstackrabbit_password = 123456 重启进程systemctl restart openstack-glance-api.service openstack-glance-registry.service 配置nova的ceilometer统计安装软件yum install openstack-ceilometer-compute python-ceilometerclient python-pecan 12345678910111213141516171819202122232425262728293031[DEFAULT]rpc_backend = rabbitauth_strategy = keystone[oslo_messaging_rabbit]rabbit_host = 192.168.4.6rabbit_userid = openstackrabbit_password = 123456[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = ceilometerpassword = 123456 #将密码替换成keystone创建ceilometer用户时设置的密码[service_credentials]auth_url = http://192.168.4.6:5000project_domain_id = defaultuser_domain_id = defaultauth_type = passwordusername = ceilometerproject_name = servicepassword = 123456 #将密码替换成keystone创建ceilometer用户时设置的密码interface = internalURLregion_name = RegionOne 修改nova-compute配置文件 vim &#x2F;etc&#x2F;nova&#x2F;nova.conf 1234567[DEFAULT]instance_usage_audit = Trueinstance_usage_audit_period = hournotify_on_state_change = vm_and_task_state[oslo_messaging_amqp]driver = messagingv2 设置开机启动systemctl enable openstack-ceilometer-compute.service启动ceilometer-compute进程systemctl start openstack-ceilometer-compute.service重启nova-computesystemctl restart openstack-nova-compute.service 配置块设备使用ceilometer计量服务 验证ceilometer meter-list 正常情况下是会出现如上图一些资源的数据的，但我这里默认报 打debug 访问被拒绝解决办法：修改httpd.conf systemctl restart httpd在测试应该没问题了。 Aodh报警服务：创件aodh数据库create database aodh; 授权 123GRANT ALL PRIVILEGES ON aodh.* TO &#x27;aodh&#x27;@&#x27;localhost&#x27; identified by &#x27;123456&#x27;; GRANT ALL PRIVILEGES ON aodh.* TO &#x27;aodh&#x27;@&#x27;%&#x27; identified by &#x27;123456&#x27;; GRANT ALL PRIVILEGES ON aodh.* TO &#x27;aodh&#x27;@&#x27;control-node1.novalocal&#x27; identified by &#x27;123456&#x27;; 创建用户openstack user create --domain default --password-prompt aodh 给adoh帐户添加admin权限openstack role add --project service --user aodh admin 添加服务openstack service create --name aodh --description &quot;Telemetry&quot; alarming 创建endpoint 123openstack endpoint create --region RegionOne alarming public http://192.168.122.2:8042 openstack endpoint create --region RegionOne alarming internal http://192.168.4.6:8042 openstack endpoint create --region RegionOne alarming admin http://192.168.4.6:8042 安装软件yum install openstack-aodh-api \\openstack-aodh-evaluator \\openstack-aodh-notifier \\openstack-aodh-listener \\openstack-aodh-expirer \\python-aodhclient 修改配置文件 vim &#x2F;etc&#x2F;aodh&#x2F;aodh.conf 1234567891011121314151617181920212223242526272829[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6auth_strategy = keystone[database]connection = mysql+pymysql://aodh:123456@192.168.4.6/aodh[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = aodhpassword = 123456 #填写通过keystone创建帐户时设置的帐号和密码[service_credentials]auth_type = passwordauth_url = http://192.168.4.6:5000/v3project_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = aodhpassword = 123456 #填写通过keystone创建帐户时设置的帐号和密码interface = internalURLregion_name = RegionOne systemctl enable openstack-aodh-api.service \\openstack-aodh-evaluator.service \\openstack-aodh-notifier.service \\openstack-aodh-listener.service systemctl start openstack-aodh-api.service \\openstack-aodh-evaluator.service \\openstack-aodh-notifier.service \\openstack-aodh-listener.service","categories":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/categories/openstack/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/tags/openstack/"}]},{"title":"Fastnetmon配置与使用","slug":"fastnetmon","date":"2016-12-09T11:50:27.000Z","updated":"2018-11-07T07:52:04.545Z","comments":true,"path":"2016/12/09/fastnetmon/","permalink":"http://yoursite.com/2016/12/09/fastnetmon/","excerpt":"","text":"Fastnetmon介绍FastNetmon是一个基于多种抓包引擎来对数据包进行统计分析的DOS&#x2F;DDOS工具，可以探测和分析网络中的异常流量情况，同时，也可以对捕获的异常调用外部脚本进行处理警报啊或者进行阻断处理，全靠外部脚本是如何定义。 项目首页http://www.open-open.com/lib/view/home/143193107973 部署架构 安装和配置方法安装 下载自动安装脚本wget https://raw.githubusercontent.com/FastVPSEestiOu/fastnetmon/master/src/fastnetmon_install.pl -Ofastnetmon_install.pl 将整个项目克隆下来https://github.com/FastVPSEestiOu/fastnetmon 安装注意若安装报连接错误，请将DNS改为8.8.8.8或223.5.5.5，因为下载地址都是在国外，有些DNS可能解析不够好。perl fastnetmon_install.pl 启动安装脚本。 配置 配置文件，cd到你刚刚克隆的项目里面。 cp src&#x2F;Fastnetmon.conf 到&#x2F;etc&#x2F; cp src&#x2F;fastnetmon_init_script_centos6 &#x2F;etc&#x2F;init.d&#x2F;fastnetmon #cp启动脚本到&#x2F;etc&#x2F;init.d&#x2F; chmod 755 &#x2F;etc&#x2F;init.d&#x2F;fastnetmon #修改启动脚本权限 cp src&#x2F;notify_about_attack.sh &#x2F;usr&#x2F;local&#x2F;bin&#x2F; #cp 通知脚本 &#x2F;etc&#x2F;fastentmon配置项 ban_time &#x3D; 1900 #对检测到攻击的ip进行多久封锁。 enable_subnet_counters &#x3D; on #检测每个子网进出流量 enable_connection_tracking &#x3D; on #开启攻击追踪检测，通过这个选项在日志文件里可以详细看见攻击者ip和其他一些详细情况 ban_for_pps &#x3D; on，ban_for_bandwidth &#x3D; on，ban_for_flows &#x3D; on #检测的选项pps(每秒包)，bandwidth(带宽)，flows(流量)。 threshold_pps &#x3D; 20000，threshold_mbps &#x3D; 1000，threshold_flows &#x3D; 3500 #监控的限定值 抓包引擎选择 mirrof&#x3D;off 没有安装PF_RING就不要开启，不然会启动报错。 pfring_sampling_ratio &#x3D; 1 #端口镜像采样率 mirror_netmap &#x3D; off 没有安装Netmap就不要开启，不然会会启动报错。 mirror_snabbswitch&#x3D;on #开启snabbswitch流量捕获。 mirror_afpacket &#x3D;on #AF_PACKET捕获引擎开启 netmap_sampling_ratio &#x3D; 1 #端口镜像抽样比 pcap&#x3D;on #pcap引擎开启 netflopw&#x3D;on #使用Netflow捕获方法 interfaces&#x3D;enp5s0f1 #监控的端口，我这里使用的是镜像端口,不然监控不到整个网端流量 notify_script_path&#x3D;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;notify_about_attack.sh #触发脚本位置 monitor_local_ip_addresses &#x3D; on #监控本地地址 sort_parameter &#x3D; packets #在控制台排序单位，包 max_ips_in_list &#x3D;400 #在控制台显示多少地址 编辑要监控的网段 vim &#x2F;etc&#x2F;networks_list #编辑networks_list加入要监控的网段这里我加入 211.156.182.0&#x2F;24 220.242.2.0&#x2F;24 &#x2F;etc&#x2F;init.d&#x2F;fastnetmon start #启动fastentmon，启动失败查看&#x2F;var&#x2F;log&#x2F;fastnetmon.log &#x2F;opt&#x2F;fastnetmon&#x2F;fastnetmon #打开监控控制台 修改监控脚本1,要先外发邮件必须配置mailx 安装mailx，配置SMTP vim &#x2F;etc&#x2F;mail.rc 我这里配置的是我163的邮箱 set bsdcompat set from&#x3D;&#x78;&#120;&#120;&#x78;&#x78;&#64;&#49;&#x36;&#x33;&#x2e;&#99;&#x6f;&#109; smtp&#x3D;smtp.163.com set smpt-auth-user&#x3D;&#120;&#120;&#120;&#120;&#64;&#x31;&#54;&#x33;&#x2e;&#99;&#111;&#x6d; set smtp-auth-user&#x3D;&#120;&#x78;&#120;&#x78;&#x78;&#x78;&#x40;&#49;&#x36;&#x33;&#x2e;&#x63;&#111;&#x6d; smtp-auth-password&#x3D;xxxxxx smtp-auth&#x3D;login vim &#x2F;usr&#x2F;local&#x2F;bin&#x2F;notify_about_attack.sh #这里填要触发监控的脚本 修改邮件地址为接受人。cat | mail -s “FastNetMon Guard: IP $1 blocked because $2 attack with power $3 pps” $email_notify; 测试使用低轨道等离子炮（Loic）进行测试这里我对我们211.156.182.143进行Tcp DDOS攻击我们先把这些值调低threshold_pps &#x3D; 2000，threshold_mbps &#x3D; 100，threshold_flows &#x3D; 350 #监控的限定值然后重启Fastnetmon 测试完修改回值","categories":[{"name":"安全","slug":"安全","permalink":"http://yoursite.com/categories/%E5%AE%89%E5%85%A8/"}],"tags":[{"name":"安全","slug":"安全","permalink":"http://yoursite.com/tags/%E5%AE%89%E5%85%A8/"}]}]}