<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[手动安装ceph]]></title>
      <url>%2F2017%2F01%2F01%2Fmanual_ceph%2F</url>
      <content type="text"><![CDATA[环镜操作系统centos6.5网络规划Cluster_net：192.168.20.0/24Public_net：192.168.2.0/24 角色 Public_net Cluster_net Ceph-mon 192.168.2.4 192.168.20.2 Ceph-mon2 192.168.2.5 192.168.20.5 Ceph-mon3 192.168.2.6 192.168.20.6 Ceph-osd1 192.168.2.7 192.168.20.7 Ceph-osd2 192.168.2.8 192.168.20.8 Ceph-osd3 192.168.2.9 192.168.20.9 绑定hosts 123456192.168.2.4 ceph-mon.novalocal mon192.168.2.5 ceph-mon2.novalocal mon2192.168.2.6 ceph-mon3.novalocal mon3192.168.2.7 ceph-osd1.novalocal osd1192.168.2.8 ceph-osd2.novalocal osd2192.168.2.9 ceph-osd3.novalocal osd3 osd数规划每台osd节点挂载3块SSD硬盘 配置yum源根据操作系统版本任意调整(所有节点)vim /etc/yum.repos.d/ceph.repo12345678910[ceph]name=cephbaseurl=http://mirrors.aliyun.com/ceph/rpm-hammer/el6/x86_64/gpgcheck=0enablde=1[ceph-noarch]name=cephnoarchbaseurl=http://mirrors.aliyun.com/ceph/rpm-hammer/el6/x86_64/gpgcheck=0enablde=1 yum cleanyum makecache 在第一台mon节点安装pssh批量执行操作yum install pssh创建hosts（会被批量执行的主机）vim /root/hosts.txt12345192.168.2.5192.168.2.6192.168.2.7192.168.2.8192.168.2.9 先进行测试 安装软件（所有节点）pssh -P -h hosts.txt yum install ceph –y同步hostspscp.pssh -h /root/hosts.txt /etc/hosts /etc/ 都success进行下一步 配置ceph-mon创建集群fsiduuidgenfdc3d06b-7e05-44a8-b982-e8e04e4156db创建/etc/ceph/ceph.conf将fsid写入配置文件[global]fsid = fdc3d06b-7e05-44a8-b982-e8e04e4156db 将ceph-mon写入配置文件(有多个mon，用逗号隔开)mon_initial_members = mon,mon2,mon3将mon节点ip写入ceph配置文件mon_host = 192.168.2.4,192.168.2.5,192.168.2.6为集群创建mon密钥ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon &#39;allow *&#39; 生成管理员密钥ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --set-uid=0 --cap mon &#39;allow *&#39; --cap osd &#39;allow *&#39; --cap mds &#39;allow&#39; 将client.admin密钥加入到ceph.mon.keyringceph-authtool/tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring 生成mon mapmonmaptool --create --add mon 192.168.2.4 --fsid fdc3d06b-7e05-44a8-b982-e8e04e4156db /tmp/monmap在每个mon节点数据目录分别在mon、mon2、mon3上执行格式为(默认cluster-name为ceph)mkdir /var/lib/ceph/mon/{cluster-name}-{hostname}如mon为mkdir /var/lib/ceph/mon/ceph-monmon初始化(-i后接hostname)ceph-mon --mkfs -i mon --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring ceph.conf增加如下配置1234public network = 192.168.2.0/24auth cluster required = cephxauth service required = cephxauth client required = cephx 创建两个空文件touch /var/lib/ceph/mon/ceph-mon/donetouch /var/lib/ceph/mon/ceph-mon/sysvinit启动第一个ceph-mon/etc/init.d/ceph status mon.mon部署第二个mon将keyring复制到mon2scp /tmp/ceph.mon.keyring mon2:/tmp/在mon2节点上建立一个/var/lib/ceph/mon/ceph-mon2目录mkdir –p /var/lib/ceph/mon/ceph-mon2/在mon2节点上初始化mon节点ceph-mon --mkfs -i mon2 --keyring /tmp/ceph.mon.keyring为了防止重新被安装，初始化一个空的done文件touch /var/lib/ceph/mon/ceph-mon2/donetouch /var/lib/ceph/mon/ceph-mon2/sysvinit/etc/init.d/ceph start mon.mon2检查进程 第三个mon同上完后检查 发现有时种偏移问题，默认ceph是0.05s，为了方便同步直接把时钟偏移设置成0.5s修改ceph配置文件增加两条配置1234[global]mon osd down out interval = 900 #设置osd节点down后900s，把此osd节点逐出ceph集群，把之前映射到此节点的数据映射到其他节点。 [mon] mon clock drift allowed = .50 同步配置 pscp.pssh -h hosts.txt /etc/ceph/ceph.conf /etc/ceph/重启进程pssh -h /root/hosts.txt /etc/init.d/ceph restart 配置osd节点将keyring同步到osd节点pscp.pssh -h /root/hosts.txt /etc/ceph/ceph.client.admin.keyring /etc/ceph/ 为osd分配uuid(我每台osd节点有3个osd所以创建3个uuid)uuidgen19ebc47d-9b29-4cf3-9720-b62896ce6f33uuidgen1721ce0b-7f65-43ef-9dfc-c49e6210d375uuidgenf5e0f54b-2ee3-41df-bf25-ad37371ab6ce 创建3个osd123ceph osd create 19ebc47d-9b29-4cf3-9720-b62896ce6f33ceph osd create 1721ce0b-7f65-43ef-9dfc-c49e6210d375ceph osd create f5e0f54b-2ee3-41df-bf25-ad37371ab6ce 创建数据存储目录1234567mkdir /var/lib/ceph/osd/&#123;cluster-name&#125;-&#123;osd-number&#125;mkdir /var/lib/ceph/osd/ceph-0mkdir /var/lib/ceph/osd/ceph-1mkdir /var/lib/ceph/osd/ceph-2mkfs.xfs /dev/vdbmkfs.xfs /dev/vdcmkfs.xfs /dev/vdd 挂载123mount -o defaults,_netdev /dev/vdb /var/lib/ceph/osd/ceph-0mount -o defaults,_netdev /dev/vdc /var/lib/ceph/osd/ceph-1mount -o defaults,_netdev /dev/vdd /var/lib/ceph/osd/ceph-2 修改fstable 初始化osd目录 123ceph-osd -i 0 --mkfs --mkkey --osd-uuid 19ebc47d-9b29-4cf3-9720-b62896ce6f33ceph-osd -i 1 --mkfs --mkkey --osd-uuid 1721ce0b-7f65-43ef-9dfc-c49e6210d375ceph-osd -i 2 --mkfs --mkkey --osd-uuid f5e0f54b-2ee3-41df-bf25-ad37371ab6ce 注册此osd密钥123ceph auth add osd.0 osd &apos;allow *&apos; mon &apos;allow profile osd&apos; -i /var/lib/ceph/osd/ceph-0/keyringceph auth add osd.1 osd &apos;allow *&apos; mon &apos;allow profile osd&apos; -i /var/lib/ceph/osd/ceph-1/keyringceph auth add osd.2 osd &apos;allow *&apos; mon &apos;allow profile osd&apos; -i /var/lib/ceph/osd/ceph-2/keyring 加入crush map ceph osd crush add-bucket ceph-osd1 hostceph osd crush move ceph-osd1 root=default 设置权重123ceph osd crush add osd.0 1.0 host=ceph-osd1 ceph osd crush add osd.1 1.0 host=ceph-osd1ceph osd crush add osd.2 1.0 host=ceph-osd1 要已守护进程开机启动，须创建一个空文件123touch /var/lib/ceph/osd/ceph-0/sysvinittouch /var/lib/ceph/osd/ceph-1/sysvinittouch /var/lib/ceph/osd/ceph-2/sysvinit 启动osd进程123/etc/init.d/ceph start osd.0/etc/init.d/ceph start osd.1/etc/init.d/ceph start osd.2 查看osd 树按上述方法配置osd2、osd33台节点添加完毕 这里有个warn，是pool的pg问题，我们重新计算，修改。查看我们现在拥有的poolceph osd lspools0 rbd, 查看默认的rbd pool的pg1234567ceph osd pool get rbd pg_numpg_num: 64ceph osd pool get rbd pg_numpg_num: 64修改为128ceph osd pool set rbd pg_num 128ceph osd pool set rbd pgp_num 128 再查看 修改ceph.conf默认数据存两份，默认pg为128同步配置重启ceph 配置ceph-radosgw直接将第一个mon节点当radosgw，ceph在0.80及以上版本可以直接使用civeweb来构建对象网关，可以不需要使用apache或nginx+fastcgi了，所以我这里用civetweb。 安装软件yum install ceph-radosgw没有www-data用户创建useradd –r –s /sbin/nologin www-data创建gateway keyring并授权 ceph auth get-or-create client.radosgw.gateway osd &#39;allow rwx&#39; mon &#39;allow rwx&#39; -o /etc/ceph/keyring.radosgw.gateway 编辑ceph.conf文件增加下面内容1234567891011[client.radosgw.gateway]keyring = /etc/ceph/keyring.radosgw.gatewayrgw_socket_path = /tmp/radosgw.sockrgw_frontends= &quot;civetweb port=7480&quot;host = ceph-monrgw_dns_name = *.domain.tldrgw_print_continue = Truergw_data = /var/lib/ceph/radosgwuser = www-datargw s3 auth use keystone = truelog file =/var/log/radosgw/client.radosgw.gateway.log 重启进程/etc/init.d/ceph-radosgw restart查看是否启动成功 新建用户radosgw-admin user create –uid=”testuser” –display-name=”First User”得到如下结果]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mariadb galera]]></title>
      <url>%2F2016%2F12%2F25%2Fmariadb-base%2F</url>
      <content type="text"><![CDATA[传统的数据同步方式传统的mysql数据库同步是使用二进制日志进行主从同步也就是semi-sync，这种同步也只是一种半同步，并不是完全的实时同步，在mysql 5.7.17中推出了MySQL Group Replication，这种实现方式与Galera cluster基本完全一样，不过MGR比Galera的优势在于 Mysql官方出品，能得到后续技术支持，Galera是芬兰的一家叫codership的公司开发的。 MGR使用Paxos协议，性能比Galera好，并且性能稳定，Galera实际只能使用三个节点，有性能和稳定性问题。 Galera目前只支持linux，MGR支持多个平台。 mysql 5.6下semi-sync数据同步方式master的dump进程需要发送binlog日志到slave，master要等待至少一个slave通知，slave将已经接收到传过来的events并写入relay log，salve发送ack信息到master，这个事务才能提交。 5.6版semi-sync的缺陷，dump thread要承但两份任务，传送binlog给slave还要等slave的返回，并且这两个任务是串行的，也就是说，dump thread要先传送binlog给slave，并还要slave返回才能传送下一个events事务，这样的dump thread成为性能瓶径。 mysql 5.7下semi-sync数据同步方式5.7.4为解决上述的问题，在master端增加了ack进程。这样事务写发送binlog与接收ack可以并行进行，提高semi-sync的效率。 GaleraGalera cluster是可以实现mariadb多主集群的软件，它目前只能用于linux和只支持XtraDB和Innodb存储引擎 ，mariadb和perconna提供了原生的Galera cluster的支持，所以可以直接使用Galera cluster，mysql要使用Galera cluster需要使用Galera cluster提供的插件。传统主从只能有一个数据库进行写服务，Galera集群，每个节点都可读可写，在Galera上层部署负载均衡软件如lvs和haproxy进行流量分担是常用做法。 原理各个节点的数据同步由wsrep接口实现。client发起一个commit命令时，所有本事务内对数据库的操作和primary_key都会打包写入到write-set，write-set随后会复制到其他节点，各个节点接收后，会根据write-set传送的primary key进行检验，检查是否与本地事务种read-write或write-write锁冲突，冲突则回滚，没有冲突就执行，完成后返回success。如果其他节点没有执行成功则存放在队列种，稍后会重新尝式。 Galera集群的优点 支持多个节点的数据写入，能保证数据的强一致性。 同步复制，各个节点无延迟，不会因为一台宕机导致数据丢失。 故障节点将自动从集群中移除 基于行级的并行复制 缺点 只支持Innodb存储引擎 只支持linux平台， 集群写入的tps由最弱节点限制，如果一个节点变慢，整的集群就是缓慢的，所以一般情况下部署，要求统一的硬件配置。 会因为网络抖动造成性能和稳定性问题。 参考链接:http://www.oschina.net/news/79983/galera-will-die-mysql-group-replication-realeasehttp://www.itbaofeng.com/?p=236]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mariadb galera集群搭建]]></title>
      <url>%2F2016%2F12%2F22%2Fmariadb-galera-2%2F</url>
      <content type="text"><![CDATA[环境配置准备3台服务器 192.168.1.16 mariadb-1.novalocal mariadb-1192.168.1.19 mariadb-2.novalocal mariadb-2192.168.1.18 mariadb-3.novalocal mariadb-3 配置repo文件 vim /etc/yum.repos.d/MariaDB.repo12345[mariadb]name = MariaDBbaseurl = http://yum.mariadb.org/10.1/centos7-amd64gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDBgpgcheck=1 yum makecache 禁用防火墙和selinux 如果要使用防火墙添加允许3306、和4567 端口规则。 安装Mariadb和galera cluster（三个节点都执行） yum install MariaDB-server MariaDB-client galera 启动mariadb systemctl start mariadb 一些初始化安全配置 /usr/bin/mysql_secure_installation 关闭数据库systemctl stop mariadb 修改mariadb-1上的/etc/my.cnf.d/server.cnf文件如下1234567891011121314[galera]wsrep_provider = /usr/lib64/galera/libgalera_smm.sowsrep_cluster_address = &quot;gcomm://192.168.1.16,192.168.1.18,192.168.1.19&quot;wsrep_node_name = mariadb-1wsrep_node_address=192.168.1.16wsrep_on=ONbinlog_format=ROWdefault_storage_engine=InnoDBinnodb_autoinc_lock_mode=2#bind-address=0.0.0.0wsrep_slave_threads=1innodb_flush_log_at_trx_commit=0innodb_buffer_pool_size=122Mwsrep_sst_method=rsync 将此文件复制到mariadb-2、mariadb-3，注意要把 wsrep_node_name 和 wsrep_node_address 改成相应节点的 hostname 和 ip。 启动 MariaDB Galera Cluster 服务 /usr/sbin/mysqld –wsrep-new-cluster –user=root &amp;–wsrep-new-cluster 这个参数只能在初始化集群使用，且只能在一个节点使用。 观察日志： [root@node4 ~]# tail -f /var/log/message123150701 19:54:17 [Note] WSREP: wsrep_load(): loading provider library &apos;none&apos;150701 19:54:17 [Note] /usr/libexec/mysqld: ready for connections.Version: &apos;5.5.40-MariaDB-wsrep&apos; socket: &apos;/var/lib/mysql/mysql.sock&apos; port: 3306 MariaDB Server, wsrep_25.11.r4026 出现 ready for connections ,证明我们启动成功 查看是否启用galera插件连接mariadb,查看是否启用galera插件 目前集群机器数 查看集群状态show status like ‘wsrep%’; 查看连接的主机 另外两个节点mariadb会自动加入集群systemctl start mariadb这时查看galera集群机器数量 已经连接机器的ip 测试在mariadb-1上创建数据库，创建表，插入数据123456789101112131415161718192021MariaDB [(none)]&gt; create database test1；MariaDB [test1]&gt; insert into test values(1);Query OK, 1 row affected (0.01 sec)MariaDB [test1]&gt; insert into test values(2);Query OK, 1 row affected (0.01 sec)MariaDB [test1]&gt; insert into test values(3);Query OK, 1 row affected (0.00 sec)MariaDB [test1]&gt; insert into test values(4);Query OK, 1 row affected (0.00 sec)MariaDB [test1]&gt; insert into test values(5);Query OK, 1 row affected (0.03 sec)MariaDB [test1]&gt; insert into test values(6);Query OK, 1 row affected (0.01 sec)MariaDB [test1]&gt; insert into test values(7);Query OK, 1 row affected (0.01 sec) 在另外两台mariadb-2、mariadb-3上可以看见刚刚插入的数据，说明数据同步了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[win10启用ubuntu子shell]]></title>
      <url>%2F2016%2F12%2F22%2Fwin10_shell%2F</url>
      <content type="text"><![CDATA[以前想用linux的shell都得装第三方软件实现，比如git bash等，在win10周年更新版14393直接集成了bash on ubuntu了集成新的 Windows Subsystem for Linux 子系统，这样能直接在 Bash on Ubuntu 环境里编译运行 Linux 程序非常爽。 开启win10开发者模式win10设置 打开控制面板，启用或关闭windows功能 点确定，然后重启然后打开powershell或cmd直接在里面输入bash命令 它会询问你是否安装canonical 分发的ubuntu，输入y，然后等待。这里推荐使用vpn，然后安装完后，设置用户名，输入root用root权限吧。安装完后，然后你打开powershell和cmd输入bash就直接进入ubuntu子系统了 可以看出子bash还是非常给力的是ubuntu 14.04 3.4的内核。以后可以直接在上面跑python脚本了。不需要专门虚拟机。 默认字体蓝色不好看清楚，修改powershell或cmd的背景色即可。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack 本地yum源搭建]]></title>
      <url>%2F2016%2F12%2F12%2Fopenstack-localyumbuild%2F</url>
      <content type="text"><![CDATA[我们在部署openstack时用国外yum源的快，经常会很慢导致等待时间太久，所以建议使用本地yum源安装 这里以newton版centos7平台为例 首先下载官方repoyum install https://rdoproject.org/repos/rdo-release.rpm这时侯/etc/yum.repos.d里面会产生3个文件12[root@test yum.repos.d]# ls rdo-qemu-ev.repo rdo-release.repo rdo-testing.repo 我这里打算用http搭建我的本地yum服务器 先安装httpdyum install httpd mkdir /var/www/html/newton 待会将同步下来的包放这个目录cd /vaw/www/html/newton yum repolist –列出你所有的仓库 前面是repo id不包含x86_64 这里我只需要openstack-newton、和rdo-qemu-ev这两个软件库 先同步openstack-newtonreposync --repoid=openstack-newton 指定要下载的仓库id，会通过网络全部下载到当前目录下载下来。 同步完第一个继续同步第二个reposync --repoid=rdo-qemu-ev 同步完后这时查看 /vaw/www/html/newton里面已经有很多包了，只有软件包，没有repodate清单，所以需要自己重新createrepo来创建清单没有createrepo自己安装，创建软件清单createrepo /var/www/html/newton/ 然后启动httpd服务，其他机器通过httpd服务来访问yum源 例如控制节点yum源配置vim /etc/yum.repos.d/openstack.repo123456[openstack]name=openstackbaseurl=http://192.168.4.3/newtonenabled=1gpgcheck=0~ yum makecache 其他节点一样。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack newton 安装]]></title>
      <url>%2F2016%2F12%2F12%2Fopenstack-newton-install%2F</url>
      <content type="text"><![CDATA[环境配置机器配置：3台8v8G的虚拟机，1台做控制节点2台做融合节点。 网络划分：192.168.122.0/24 public网络192.168.3.0/24 存储网络192.168.4.0/24管理网络、sdn隧道网络 我这里配置了本地源，就不用在手动配置官网源本地源的搭建和配置会在另外一个文档说明。节点网络信息 ： 管理网络和随道网络 存储网络 公网 控制节点 192.168.4.6 192.168.3.5 192.168.122.2 计算节点 192.168.4.7 192.168.3.6 192.168.125.5 计算节点 192.168.4.8 192.168.3.7 192.168.122.6 网络拓扑 安装chrony控制节点向外同步时间，其他节点如计算节点都直接同步控制节点yum install chrony 修改配置文件vim /etc/chrony.conf添加下面这两条server cn.ntp.org.cn iburstallow 192.168.4.0/24 设置开机启动systemctl enable chronysystemctl start chrony 其他节点：yum install chrony 修改配置文件vim /etc/chrony.conf 添加下面这两条server 192.168.4.6 iburst 设置开机启动systemctl enable chrony 启动进程systemctl start chrony 安装openstack客户端yum install python-openstackclient 安装Mariadb（数据库服务）vim /etc/my.cnf.d/openstack.cnf1234567[mysqld] bind-address = 192.168.4.6 #填写管理网段ip default-storage-engine = innodb innodb_file_per_table max_connections = 4096collation-server = utf8_general_cicharacter-set-server = utf8 设置开机启动 systemctl enable mariadb 启动Mariadb systemctl start mariadb 安装rabbitmq（用于消息队列）yum install rabbitmq-server 设置开机启动 systemctl enable rabbitmq-server 开启rabbitmq systemctl start rabbitmq-server 创建openstack用户和配置密码 rabbitmqctl add_user openstack 123456 给openstack用户配置读和写权限 rabbitmqctl set_permissions openstack &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; 安装memcache（缓存token）yum install memcached phython-memcached systemctl enable memcached systemctl start memcached 安装keystone（认证服务）连接数据库 [root@control-node1 yum.repos.d]# mysql 创建keystone数据库 create database keystone; 数据库授权 密码自己设置，这里为了方便设置123456grant all privileges on keystone.* to &#39;keystone&#39;@&#39;localhost&#39; identified by &#39;123456&#39;; grant all privileges on keystone.* to &#39;keystone&#39;@&#39;%&#39; identified by &#39;123456&#39;; keystone使用httpd的mod_wsgi运行在端口5000和35357处理认证服务请求。默认情况下，keystone服务依然监听在5000和35357端口。 安装 keystone和wsgi yum install openstack-keystone httpd mod_wsgi 修改keystone配置文件 vim /etc/keystone/keystone.conf connection = mysql+pymysql://keystone:123456@192.168.4.6/keystone #加入连接数据库配置 配置使用哪种产生token方式目前keystone支持4种(UUID、PKI、PKIZ、Fernet)这里我们配置fernethttp://www.tuicool.com/articles/jQJNFrn 这篇文章有几种模式的详细介绍。 [token]provider = fernet 同步数据库 su -s /bin/sh -c “keystone-manage db_sync” keystone #发现同步数据库就是错了也没有反应，需要检查keystone的日志文件查看是否正确 初始化fernet keykeystone-manage fernet_setup –keystone-user keystone –keystone-group keystone keystone-manage credential_setup –keystone-user keystone –keystone-group keystoen 创建Bootstrap the Identity service(就是创建admin用户的帐号信息) 12345keystone-manage bootstrap --bootstrap-password 123456 \ --bootstrap-admin-url http://192.168.4.6:35357/v3/ \ --bootstrap-internal-url http://192.168.4.6:35357/v3/ \ --bootstrap-public-url http://192.168.122.2:5000/v3/ \ --bootstrap-region-id RegionOne 配置apache服务器vim /etc/httpd/conf/httpd.conf 配置成管理网段的ipServerName 192.168.4.6 将keystone的配置文件软链接到apache的配置文件ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/ 设置开机启动systemctl enable httpd 启动httpdsystemctl start httpd检查端口 12345678lsof -i:5000COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEhttpd 18883 root 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18894 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18895 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18896 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18897 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18898 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN) 到root下创建环境变量文件 123456789vim /root/openrc#!/bin/bashexport OS_USERNAME=adminexport OS_PASSWORD=123456 #这个密码是上面Bootstrap the Identity service填的密码export OS_PROJECT_NAME=adminexport OS_USER_DOMAIN_NAME=defaultexport OS_PROJECT_DOMAIN_NAME=defaultexport OS_AUTH_URL=http://192.168.4.6:35357/v3export OS_IDENTITY_API_VERSION=3 创建域、项目、用户创建service projectopenstack project create --domain default --description &quot;Service Project&quot; service 创建user角色openstack role create user这里不创建普通用户了测试admin用户获取tokenopenstack --os-auth-url http://192.168.4.6:35357/v3 token issue 安装glance镜像服务连接Mariadb创建数据库 create database glance; 授权123grant all privileges on glance.* to &apos;glance&apos;@&apos;localhost&apos; identified by &apos;123456&apos;; grant all privileges on glance.* to &apos;glance&apos;@&apos;%&apos; identified by &apos;123456&apos;; grant all privileges on glance.* to &apos;glance&apos;@&apos;control-node1.novalocal&apos; identified by &apos;123456&apos;; control-xxx换成主机名，我这里就算api.conf里面配置的ip默认还是去连接host是主机名，所以只能在加个主机名授权。 创建glance用户并设置密码openstack user create –domain default –password-prompt glance 给glance用户添加admin角色权限openstack role add –project service –user glance admin 创建glance serviceopenstack service create –name glance –description “OpenStack Image” image 创建glance endpoint123openstack endpoint create --region RegionOne image public http://192.168.122.2:9292openstack endpoint create --region RegionOne image internal http://192.168.4.6:9292openstack endpoint create --region RegionOne image admin http://192.168.4.6:9292 安装软件包yum install openstack-glance 配置glancevim /etc/glance/glance-api.conf 配置数据库12[database]connection = mysql+pymysql://glance:123456@192.168.4.6/glance 配置glancevim /etc/glance/glance-api.conf配置数据库12345678910111213141516[database]connection = mysql+pymysql://glance:123456@192.168.4.6/glance配置keystone[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = glancepassword = 123456[paste_deploy]flavor = keystone 配置镜像存储1234[glance_store]stores = file,httpdefault_store = filefilesystem_store_datadir = /var/lib/glance/images/ vim /etc/glance/glance-registry.conf12345678910111213141516[database]connection = mysql+pymysql://glance:123456@192.168.4.6/glance配置keystone[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = glancepassword = 123456[paste_deploy]flavor = keystone 同步数据库su -s /bin/sh -c “glance-manage db_sync” glance 设置开机启动systemctl enable openstack-glance-api 启动服务systemctl start openstack-glance-api 这些做完后最好在查看下日志，看看是否有错误，每部署完一个组件都这样，这样出错的可以很快定位。 下载cirros测试一下 wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.imgopenstack image create &quot;cirros&quot; --file cirros-0.3.4-x86_64-disk.img --disk-format qcow2 --container-format bare --public glance image-list 检查一下镜像是否上传成功 安装nova组件控制节点安装创建数据库create database nova_api;create database nova; 授权12345678GRANT ALL PRIVILEGES ON nova_api.* TO &apos;nova&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON nova_api.* TO &apos;nova&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON nova_api.* TO &apos;nova&apos;@&apos;control-node1.novalocal&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON nova.* TO &apos;nova&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON nova.* TO &apos;nova&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON nova.* TO &apos;nova&apos;@&apos;control-node1.novalocal&apos; IDENTIFIED BY &apos;123456&apos;; 创建为nova组件创建用户、service、endpointopenstack user create --domain default --password-prompt nova 给nova用户添加admin角色权限openstack role add --project service --user nova admin 创建serviceopenstack service create --name nova --description &quot;OpenStack Compute&quot; compute 创建endpoint12345openstack endpoint create --region RegionOne compute public http://192.168.122.2:8774/v2.1/%\(tenant_id\)s openstack endpoint create --region RegionOne compute internal http://192.168.4.6:8774/v2.1/%\(tenant_id\)s openstack endpoint create --region RegionOne compute admin http://192.168.4.6:8774/v2.1/%\(tenant_id\)s 安装nova-api组件yum install openstack-nova-api openstack-nova-conductor openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler 配置novavim /etc/nova/nova.conf 123456789101112131415161718192021222324252627282930313233[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6 #配置rabbitmq帐户和密码my_ip = 192.168.4.6use_neutron = Truefirewall_drive = nova.virt.firewall.NoopFirewallDriverenabled_apis=osapi_compute,metadataauth_strategy=keystone[api_database]connection = mysql+pymysql://nova:123456@192.168.4.6/nova_api #配置nova连接数据库[database]connection = mysql+pymysql://nova:123456@192.168.4.6/nova[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = novapassword = 123456配置novncnovncproxy_port=6080novncproxy_base_url=http://211.156.182.144:6080/vnc_auto.htmlvncserver_listen=192.168.4.6[glance]api_servers = http://192.168.4.6:9292[oslo_concurrency]lock_path = /var/lib/nova/tmp 同步数据库su -s /bin/sh -c “nova-manage api_db sync” novasu -s /bin/sh -c “nova-manage db sync” nova 1234567891011systemctl enable openstack-nova-api.service \openstack-nova-consoleauth.service \openstack-nova-scheduler.service \openstack-nova-conductor.service \openstack-nova-novncproxy.servicesystemctl start openstack-nova-api.service \openstack-nova-consoleauth.service \openstack-nova-scheduler.service \openstack-nova-conductor.service \openstack-nova-novncproxy.service 计算机节点安装安装nova-computeyum install openstack-nova-compute 配置nova-computevim /etc/nova/nova.conf123456789101112131415161718192021222324252627282930313233[DEFAULT]enabled_apis = osapi_compute,metadatatransport_url = rabbit://openstack:123456@192.168.4.6 #配置rabbitmq帐号和密码auth_strategy = keystonemy_ip = 192.168.4.7use_neutron = Truefirewall_driver = nova.virt.firewall.NoopFirewallDriver[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = novapassword = 123456[vnc]enabled=truevncserver_listen=0.0.0.0vncserver_proxyclient_address=192.168.4.7 #填写本机ipnovncproxy_base_url=http://211.156.182.144:6080/vnc_auto.html #这个填你要用控制节点的public ip[glance]api_servers = http://192.168.4.6:9292配置锁路径[oslo_concurrency]lock_path = /var/lib/nova/tmp[libvirt]virt_type = qemu #物理服务器就配置kvm虚拟机就配置qemu 设置开机启动systemctl enable libvirtd.service openstack-nova-compute.service 启动nova-computesystemctl start libvirtd.service openstack-nova-compute.service 在控制节点查看检查一下compute进程根控制节点连接 配置neutron控制节点安装我这里使用openvswitch不使用linux bridge，因为openvswitch功能比linux Brige功能强太多了。但配置稍微复杂点。 创建数据库create database neutron; 授权123GRANT ALL PRIVILEGES ON neutron.* TO &apos;neutron&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON neutron.* TO &apos;neutron&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON neutron.* TO &apos;neutron&apos;@&apos;control-node1.novalocal&apos; IDENTIFIED BY &apos;123456&apos;; 创建neutron用户并设置密码openstack user create --domain default --password-prompt neutron 给neutron用户添加admin角色权限openstack role add --project service --user neutron admin 创建neutron serviceopenstack service create --name neutron --description &quot;OpenStack Networking&quot; network 创建neutron endpoint123openstack endpoint create --region RegionOne network public http://192.168.122.2:9696 openstack endpoint create --region RegionOne network admin http://192.168.4.6:9696 openstack endpoint create --region RegionOne network internal http://192.168.4.6:9696 安装neutron组件yum -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch vim /etc/neutron/neutron.conf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[DEFAULT]service_plugins = routertransport_url = rabbit://openstack:123456@192.168.4.6auth_strategy = keystonenotify_nova_on_port_status_changes = Truenotify_nova_on_port_data_changes = Truestate_path = /var/lib/neutronuse_syslog = Truesyslog_log_facility = LOG_LOCAL4log_dir =/var/log/neutroncore_plugin = neutron.plugins.ml2.plugin.Ml2Pluginbase_mac = fa:16:3e:00:00:00mac_generation_retries = 32dhcp_lease_duration = 600dhcp_agent_notification = Trueallow_bulk = Trueallow_pagination = Falseallow_sorting = Falseallow_overlapping_ips = Trueadvertise_mtu = Trueagent_down_time = 30router_scheduler_driver = neutron.scheduler.l3_agent_scheduler.ChanceSchedulerallow_automatic_l3agent_failover = Truedhcp_agents_per_network = 2api_workers = 9rpc_workers = 9network_device_mtu=1450[database]connection = mysql+pymysql://neutron:123456@192.168.4.6/neutron[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = neutronpassword = 123456[nova]auth_url = http://192.168.4.6:35357auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = novapassword = 123456[oslo_concurrency]lock_path = /var/lib/neutron/tmp 配置modular layer 2（ml2）插件 vim /etc/neutron/plugins/ml2/ ml2_conf.ini1234567891011121314151617181920212223242526272829[DEFAULT]type_drivers = flat,vxlantenant_network_types = vxlanmechanism_drivers = openvswitch,l2populationextension_drivers = port_security[ml2]path_mtu = 1450type_drivers = flat,vxlantenant_network_types = vxlanphysical_network_mtus =physnet1:1500[ml2_type_flat]flat_networks =*[ml2_type_vxlan]vni_ranges =2:65535vxlan_group =224.0.0.1[securitygroup]enable_security_group = Truefirewall_driver=neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver[ovs]local_ip=192.168.4.6tunnel_bridge=br-tunenable_tunneling=Trueintegration_bridge=br-intbridge_mappings=physnet1:br-ex 配置l3-agentvim /etc/neutron/l3_agent.ini12345678910[DEFAULT]debug = Falseinterface_driver =neutron.agent.linux.interface.OVSInterfaceDriverhandle_internal_only_routers = Truemetadata_port = 8775send_arp_for_ha = 3periodic_interval = 40periodic_fuzzy_delay = 5enable_metadata_proxy = Truerouter_delete_namespaces = True 配置dhcp_agentvim /etc/neutron/dhcp_agent.ini12345678910[DEFAULT]resync_interval = 30interface_driver =neutron.agent.linux.interface.OVSInterfaceDriverenable_isolated_metadata = Trueenable_metadata_network = Falsedhcp_domain = openstacklocaldhcp_broadcast_reply = Falsedhcp_delete_namespaces = Trueroot_helper=sudo neutron-rootwrap /etc/neutron/rootwrap.confstate_path=/var/lib/neutron vim /etc/neutron/plugins/ml2/openvswitch_agent.ini1234567891011121314151617[agent]polling_interval = 2tunnel_types = vxlanvxlan_udp_port = 4789l2_population = Trueprevent_arp_spoofing = Falseextensions =[ovs]local_ip=192.168.4.6tunnel_bridge=br-tunenable_tunneling=Trueintegration_bridge=br-intbridge_mappings=physnet1:br-ex[securitygroup]firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriverenable_security_group = true 设置openvswitch开机启动systemctl enable openvswitch.service 启动openvswitchsystemctl start openvswitch 创建br-ex br-tun br-intovs-vsctl add-br br-intovs-vsctl add-br br-exovs-vsctl add-br br-tun 将上外网网卡挂载到br-ex上ovs-vsctl add-port br-ex eth2 设置开机启动项systemctl enable neutron-openvswitch-agent.service启动进程systemctl start neutron-openvswitch-agent.service 配置计算节点neutron配置一下内核参数修改配置文件 /etc/sysctl.conf12net.ipv4.conf.all.rp_filter=0net.ipv4.conf.default.rp_filter=0 sysctl –pyum -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch vim /etc/neutron/neutron.conf 1234567891011121314151617[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6auth_strategy = keystone[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = neutronpassword = 123456[oslo_concurrency]lock_path = /var/lib/neutron/tmp vim /etc/neutron/plugins/ml2/ml2_conf.ini1234567891011[ml2]path_mtu = 1450type_drivers = flat,vxlantenant_network_types = vxlanphysical_network_mtus =physnet1:1500mechanism_drivers = openvswitch,l2populationextension_drivers = port_security[securitygroup]enable_ipset = truefirewall_driver=neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver vim /etc/neutron/plugins/ml2/openvswitch_agent.ini1234567891011121314151617181920[ovs]local_ip=192.168.4.7tunnel_bridge=br-tunenable_tunneling=Trueintegration_bridge=br-intbridge_mappings=physnet1:br-ex[agent]enable_distributed_routing=Trueprevent_arp_spoofing=Truearp_responder=Truepolling_interval=2drop_flows_on_start=Falsevxlan_udp_port=4789l2_population=Truetunnel_types=vxlan[securitygroup]firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriverenable_security_group = true systemctl enable openvswitch.servicesystemctl start openvswitch.service 创建br-ex、br-int、br-tunovs-vsctl add-br br-intovs-vsctl add-br br-exovs-vsctl add-br br-tun vim /etc/nova/nova.conf123456789101112131415[DEFAULT]network_api_class = nova.network.neutronv2.api.APIsecurity_group_api = neutronlinuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver[neutron]url = http://192.168.4.6:9696auth_url = http://192.168.4.6:35357auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = neutronpassword = 123456 systemctl restart neutron-openvswitch-agent.servicesystemctl restart openstack-nova-compute 安装完后可以在在控制节点检查是否安装成功 安装控制台yum install openstack-dashboardvim /etc/openstack-dashboard/local_settings12345678910111213141516171819202122232425262728这里配置控制节点ipOPENSTACK_HOST = &quot;192.168.4.6&quot;配置允许所有节点访问ALLOWED_HOSTS = [&apos;*&apos;, ]配置memcacheSESSION_ENGINE = &apos;django.contrib.sessions.backends.cache&apos;CACHES = &#123; &apos;default&apos;: &#123; &apos;BACKEND&apos;: &apos;django.core.cache.backends.memcached.MemcachedCache&apos;, &apos;LOCATION&apos;: &apos;192.168.4.6:11211&apos;, &#125;&#125;配置keystone v3验证OPENSTACK_KEYSTONE_URL = &quot;http://%s:5000/v3&quot; % OPENSTACK_HOST配置域OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = &apos;default&apos;配置api版本OPENSTACK_API_VERSIONS = &#123; &quot;identity&quot;: 3, &quot;image&quot;: 2, &quot;volume&quot;: 2,&#125;设置通过控制台默认创建用户的角色是userOPENSTACK_KEYSTONE_DEFAULT_ROLE = &quot;user&quot; 重启服务systemctl restart httpd.service memcached.service 通过http://control_ip/dashboard可以访问 Admin 登录，密码是你通过keystone创建的，如果不记得查看openrc 创建flat网络做float_ip池管理员—&gt;网络——&gt;创建网络 Phynet1是在ml2.ini里面bridge_mappings定义的br-ex对应的名字，创建完后增加子网，然后在创建个普通网络，创建个路由器，路由器绑定普通子网，创建个主机配置，然后创建vm加入到你创建的普通网络 这时在vm所在的计算节点或控制节点 ovs-vsctl show 可以看见计算节点根网络节道隧道已经建立。 Cinder配置配置控制节点创建数据库create database cinder;用户授权123GRANT ALL PRIVILEGES ON cinder.* TO &apos;cinder&apos;@&apos;localhost&apos; identified by &apos;123456&apos;; GRANT ALL PRIVILEGES ON cinder.* TO &apos;cinder&apos;@&apos;%&apos; identified by &apos;123456&apos;; GRANT ALL PRIVILEGES ON cinder.* TO &apos;cinder&apos;@&apos;control-node1.novalocal&apos; identified by &apos;123456&apos;; 创建用户openstack user create --domain default --password-prompt cinder 给cinder用户赋予admin权限openstack role add --project service --user cinder admin openstack service create --name cinder --description &quot;OpenStack Block Storage&quot; volume openstack service create --name cinderv2 --description &quot;OpenStack Block Storage&quot; volumev2 创建endpoint1234567891011121314151617openstack endpoint create --region RegionOne \volume public http://192.168.122.2:8776/v1/%\(tenant_id\)sopenstack endpoint create --region RegionOne \volume internal http://192.168.4.6:8776/v1/%\(tenant_id\)sopenstack endpoint create --region RegionOne \volume admin http://192.168.4.6:8776/v1/%\(tenant_id\)sopenstack endpoint create --region RegionOne \volumev2 public http://192.168.122.2:8776/v2/%\(tenant_id\)sopenstack endpoint create --region RegionOne \volumev2 internal http://192.168.4.6:8776/v2/%\(tenant_id\)sopenstack endpoint create --region RegionOne \volumev2 admin http://192.168.4.6:8776/v2/%\(tenant_id\)s 安装cinderyum install openstack-cinder vim /etc/cinder/cinder.conf1234567891011121314151617181920[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6auth_strategy = keystone[database]connection = mysql+pymysql://cinder:123456@192.168.4.6/cinder[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = cinderpassword = 123456[oslo_concurrency]lock_path = /var/lib/cinder/tmp 同步数据库su -s /bin/sh -c “cinder-manage db sync” cinder 配置计算机节点使用cindervim /etc/nova/nova.conf [cinder]os_region_name = RegionOne 重启服务systemctl restart openstack-nova-api.service 设置开机自启cindersystemctl enable openstack-cinder-api.service openstack-cinder-scheduler.service systemctl start openstack-cinder-api.service openstack-cinder-scheduler.service 配置一个存储节点安装lvmyum install lvm2 systemctl enable lvm2-lvmetad.servicesystemctl start lvm2-lvmetad.service 创建个lvm卷pvcreate /dev/vdb 创建vgvgcreate cinder-volumes /dev/vdb vim /etc/lvm/lvm.conf123devices &#123;filter = [ &quot;a/vdb/&quot;, &quot;r/.*/&quot;]&#125; 安装软件yum install openstack-cinder targetcli python-keystone 修改cinder配置文件vim /etc/cinder/cinder.conf1234567891011121314151617181920212223242526272829[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6verbose = Trueauth_strategy = keystoneenabled_backends = lvmglance_api_servers = http://192.168.4.6:9292[database]connection = mysql+pymysql://cinder:123456@192.168.4.6/cinder[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = cinderpassword = 123456[lvm]volume_driver = cinder.volume.drivers.lvm.LVMVolumeDrivervolume_group = cinder-volumesiscsi_protocol = iscsiiscsi_helper = lioadm[oslo_concurrency]lock_path = /var/lib/cinder/tmpsystemctl enable openstack-cinder-volume.service target.service systemctl start openstack-cinder-volume.service target.service 到控制台创建个卷，并挂载到云主机。 Ceilometer配置Ceilometer使用Mongdb存储meter数据，所以需要先在控制节点安装Mongdb 在控制节点安装Mongdbyum install mongodb-server mongodb 配置Mongdbvim /etc/mongod.confsmallfiles = true #限制日志大小创建Mongdb数据库和帐户授权替换123456为你自己设置的密码创建用户openstack user create –domain default –password-prompt ceilometer 给ceilometer用户添加admin角色权限openstack role add –project service –user ceilometer admin创建ceilometer serviceopenstack service create –name ceilometer –description “Telemetry” metering 创建ceilometer endpoint123openstack endpoint create --region RegionOne metering public http://192.168.122.2:8777openstack endpoint create --region RegionOne metering admin http://192.168.4.6:8777openstack endpoint create --region RegionOne metering internal http://192.168.4.6:8777 安装包yum install openstack-ceilometer-api \openstack-ceilometer-collector \openstack-ceilometer-notification \openstack-ceilometer-central \python-ceilometerclient 配置ceilometervim /etc/ceilometer/ceilometer.conf123456789101112131415161718192021222324252627282930[DEFAULT]rpc_backend = rabbitauth_strategy = keystone[oslo_messaging_rabbit]rabbit_host = 192.168.4.6rabbit_userid = openstackrabbit_password = 123456[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = ceilometerpassword = 123456[service_credentials]auth_type = passwordauth_url = http://192.168.4.6:5000/v3project_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = ceilometerpassword = 123456 ##密码替换成在keystone创建ceilometer时设置的密码interface = internalURLregion_name = RegionOne 创建ceilometer的vhost vim /etc/httpd/conf.d/wsgi-ceilometer.conf 123456789101112Listen 8777&lt;VirtualHost *:8777&gt; WSGIDaemonProcess ceilometer-api processes=2 threads=10 user=ceilometer group=ceilometer display-name=%&#123;GROUP&#125; WSGIProcessGroup ceilometer-api WSGIScriptAlias / /usr/lib/python2.7/site-packages/ceilometer/api/app.wsgi WSGIApplicationGroup %&#123;GLOBAL&#125; ErrorLog /var/log/httpd/ceilometer_error.log CustomLog /var/log/httpd/ceilometer_access.log combined&lt;/VirtualHost&gt;WSGISocketPrefix /var/run/httpd 重启httpdsystemctl reload httpd.service设置服务开机启动systemctl enable openstack-ceilometer-notification.service \openstack-ceilometer-central.service \openstack-ceilometer-collector.service 启动进程systemctl start openstack-ceilometer-notification.service \openstack-ceilometer-central.service \openstack-ceilometer-collector.service 配置glance的ceilometer统计vim /etc/glance/glance-api.conf 12345678910[DEFAULT]rpc_backend = rabbit[oslo_messaging_amqp]driver = messagingv2[oslo_messaging_rabbit]rabbit_host = 192.168.4.6rabbit_userid = openstackrabbit_password = 123456 重启进程systemctl restart openstack-glance-api.service openstack-glance-registry.service 配置nova的ceilometer统计安装软件yum install openstack-ceilometer-compute python-ceilometerclient python-pecan 123456789101112131415161718192021222324252627282930[DEFAULT]rpc_backend = rabbitauth_strategy = keystone[oslo_messaging_rabbit]rabbit_host = 192.168.4.6rabbit_userid = openstackrabbit_password = 123456[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = ceilometerpassword = 123456 #将密码替换成keystone创建ceilometer用户时设置的密码[service_credentials]auth_url = http://192.168.4.6:5000project_domain_id = defaultuser_domain_id = defaultauth_type = passwordusername = ceilometerproject_name = servicepassword = 123456 #将密码替换成keystone创建ceilometer用户时设置的密码interface = internalURLregion_name = RegionOne 修改nova-compute配置文件 vim /etc/nova/nova.conf 1234567[DEFAULT]instance_usage_audit = Trueinstance_usage_audit_period = hournotify_on_state_change = vm_and_task_state[oslo_messaging_amqp]driver = messagingv2 设置开机启动systemctl enable openstack-ceilometer-compute.service启动ceilometer-compute进程systemctl start openstack-ceilometer-compute.service重启nova-computesystemctl restart openstack-nova-compute.service 配置块设备使用ceilometer计量服务 验证ceilometer meter-list 正常情况下是会出现如上图一些资源的数据的，但我这里默认报 打debug 访问被拒绝解决办法：修改httpd.conf systemctl restart httpd在测试应该没问题了。 Aodh报警服务：创件aodh数据库create database aodh; 授权123GRANT ALL PRIVILEGES ON aodh.* TO &apos;aodh&apos;@&apos;localhost&apos; identified by &apos;123456&apos;; GRANT ALL PRIVILEGES ON aodh.* TO &apos;aodh&apos;@&apos;%&apos; identified by &apos;123456&apos;; GRANT ALL PRIVILEGES ON aodh.* TO &apos;aodh&apos;@&apos;control-node1.novalocal&apos; identified by &apos;123456&apos;; 创建用户openstack user create --domain default --password-prompt aodh 给adoh帐户添加admin权限openstack role add --project service --user aodh admin 添加服务openstack service create --name aodh --description &quot;Telemetry&quot; alarming 创建endpoint123openstack endpoint create --region RegionOne alarming public http://192.168.122.2:8042 openstack endpoint create --region RegionOne alarming internal http://192.168.4.6:8042 openstack endpoint create --region RegionOne alarming admin http://192.168.4.6:8042 安装软件yum install openstack-aodh-api \openstack-aodh-evaluator \openstack-aodh-notifier \openstack-aodh-listener \openstack-aodh-expirer \python-aodhclient 修改配置文件 vim /etc/aodh/aodh.conf12345678910111213141516171819202122232425262728[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6auth_strategy = keystone[database]connection = mysql+pymysql://aodh:123456@192.168.4.6/aodh[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = aodhpassword = 123456 #填写通过keystone创建帐户时设置的帐号和密码[service_credentials]auth_type = passwordauth_url = http://192.168.4.6:5000/v3project_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = aodhpassword = 123456 #填写通过keystone创建帐户时设置的帐号和密码interface = internalURLregion_name = RegionOne systemctl enable openstack-aodh-api.service \openstack-aodh-evaluator.service \openstack-aodh-notifier.service \openstack-aodh-listener.service systemctl start openstack-aodh-api.service \openstack-aodh-evaluator.service \openstack-aodh-notifier.service \openstack-aodh-listener.service]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Fastnetmon配置与使用]]></title>
      <url>%2F2016%2F12%2F09%2Ffastnetmon%2F</url>
      <content type="text"><![CDATA[Fastnetmon介绍FastNetmon是一个基于多种抓包引擎来对数据包进行统计分析的DOS/DDOS工具，可以探测和分析网络中的异常流量情况，同时，也可以对捕获的异常调用外部脚本进行处理警报啊或者进行阻断处理，全靠外部脚本是如何定义。 项目首页http://www.open-open.com/lib/view/home/143193107973 部署架构 安装和配置方法安装 下载自动安装脚本wget https://raw.githubusercontent.com/FastVPSEestiOu/fastnetmon/master/src/fastnetmon_install.pl -Ofastnetmon_install.pl 将整个项目克隆下来https://github.com/FastVPSEestiOu/fastnetmon 安装注意若安装报连接错误，请将DNS改为8.8.8.8或223.5.5.5，因为下载地址都是在国外，有些DNS可能解析不够好。perl fastnetmon_install.pl 启动安装脚本。 配置 配置文件，cd到你刚刚克隆的项目里面。cp src/Fastnetmon.conf 到/etc/cp src/fastnetmon_init_script_centos6 /etc/init.d/fastnetmon #cp启动脚本到/etc/init.d/chmod 755 /etc/init.d/fastnetmon #修改启动脚本权限cp src/notify_about_attack.sh /usr/local/bin/ #cp 通知脚本 /etc/fastentmon配置项ban_time = 1900 #对检测到攻击的ip进行多久封锁。enable_subnet_counters = on #检测每个子网进出流量enable_connection_tracking = on #开启攻击追踪检测，通过这个选项在日志文件里可以详细看见攻击者ip和其他一些详细情况ban_for_pps = on，ban_for_bandwidth = on，ban_for_flows = on #检测的选项pps(每秒包)，bandwidth(带宽)，flows(流量)。threshold_pps = 20000，threshold_mbps = 1000，threshold_flows = 3500 #监控的限定值抓包引擎选择mirrof=off没有安装PF_RING就不要开启，不然会启动报错。pfring_sampling_ratio = 1 #端口镜像采样率mirror_netmap = off没有安装Netmap就不要开启，不然会会启动报错。mirror_snabbswitch=on #开启snabbswitch流量捕获。mirror_afpacket =on #AF_PACKET捕获引擎开启netmap_sampling_ratio = 1 #端口镜像抽样比pcap=on #pcap引擎开启netflopw=on #使用Netflow捕获方法interfaces=enp5s0f1 #监控的端口，我这里使用的是镜像端口,不然监控不到整个网端流量notify_script_path=/usr/local/bin/notify_about_attack.sh #触发脚本位置monitor_local_ip_addresses = on #监控本地地址sort_parameter = packets #在控制台排序单位，包max_ips_in_list =400 #在控制台显示多少地址 编辑要监控的网段vim /etc/networks_list #编辑networks_list加入要监控的网段这里我加入211.156.182.0/24 220.242.2.0/24/etc/init.d/fastnetmon start #启动fastentmon，启动失败查看/var/log/fastnetmon.log/opt/fastnetmon/fastnetmon #打开监控控制台 修改监控脚本1,要先外发邮件必须配置mailx 安装mailx，配置SMTP vim /etc/mail.rc 我这里配置的是我163的邮箱set bsdcompatset from=xxxxx@163.com smtp=smtp.163.comset smpt-auth-user=xxxx@163.com set smtp-auth-user=xxxxxx@163.com smtp-auth-password=xxxxxx smtp-auth=login vim /usr/local/bin/notify_about_attack.sh #这里填要触发监控的脚本修改邮件地址为接受人。cat | mail -s “FastNetMon Guard: IP $1 blocked because $2 attack with power $3 pps” $email_notify; 测试使用低轨道等离子炮（Loic）进行测试这里我对我们211.156.182.143进行Tcp DDOS攻击我们先把这些值调低threshold_pps = 2000，threshold_mbps = 100，threshold_flows = 350 #监控的限定值然后重启Fastnetmon 测试完修改回值]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hello]]></title>
      <url>%2F2016%2F12%2F06%2Fhello%2F</url>
      <content type="text"><![CDATA[你好,欢迎来到我的个人技术博客.]]></content>
    </entry>

    
  
  
</search>
