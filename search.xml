<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[OpenStack对接vmware]]></title>
      <url>%2F2017%2F02%2F25%2Fvmware%26openstack%2F</url>
      <content type="text"><![CDATA[openstack版本：libertyvsphere版本：6.0 拓扑图 架构图 vmware对接openstack有两种驱动1，VCDriver，以nova driver的方式通过vcenter来控制计算和网络2，VMDKDriver，以cinder driver的方式通过vcenter来控制datastore。 这里采用vcdriver的方式这种方式目前有几个问题1，就是已经存在vmware集群的vm是不能被openstack管理了，只有新创建的机器可以被openstack管理。 2，vlan模式下，VCDriver要求vCenter里被使用的VLAN port group的名字必须和OpenStack里integration bridge（默认是br-int）的名字一样，而且vlan 号也得一样，这里我配置port group为0，表示不做限制。 3，首次创建虚机会花费很长时间，因为需要将镜像从glance通过vcenter 拷贝到cluster后端的datasotre，后面再创建速度就会很快。 要点：一台计算节点对接vsphere，需要这台计算节点能根vcenter通信。 Vsphere上的配置配置好对应的openstack计算节点对应的cluster如这个vsphere的cluster_name为UAT_Lenovo_6.0 cluster对应的storage名字 创建对应的port_group nova配置需要注意对应的物理适配器，待会需要写到配置项内。12345678910openstack平台对接vcenter配置参考compute_driver #对应的驱动host_ip #连接vsphere的ip地址host_username #管理员帐户host_password #管理员密码datastore_rege #cluster对应的后端存储cluster_name #对应的cluster 名字vlan_interface #port group对应的物理网卡vmware.integration_bridge #对应的port_groupinsecure=True #关闭ssl连接不然报下面错 修改/etc/nova/nova.conf配置文件：123456789101112[default]compute_driver = vmwareapi.VMwareVCDriver[vmware]host_ip=10.3.1.11host_username=administrator@UAT.comhost_password=1qaz@WSXdatastore_regex=Huawei_S5500_01cluster_name=UAT_Lenovo_6.0api_retry_count=10vlan_interface=vmnic0vmware.integration_bridge=br-intinsecure=True 需要注意这些参数仔细核对。 启动openstack-nova-compute服务：systemctl restart openstack-nova-compute 此时应该可以看见vmware 对应的hypervisor 转换镜像将一个qcow2的镜像转换成vmware用的vmdk格式 glance配置转换镜像格式为vmdk：qemu-img convert -f raw -O vmdk centos_6.5.raw centos_6.5.vmdk 上传镜像：1glance image-create --name=centos-6.5.vmdk --disk-format=vmdk --property hypervisor_type=&quot;vmware&quot; --property vmware_adaptertype=&quot;ide&quot; --property vmware_disktype=&quot;sparse&quot; --is-public=True --property vmware_ostype=&quot;otherLinuxGuest&quot; --container-format=bare &lt; centos_6.5.vmdk 创建虚拟机进行测试 第一次创建时间比较久因为要将镜像从glance 拷贝到vmware 的storage内 可以看见vsphere内已经有以uuid命名的vm了，表示创建成功 后面为可选配置，配置cinder对接vmware存储 cinder配置编辑/etc/cinder/cinder.conf配置文件，修改如下参数：12345volume_driver=cinder.volume.drivers.vmware.vmdk.VMwareVcVmdkDrivervolume_backend_name=vmwarevmware_host_ip=10.68.35.24vmware_host_username=administratorvmware_host_password=admin123 重启cinder-volume服务：/etc/init.d/openstack-cinder-volume restart创建多类型存储：cinder type-create vmwarecinder type-key vmware set volume_backend_name=vmwarecinder type-create cephcinder type-key ceph set volume_backend_name=DEFAULT 创建云硬盘，挂在云主机进行测试 参考链接1234http://blog.csdn.net/jmilk/article/details/52102020http://blog.csdn.net/halcyonbaby/article/details/37818789http://www.cnblogs.com/zhoumingang/p/5514556.htmlhttp://www.iyunv.com/forum.php?mod=viewthread&amp;tid=86702]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[vxlan]]></title>
      <url>%2F2017%2F02%2F24%2Fvxlan%2F</url>
      <content type="text"><![CDATA[什么是vxlan？VXLAN（Virtual Extensible LAN）虚拟可扩展局域网是目前NVO3(Network Virtualization Over Layer 3 基于三层IP overlay网络构建虚拟网络技术统称NVO3)，它是目前NVO3中影响力最为广泛的一种，它通过L2 over L4 (MAC in UDP)的报文封装方式，实现基于IP overlay的虚拟局域网。 传统网络存在的一些问题1，传统的二层网络，交换机通过mac地址进行数据转发，mac地址一多会造成交换机的转发速率变慢，同时交换机的mac地址表的大小是有限制的，在云计算中，mac表容量限制了虚拟机的数量。2，VLAN的vlan id是一个12bit，最大只支持4096个vlan，这在云计算中是远远不够的。 vxlan的提出很好的解决了上述问题1，vxlan采用MAC in UDP的方式将vm主机的数据报文封装在UDP中，并使用物理网络VTEP的IP和MAC地址在外层包头封装进行数据传输，对外表现为VTEP之间的mac地址，极大的降低了交换机mac地址使用。 2，VXLAN报文中拥有一个24bit vni段，vxlan隔离不同租户就是通过vni来进行，一个vni表示一个租户，不同vni之间二层不能直接通信，但可以通过vxlan三层网关进行通信。即使多个终端用户属于同一个vni也属于同一个租户。 VXLAN的优点1，基于ip的overlay网络，仅需要边界VTEP设备间可通信2，ip overlay TTL避免环路。3，overlay+vni构建虚拟网络支持多达16M的虚拟网络，充分满足多租户的需求4，接入交换机只需要学习物理服务器的mac地址，不需要学习每台VM的mac地址减轻了交换机的负担 概念NVE(Network virtrualization Edge网络虚拟边缘节点）是实现网络虚拟化功能的实体。VTEP(vxlan tunnel end point vxlan 隧道端点）vxlan网络中的NVE以VTEP为标识；每个NVE至少得有一个VTEP；VTEP使用NVE的IP地址表示；两个VTEP之间可以确立一条VXLAN隧道，VTEP间的这条VXLAN隧道将两个NVE之间的所有的VNI所公用。VTEP可以由专有硬件来实现，也可以使用纯软件实现，硬件的实现是通过一些SDN交换机，软件的实现主要有 1，带vxlan内核模块的Linux2，openvswitch vxlan报文格式 端口默认使用4789端口报文中源IP为报文的虚拟机所属的VTEP的IP地址，目的IP为目的虚拟机所属的VTEP的IP，目标ip地址可以为单播地址也可以为组播地址 vxlan网络架构图 vxlan使用 MAC IN UDP的方式来延伸二层网络，是一种扩展大二层网络的隧道封装技术。 NVE负责将vm的报文封装，建立隧道网络。如服务器上的openvswitch就是一个NVE。 VXLAN报文的转发一种是BUM(broadcast&amp;unknown-unicast&amp;multicast)报文转发，一种是已知的单播报文转发 BUM报文转发：采用头端复制的方式(vm上层VTEP接口收到BUM报文后本地VTEP通过控制平面获取同一VNI的VTEP列表，将收到的BUM报文根据VTEP列表进行复制并发送给属于同一VNI的所有VTEP)并进行报文封装。 1，switch_1收到来终端A发出的报文后，提取报文信息，并判断报文的目的MAC地址是否为BUM MAC. 是，在对应的二层广播域内广播，并跳转2。 不是，走已知单播报文转发流程。2，switch_1根据报文中的VNI信息获取同一VNI的VTEP列表获取对应的二层广播域，进行vxlan封装，基于出端口和vxlan封装信息封装vxlan头和外层IP信息，进行二层广播。3，switch_2和switch_3上VTEP收到VXLAN报文后，根据UDP目的端口号、源和目的IP地址VNI判断VXLAN报文的合法有效性，依据VNI获取对应的二层广播域，然后进行VXLAN解封装，获取内层二层报文，判断报文的目的MAC是否为BUM MAC 是，在对应的二层广播域内非VXLAN侧进行广播处理。 不是，再判断是否是本机的MAC地址。 是，本机MAC，上送主机处理。 不是，在对应的二层广播域内查找出接口和封装信息并跳转4.4，switch_2和switch_3根据查找到的出接口和封装信息，为报文添加VLAN tag，转发给对应的终端B/C. 已知单播报文转发 1，switch_1收到来自终端A的报文，根据报文信息接入端口和VLAN信息获取对应的二层广播域，并判断报文的目的MAC是否为已知的单播MAC 是，在判断是否为本机MAC. 是，上送主机处理。 不是，在对应的二层广播域内查找出接口和封装信息，并跳装到2. 2， switch_1上VTEP根据查找到的出接口和封装信息进行VXLAN封装和报文转发。3，switch_2上VTEP收到VXLAN报文后，根据UDP目的端口号，源/目的IP地址，VNI判断VXLAN报文的合法有效性，依据VNI获取对应的二层广播域，然后进行VXLAN解封装、获取内层二层报文，判断报文的目的MAC是否为已知单播报文MAC。 是，在对应的二层广播域内查找出接口和封装信息，并跳转到4. 不是，在判断是否是本机的mac。 是，上送主机处理。 不是，走BUM报文转发流程。4，switch_2根据查找的出接口和封装信息，为报文添加VLAN tag，转发给对应的终端B。 为了防止不同vm之间通信发送arp广播引发广播风暴，vtep有一个arp proxy的功能也就是说在请求这个VTEP节点上的虚拟机的mac地址都是由VTEP的mac地址应答。也就是l2population。 vxlan网关vxlan下vm之间的通信方式有3种，同VNI下不同VM，不同VNI下跨网访问，vxlan和非vxlan之间访问。 vxlan网关分为二层网关、三层网关。二层网关：主要用于解决同VNI下不同VM之间通信，一般为vetp IP三层网关：用于解决不同VNI下跨网访问，和vxlan和非vxlan之间访问。 通俗理解就是在一台实体服务器上可以虚拟出一个交换机来，这个交换机就是VSwitch，而这个VSwitch下挂的不再是实体服务器，而是一个个VM，一个VM其实就是一个租户租用的服务器，不同租户之间肯定是不能互访的，要不然租户数据的安全性如何保障，这个隔离就是靠的VNI这个ID，其实这个你可以向下VLAN是如何隔离的，目的就是为了隔离租户。我一个租户有2个VM的话，那么我这2个之间应该可以互访吧。所以说基于VNI定义的租户，而非基于VM。内部的结构说清楚了再来说上行如何访问，在一个L2交换机你要跨网访问必然要经过网关，这个网关的IP地址就是VTEP IP，在网络上有个概念叫arp-proxy，一般用途是为了保护内部私有网络，外界的所有应答都有网关来代替回答（可以理解为门卫）。在这里外界只需要你的VTEP IP即可，对端报文到达VTEP这个网关后自己在内部走L2进行转发。因此VXLAN报文中的目的IP就是对端的网关（VTEP IP），而源地址自然也是自己的网关（VTEP IP）。而对于不同leaf上的同一VNI的VM来说，他们的VTEP IP肯定要配置相同，想下同一vlan下的服务器的网关是如何配置的就明白了 http://www.cnblogs.com/hbgzy/p/5279269.htmlhttp://blog.csdn.net/zztflyer/article/details/51883523 vxlan实验环境操作系统：centos7.2内核版本：3.10openvswitch版本：2.5 在两台虚拟机上安装好ovs，并启动这个实验目的，就是HOST 1的 br1，没有根物理网卡绑定，但可以通过建利vxlan隧道与HOST2的br1通信。 配置host1创建两个网桥br0、br1ovs-vsctl add-br br0ovs-vsctl add-br br1将eth0挂载到br0上ovs-vsctl add-port br0 eth0 将eth0的ip分配到br0上ifconfig eth0 0 &amp;&amp; ifconfig br0 192.168.1.2/24 给br1分配一个ipifconfig br1 192.168.2.2/24 配置host2创建两个网桥br0、br1ovs-vsctl add-br br0ovs-vsctl add-br br1将eth0挂载到br0上 ovs-vsctl add-port br0 eth0 将eth0的ip分配到br0上ifconfig eth0 0 &amp;&amp; ifconfig br0 192.168.1.3/24 给br1分配一个ipifconfig br1 192.168.2.3/24 此时在hots1上ping host2上的br0的ip是可以通的 ping br1的ip是pint不通的 通过建立vxlan隧道来实现br1之间通信在host1上执行给br1上增加一个vxlan0接口类型为vxlan，远程ip为 host2上的br0 ip， vni为100在host2上执行远端ip为host1上br0的ip 此时在host1上ping host2的br1的ip 就可以ping通了 在host2上的 eth0上抓包 有两层报文封装第一层源ip为192.168.2.2 目标ip为192.168.2.3，然后被udp封装，走vxlan隧道，第二层源ip为192.168.1.2 目标ip为192.168.1.3 网络包经过 vxlan interface 到达 eth1 的过程中，Linux vxlan 内核模块会将网络包二层帧封装成 UDP 包，因此，vxlan interface 必须设置适当的 MTU 来限制通过它的网络包的大小（vxlan interface 的 MTU 需要比它所绑定的物理网卡的 MTU 小 50），否则，封装后的包会被 eth1 丢弃。 VTEP (vxlan 随道端点）vswitch生成br-tun连接各个节点 像这些 为什么vlan只支持4096个因为vid只有12bit 2的12次方=4096 为什么用vxlan不用vlan？因为vlan最大只有4096个虚拟化技术的话用vlan运行vm一多，交换机上的mac地址会很多，影响交换机性能。stp算法会产生大量多路路径冗余 vxlan：建立在物理网络上的虚拟以太网vlxan是一种将二层报文用三层协议进行封装的技术，它进行传输的标识是通过VNI vni包含24bit所以vxlan最大支持2的24次方约16m个，会将二层数据包封装成udp报文通过隧道组播传输，一般配置的组播地址224.0.0.1发送arp，udp端口是4789，共50字节封装报头 UDP校验和：一般为0，非0则此包将会被丢弃。 数据包都是通过vtep进行封装传输在 OVS 中, 有几个非常重要的概念： Bridge: Bridge 代表一个以太网交换机（Switch），一个主机中可以创建一个或者多个 Bridge 设备。 Port: 端口与物理交换机的端口概念类似，每个 Port 都隶属于一个 Bridge。 Interface: 连接到 Port 的网络接口设备。在通常情况下，Port 和 Interface 是一对一的关系, 只有在配置 Port 为 bond 模式后，Port 和 Interface 是一对多的关系。 Controller: OpenFlow 控制器。OVS 可以同时接受一个或者多个 OpenFlow 控制器的管理。 datapath: 在 OVS 中，datapath 负责执行数据交换，也就是把从接收端口收到的数据包在流表中进行匹配，并执行匹配到的动作。 Flow table: 每个 datapath 都和一个“flow table”关联，当 datapath 接收到数据之后， OVS 会在flow table 中查找可以匹配的 flow，执行对应的操作, 例如转发数据到另外的端口。 veth-pair：一对接口，一个接口收另外一个接口同时也能收到，用于连接两个Brigge 设置网络接口设备的类型为“internal”。对于 internal 类型的的网络接口，OVS 会同时在 Linux 系统中创建一个可以用来收发数据的模拟网络设备。我们可以为这个网络设备配置 IP 地址、进行数据监听等等。 参考链接http://www.360doc.com/content/16/0227/12/3038654_537760576.shtmlhttp://blog.csdn.net/xjtuse2014/article/details/51376123?locationNum=7http://www.cnblogs.com/hbgzy/p/5279269.htmlhttp://blog.csdn.net/zztflyer/article/details/51883523]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[kolla 3.0.3部署openstack newton]]></title>
      <url>%2F2017%2F02%2F09%2Fkillo_newton%2F</url>
      <content type="text"><![CDATA[kolla是什么？容器化部署openstack，简而言之就是openstack里面每个组件都是用docker封装好了，对应docker的一个image。 容器化好处是什么?通过docker，降低openstack升级的难度，使升级对openstack影响最小，一旦升级失败，直接回滚旧的docker image，升级只需要三步：Pull新版本的容器镜像，停止老版本的容器服务，然后启动新版本容器。回滚也不需要重新安装包了，直接启动老版本容器服务就行，非常方便。对比过目前生产环境主流的部署方式，fuel、puppet、ROD， 我个人认为容器化部署将是未来的趋势。kolla底层通过ansoble去启动配置 已经封装好了的docker image。 缺点：要熟悉kolla，不仅仅是要熟悉openstack那么简单了，还要熟悉docker、和ansiable。 环境cpu：8核内存：16G根分区大小：100G至少需要2个网络接口（一个管理口，一个外网口）管理地址 192.168.122.77haproxy vip：192.168.122.76 安装时注意审查软件版本，我这里是安装newton版对应的是koll 3 ，2月24日发布的ocata版对应的是kolla 4 搭建docker本地镜像库 安装epel源和python-pip 安装dockercurl -sSL https://get.docker.io | bash 查看docker版本，确认docker是否安装成功。 修改docker参数，如果没有修改的话，会造成部署 neutron-dhcp-agent container 和访问APIError/HTTPError mkdir -p /etc/systemd/system/docker.service.d vi /etc/systemd/system/docker.service.d/koll.conf12345[Service]MountFlags=sharedEnvironmentFile=/etc/sysconfig/dockerExecStart=ExecStart=/usr/bin/docker daemon $other_args 保存退出修改/etc/sysconfig/docker参数并添加下面一行，目的是为了配置本地镜像仓库。other_args=&quot;--insecure-registry 192.168.122.77:4000&quot; 重启docker进程systemctl daemon-reloadsystemctl enable dockersystemctl restart docker 制做docker本地镜像仓库链接：http://pan.baidu.com/s/1gf5LTV9 密码：aysu下载解压将已经build好的openstack镜像解压到本地tar -xvf kolla-image-newton-latest.tgz 加载下载好的docker registry，docker搭建私有镜像仓库使用registry这个软件docker load &lt; ./registry-server.tar将镜像文件放到/home/下docker run -d -p 4000:5000 --restart=always -e REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY=/tmp/registry -v /home/tmp/registry/:/tmp/registry --name registry registry:2 测试是否搭建本地镜像仓库成功curl -XGET http://127.0.0.1:4000/v2/_catalog #正常情况会输出很多image name。仓库里面存在的镜像查看该镜像的tagcurl -XGET http://127.0.0.1:4000/v2/kolla/centos-binary-nova-compute/tags/list 3.0.3是版本号查看仓库数据 安装ansibleyum install ansibleclone newton分支git clone -b stable/newton https://github.com/openstack/kollacd kollacp -r etc/kolla /etc/ 安装kollapip install kolla vim /etc/kolla/globals.yml12345678openstack_release: &quot;3.0.3&quot; #上面搭建的本地仓库里面curl出来的tag号，写错了的话会导致找不到image。network_interface: &quot;ens3&quot;kolla_internal_vip_address: &quot;192.168.123.76&quot;#配置给高可用的vipneutron_external_interface: &quot;eth0&quot;docker_registry: &quot;192.168.122.77:4000&quot; #本地仓库地址docker_namespace: &quot;kolla&quot; 生成密码文件 kolla-genpwd路径/etc/kolla/password.yml 执行部署cd /root/kolla/tools/./kolla-ansible deploy -i /root/kolla/ansible/inventory/all-in-one这里需要注意的是，我是部署all-in-one，如果需要部署多节点的用multinode 修改一下里面的hostname部署完成查看容器pip install python-openstackclientpip install python-neutronclient 输入以下命令生成一个环境变量文件kolla-ansible post-deploy 文件路径为/etc/kolla/admin-openrc.sh cp /etc/kolla/admin-openrc.sh /root/ source /root/admin-openrc.sh 查看nova service查看neutron agent 打开控制台访问 帐户admin，密码通过刚刚生成的admin-openrc.sh获取 执行cd /usr/share/kolla ./init-runonce #一个测试脚本，自动下载镜像，上传，创建网络，创建路由器…… 最后创建虚拟机 需要注意的是如果是在虚拟机中测试kolla需要在宿主机上修改nova-compute的配置文件 为virt_type=qemu不然默认用的是kvm，会造成创建云主机失败。vim /etc/kolla/nova-compute/nova.conf 重启这个容器。docker restart nova_compute创建云主机测试 最后 Docker使用heka来展现收集到的日志信息。这些openstack容器的log都在heka 容器内展现默认是没有安装cinder和其他一些软件的，如果需要安装在部署时可以修改/etc/kolla/globals.yml 参考链接http://blog.csdn.net/u011211976/article/details/52085891http://docs.openstack.org/developer/kolla-ansible/quickstart.htmlhttp://geek.csdn.net/news/detail/60805?utm_source=tuicool&amp;utm_medium=referral]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack N版对接ceph jewel版]]></title>
      <url>%2F2017%2F02%2F06%2Fopenstac%26ceph%2F</url>
      <content type="text"><![CDATA[环境网络配置： public_network cluster_network 控制节点/ceph-mon 192.168.4.6 192.168.3.5 ceph-osd1 192.168.4.12 192.168.3.9 ceph-osd2 192.168.4.8 192.168.3.7 硬件配置：ceph-moncpu：4核内存：4G ceph-osd1cpu：4核内存：4G硬盘：3块100G磁盘 ceph-osd2cpu：4核内存：4G硬盘：3块100G磁盘将selinux和firewalld关闭，或配置防火墙规则 配置软件源：163 yum源wget -O /etc/yum.repo/ CentOS7-Base-163.repo http://mirrors.163.com/.help/CentOS7-Base-163.repo 配置epel源wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo 增加ceph源vim /etc/yum.repos.d/ceph.repo12345678[ceph]name=cephbaseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/x86_64/gpgcheck=0[ceph-noarch]name=cephnoarchbaseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/noarch/gpgcheck=0 清理yum缓存yum clean all创建缓存yum makecache所有节点安装cephyum install ceph –y 开始部署在部署节点安装我这里的是openstack的controller节点安装ceph-deploy，手动部署参考上一篇文章 http://www.bladewan.com/2017/01/01/manual_ceph/#more yum install ceph-deploy –y 在部署节点创建部署目录mkdir /etc/cephcd /etc/ceph/ceph-deploy new control-node1没有erro继续向下 此时目录下有ceph.conf、ceph-deploy-ceph.log、ceph.mon.keyring 修改ceph.conf添加public_network和cluster_network，同时增加允许时钟偏移vim /etc/ceph/ceph.conf 开始monitor在controller上执行ceph-deploy mon create-initial …… 部署目录多了以下文件 查看ceph状态ceph -s此时ceph状态应该是ERROR的health HEALTH_ERRno osdsMonitor clock skew detected 部署osd ceph-deploy --overwrite-conf osd prepare ceph-osd1:/dev/vdb /dev/vdc /dev/vdd ceph-osd2:/dev/vdb /dev/vdbc /dev/vdd --zap-disk部署完后查看ceph状态 查看osd tree 推送配置ceph-deploy --overwrite-conf config push ceph-osd1 ceph-osd2 重启ceph进程mon节点systemctl restart ceph-mon@control-node1.service osd节点重启systemctl restart ceph-osd@x 查看public_network和cluster_network配置是否生效 根openstack对接Ceph创建pool根据公式计算出每个pool的合适pg数 PG NumberPG和PGP数量一定要根据OSD的数量进行调整，计算公式如下，但是最后算出的结果一定要接近或者等于一个2的指数。Total PGs = (Total_number_of_OSD * 100) / max_replication_count 例：有6个osd，2副本，3个pool Total PGs =6*100/2=300每个pool 的PG=300/3=100，那么创建pool的时候就指定pg为128ceph osd pool create pool_name 128ceph osd pool create pool_name 128 创建3个poolceph osd pool create volumes 128ceph osd pool create images 128ceph osd pool create vms 128 创建nova、cinder、glance、backup用户并授权12345ceph auth get-or-create client.cinder mon &apos;allow r&apos; osd &apos;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rwx pool=images&apos;ceph auth get-or-create client.glance mon &apos;allow r&apos; osd &apos;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rwx pool=images&apos;ceph auth get-or-create client.nova mon &apos;allow r&apos; osd &apos;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rwx pool=images&apos; 生成keyring文件控制节点 ceph auth get-or-create client.cinder | tee /etc/ceph/ceph.client.cinder.keyringceph auth get-or-create client.glance | tee /etc/ceph/ceph.client.glance.keyring 修改文件属组chown cinder:cinder /etc/ceph/ceph.client.cinder.keyringchown glance:glance /etc/ceph/ceph.client.glance.keyring 计算节点ceph auth get-or-create client.cinder |tee /etc/ceph/ceph.client.cinder.keyringceph auth get-or-create client.nova |tee /etc/ceph/ceph.client.nova.keyringceph auth get-or-create client.glance |tee /etc/ceph/ceph.client.glance.keyring 修改文件属组chown cinder:cinder /etc/ceph/ceph.client.cinder.keyringchown nova:nova /etc/ceph/ceph.client.nova.keyring 在计算节点上生成uuidgen（所有计算节点用一个就可以）uuidgenf77169a0-7d56-4fc3-a436-35298081f9f9 创建secret.xmlvim secret.xml123456&lt;secret ephemeral=&apos;no&apos; private=&apos;no&apos;&gt; &lt;uuid&gt;f77169a0-7d56-4fc3-a436-35298081f9f9&lt;/uuid&gt; &lt;usage type=&apos;ceph&apos;&gt; &lt;name&gt;client.nova secret&lt;/name&gt; &lt;/usage&gt;&lt;/secret&gt; 导出nova的keyring ceph auth get-key client.nova | tee client.nova.key virsh secret-define –file secret.xmlvirsh secret-set-value –secret f77169a0-7d56-4fc3-a436-35298081f9f9 –base64 $(cat client.nova.key ) 查看secret-value 另外一台计算节点一样 修改openstack组件配置 glancecp /etc/glance/glance-api.conf /etc/glance/glance-api.conf.bakvim /etc/glance/glance-api.conf1234567891011[DEFAULT]...show_image_direct_url = True...[glance_store]stores=glance.store.rbd.Storedefault_store = rbdrbd_store_pool = imagesrbd_store_user = glancerbd_store_ceph_conf = /etc/ceph/ceph.confrbd_store_chunk_size = 8 重启glance-api和glance-registrysystemctl restart openstack-glance-apisystemctl restart openstack-glance-registry cindercp /etc/cinder/cinder.conf /etc/cinder/cinder.conf.bakvim /etc/cinder/cinder.conf12345678910111213enabled_backends = rbd[rbd]volume_driver = cinder.volume.drivers.rbd.RBDDriverrbd_pool = volumesrbd_ceph_conf = /etc/ceph/ceph.confrbd_flatten_volume_from_snapshot = falserbd_max_clone_depth = 5rbd_store_chunk_size = 4rados_connect_timeout = -1glance_api_version = 1rbd_user = cinderrbd_secret_uuid = f77169a0-7d56-4fc3-a436-35298081f9f9volume_backend_name = rbd 重启cinder-api、cinder-schedule、cinder-volume systemctl restart openstack-cinder-apisystemctl restart openstack-cinder-volumesystemctl restart openstack-cinder-scheduler nova修改nova-computecp /etc/nova/nova.conf /etc/nova/nova.conf.bak修改nova.conf添加如下配置12345678[libvirt]virt_type = qemuimages_type = rbdimages_rbd_pool = vmsimages_rbd_ceph_conf = /etc/ceph/ceph.confrbd_user = novarbd_secret_uuid = f77169a0-7d56-4fc3-a436-35298081f9f9live_migration_flag=&quot;VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE,VIR_MIGRATE_PERSIST_DEST,VIR_MIGRATE_TUNNELLED 重启nova-computesystemctl restart openstack-nova-compute 测试glance上传镜像，在ceph pool中查看是否存在openstack image create &quot;cirros&quot; --file cirros-0.3.4-x86_64-disk.img --disk-format qcow2 --container-format bare –public 存在说明对接正常 cinder在控制台创建个云硬盘，创建成功后在ceph的volumes pool池中可以看见刚刚创建的云硬盘说明创建成功 nova在控制台创建个云主记，创建成功后在ceph的vm pool池中可以看见刚刚创建的云主机说明创建成功]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[手动安装ceph]]></title>
      <url>%2F2017%2F01%2F01%2Fmanual_ceph%2F</url>
      <content type="text"><![CDATA[环镜操作系统centos6.5网络规划Cluster_net：192.168.20.0/24Public_net：192.168.2.0/24 角色 Public_net Cluster_net Ceph-mon 192.168.2.4 192.168.20.2 Ceph-mon2 192.168.2.5 192.168.20.5 Ceph-mon3 192.168.2.6 192.168.20.6 Ceph-osd1 192.168.2.7 192.168.20.7 Ceph-osd2 192.168.2.8 192.168.20.8 Ceph-osd3 192.168.2.9 192.168.20.9 绑定hosts 123456192.168.2.4 ceph-mon.novalocal mon192.168.2.5 ceph-mon2.novalocal mon2192.168.2.6 ceph-mon3.novalocal mon3192.168.2.7 ceph-osd1.novalocal osd1192.168.2.8 ceph-osd2.novalocal osd2192.168.2.9 ceph-osd3.novalocal osd3 osd数规划每台osd节点挂载3块SSD硬盘 配置yum源根据操作系统版本任意调整(所有节点)vim /etc/yum.repos.d/ceph.repo12345678910[ceph]name=cephbaseurl=http://mirrors.aliyun.com/ceph/rpm-hammer/el6/x86_64/gpgcheck=0enablde=1[ceph-noarch]name=cephnoarchbaseurl=http://mirrors.aliyun.com/ceph/rpm-hammer/el6/x86_64/gpgcheck=0enablde=1 yum cleanyum makecache 在第一台mon节点安装pssh批量执行操作yum install pssh创建hosts（会被批量执行的主机）vim /root/hosts.txt12345192.168.2.5192.168.2.6192.168.2.7192.168.2.8192.168.2.9 先进行测试 安装软件（所有节点）pssh -P -h hosts.txt yum install ceph –y同步hostspscp.pssh -h /root/hosts.txt /etc/hosts /etc/ 都success进行下一步 配置ceph-mon创建集群fsiduuidgenfdc3d06b-7e05-44a8-b982-e8e04e4156db创建/etc/ceph/ceph.conf将fsid写入配置文件[global]fsid = fdc3d06b-7e05-44a8-b982-e8e04e4156db 将ceph-mon写入配置文件(有多个mon，用逗号隔开)mon_initial_members = mon,mon2,mon3将mon节点ip写入ceph配置文件mon_host = 192.168.2.4,192.168.2.5,192.168.2.6为集群创建mon密钥ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon &#39;allow *&#39; 生成管理员密钥ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --set-uid=0 --cap mon &#39;allow *&#39; --cap osd &#39;allow *&#39; --cap mds &#39;allow&#39; 将client.admin密钥加入到ceph.mon.keyringceph-authtool/tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring 生成mon mapmonmaptool --create --add mon 192.168.2.4 --fsid fdc3d06b-7e05-44a8-b982-e8e04e4156db /tmp/monmap在每个mon节点数据目录分别在mon、mon2、mon3上执行格式为(默认cluster-name为ceph)mkdir /var/lib/ceph/mon/{cluster-name}-{hostname}如mon为mkdir /var/lib/ceph/mon/ceph-monmon初始化(-i后接hostname)ceph-mon --mkfs -i mon --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring ceph.conf增加如下配置1234public network = 192.168.2.0/24auth cluster required = cephxauth service required = cephxauth client required = cephx 创建两个空文件touch /var/lib/ceph/mon/ceph-mon/donetouch /var/lib/ceph/mon/ceph-mon/sysvinit启动第一个ceph-mon/etc/init.d/ceph status mon.mon部署第二个mon将keyring复制到mon2scp /tmp/ceph.mon.keyring mon2:/tmp/在mon2节点上建立一个/var/lib/ceph/mon/ceph-mon2目录mkdir –p /var/lib/ceph/mon/ceph-mon2/在mon2节点上初始化mon节点ceph-mon --mkfs -i mon2 --keyring /tmp/ceph.mon.keyring为了防止重新被安装，初始化一个空的done文件touch /var/lib/ceph/mon/ceph-mon2/donetouch /var/lib/ceph/mon/ceph-mon2/sysvinit/etc/init.d/ceph start mon.mon2检查进程 第三个mon同上完后检查 发现有时种偏移问题，默认ceph是0.05s，为了方便同步直接把时钟偏移设置成0.5s修改ceph配置文件增加两条配置1234[global]mon osd down out interval = 900 #设置osd节点down后900s，把此osd节点逐出ceph集群，把之前映射到此节点的数据映射到其他节点。 [mon] mon clock drift allowed = .50 同步配置 pscp.pssh -h hosts.txt /etc/ceph/ceph.conf /etc/ceph/重启进程pssh -h /root/hosts.txt /etc/init.d/ceph restart 配置osd节点将keyring同步到osd节点pscp.pssh -h /root/hosts.txt /etc/ceph/ceph.client.admin.keyring /etc/ceph/ 为osd分配uuid(我每台osd节点有3个osd所以创建3个uuid)uuidgen19ebc47d-9b29-4cf3-9720-b62896ce6f33uuidgen1721ce0b-7f65-43ef-9dfc-c49e6210d375uuidgenf5e0f54b-2ee3-41df-bf25-ad37371ab6ce 创建3个osd123ceph osd create 19ebc47d-9b29-4cf3-9720-b62896ce6f33ceph osd create 1721ce0b-7f65-43ef-9dfc-c49e6210d375ceph osd create f5e0f54b-2ee3-41df-bf25-ad37371ab6ce 创建数据存储目录1234567mkdir /var/lib/ceph/osd/&#123;cluster-name&#125;-&#123;osd-number&#125;mkdir /var/lib/ceph/osd/ceph-0mkdir /var/lib/ceph/osd/ceph-1mkdir /var/lib/ceph/osd/ceph-2mkfs.xfs /dev/vdbmkfs.xfs /dev/vdcmkfs.xfs /dev/vdd 挂载123mount -o defaults,_netdev /dev/vdb /var/lib/ceph/osd/ceph-0mount -o defaults,_netdev /dev/vdc /var/lib/ceph/osd/ceph-1mount -o defaults,_netdev /dev/vdd /var/lib/ceph/osd/ceph-2 修改fstable 初始化osd目录 123ceph-osd -i 0 --mkfs --mkkey --osd-uuid 19ebc47d-9b29-4cf3-9720-b62896ce6f33ceph-osd -i 1 --mkfs --mkkey --osd-uuid 1721ce0b-7f65-43ef-9dfc-c49e6210d375ceph-osd -i 2 --mkfs --mkkey --osd-uuid f5e0f54b-2ee3-41df-bf25-ad37371ab6ce 注册此osd密钥123ceph auth add osd.0 osd &apos;allow *&apos; mon &apos;allow profile osd&apos; -i /var/lib/ceph/osd/ceph-0/keyringceph auth add osd.1 osd &apos;allow *&apos; mon &apos;allow profile osd&apos; -i /var/lib/ceph/osd/ceph-1/keyringceph auth add osd.2 osd &apos;allow *&apos; mon &apos;allow profile osd&apos; -i /var/lib/ceph/osd/ceph-2/keyring 加入crush map ceph osd crush add-bucket ceph-osd1 hostceph osd crush move ceph-osd1 root=default 设置权重123ceph osd crush add osd.0 1.0 host=ceph-osd1 ceph osd crush add osd.1 1.0 host=ceph-osd1ceph osd crush add osd.2 1.0 host=ceph-osd1 要已守护进程开机启动，须创建一个空文件123touch /var/lib/ceph/osd/ceph-0/sysvinittouch /var/lib/ceph/osd/ceph-1/sysvinittouch /var/lib/ceph/osd/ceph-2/sysvinit 启动osd进程123/etc/init.d/ceph start osd.0/etc/init.d/ceph start osd.1/etc/init.d/ceph start osd.2 查看osd 树按上述方法配置osd2、osd33台节点添加完毕 这里有个warn，是pool的pg问题，我们重新计算，修改。查看我们现在拥有的poolceph osd lspools0 rbd, 查看默认的rbd pool的pg1234567ceph osd pool get rbd pg_numpg_num: 64ceph osd pool get rbd pg_numpg_num: 64修改为128ceph osd pool set rbd pg_num 128ceph osd pool set rbd pgp_num 128 再查看 修改ceph.conf默认数据存两份，默认pg为128同步配置重启ceph 配置ceph-radosgw直接将第一个mon节点当radosgw，ceph在0.80及以上版本可以直接使用civeweb来构建对象网关，可以不需要使用apache或nginx+fastcgi了，所以我这里用civetweb。 安装软件yum install ceph-radosgw没有www-data用户创建useradd –r –s /sbin/nologin www-data创建gateway keyring并授权 ceph auth get-or-create client.radosgw.gateway osd &#39;allow rwx&#39; mon &#39;allow rwx&#39; -o /etc/ceph/keyring.radosgw.gateway 编辑ceph.conf文件增加下面内容1234567891011[client.radosgw.gateway]keyring = /etc/ceph/keyring.radosgw.gatewayrgw_socket_path = /tmp/radosgw.sockrgw_frontends= &quot;civetweb port=7480&quot;host = ceph-monrgw_dns_name = *.domain.tldrgw_print_continue = Truergw_data = /var/lib/ceph/radosgwuser = www-datargw s3 auth use keystone = truelog file =/var/log/radosgw/client.radosgw.gateway.log 重启进程/etc/init.d/ceph-radosgw restart查看是否启动成功 新建用户radosgw-admin user create –uid=”testuser” –display-name=”First User”得到如下结果]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mariadb galera]]></title>
      <url>%2F2016%2F12%2F25%2Fmariadb-base%2F</url>
      <content type="text"><![CDATA[传统的数据同步方式传统的mysql数据库同步是使用二进制日志进行主从同步也就是semi-sync，这种同步也只是一种半同步，并不是完全的实时同步，在mysql 5.7.17中推出了MySQL Group Replication，这种实现方式与Galera cluster基本完全一样，不过MGR比Galera的优势在于 Mysql官方出品，能得到后续技术支持，Galera是芬兰的一家叫codership的公司开发的。 MGR使用Paxos协议，性能比Galera好，并且性能稳定，Galera实际只能使用三个节点，有性能和稳定性问题。 Galera目前只支持linux，MGR支持多个平台。 mysql 5.6下semi-sync数据同步方式master的dump进程需要发送binlog日志到slave，master要等待至少一个slave通知，slave将已经接收到传过来的events并写入relay log，salve发送ack信息到master，这个事务才能提交。 5.6版semi-sync的缺陷，dump thread要承但两份任务，传送binlog给slave还要等slave的返回，并且这两个任务是串行的，也就是说，dump thread要先传送binlog给slave，并还要slave返回才能传送下一个events事务，这样的dump thread成为性能瓶径。 mysql 5.7下semi-sync数据同步方式5.7.4为解决上述的问题，在master端增加了ack进程。这样事务写发送binlog与接收ack可以并行进行，提高semi-sync的效率。 GaleraGalera cluster是可以实现mariadb多主集群的软件，它目前只能用于linux和只支持XtraDB和Innodb存储引擎 ，mariadb和perconna提供了原生的Galera cluster的支持，所以可以直接使用Galera cluster，mysql要使用Galera cluster需要使用Galera cluster提供的插件。传统主从只能有一个数据库进行写服务，Galera集群，每个节点都可读可写，在Galera上层部署负载均衡软件如lvs和haproxy进行流量分担是常用做法。 原理各个节点的数据同步由wsrep接口实现。client发起一个commit命令时，所有本事务内对数据库的操作和primary_key都会打包写入到write-set，write-set随后会复制到其他节点，各个节点接收后，会根据write-set传送的primary key进行检验，检查是否与本地事务种read-write或write-write锁冲突，冲突则回滚，没有冲突就执行，完成后返回success。如果其他节点没有执行成功则存放在队列种，稍后会重新尝式。 Galera集群的优点 支持多个节点的数据写入，能保证数据的强一致性。 同步复制，各个节点无延迟，不会因为一台宕机导致数据丢失。 故障节点将自动从集群中移除 基于行级的并行复制 缺点 只支持Innodb存储引擎 只支持linux平台， 集群写入的tps由最弱节点限制，如果一个节点变慢，整的集群就是缓慢的，所以一般情况下部署，要求统一的硬件配置。 会因为网络抖动造成性能和稳定性问题。 参考链接:http://www.oschina.net/news/79983/galera-will-die-mysql-group-replication-realeasehttp://www.itbaofeng.com/?p=236]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[win10启用ubuntu子shell]]></title>
      <url>%2F2016%2F12%2F22%2Fwin10_shell%2F</url>
      <content type="text"><![CDATA[以前想用linux的shell都得装第三方软件实现，比如git bash等，在win10周年更新版14393直接集成了bash on ubuntu了集成新的 Windows Subsystem for Linux 子系统，这样能直接在 Bash on Ubuntu 环境里编译运行 Linux 程序非常爽。 开启win10开发者模式win10设置 打开控制面板，启用或关闭windows功能 点确定，然后重启然后打开powershell或cmd直接在里面输入bash命令 它会询问你是否安装canonical 分发的ubuntu，输入y，然后等待。这里推荐使用vpn，然后安装完后，设置用户名，输入root用root权限吧。安装完后，然后你打开powershell和cmd输入bash就直接进入ubuntu子系统了 可以看出子bash还是非常给力的是ubuntu 14.04 3.4的内核。以后可以直接在上面跑python脚本了。不需要专门虚拟机。 默认字体蓝色不好看清楚，修改powershell或cmd的背景色即可。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mariadb galera集群搭建]]></title>
      <url>%2F2016%2F12%2F22%2Fmariadb-galera-2%2F</url>
      <content type="text"><![CDATA[环境配置准备3台服务器 192.168.1.16 mariadb-1.novalocal mariadb-1192.168.1.19 mariadb-2.novalocal mariadb-2192.168.1.18 mariadb-3.novalocal mariadb-3 配置repo文件 vim /etc/yum.repos.d/MariaDB.repo12345[mariadb]name = MariaDBbaseurl = http://yum.mariadb.org/10.1/centos7-amd64gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDBgpgcheck=1 yum makecache 禁用防火墙和selinux 如果要使用防火墙添加允许3306、和4567 端口规则。 安装Mariadb和galera cluster（三个节点都执行） yum install MariaDB-server MariaDB-client galera 启动mariadb systemctl start mariadb 一些初始化安全配置 /usr/bin/mysql_secure_installation 关闭数据库systemctl stop mariadb 修改mariadb-1上的/etc/my.cnf.d/server.cnf文件如下1234567891011121314[galera]wsrep_provider = /usr/lib64/galera/libgalera_smm.sowsrep_cluster_address = &quot;gcomm://192.168.1.16,192.168.1.18,192.168.1.19&quot;wsrep_node_name = mariadb-1wsrep_node_address=192.168.1.16wsrep_on=ONbinlog_format=ROWdefault_storage_engine=InnoDBinnodb_autoinc_lock_mode=2#bind-address=0.0.0.0wsrep_slave_threads=1innodb_flush_log_at_trx_commit=0innodb_buffer_pool_size=122Mwsrep_sst_method=rsync 将此文件复制到mariadb-2、mariadb-3，注意要把 wsrep_node_name 和 wsrep_node_address 改成相应节点的 hostname 和 ip。 启动 MariaDB Galera Cluster 服务 /usr/sbin/mysqld –wsrep-new-cluster –user=root &amp;–wsrep-new-cluster 这个参数只能在初始化集群使用，且只能在一个节点使用。 观察日志： [root@node4 ~]# tail -f /var/log/message123150701 19:54:17 [Note] WSREP: wsrep_load(): loading provider library &apos;none&apos;150701 19:54:17 [Note] /usr/libexec/mysqld: ready for connections.Version: &apos;5.5.40-MariaDB-wsrep&apos; socket: &apos;/var/lib/mysql/mysql.sock&apos; port: 3306 MariaDB Server, wsrep_25.11.r4026 出现 ready for connections ,证明我们启动成功 查看是否启用galera插件连接mariadb,查看是否启用galera插件 目前集群机器数 查看集群状态show status like ‘wsrep%’; 查看连接的主机 另外两个节点mariadb会自动加入集群systemctl start mariadb这时查看galera集群机器数量 已经连接机器的ip 测试在mariadb-1上创建数据库，创建表，插入数据123456789101112131415161718192021MariaDB [(none)]&gt; create database test1；MariaDB [test1]&gt; insert into test values(1);Query OK, 1 row affected (0.01 sec)MariaDB [test1]&gt; insert into test values(2);Query OK, 1 row affected (0.01 sec)MariaDB [test1]&gt; insert into test values(3);Query OK, 1 row affected (0.00 sec)MariaDB [test1]&gt; insert into test values(4);Query OK, 1 row affected (0.00 sec)MariaDB [test1]&gt; insert into test values(5);Query OK, 1 row affected (0.03 sec)MariaDB [test1]&gt; insert into test values(6);Query OK, 1 row affected (0.01 sec)MariaDB [test1]&gt; insert into test values(7);Query OK, 1 row affected (0.01 sec) 在另外两台mariadb-2、mariadb-3上可以看见刚刚插入的数据，说明数据同步了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack 本地yum源搭建]]></title>
      <url>%2F2016%2F12%2F12%2Fopenstack-localyumbuild%2F</url>
      <content type="text"><![CDATA[我们在部署openstack时用国外yum源的快，经常会很慢导致等待时间太久，所以建议使用本地yum源安装 这里以newton版centos7平台为例 首先下载官方repoyum install https://rdoproject.org/repos/rdo-release.rpm这时侯/etc/yum.repos.d里面会产生3个文件12[root@test yum.repos.d]# ls rdo-qemu-ev.repo rdo-release.repo rdo-testing.repo 我这里打算用http搭建我的本地yum服务器 先安装httpdyum install httpd mkdir /var/www/html/newton 待会将同步下来的包放这个目录cd /vaw/www/html/newton yum repolist –列出你所有的仓库 前面是repo id不包含x86_64 这里我只需要openstack-newton、和rdo-qemu-ev这两个软件库 先同步openstack-newtonreposync --repoid=openstack-newton 指定要下载的仓库id，会通过网络全部下载到当前目录下载下来。 同步完第一个继续同步第二个reposync --repoid=rdo-qemu-ev 同步完后这时查看 /vaw/www/html/newton里面已经有很多包了，只有软件包，没有repodate清单，所以需要自己重新createrepo来创建清单没有createrepo自己安装，创建软件清单createrepo /var/www/html/newton/ 然后启动httpd服务，其他机器通过httpd服务来访问yum源 例如控制节点yum源配置vim /etc/yum.repos.d/openstack.repo123456[openstack]name=openstackbaseurl=http://192.168.4.3/newtonenabled=1gpgcheck=0~ yum makecache 其他节点一样。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack newton 安装]]></title>
      <url>%2F2016%2F12%2F12%2Fopenstack-newton-install%2F</url>
      <content type="text"><![CDATA[环境配置机器配置：3台8v8G的虚拟机，1台做控制节点2台做融合节点。 网络划分：192.168.122.0/24 public网络192.168.3.0/24 存储网络192.168.4.0/24管理网络、sdn隧道网络 我这里配置了本地源，就不用在手动配置官网源本地源的搭建和配置会在另外一个文档说明。节点网络信息 ： 管理网络和随道网络 存储网络 公网 控制节点 192.168.4.6 192.168.3.5 192.168.122.2 计算节点 192.168.4.7 192.168.3.6 192.168.125.5 计算节点 192.168.4.8 192.168.3.7 192.168.122.6 网络拓扑 安装chrony控制节点向外同步时间，其他节点如计算节点都直接同步控制节点yum install chrony 修改配置文件vim /etc/chrony.conf添加下面这两条server cn.ntp.org.cn iburstallow 192.168.4.0/24 设置开机启动systemctl enable chronysystemctl start chrony 其他节点：yum install chrony 修改配置文件vim /etc/chrony.conf 添加下面这两条server 192.168.4.6 iburst 设置开机启动systemctl enable chrony 启动进程systemctl start chrony 安装openstack客户端yum install python-openstackclient 安装Mariadb（数据库服务）vim /etc/my.cnf.d/openstack.cnf1234567[mysqld] bind-address = 192.168.4.6 #填写管理网段ip default-storage-engine = innodb innodb_file_per_table max_connections = 4096collation-server = utf8_general_cicharacter-set-server = utf8 设置开机启动 systemctl enable mariadb 启动Mariadb systemctl start mariadb 安装rabbitmq（用于消息队列）yum install rabbitmq-server 设置开机启动 systemctl enable rabbitmq-server 开启rabbitmq systemctl start rabbitmq-server 创建openstack用户和配置密码 rabbitmqctl add_user openstack 123456 给openstack用户配置读和写权限 rabbitmqctl set_permissions openstack &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; 安装memcache（缓存token）yum install memcached phython-memcached systemctl enable memcached systemctl start memcached 安装keystone（认证服务）连接数据库 [root@control-node1 yum.repos.d]# mysql 创建keystone数据库 create database keystone; 数据库授权 密码自己设置，这里为了方便设置123456grant all privileges on keystone.* to &#39;keystone&#39;@&#39;localhost&#39; identified by &#39;123456&#39;; grant all privileges on keystone.* to &#39;keystone&#39;@&#39;%&#39; identified by &#39;123456&#39;; keystone使用httpd的mod_wsgi运行在端口5000和35357处理认证服务请求。默认情况下，keystone服务依然监听在5000和35357端口。 安装 keystone和wsgi yum install openstack-keystone httpd mod_wsgi 修改keystone配置文件 vim /etc/keystone/keystone.conf connection = mysql+pymysql://keystone:123456@192.168.4.6/keystone #加入连接数据库配置 配置使用哪种产生token方式目前keystone支持4种(UUID、PKI、PKIZ、Fernet)这里我们配置fernethttp://www.tuicool.com/articles/jQJNFrn 这篇文章有几种模式的详细介绍。 [token]provider = fernet 同步数据库 su -s /bin/sh -c “keystone-manage db_sync” keystone #发现同步数据库就是错了也没有反应，需要检查keystone的日志文件查看是否正确 初始化fernet keykeystone-manage fernet_setup –keystone-user keystone –keystone-group keystone keystone-manage credential_setup –keystone-user keystone –keystone-group keystoen 创建Bootstrap the Identity service(就是创建admin用户的帐号信息) 12345keystone-manage bootstrap --bootstrap-password 123456 \ --bootstrap-admin-url http://192.168.4.6:35357/v3/ \ --bootstrap-internal-url http://192.168.4.6:35357/v3/ \ --bootstrap-public-url http://192.168.122.2:5000/v3/ \ --bootstrap-region-id RegionOne 配置apache服务器vim /etc/httpd/conf/httpd.conf 配置成管理网段的ipServerName 192.168.4.6 将keystone的配置文件软链接到apache的配置文件ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/ 设置开机启动systemctl enable httpd 启动httpdsystemctl start httpd检查端口 12345678lsof -i:5000COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEhttpd 18883 root 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18894 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18895 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18896 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18897 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18898 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN) 到root下创建环境变量文件 123456789vim /root/openrc#!/bin/bashexport OS_USERNAME=adminexport OS_PASSWORD=123456 #这个密码是上面Bootstrap the Identity service填的密码export OS_PROJECT_NAME=adminexport OS_USER_DOMAIN_NAME=defaultexport OS_PROJECT_DOMAIN_NAME=defaultexport OS_AUTH_URL=http://192.168.4.6:35357/v3export OS_IDENTITY_API_VERSION=3 创建域、项目、用户创建service projectopenstack project create --domain default --description &quot;Service Project&quot; service 创建user角色openstack role create user这里不创建普通用户了测试admin用户获取tokenopenstack --os-auth-url http://192.168.4.6:35357/v3 token issue 安装glance镜像服务连接Mariadb创建数据库 create database glance; 授权123grant all privileges on glance.* to &apos;glance&apos;@&apos;localhost&apos; identified by &apos;123456&apos;; grant all privileges on glance.* to &apos;glance&apos;@&apos;%&apos; identified by &apos;123456&apos;; grant all privileges on glance.* to &apos;glance&apos;@&apos;control-node1.novalocal&apos; identified by &apos;123456&apos;; control-xxx换成主机名，我这里就算api.conf里面配置的ip默认还是去连接host是主机名，所以只能在加个主机名授权。 创建glance用户并设置密码openstack user create –domain default –password-prompt glance 给glance用户添加admin角色权限openstack role add –project service –user glance admin 创建glance serviceopenstack service create –name glance –description “OpenStack Image” image 创建glance endpoint123openstack endpoint create --region RegionOne image public http://192.168.122.2:9292openstack endpoint create --region RegionOne image internal http://192.168.4.6:9292openstack endpoint create --region RegionOne image admin http://192.168.4.6:9292 安装软件包yum install openstack-glance 配置glancevim /etc/glance/glance-api.conf 配置数据库12[database]connection = mysql+pymysql://glance:123456@192.168.4.6/glance 配置glancevim /etc/glance/glance-api.conf配置数据库12345678910111213141516[database]connection = mysql+pymysql://glance:123456@192.168.4.6/glance配置keystone[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = glancepassword = 123456[paste_deploy]flavor = keystone 配置镜像存储1234[glance_store]stores = file,httpdefault_store = filefilesystem_store_datadir = /var/lib/glance/images/ vim /etc/glance/glance-registry.conf12345678910111213141516[database]connection = mysql+pymysql://glance:123456@192.168.4.6/glance配置keystone[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = glancepassword = 123456[paste_deploy]flavor = keystone 同步数据库su -s /bin/sh -c “glance-manage db_sync” glance 设置开机启动systemctl enable openstack-glance-api 启动服务systemctl start openstack-glance-api 这些做完后最好在查看下日志，看看是否有错误，每部署完一个组件都这样，这样出错的可以很快定位。 下载cirros测试一下 wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.imgopenstack image create &quot;cirros&quot; --file cirros-0.3.4-x86_64-disk.img --disk-format qcow2 --container-format bare --public glance image-list 检查一下镜像是否上传成功 安装nova组件控制节点安装创建数据库create database nova_api;create database nova; 授权12345678GRANT ALL PRIVILEGES ON nova_api.* TO &apos;nova&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON nova_api.* TO &apos;nova&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON nova_api.* TO &apos;nova&apos;@&apos;control-node1.novalocal&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON nova.* TO &apos;nova&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON nova.* TO &apos;nova&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON nova.* TO &apos;nova&apos;@&apos;control-node1.novalocal&apos; IDENTIFIED BY &apos;123456&apos;; 创建为nova组件创建用户、service、endpointopenstack user create --domain default --password-prompt nova 给nova用户添加admin角色权限openstack role add --project service --user nova admin 创建serviceopenstack service create --name nova --description &quot;OpenStack Compute&quot; compute 创建endpoint12345openstack endpoint create --region RegionOne compute public http://192.168.122.2:8774/v2.1/%\(tenant_id\)s openstack endpoint create --region RegionOne compute internal http://192.168.4.6:8774/v2.1/%\(tenant_id\)s openstack endpoint create --region RegionOne compute admin http://192.168.4.6:8774/v2.1/%\(tenant_id\)s 安装nova-api组件yum install openstack-nova-api openstack-nova-conductor openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler 配置novavim /etc/nova/nova.conf 123456789101112131415161718192021222324252627282930313233[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6 #配置rabbitmq帐户和密码my_ip = 192.168.4.6use_neutron = Truefirewall_drive = nova.virt.firewall.NoopFirewallDriverenabled_apis=osapi_compute,metadataauth_strategy=keystone[api_database]connection = mysql+pymysql://nova:123456@192.168.4.6/nova_api #配置nova连接数据库[database]connection = mysql+pymysql://nova:123456@192.168.4.6/nova[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = novapassword = 123456配置novncnovncproxy_port=6080novncproxy_base_url=http://211.156.182.144:6080/vnc_auto.htmlvncserver_listen=192.168.4.6[glance]api_servers = http://192.168.4.6:9292[oslo_concurrency]lock_path = /var/lib/nova/tmp 同步数据库su -s /bin/sh -c “nova-manage api_db sync” novasu -s /bin/sh -c “nova-manage db sync” nova 1234567891011systemctl enable openstack-nova-api.service \openstack-nova-consoleauth.service \openstack-nova-scheduler.service \openstack-nova-conductor.service \openstack-nova-novncproxy.servicesystemctl start openstack-nova-api.service \openstack-nova-consoleauth.service \openstack-nova-scheduler.service \openstack-nova-conductor.service \openstack-nova-novncproxy.service 计算机节点安装安装nova-computeyum install openstack-nova-compute 配置nova-computevim /etc/nova/nova.conf123456789101112131415161718192021222324252627282930313233[DEFAULT]enabled_apis = osapi_compute,metadatatransport_url = rabbit://openstack:123456@192.168.4.6 #配置rabbitmq帐号和密码auth_strategy = keystonemy_ip = 192.168.4.7use_neutron = Truefirewall_driver = nova.virt.firewall.NoopFirewallDriver[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = novapassword = 123456[vnc]enabled=truevncserver_listen=0.0.0.0vncserver_proxyclient_address=192.168.4.7 #填写本机ipnovncproxy_base_url=http://211.156.182.144:6080/vnc_auto.html #这个填你要用控制节点的public ip[glance]api_servers = http://192.168.4.6:9292配置锁路径[oslo_concurrency]lock_path = /var/lib/nova/tmp[libvirt]virt_type = qemu #物理服务器就配置kvm虚拟机就配置qemu 设置开机启动systemctl enable libvirtd.service openstack-nova-compute.service 启动nova-computesystemctl start libvirtd.service openstack-nova-compute.service 在控制节点查看检查一下compute进程根控制节点连接 配置neutron控制节点安装我这里使用openvswitch不使用linux bridge，因为openvswitch功能比linux Brige功能强太多了。但配置稍微复杂点。 创建数据库create database neutron; 授权123GRANT ALL PRIVILEGES ON neutron.* TO &apos;neutron&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON neutron.* TO &apos;neutron&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON neutron.* TO &apos;neutron&apos;@&apos;control-node1.novalocal&apos; IDENTIFIED BY &apos;123456&apos;; 创建neutron用户并设置密码openstack user create --domain default --password-prompt neutron 给neutron用户添加admin角色权限openstack role add --project service --user neutron admin 创建neutron serviceopenstack service create --name neutron --description &quot;OpenStack Networking&quot; network 创建neutron endpoint123openstack endpoint create --region RegionOne network public http://192.168.122.2:9696 openstack endpoint create --region RegionOne network admin http://192.168.4.6:9696 openstack endpoint create --region RegionOne network internal http://192.168.4.6:9696 安装neutron组件yum -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch vim /etc/neutron/neutron.conf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[DEFAULT]service_plugins = routertransport_url = rabbit://openstack:123456@192.168.4.6auth_strategy = keystonenotify_nova_on_port_status_changes = Truenotify_nova_on_port_data_changes = Truestate_path = /var/lib/neutronuse_syslog = Truesyslog_log_facility = LOG_LOCAL4log_dir =/var/log/neutroncore_plugin = neutron.plugins.ml2.plugin.Ml2Pluginbase_mac = fa:16:3e:00:00:00mac_generation_retries = 32dhcp_lease_duration = 600dhcp_agent_notification = Trueallow_bulk = Trueallow_pagination = Falseallow_sorting = Falseallow_overlapping_ips = Trueadvertise_mtu = Trueagent_down_time = 30router_scheduler_driver = neutron.scheduler.l3_agent_scheduler.ChanceSchedulerallow_automatic_l3agent_failover = Truedhcp_agents_per_network = 2api_workers = 9rpc_workers = 9network_device_mtu=1450[database]connection = mysql+pymysql://neutron:123456@192.168.4.6/neutron[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = neutronpassword = 123456[nova]auth_url = http://192.168.4.6:35357auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = novapassword = 123456[oslo_concurrency]lock_path = /var/lib/neutron/tmp 配置modular layer 2（ml2）插件 vim /etc/neutron/plugins/ml2/ ml2_conf.ini1234567891011121314151617181920212223242526272829[DEFAULT]type_drivers = flat,vxlantenant_network_types = vxlanmechanism_drivers = openvswitch,l2populationextension_drivers = port_security[ml2]path_mtu = 1450type_drivers = flat,vxlantenant_network_types = vxlanphysical_network_mtus =physnet1:1500[ml2_type_flat]flat_networks =*[ml2_type_vxlan]vni_ranges =2:65535vxlan_group =224.0.0.1[securitygroup]enable_security_group = Truefirewall_driver=neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver[ovs]local_ip=192.168.4.6tunnel_bridge=br-tunenable_tunneling=Trueintegration_bridge=br-intbridge_mappings=physnet1:br-ex 配置l3-agentvim /etc/neutron/l3_agent.ini12345678910[DEFAULT]debug = Falseinterface_driver =neutron.agent.linux.interface.OVSInterfaceDriverhandle_internal_only_routers = Truemetadata_port = 8775send_arp_for_ha = 3periodic_interval = 40periodic_fuzzy_delay = 5enable_metadata_proxy = Truerouter_delete_namespaces = True 配置dhcp_agentvim /etc/neutron/dhcp_agent.ini12345678910[DEFAULT]resync_interval = 30interface_driver =neutron.agent.linux.interface.OVSInterfaceDriverenable_isolated_metadata = Trueenable_metadata_network = Falsedhcp_domain = openstacklocaldhcp_broadcast_reply = Falsedhcp_delete_namespaces = Trueroot_helper=sudo neutron-rootwrap /etc/neutron/rootwrap.confstate_path=/var/lib/neutron vim /etc/neutron/plugins/ml2/openvswitch_agent.ini1234567891011121314151617[agent]polling_interval = 2tunnel_types = vxlanvxlan_udp_port = 4789l2_population = Trueprevent_arp_spoofing = Falseextensions =[ovs]local_ip=192.168.4.6tunnel_bridge=br-tunenable_tunneling=Trueintegration_bridge=br-intbridge_mappings=physnet1:br-ex[securitygroup]firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriverenable_security_group = true 设置openvswitch开机启动systemctl enable openvswitch.service 启动openvswitchsystemctl start openvswitch 创建br-ex br-tun br-intovs-vsctl add-br br-intovs-vsctl add-br br-exovs-vsctl add-br br-tun 将上外网网卡挂载到br-ex上ovs-vsctl add-port br-ex eth2 设置开机启动项systemctl enable neutron-openvswitch-agent.service启动进程systemctl start neutron-openvswitch-agent.service 配置计算节点neutron配置一下内核参数修改配置文件 /etc/sysctl.conf12net.ipv4.conf.all.rp_filter=0net.ipv4.conf.default.rp_filter=0 sysctl –pyum -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch vim /etc/neutron/neutron.conf 1234567891011121314151617[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6auth_strategy = keystone[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = neutronpassword = 123456[oslo_concurrency]lock_path = /var/lib/neutron/tmp vim /etc/neutron/plugins/ml2/ml2_conf.ini1234567891011[ml2]path_mtu = 1450type_drivers = flat,vxlantenant_network_types = vxlanphysical_network_mtus =physnet1:1500mechanism_drivers = openvswitch,l2populationextension_drivers = port_security[securitygroup]enable_ipset = truefirewall_driver=neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver vim /etc/neutron/plugins/ml2/openvswitch_agent.ini1234567891011121314151617181920[ovs]local_ip=192.168.4.7tunnel_bridge=br-tunenable_tunneling=Trueintegration_bridge=br-intbridge_mappings=physnet1:br-ex[agent]enable_distributed_routing=Trueprevent_arp_spoofing=Truearp_responder=Truepolling_interval=2drop_flows_on_start=Falsevxlan_udp_port=4789l2_population=Truetunnel_types=vxlan[securitygroup]firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriverenable_security_group = true systemctl enable openvswitch.servicesystemctl start openvswitch.service 创建br-ex、br-int、br-tunovs-vsctl add-br br-intovs-vsctl add-br br-exovs-vsctl add-br br-tun vim /etc/nova/nova.conf123456789101112131415[DEFAULT]network_api_class = nova.network.neutronv2.api.APIsecurity_group_api = neutronlinuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver[neutron]url = http://192.168.4.6:9696auth_url = http://192.168.4.6:35357auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = neutronpassword = 123456 systemctl restart neutron-openvswitch-agent.servicesystemctl restart openstack-nova-compute 安装完后可以在在控制节点检查是否安装成功 安装控制台yum install openstack-dashboardvim /etc/openstack-dashboard/local_settings12345678910111213141516171819202122232425262728这里配置控制节点ipOPENSTACK_HOST = &quot;192.168.4.6&quot;配置允许所有节点访问ALLOWED_HOSTS = [&apos;*&apos;, ]配置memcacheSESSION_ENGINE = &apos;django.contrib.sessions.backends.cache&apos;CACHES = &#123; &apos;default&apos;: &#123; &apos;BACKEND&apos;: &apos;django.core.cache.backends.memcached.MemcachedCache&apos;, &apos;LOCATION&apos;: &apos;192.168.4.6:11211&apos;, &#125;&#125;配置keystone v3验证OPENSTACK_KEYSTONE_URL = &quot;http://%s:5000/v3&quot; % OPENSTACK_HOST配置域OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = &apos;default&apos;配置api版本OPENSTACK_API_VERSIONS = &#123; &quot;identity&quot;: 3, &quot;image&quot;: 2, &quot;volume&quot;: 2,&#125;设置通过控制台默认创建用户的角色是userOPENSTACK_KEYSTONE_DEFAULT_ROLE = &quot;user&quot; 重启服务systemctl restart httpd.service memcached.service 通过http://control_ip/dashboard可以访问 Admin 登录，密码是你通过keystone创建的，如果不记得查看openrc 创建flat网络做float_ip池管理员—&gt;网络——&gt;创建网络 Phynet1是在ml2.ini里面bridge_mappings定义的br-ex对应的名字，创建完后增加子网，然后在创建个普通网络，创建个路由器，路由器绑定普通子网，创建个主机配置，然后创建vm加入到你创建的普通网络 这时在vm所在的计算节点或控制节点 ovs-vsctl show 可以看见计算节点根网络节道隧道已经建立。 Cinder配置配置控制节点创建数据库create database cinder;用户授权123GRANT ALL PRIVILEGES ON cinder.* TO &apos;cinder&apos;@&apos;localhost&apos; identified by &apos;123456&apos;; GRANT ALL PRIVILEGES ON cinder.* TO &apos;cinder&apos;@&apos;%&apos; identified by &apos;123456&apos;; GRANT ALL PRIVILEGES ON cinder.* TO &apos;cinder&apos;@&apos;control-node1.novalocal&apos; identified by &apos;123456&apos;; 创建用户openstack user create --domain default --password-prompt cinder 给cinder用户赋予admin权限openstack role add --project service --user cinder admin openstack service create --name cinder --description &quot;OpenStack Block Storage&quot; volume openstack service create --name cinderv2 --description &quot;OpenStack Block Storage&quot; volumev2 创建endpoint1234567891011121314151617openstack endpoint create --region RegionOne \volume public http://192.168.122.2:8776/v1/%\(tenant_id\)sopenstack endpoint create --region RegionOne \volume internal http://192.168.4.6:8776/v1/%\(tenant_id\)sopenstack endpoint create --region RegionOne \volume admin http://192.168.4.6:8776/v1/%\(tenant_id\)sopenstack endpoint create --region RegionOne \volumev2 public http://192.168.122.2:8776/v2/%\(tenant_id\)sopenstack endpoint create --region RegionOne \volumev2 internal http://192.168.4.6:8776/v2/%\(tenant_id\)sopenstack endpoint create --region RegionOne \volumev2 admin http://192.168.4.6:8776/v2/%\(tenant_id\)s 安装cinderyum install openstack-cinder vim /etc/cinder/cinder.conf1234567891011121314151617181920[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6auth_strategy = keystone[database]connection = mysql+pymysql://cinder:123456@192.168.4.6/cinder[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = cinderpassword = 123456[oslo_concurrency]lock_path = /var/lib/cinder/tmp 同步数据库su -s /bin/sh -c “cinder-manage db sync” cinder 配置计算机节点使用cindervim /etc/nova/nova.conf [cinder]os_region_name = RegionOne 重启服务systemctl restart openstack-nova-api.service 设置开机自启cindersystemctl enable openstack-cinder-api.service openstack-cinder-scheduler.service systemctl start openstack-cinder-api.service openstack-cinder-scheduler.service 配置一个存储节点安装lvmyum install lvm2 systemctl enable lvm2-lvmetad.servicesystemctl start lvm2-lvmetad.service 创建个lvm卷pvcreate /dev/vdb 创建vgvgcreate cinder-volumes /dev/vdb vim /etc/lvm/lvm.conf123devices &#123;filter = [ &quot;a/vdb/&quot;, &quot;r/.*/&quot;]&#125; 安装软件yum install openstack-cinder targetcli python-keystone 修改cinder配置文件vim /etc/cinder/cinder.conf1234567891011121314151617181920212223242526272829[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6verbose = Trueauth_strategy = keystoneenabled_backends = lvmglance_api_servers = http://192.168.4.6:9292[database]connection = mysql+pymysql://cinder:123456@192.168.4.6/cinder[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = cinderpassword = 123456[lvm]volume_driver = cinder.volume.drivers.lvm.LVMVolumeDrivervolume_group = cinder-volumesiscsi_protocol = iscsiiscsi_helper = lioadm[oslo_concurrency]lock_path = /var/lib/cinder/tmpsystemctl enable openstack-cinder-volume.service target.service systemctl start openstack-cinder-volume.service target.service 到控制台创建个卷，并挂载到云主机。 Ceilometer配置Ceilometer使用Mongdb存储meter数据，所以需要先在控制节点安装Mongdb 在控制节点安装Mongdbyum install mongodb-server mongodb 配置Mongdbvim /etc/mongod.confsmallfiles = true #限制日志大小创建Mongdb数据库和帐户授权替换123456为你自己设置的密码创建用户openstack user create –domain default –password-prompt ceilometer 给ceilometer用户添加admin角色权限openstack role add –project service –user ceilometer admin创建ceilometer serviceopenstack service create –name ceilometer –description “Telemetry” metering 创建ceilometer endpoint123openstack endpoint create --region RegionOne metering public http://192.168.122.2:8777openstack endpoint create --region RegionOne metering admin http://192.168.4.6:8777openstack endpoint create --region RegionOne metering internal http://192.168.4.6:8777 安装包yum install openstack-ceilometer-api \openstack-ceilometer-collector \openstack-ceilometer-notification \openstack-ceilometer-central \python-ceilometerclient 配置ceilometervim /etc/ceilometer/ceilometer.conf123456789101112131415161718192021222324252627282930[DEFAULT]rpc_backend = rabbitauth_strategy = keystone[oslo_messaging_rabbit]rabbit_host = 192.168.4.6rabbit_userid = openstackrabbit_password = 123456[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = ceilometerpassword = 123456[service_credentials]auth_type = passwordauth_url = http://192.168.4.6:5000/v3project_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = ceilometerpassword = 123456 ##密码替换成在keystone创建ceilometer时设置的密码interface = internalURLregion_name = RegionOne 创建ceilometer的vhost vim /etc/httpd/conf.d/wsgi-ceilometer.conf 123456789101112Listen 8777&lt;VirtualHost *:8777&gt; WSGIDaemonProcess ceilometer-api processes=2 threads=10 user=ceilometer group=ceilometer display-name=%&#123;GROUP&#125; WSGIProcessGroup ceilometer-api WSGIScriptAlias / /usr/lib/python2.7/site-packages/ceilometer/api/app.wsgi WSGIApplicationGroup %&#123;GLOBAL&#125; ErrorLog /var/log/httpd/ceilometer_error.log CustomLog /var/log/httpd/ceilometer_access.log combined&lt;/VirtualHost&gt;WSGISocketPrefix /var/run/httpd 重启httpdsystemctl reload httpd.service设置服务开机启动systemctl enable openstack-ceilometer-notification.service \openstack-ceilometer-central.service \openstack-ceilometer-collector.service 启动进程systemctl start openstack-ceilometer-notification.service \openstack-ceilometer-central.service \openstack-ceilometer-collector.service 配置glance的ceilometer统计vim /etc/glance/glance-api.conf 12345678910[DEFAULT]rpc_backend = rabbit[oslo_messaging_amqp]driver = messagingv2[oslo_messaging_rabbit]rabbit_host = 192.168.4.6rabbit_userid = openstackrabbit_password = 123456 重启进程systemctl restart openstack-glance-api.service openstack-glance-registry.service 配置nova的ceilometer统计安装软件yum install openstack-ceilometer-compute python-ceilometerclient python-pecan 123456789101112131415161718192021222324252627282930[DEFAULT]rpc_backend = rabbitauth_strategy = keystone[oslo_messaging_rabbit]rabbit_host = 192.168.4.6rabbit_userid = openstackrabbit_password = 123456[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = ceilometerpassword = 123456 #将密码替换成keystone创建ceilometer用户时设置的密码[service_credentials]auth_url = http://192.168.4.6:5000project_domain_id = defaultuser_domain_id = defaultauth_type = passwordusername = ceilometerproject_name = servicepassword = 123456 #将密码替换成keystone创建ceilometer用户时设置的密码interface = internalURLregion_name = RegionOne 修改nova-compute配置文件 vim /etc/nova/nova.conf 1234567[DEFAULT]instance_usage_audit = Trueinstance_usage_audit_period = hournotify_on_state_change = vm_and_task_state[oslo_messaging_amqp]driver = messagingv2 设置开机启动systemctl enable openstack-ceilometer-compute.service启动ceilometer-compute进程systemctl start openstack-ceilometer-compute.service重启nova-computesystemctl restart openstack-nova-compute.service 配置块设备使用ceilometer计量服务 验证ceilometer meter-list 正常情况下是会出现如上图一些资源的数据的，但我这里默认报 打debug 访问被拒绝解决办法：修改httpd.conf systemctl restart httpd在测试应该没问题了。 Aodh报警服务：创件aodh数据库create database aodh; 授权123GRANT ALL PRIVILEGES ON aodh.* TO &apos;aodh&apos;@&apos;localhost&apos; identified by &apos;123456&apos;; GRANT ALL PRIVILEGES ON aodh.* TO &apos;aodh&apos;@&apos;%&apos; identified by &apos;123456&apos;; GRANT ALL PRIVILEGES ON aodh.* TO &apos;aodh&apos;@&apos;control-node1.novalocal&apos; identified by &apos;123456&apos;; 创建用户openstack user create --domain default --password-prompt aodh 给adoh帐户添加admin权限openstack role add --project service --user aodh admin 添加服务openstack service create --name aodh --description &quot;Telemetry&quot; alarming 创建endpoint123openstack endpoint create --region RegionOne alarming public http://192.168.122.2:8042 openstack endpoint create --region RegionOne alarming internal http://192.168.4.6:8042 openstack endpoint create --region RegionOne alarming admin http://192.168.4.6:8042 安装软件yum install openstack-aodh-api \openstack-aodh-evaluator \openstack-aodh-notifier \openstack-aodh-listener \openstack-aodh-expirer \python-aodhclient 修改配置文件 vim /etc/aodh/aodh.conf12345678910111213141516171819202122232425262728[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6auth_strategy = keystone[database]connection = mysql+pymysql://aodh:123456@192.168.4.6/aodh[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = aodhpassword = 123456 #填写通过keystone创建帐户时设置的帐号和密码[service_credentials]auth_type = passwordauth_url = http://192.168.4.6:5000/v3project_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = aodhpassword = 123456 #填写通过keystone创建帐户时设置的帐号和密码interface = internalURLregion_name = RegionOne systemctl enable openstack-aodh-api.service \openstack-aodh-evaluator.service \openstack-aodh-notifier.service \openstack-aodh-listener.service systemctl start openstack-aodh-api.service \openstack-aodh-evaluator.service \openstack-aodh-notifier.service \openstack-aodh-listener.service]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Fastnetmon配置与使用]]></title>
      <url>%2F2016%2F12%2F09%2Ffastnetmon%2F</url>
      <content type="text"><![CDATA[Fastnetmon介绍FastNetmon是一个基于多种抓包引擎来对数据包进行统计分析的DOS/DDOS工具，可以探测和分析网络中的异常流量情况，同时，也可以对捕获的异常调用外部脚本进行处理警报啊或者进行阻断处理，全靠外部脚本是如何定义。 项目首页http://www.open-open.com/lib/view/home/143193107973 部署架构 安装和配置方法安装 下载自动安装脚本wget https://raw.githubusercontent.com/FastVPSEestiOu/fastnetmon/master/src/fastnetmon_install.pl -Ofastnetmon_install.pl 将整个项目克隆下来https://github.com/FastVPSEestiOu/fastnetmon 安装注意若安装报连接错误，请将DNS改为8.8.8.8或223.5.5.5，因为下载地址都是在国外，有些DNS可能解析不够好。perl fastnetmon_install.pl 启动安装脚本。 配置 配置文件，cd到你刚刚克隆的项目里面。cp src/Fastnetmon.conf 到/etc/cp src/fastnetmon_init_script_centos6 /etc/init.d/fastnetmon #cp启动脚本到/etc/init.d/chmod 755 /etc/init.d/fastnetmon #修改启动脚本权限cp src/notify_about_attack.sh /usr/local/bin/ #cp 通知脚本 /etc/fastentmon配置项ban_time = 1900 #对检测到攻击的ip进行多久封锁。enable_subnet_counters = on #检测每个子网进出流量enable_connection_tracking = on #开启攻击追踪检测，通过这个选项在日志文件里可以详细看见攻击者ip和其他一些详细情况ban_for_pps = on，ban_for_bandwidth = on，ban_for_flows = on #检测的选项pps(每秒包)，bandwidth(带宽)，flows(流量)。threshold_pps = 20000，threshold_mbps = 1000，threshold_flows = 3500 #监控的限定值抓包引擎选择mirrof=off没有安装PF_RING就不要开启，不然会启动报错。pfring_sampling_ratio = 1 #端口镜像采样率mirror_netmap = off没有安装Netmap就不要开启，不然会会启动报错。mirror_snabbswitch=on #开启snabbswitch流量捕获。mirror_afpacket =on #AF_PACKET捕获引擎开启netmap_sampling_ratio = 1 #端口镜像抽样比pcap=on #pcap引擎开启netflopw=on #使用Netflow捕获方法interfaces=enp5s0f1 #监控的端口，我这里使用的是镜像端口,不然监控不到整个网端流量notify_script_path=/usr/local/bin/notify_about_attack.sh #触发脚本位置monitor_local_ip_addresses = on #监控本地地址sort_parameter = packets #在控制台排序单位，包max_ips_in_list =400 #在控制台显示多少地址 编辑要监控的网段vim /etc/networks_list #编辑networks_list加入要监控的网段这里我加入211.156.182.0/24 220.242.2.0/24/etc/init.d/fastnetmon start #启动fastentmon，启动失败查看/var/log/fastnetmon.log/opt/fastnetmon/fastnetmon #打开监控控制台 修改监控脚本1,要先外发邮件必须配置mailx 安装mailx，配置SMTP vim /etc/mail.rc 我这里配置的是我163的邮箱set bsdcompatset from=xxxxx@163.com smtp=smtp.163.comset smpt-auth-user=xxxx@163.com set smtp-auth-user=xxxxxx@163.com smtp-auth-password=xxxxxx smtp-auth=login vim /usr/local/bin/notify_about_attack.sh #这里填要触发监控的脚本修改邮件地址为接受人。cat | mail -s “FastNetMon Guard: IP $1 blocked because $2 attack with power $3 pps” $email_notify; 测试使用低轨道等离子炮（Loic）进行测试这里我对我们211.156.182.143进行Tcp DDOS攻击我们先把这些值调低threshold_pps = 2000，threshold_mbps = 100，threshold_flows = 350 #监控的限定值然后重启Fastnetmon 测试完修改回值]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hello]]></title>
      <url>%2F2016%2F12%2F06%2Fhello%2F</url>
      <content type="text"><![CDATA[你好,欢迎来到我的个人技术博客.]]></content>
    </entry>

    
  
  
</search>
