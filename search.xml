<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[kubernetes基础理解]]></title>
      <url>%2F2018%2F01%2F15%2Fkubernetes_base%2F</url>
      <content type="text"><![CDATA[概念clustercluster是计算、存储、网络的资源的集合，kubernetes利用这些资源运行各种基于容器的应用; master节点master是cluster的大脑，类似openstack的控制节点，运行着apiserver、controller-manager、scheduler它的主要职责是对资源管理、调度，还有认证、弹性伸缩、安全认证。 node节点运行容器应用，由mater管理，接收master节点的各类请求，进行容器的创建和管理，并将运行在上面的容器应用上报到master节点，类似openstack的计算节点。 PODpod是kubernetes最小的调度单元，一个pod对应一个或多个container，通常会将紧密相连的一组应用放到一个pod中，同一个pod的container共享ip和namespace。 为什么使用pod?方便管理在kubernetes中pod是container的载体，一个pod里面拥有一个或多个container，做为一个逻辑单元，方便管理。 资源共享和通信同一个pod中的container共享一个网络栈和存储，相互之间可以直接通过localhost进行通信，同时也共享同一块存储卷 灵活kubernetes直接管理对象是pod，而不是底层的docker，对于docker的操作，被封装在pod中，不会直接操作，这也意味着，pod包含也可以是其他公司的容器产品，比如coreos的rtk或阿里巴巴的pouch。 controllerkubernetes使用controller来创建和管理pod，controller中定义了pod的部署特性，比如部署几个副本，在什么样的node上运行。为满足不同的业务需求，kubernetes提供了如下controller。 Deployment是最常用的 Controller，比如前面就是通过创建 Deployment 来部署应用的。Deployment 可以管理 Pod 的多个副本，并确保 Pod 按照期望的状态运行。 Replicaset实现了 Pod 的多副本管理。使用 Deployment 时会自动创建 ReplicaSet，也就是说 Deployment 是通过 ReplicaSet 来管理 Pod 的多个副本，我们通常不需要直接使用 ReplicaSet。 Daemonset 用于每个 Node 最多只运行一个 Pod 副本的场景。正如其名称所揭示的，DaemonSet 通常用于运行 daemon。 StatefuleSet能够保证 Pod 的每个副本在整个生命周期中名称是不变的。而其他 Controller 不提供这个功能，当某个 Pod 发生故障需要删除并重新启动时，Pod 的名称会发生变化。同时 StatefuleSet 会保证副本按照固定的顺序启动、更新或者删除。 Job用于运行结束就删除的应用。而其他 Controller 中的 Pod 通常是长期持续运行。 servicedeployment有多个副本pod，每个pod有自己的ip，外界通过service来访问pod，service有自己的ip和端口，service为pod提供负载均衡，简单说，service是为一组功能相同的pod提供统一入口。 namespace这里根linux的namespace不一样通过namespace将用户创建的controller和pod等资源分开。namespace可以将一个物理的cluster划成多个虚拟的cluster，每个cluster就是一个namespace，不同的namespace里面资源是完全隔离的。kubernetes默认创建了三个namespace，default、kube-public、kube-systemdefault：默认创建的资源将放到这个namespace里面。kube-system：kubernetes自己创建的系统资源将放这里。kube-public：kubernetes自己创建的命名空间，用于确保整个集群中公开的看到某些资源。创建pod时指定namespace1kubectl --namespace=&lt;insert-namespace-name-here&gt; run nginx --image=nginx 组件构成master节点etcdkubernetes的后端数据存储系统，cluster中的所有资源对象数据都存储在这，用于配置共享和服务发现。 kube-apiserver1、整个集群管理的api接口，所有对集群的操作和查询都需要经过api-server。2、集群内各个模块通信的枢钮，集群内其他模块，互相之间并不能直接通信，都是通过api-server打交道，来完成自己那部分工作。3、集群安全控制，apiserver提供了集群的安全验证，和角色权限分配。4、连接etcd，集群内所有组件都不能直接连接etcd只能通过api-server去连接etcd。 kube-controller-manager1、负责集群的故障检测和修复。2、根据deployment的定义，维持正确的pod副本数。3、根据Service与Pod的管理关系，完成服务的Endpoints对象的创建和更新；4、为新的namespace创建帐户和api访问token。 kube-scheduler创建pod时，选择合适的node进行调度。 kube-dnskubernetes中的域名解析服务器。 flannelflannel的网络组件。 kube-dashboardkubernetes的web控制端。 node节点kubelet负责node节点容器的创建、删除监控等生命周期管理，上报node信息如cpu、内存、pod的ip等信息导master节点的api-serverkube-proxy用于实现端口映射和负载均衡。kube-dnskubernetes的域名解析服务器flannel网络flannel的组件 根openstack类比角色上kubernetes分为master节点和node节点，其中master节点是大脑，node节点上承载master调度过来的资源，接收master节点的各类请求。这点根openstack的control节点和compute节点基本一样。组件上master节点先从etcd说起，etcd做为kubernetes的后端数据存储系统用于配置共享和服务发现，和openstack中的控制节点的数据库和消息队列服务提供的服务相似。kube-api-server在kubernetes提供集群管理的api接口，角色的权限分配和安全验证，和openstack中的keystone和各类组件的api-server相似。kube-scheduler在kubernetes中为pod选择最合适的node节点。与openstack的nova-scheduler提供功能相似。node节点kubelet在node节点上负责pod的创建和生命周期的管理，同时对宿主机资源使用情况进行上报到master，与openstack的nova-compute所提供的功能非常一致。docker是运行容器的引擎根openstack的hypervisor提供的功能一致。kubelet调用docker创建容器，nova-compute调用kvm去创建虚拟机。 例子(参考cloudmanhttps://steemit.com/kubernetes/@cloudman6/k8s-kubernetes-3)部署应用 部署应用（kubernetes-bootcamp)1kubectl run kubernetes-bootcamp --image=docker.io/jocatalin/kubernetes-bootcamp:v1 --port=8080 查看部署的应用，这里deployment为kubernetes术语，理解为应用。1kubectl get deployment docker镜像通过–image指定–port设置应用对外服务的端口。 流程:1、kubectl发送应用部署请求到kube-apiserver，kube-apiserver将请求写入etcd中。2、api-server通知kube-controller-manger，创建一个deployment对象。将结果通过api-server写入etcd。3、Scheduler发现后，执行调度流程，为这个新Pod选定一个落户的Node，然后通过API Server将结果写入到etcd中。4、目标节点kubelet进程通过api-server检测这个新生的pod，并按照定义创建pod。 查看当前pod1kubectl get pods 默认情况下使用pod只能在集群内部进行访问，为了能在外部访问容器，需要将容器应用的端口映射到node节点端口。修改kubernetes-bootcamp的端口类型为NodePort，容器内端口为8080，映射的端口会自动从30000~32767中挑选一个。services可以认为是端口映射。 kubernetes-bootcamp分配到了node-2上，可以直接访问node-2的:31079 scale应用（弹性伸缩)当前kubernetes-bootcamp副本数还是为1扩展应用为31kubectl scale deployment/kubernetes-bootcamp --replicas=3 pod数也增长为3了。 每次访问node-1:31079，都是不同的pod给回的返回结果。可以发现每次请求发到了不同的pod上从而实现了负载均衡。 缩减pod数为21kubectl scale deployment/kubernetes-bootcamp --replicas=2 滚动更新当前应用image版本为v1，升级为v2在次访问，版本是v2了回退版本验证结果 参考链接:http://blog.csdn.net/liukuan73/article/details/54971854https://steemit.com/kubernetes/@cloudman6/k8s-kubernetes-3]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[kubernetes V1.9安装(附离线安装包和离线镜像）]]></title>
      <url>%2F2018%2F01%2F02%2Fkubernetes_install%2F</url>
      <content type="text"><![CDATA[简介环境信息（采用一个master节点+两个node节点)master 192.168.2.110node-1 192.168.2.112node-2 192.168.2.113 软件版本kubernetes v1.9docker：17.03kubeadm:v1.9.0kube-apiserver:v1.9.0kube-controller-manager:v1.9.0kube-scheduler:v1.9.0k8s-dns-sidecar:1.14.7k8s-dns-kube-dns:1.14.7k8s-dns-dnsmasq-nanny:1.14.7kube-proxy:v1.9.0etcd:3.1.10pause :3.0flannel:v0.9.1kubernetes-dashboard:v1.8.1 采用kubeadm安装 kubeadm为kubernetes官方推荐的自动化部署工具，他将kubernetes的组件以pod的形式部署在master和node节点上，并自动完成证书认证等操作。因为kubeadm默认要从google的镜像仓库下载镜像，但目前国内无法访问google镜像仓库，所以这里我提交将镜像下好了，只需要将离线包的镜像导入到节点中就可以了。 开始安装所有节点操作下载链接: https://pan.baidu.com/s/1c2O1gIW 密码: 9s92比对md5解压离线包 MD5 (k8s_images.tar.bz2) = b60ad6a638eda472b8ddcfa9006315ee 解压下载下来的离线包1tar -xjvf k8s_images.tar.bz2 安装docker-ce17.03(kubeadmv1.9最大支持docker-ce17.03)12rpm -ihv docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpmrpm -ivh docker-ce-17.03.2.ce-1.el7.centos.x86_64.rpm 修改docker的镜像源为国内的daocloud的。 1curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://a58c8480.m.daocloud.io 启动docker-ce1systemctl start docker &amp;&amp; systemctl enable docker 绑定hosts将master和node-1、node-2绑定hosts master节点与node节点做互信123[root@master ~]# ssh-keygen[root@master ~]# ssh-copy-id node-1[root@master ~]# ssh-copy-id node-2 关闭防火墙和selinux1systemctl stop firewalld &amp;&amp; systemctl disable firewalld vim /etc/selinux/configSELINUX=disabled1setenforce 0 配置系统路由参数,防止kubeadm报路由警告12345echo &quot;net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1&quot; &gt;&gt; /etc/sysctl.confsysctl -p 导入镜像 1234567891011docker load &lt; /root/k8s_images/docker_images/etcd-amd64_v3.1.10.tardocker load &lt;/root/k8s_images/docker_images/flannel\:v0.9.1-amd64.tardocker load &lt;/root/k8s_images/docker_images/k8s-dns-dnsmasq-nanny-amd64_v1.14.7.tardocker load &lt;/root/k8s_images/docker_images/k8s-dns-kube-dns-amd64_1.14.7.tardocker load &lt;/root/k8s_images/docker_images/k8s-dns-sidecar-amd64_1.14.7.tardocker load &lt;/root/k8s_images/docker_images/kube-apiserver-amd64_v1.9.0.tardocker load &lt;/root/k8s_images/docker_images/kube-controller-manager-amd64_v1.9.0.tardocker load &lt;/root/k8s_images/docker_images/kube-scheduler-amd64_v1.9.0.tardocker load &lt; /root/k8s_images/docker_images/kube-proxy-amd64_v1.9.0.tardocker load &lt;/root/k8s_images/docker_images/pause-amd64_3.0.tardocker load &lt; /root/k8s_images/docker_images/kubernetes-dashboard_v1.8.1.tar 安装安装kubelet kubeadm kubectl包1234rpm -ivh socat-1.7.3.2-2.el7.x86_64.rpmrpm -ivh kubernetes-cni-0.6.0-0.x86_64.rpm kubelet-1.9.9-9.x86_64.rpm kubectl-1.9.0-0.x86_64.rpmrpm -ivh kubectl-1.9.0-0.x86_64.rpmrpm -ivh kubeadm-1.9.0-0.x86_64.rpm master节点操作启动kubelete1systemctl enable kubelet &amp;&amp; sudo systemctl start kubelet 开始初始化master1kubeadm init --kubernetes-version=v1.9.0 --pod-network-cidr=10.244.0.0/16 kubernetes默认支持多重网络插件如flannel、weave、calico，这里使用flanne，就必须要设置–pod-network-cidr参数，10.244.0.0/16是kube-flannel.yml里面配置的默认网段，如果需要修改的话，需要把kubeadm init的–pod-network-cidr参数和后面的kube-flannel.yml里面修改成一样的网段就可以了。 发现kubelet启动不了查看日志/var/log/message 发现原来是kubelet默认的cgroup的driver和docker的不一样，docker默认的cgroupfs，kubelet默认为systemd修改1vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 重启reload1systemctl daemon-reload &amp;&amp; systemctl restart kubelet 查看状态 此时记得将环境reset一下执行1kubeadm reset 在重新执行1kubeadm init --kubernetes-version=v1.9.0 --pod-network-cidr=10.244.0.0/16 将kubeadm join xxx保存下来，等下node节点需要使用如果忘记了，可以在master上通过kubeadmin token list得到 按照上面提示，此时root用户还不能使用kubelet控制集群需要，配置下环境变量对于非root用户123mkdir -p $HOME/.kubecp -i /etc/kubernetes/admin.conf $HOME/.kube/configchown $(id -u):$(id -g) $HOME/.kube/config 对于root用户 1export KUBECONFIG=/etc/kubernetes/admin.conf 也可以直接放到~/.bash_profile1echo &quot;export KUBECONFIG=/etc/kubernetes/admin.conf&quot; &gt;&gt; ~/.bash_profile source一下环境变量1source ~/.bash_profile kubectl version测试 安装网络，可以使用flannel、calico、weave、macvlan这里我们用flannel。 下载此文件1wget https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml 或直接使用离线包里面的若要修改网段，需要kubeadm –pod-network-cidr=和这里同步vim kube-flannel.yml修改network项1&quot;Network&quot;: &quot;10.244.0.0/16&quot;, 执行 1kubectl create -f kube-flannel.yml node节点操作修改kubelet配置文件根上面有一将cgroup的driver由systemd改为cgroupfsvim /etc/systemd/system/kubelet.service.d/10-kubeadm.confEnvironment=”KUBELET_CGROUP_ARGS=–cgroup-driver=cgroupfs”1systemctl daemon-reload 1systemctl enable kubelet&amp;&amp;systemctl restart kubelet 使用刚刚执行kubeadm后的kubeadm join –xxx1kubeadm join --token 361c68.fbafaa96a5381651 192.168.2.110:6443 --discovery-token-ca-cert-hash sha256:e5e392f4ce66117635431f76512d96824b88816dfdf0178dc497972cf8631a98 多次加入报错查看/var/log/message日志.这个错是因为没有配置前面sysctl的router的环境变量 发现node节点启动不了flannel容器，查看容器log发现是host上没有默认路由，在网卡配置文件里面设置好默认路由。 加入成功在master节点上check一下 kubernetes会在每个node节点创建flannel和kube-proxy的pod 测试集群在master节点上发起个创建应用请求这里我们创建个名为httpd-app的应用，镜像为httpd，有两个副本pod1kubectl run httpd-app --image=httpd --replicas=2 检查pod可以看见pod分布在node-1和node-2上 因为创建的资源不是service所以不会调用kube-proxy直接访问测试 至此kubernetes基本集群安装完成。 部署kubernetes-dashboardkubernetes-dashboard是可选组件，因为，实在不好用，功能太弱了。建议在部署master时一起把kubernetes-dashboard一起部署了,不然在node节点加入集群后，kubernetes-dashboard会被kube-scheduler调度node节点上，这样根kube-apiserver通信需要额外配置。下载kubernetes-dashboard的配置文件或直接使用离线包里面的kubernetes-dashboard.yaml1wget https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml 修改kubernetes-dashboard.yaml 如果需要让外面访问需要修改这个yaml文件端口类型为NodePort默认为clusterport外部访问不了， nodeport端口范围30000-3276732666就是我的映射端口，根docker run -d xxx:xxx差不多创建kubernetes-dashboard1kubectl create -f kubernetes-dashboard.yaml 访问1https://master_ip:32666 默认验证方式有kubeconfig和token，这里我们都不用。这里我们使用basicauth的方式进行apiserver的验证创建/etc/kubernetes/manifests/pki/basic_auth_file 用于存放用户名和密码 #user,password,useridadmin,admin,2 给kube-apiserver添加basic_auth验证1vim /etc/kubernetes/manifests/kube-apiserver.yaml 加上这行 更新kube-apiserver容器1kubectl apply -f kube-apiserver.yaml 授权k8s1.6后版本都采用RBAC授权模型给admin授权默认cluster-admin是拥有全部权限的，将admin和cluster-admin bind这样admin就有cluster-admin的权限。 那我们将admin和cluster-admin bind在一起这样admin也拥用cluster-admin的权限了1kubectl create clusterrolebinding login-on-dashboard-with-cluster-admin --clusterrole=cluster-admin --user=admin 查看1kubectl get clusterrolebinding/login-on-dashboard-with-cluster-admin -o yaml 在此访问https://master:32666 选基本，就可以通过用户名和密码访问了 创建个应用测试 部署成功 参考链接:https://kubernetes.io/docs/setup/independent/install-kubeadm/http://tonybai.com/2017/07/20/fix-cannot-access-dashboard-in-k8s-1-6-4/]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Docker多主机网络(weave)]]></title>
      <url>%2F2017%2F12%2F15%2Fdocker_weave%2F</url>
      <content type="text"><![CDATA[此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。weave软件版本docker：17.09weave：2.1.3项目文 档：https://www.weave.works/docs/net/latest/overview/项目地址：https://github.com/weaveworks/weave环境信息container-1 192.168.111.161container-2 192.168.111.162 概述weave是weaveworks开发的容器网络解决方案，weave会创建一个大的二层网络，来将各个host上的容器连接起来，容器与容器之间可以直接通信，无须nat和端口映射,weave还提供dns服务，使容器可以直接通过hostname访问。weave不需要依赖k\v服务，而是在每个主机上运行一个weave容器路由器。来根据不同的主机交换路由信息。 注意点：1、一个Weave网络是由一定数量的Weave节点构成的。每个Weave路由器运行在不同的宿主机上。 2、Weave网络中的每个节点都有一个名字，默认是Weave网卡的地址。还有一个人类便于识别的名称，默认是主机名，也可以在Weave启动的时候指定一个名称。 3、Weave路由器节点之间会建立TCP连接，通过TCP连接，进行心跳检测和交换路由信息。通过配置可以让这条链接进行加密处理。 4、Weave路由器节点之间也会建立UDP连接，通过UDP连接，进行网络数据包的封装。同样也可以将UDP链接进行加密处理。 Weave路由器节点之间的连接(TCP连接或者UDP连接)可以穿透防火墙，意味着，Weave网络可用于跨数据中心的Docker通信。 5、Weave网络会在每个宿主机上创建一个网桥，每个容器通过veth pair连接到这个Weave 网桥。容器里面的veth网卡会获取到Weave网络分配给的IP地址和子网掩码。 6、Weave网络在不同宿主机之间路由数据的方法有两种：fast data path模式(完全工作在内核态)，sleeve模式，在这种模式中，发往非本地容器的数据会被内核捕获，然后交给用户态的Weave网络路由器处理，通过UDP发送到其他的Weave节点路由器，然后注入到内核空间，最后再转发到本地的容器中。 7、Weave网络路由器会学习其他节点特定的MAC地址，然后将已知的信息和拓扑结合起来。 8、Weave网络可以在partially connected路由数据，通过拓扑交换。比如下图中：节点1 直接连接着 节点2和节点3，如果节点1 先要发送数据到 节点4或者节点5 ，那么先必须发送到节点3。 Weave 网络如何了解网络拓扑？官网地址：https://www.weave.works/docs/net/latest/concepts/network-topology/ Peers之间的拓扑交流 连接到Weave网络的节点会捕获其他Peers节点的拓扑信息。Weave节点会将自己已知的拓扑，和改变的内容传递给其他节点，以便于所有的Weave Peers了解整过网络拓扑。 Weave节点之间的通信建立在TCP上的，有两种方法： 基于STP(生成树)的广播方法基于邻居Gossip方法(八卦算法)以下情况，拓扑信息会发送： 当一个连接新加入进来的时候，如果远程节点似乎是新加入到Weave网络中的，那么会发送整个网络拓扑，并且增量更新，广播节点两端的连接。当一个连接被标记为已经建立，则意味着远端可以从本端接受UDP报文，然后广播一个包含本端信息的数据包。当一个连接断开，一个包含本端信息的报文被广播。周期性的定时器，整个拓扑信息被gossip给邻居，这是为了拓扑敏感的随机分布系统。这是为了防止由于频繁的拓扑变化，造成广播路由表过时，而使前面提到的广播没有到达所有的peers。如果Peers更新拓扑信息之后，发现有一个Peer已经离线，那么就会清除掉这个Peer相关的所有的信息。 拓扑过期怎么办？ 将拓扑变化信息广播给所有peers不是立即发生的。这就意味着，很有可能一个节点有过期的网络拓扑视图。 如果目的peer的数据包仍然可达，那么过期的拓扑可能会导致一次低效的路由选择。 如果过期的拓扑中目的peer不可达，那么数据包会被丢弃，对于很多协议（如TCP)，数据发送会在稍后重新尝试，在这期间拓扑信息应当被正确更新。 weave安装container-1和container-2上安装weave12curl -L git.io/weave -o /usr/local/bin/weavechmod a+x /usr/local/bin/weave 在container-1上启动weave在 host1 中执行 weave launch 命令，weave的组件都是以容器启动的，weave 会从 docker hub 下载最新的 image 并启动容器。 10.32.0.0/12是weave网络使用的默认subnet，如果需要改变(所有host都要改)1weave launch --ipalloc-range 10.2.0.0/16 weave 运行了三个容器：weave 是主程序，负责建立 weave 网络，收发数据 ，提供 DNS 服务等。weavevolumes容器提供卷存储weavedb容器提供数据存储 创建了一个linux bridge桥 和weave网络 container-2执行1weave launch 192.168.111.161 这里指定的container-1的地址，这样container-2和container-1才会加入到同一个网络，也可以写多个地址，格式为 ip_addr ip_addr. 两个host要互通，首先要确认建立了集群 测试在container-1上执行特别说明：声明环境变量的作用是告诉后续的docker命令都发给weave proxy进行处理。声明环境变量12eval $(weave env)docker run -itd --name bbox1 busybox 如果要恢复之前的环境，可执行1eval $(weave env --restore) 在container-2上执行12eval $(weave env)docker run -itd --name bbox2 busybox bbox1 ping bbox2 原理单台host架构图 bbox1和bbox2的地址为10.32.0.1/12、10.44.0.0/12, 子网掩码都是12位的，实际上这3个ip都位于 weave网络的10.32.0.0/12，通过container-1和container-2上的vxlan的遂道连接在一起。 查看bbox1的网络接口 bbox1有两个网络接口eth0和ethwe，eth0桥接的是默认docker0网络。ethwe和宿主机weave网卡有关ethwe@if39表示ethwe对应编号39的interface。 在宿主机ip a看见了39的interface，vethwepl19152和ethwe是一对veth-pair，同时vethwep19152还挂载在weave 桥上。 同时挂载的还有vethwe-bridge，ip -d link查看 vethwe-bridge和vethwe-datapath是一对veth-pair，但vethwe-datapath的父设备是datapath，其中datapath是个openvswitch网桥，vxlan-6784是一个vxlan-interface，其父设备也是datapath，weave主机间是通过vxlan建立的遂道通信，等于说，veth-pair vethwe-bridge和vethwe-datapath将linux-bridge的weave桥和datapath连接在一起，其中，weave桥负责容器接入weave网络，datapath负责主机间建立vxlan隧道并收发数据。 多host时网络架构图 1、当在container-1上的bbox1 ping container-2上bbox2时 2、将数据包给ethwe 2、ethwe和vethwep是veth-pair，所以数据包到了宿主机的weave桥上，然后给vethwe-bridge，然后通过veph-pair的另外一端vethwe-datapath到datapath，通过vxlan发送给container-2。3、container-2的vxlan接口收到报文后会给datapath然后通过vethwe-datapath到weave桥收到报文后，根据目的IP地址将数据包转发给bbox2。 weave网络隔离默认weave使用一个大的subnet(10.32.0.0/12)所有主机容器都从这个地址空间分配ip，因为是同一个subnet，容器可以直接通信，如果要隔离，可以通过环境变量WEAVE_CIDR=net:xxx.xxx.xxx/24指定网段,但需要注意的是这个网段也不是随便写的，要在10.32.0.0/12这个大子网范围内。1docker run -itd -e WEAVE_CIDR=net:xxx.xxx.xxx.xxx/24 image_name weave与外网连接宿主机访问weave网络的容器因为weave默认是一个私有的vxlan网络，默认与外部网络隔离，要想host访问，需要将host加入weave。 10.32.0.2会被分配到weave网桥上 此时就可以ping通了 如果其他主机想访问这台host上的容器，只需要添加一条默认路由将下一跳指向host的地址。container-2访问container-1上的bbox1只需要在container-2上添加1ip route add 10.32.0.0/12 via 192.168.111.161 IPAM默认weave使用一个大的subnet(10.32.0.0/12)所有主机容器都从这个地址空间分配ip一个大二层。 参考链接 http://ibash.cc/frontend/article/59/]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Docker多主机网络(flannel)]]></title>
      <url>%2F2017%2F12%2F14%2Fdocker_flannel%2F</url>
      <content type="text"><![CDATA[此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。flannel软件版本docker：17.09etcd：etcd-3.2.7-1flannel：flannel-0.7.1-2项目地址：https://github.com/coreos/flannel项目地址：https://github.com/coreos/etcd 环境信息container-1 192.168.2.110 etcdcontainer-2 192.168.2.112 dockercontainer-3 192.168.2.113 docker flannel是core os开发出的docker多host的网络解决方案，flannel为每个宿主机分配一个subnet，每个宿主机上都有一个flannel的agent端，通过这个agent端可以进行根其他宿主机网络信息的共享，创建flannel网卡生成路由信息，建立vxlan遂道,各个宿主机的网络信息存储在etcd这个key-value软件中。 flannel的backend vlxan、host-gw、udp等。 安装安装etcd1yum install etcd -y 配置etcd1cp /etc/etcd/etcd.conf /etc/etcd/etcd.conf.bak vim /etc/etcd/etcd.conf1234ETCD_NAME=default #节点名称ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot; #数据存放位置ETCD_LISTEN_CLIENT_URLS=&quot;http://0.0.0.0:2379,http://0.0.0.0:4001&quot; #监听客户端地址ETCD_ADVERTISE_CLIENT_URLS=&quot;http://etcd:2379,http://etcd:4001&quot; #通知客户端地址 启动服务1systemctl start etcd 检查服务 导入网络配置key先将配置信息写到文件 flannel-config.json 中，内容为：[root@container-0 ~]# cat flannel-config.json12345678910111213&#123; &quot;Network&quot;: &quot;10.2.0.0/16&quot;, &quot;SubnetLen&quot;: 24, &quot;Backend&quot;: &#123; &quot;Type&quot;: &quot;vxlan&quot; &#125;&#125; Network 定义该网络的 IP 池为 10.2.0.0/16。 SubnetLen 指定每个主机分配到的 subnet 大小为 24 位，即10.2.X.0/24。 Backend 为 vxlan，即主机间通过 vxlan 通信，后面我们还会讨论host-gw。 将配置存入etcd /docker-test/network/config 是此 etcd 数据项的 key，其 value 为 flannel-config.json 的内容。key 可以任意指定，这个 key 后面会作为 flanneld 的一个启动参数。执行 etcdctl get 确保设置成功。 测试get value 在container-1和container-2上执行安装flannel1yum install flannel -y 配置flannel1cp /etc/sysconfig/flanneld /etc/sysconfig/flanneld.bak ip写etcd的ip 启动flannel12systemctl enable flannelsystemctl start flannel 验证可以看见flannel网卡已经出来了 配置docker使用flannel1cat /run/flannel/subnet.env 将flannel_subnet和flannel_mtu写入docker.service编辑docker.service1vim /usr/lib/systemd/system/docker.service bip和mtu为上图cat /run/flannel/subnet.env的配置项。重启docker12systemctl daemon-reloadsystemctl restart docker docker会10.2.71.1配置到docker0上，同时生成一条路由 同主机的docker使用docker0进行通信，跨主机的使用flannel1.1转发 1ps -ef|grep docker 测试连通性flannel不会创建新的网络，会使用默认的bridge网络启动容器container-112docker run -itd --name bbox1 busybox ` container-21docker run -itd --name bbox2 busybox 原理vxlan容器内网卡和物理机上一veth-xxx是一对veth-pair，同时物理机的veth-xxx挂载在docker0这个bridge上。 容器内默认路由是给10.2.71.1 10.2.71.1是docker0 查看host路由，是10.2.0.0/16这个subnet的包会交给flannel1.1,,flannel1.1封装成udp包通过ens3，发送出去。container-2 收到包解封装，发现数据包目的地址为 10.2.36.2，根据路由表将数据包发送给 flannel1..1，并通过 docker0 到达 bbox2。 flannel并不会创建什么网桥，同一主机通过docker0连接，不同主机通过flanel1.1建立vxlan遂道连接。 host-gwflannel支持的backend;VXLAN: 使用Linux的VxLan；默认的VNI 是 1 ；默认的UDP端口是 8472;host-gw: 创建IP路由的方式, 不会对数据进行封装;UDP: 使用UDP 8285 端口;AliVPC: 不能用于生产;Alloc: 不能用于生产;AWS VPC: 不能用于生产;GCE: 不能用于生产; 修改etcd导入新配置重启etcd1systemctl restart etcd 重启container-1和container-2上的flannel查看路由表，发现生成了一条到container-2的明细路由 mtu变成了1500 重新修改docker.service1vim /usr/lib/systemd/system/docker.service 修改mtu为1500 重启docker1systemctl restart docker 测试连通性 面对 host-gw 和 vxlan 这两种 backend 做个简单比较。host-gw 把每个主机都配置成网关，主机知道其他主机的 subnet 和转发地址。vxlan 则在主机间建立隧道，不同主机的容器都在一个大的网段内（比如 10.2.0.0/16）。 虽然 vxlan 与 host-gw 使用不同的机制建立主机之间连接，但对于容器则无需任何改变，bbox1 仍然可以与 bbox2 通信。 由于 vxlan 需要对数据进行额外打包和拆包，性能会稍逊于 host-gw。 IPAM:flannel为每个主机自动分配独立的subnet，用户只需要指定一个大的地址段，每个host从这个地址段里面在细分。 网络隔离:flannel都用的vxlan的vni1没有实现网络隔离。flannel 为每个主机分配了独立的 subnet，但 flannel.1 将这些 subnet 连接起来了，相互之间可以路由。本质上，flannel 将各主机上相互独立的 docker0 容器网络组成了一个互通的大网络，实现了容器跨主机通信。flannel 没有提供隔离。 http://ibash.cc/frontend/article/58/http://www.cnblogs.com/CloudMan6/p/7270551.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Docker多主机网络(macvlan)]]></title>
      <url>%2F2017%2F12%2F07%2Fdocker_macvlan%2F</url>
      <content type="text"><![CDATA[docker 版本：Docker version 17.10.0-ce, build f4ffd25操作系统版本：centos7.3 物理网卡虚拟化技术，将一张物理网卡虚拟化出多个网卡，每个都用独立的mac地址和ip地址docker用macvlan优点：1、性能好，相比其他的实现，macvlan不需要创建linux bridge、直接通过物理网卡出去 缺点：vlan子接口和需要自己提前划出来，ip地址使用需要自己手动去配置。 准备交换机：1、创建vlan 这里使用vlan 100和vlan 200。2、将两台container host连接的交换机口配置为trunk模式允许vlan 100和vlan200通过。 注意1、使用macvlan网络时因为不使用bridge了所有网卡都是一个真实的物理设备，能进行三层转发。 服务器1、使用container-1和container-2的eth0连接交换机； 操作container-1和container-2分别执行1vconfig add eth0 100 1vconfig add eth0 200 1ip link set eth0.100 up 1ip link set eth0.200 up 创建macvlan网络在container-1和container-2上执行 1docker network create -d macvlan --subnet=192.168.100.0/24 --gateway=192.168.100.1 -o parent=eth0.100 mac_net100 1docker network create -d macvlan --subnet=192.168.200.0/24 --gateway=192.168.200.1 -o parent=eth0.200 mac_net200 mac_net100和mac_net200本质上是独立的，为了避免ip冲突，在run时手动指定ip ;container-1上运行容器1docker run -itd --name mac_net1 --ip=192.168.100.10 --network mac_net100 busybox 1docker run -itd --name mac_net2 --ip=192.168.200.10 --network mac_net200 busybox container-2上运行容器1docker run -itd --name mac_net3 --ip=192.168.100.20 --network mac_net100 busybox 1docker run -itd --name mac_net4 --ip=192.168.200.20 --network mac_net200 busybox 正常情况是container-1上的mac_net1能够container-2上的mac_net3通新，mac_net2能根mac_net4通信; 原理 1、mac_net100 ping mac_net300 2、因为macvlan的包是直接走子接口出去的，所以不用桥，也不用通过封包。 IPAMMACVLAN需要用户自己管理subnet，自己分配地址，不同subnet通信依赖外部网关。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[docker-compose]]></title>
      <url>%2F2017%2F12%2F05%2Fdocker_compose%2F</url>
      <content type="text"><![CDATA[概念用来定义和管理多个容器的容器编排工具，docker-compose是docker-fig的升级版。能做什么？能够快速管理和创建多个容器，比如我直想起一个WordPress，在容器中也不需要自己去搭建lamp和然后安装WordPress了，只需要在docker hub找mysql和WordPress的镜像就可以，然后自己docker run，传对应的参数进去，映射端口，映射目录，很麻烦，同时非常容易出错，有了docker-compose，你只需要通过yaml语言，编写docker-compose脚本，然后就可以通过docker-compose一键是启动，关闭，删除这些容器了。脚本更加好维护和方便迁移。 安装docker-compose可以直接通过pip 安装，因为docker-compose是基于python写的1pip install docker-compose 原理dockerc-compose使用不同的请求将会调用不同的python方法去处理，比如使用docker-compose run -d将会调用run方法处理，最终调用docker-daemon去完成操作，docker-compose借助docker-py来完任务，docker-py是一个使用python开发并调用docker-daemon api的docker client包。 docker-compose命令1234567891011121314151617181920–-verbose：输出详细信息。-f：制定一个非docker-compose.yml命名的yaml文件。-p：设置一个项目名称，默认是当前目录的名称。-d：表示后台运行--project-directory ：设置一个项目路径默认build:构建镜像，docker-compose里面镜像来源有两种一种是通过Dockerfile构建，一种是通过镜像仓库pull下来，build在这里就是指调用Docker-compose的build参数，构建镜像config:用于检查docker-compose的yml文件语法是否有错误create:创建出来的容器状态为create状态。down：将创建出来的容器集群down掉。events:接收容器事件exec：对run的容器执行命令ps：输出正在运行容器pull:pull一个服务镜像下来 docker-compose pull server_namepush:上传一个容器镜像 docker-compose push server_namestop：停止容器集群start：启动容器集群up：创建和启动一个容器scale:设置运行容器的数量，可以进行集群伸缩，如docker-compose scale resource_name=container_num 例下面这个docker-compose.yml默认执行docker-compose up -d只会起一个web1一个db1，web1使用centos镜像，db1使用httpd镜像，并且db1依赖web1，必须等到web1安装完才能操作。 12345678910version: &quot;3&quot;services: web1: image: centos tty: true db1: image: httpd depends_on: - web1 tty: true 假设web1资源不够了需要进行扩容到5个执行1docker-compose scale web1=5 迅速扩容到5个，此时高峰期以过，不需要这么多web1容器了需要缩减到正常执行1docker-compose scale web1=1 docker-compose.yml语法解析详细参考12https://docs.docker.com/compose/compose-file/compose-versioning/https://www.cnblogs.com/freefei/p/5311294.html 1234567891011以下面这个文件为例version: &quot;3&quot;services: web1: image: centos tty: true db1: image: centos depends_on: - web1 tty: true version ：版本，docker-compose的版本，不同的版本所支持的语法是不一样的，所对应的docker版本也是不一样的如,但新版本都是向下兼容的 。 services：表示定义服务，是以容器为颗粒度的。web1：表示服务名。image：才用哪个镜像，docker-compose支持两种镜像来源，一种是从docker-hub上，或本地镜像仓库，一种是通过dockerfile构建镜像那这里就应该是build了。tty：根docker run的-t一样，为容器重新分配一个伪输入终端。depends_on:依赖那个服务，这里表示等待哪个服务先完成，在进行本服务的操作。 其中参数123links: - server_name - server_name:alias_name 链接到其他服务中的容器，使用服务名称，或别名,这样在容器中就可以通过服务名的方式联通。但在新版本docker 17.09版本中发现就算不link，docker默认也将他们信息添加到docker dns中了可以直接ping通。12external_links - container_name 链接外部已经存在容器，非docker-compose管理的容器12ports: - &quot;xxx:xxx&quot; 端口映射123volumes： -/dxx/xxx:/xxx -/dxx/xxxx:xxx:ro 卷映射,加上ro表示只读模式。12volume_from: - container_name 从另外一个服务或容器挂载他的卷。 123environment： - XXXX=XXX - XXXX=XXX 设置环境变量12extra_hosts: - &quot;www.test.com: xxx.xxx.xxx.xx&quot; 添加此项到hosts文件 container_name: xxx 设置容器名 hostname： xxx 设置主机名 restart:always 根docker run参数一样，无论怎么样，自动拉起此容器 例子python flask框架网站以docker官网例子docker-compose构建一个简单的动态网站为例创建compose文件夹，并cd到目录1mkdir /compose 1cd /compose 编写一个python文件，内容如下vim app.py12345678910111213from flask import Flaskfrom redis import Redisapp = Flask(__name__)redis = Redis(host=&apos;redis&apos;, port=6379)@app.route(&apos;/&apos;)def hello(): count = redis.incr(&apos;hits&apos;) return &apos;Hello World! I have been seen &#123;&#125; times.\n&apos;.format(count)if __name__ == &quot;__main__&quot;: app.run(host=&quot;0.0.0.0&quot;, debug=True) 其中host的redis为docker-compose里面定义的redis的service-name，因为新版本docker默认会将service_name 添加到dns所以可以不用link直接就可以通过主机名进行通信。 创建requirements.txt文件用于pip程序安装 vim requirements.txt12flaskredis 创建Dockerfile文件用于构建镜像vim Dockerfile12345FROM python:3.4-alpineADD . /codeWORKDIR /codeRUN pip install -r requirements.txtCMD [&quot;python&quot;, &quot;app.py&quot;] 创建Docker-compose文件，用于构建应用集群vim docker-compose.yml12345678910version: &apos;3&apos;services: web: build: . volumes: - .:/code ports: - &quot;5000:5000&quot; redis: image: &quot;redis:alpine&quot; 开始运行docker-compose up 访问浏览器http://localhost:5000,刷新一下times根着变一次。 查看容器1docker ps -a 如果要后台运行1docker-compose up -d 查看服务的环境变量 通过service_name控制单个服务的开关。 需要注意的是，如果没指定网络的话，默认docker-compose会创建一个以docker-compose.yml所在文件夹名为名的bridge网络。比如我的docker-compose.yml文件是放在zabbix这个目录，那么通过docker-compose构建集群后，会自动创建一个zabbix_default网络，类型为bridge。 如果需要使创建的应用集群同时创建网络，使用下面例子，创建docker集群时，同时构建指定网络名称和类型、网段的docker网络。12345678910111213141516171819version: &apos;3&apos;services: web: build: . ports: - &quot;5000:5000&quot; networks: - test redis: networks: - test image: &quot;redis:alpine&quot;networks: test: driver: bridge ipam: driver: default config: - subnet: 172.28.0.0/16 web和reids服务都通过network参数加入到test网络中，下面networks资源开始创建test网络，类型为bridge，ipadm驱动为默认，网段为172.28.0.0/16。 docker-compose默认没指定网络会创建一个自己的网络，然后将这些service加入进去。 执行docker-compose up -d 查看这个网络 如果需要加入一个已经存在的网络创建网络1docker network create --driver bridge --subnet 192.168.1.0/24 --gateway 192.168.1.1 my_net 123456789101112131415161718version: &quot;3&quot;services: web1: image: centos tty: true networks: - my_net db1: image: centos depends_on: - web1 tty: true networks: - my_netnetworks: my_net: external: true 1docker-compose up -d 在去镜像内查看就发现是192.168.1.0/24的网段了。 构建WordPress应用使用两个镜像，一个mysql，一个WordPress，WordPress已经部署好了apache和php和WordPress。 1234567891011121314151617181920212223version: &apos;3&apos;services: db1: image: mysql:5.7 volumes: - /var/lib/mysql:/var/lib/mysql environment: - MYSQL_ROOT_PASSWORD=password - MYSQL_DATABASE=wordpress - MYSQL_USER=wordpress - MYSQL_PASSWORD=wordpress wordpress: image: wordpress:latest depends_on: - db1 ports: - 8000:80 environment: - WORDPRESS_DB_HOST=db1 - WORDPRESS_DB_USER=wordpress - WORDPRESS_DB_PASSWORD=wordpress - WORDPRESS_DB_NAME=wordpress 默认docker-compose出来的容器已经自动添加service_name和ip的映射了，所以这里DB_HOST直接写db1，不用links也是可以的互通的。这些镜像的环境变量可以在docker hub上找到 构建使用下面的命令，所有的服务将使用后台模式被启动1docker-compose up -d 访问http://localhost:8000 构建zabbix监控zabbix-server分为两个部分，zabbix-server-mysql负责接收zabbix-agent的监控数据，然后将监控数据存储到mysql中，zabbix-web-nginx-mysql负责展示监控数据。 因为默认mysql数据库不是utf-8的编码所以需要重新构建下镜像vim Dockerfile12FROM mysql:5.7CMD [&quot;mysqld&quot;, &quot;--character-set-server=utf8&quot;, &quot;--collation-server=utf8_bin&quot;]， 所以在mysql镜像这里没有直接用仓库的镜像，而是采用dockerfile重新构建。123456789101112131415161718192021222324252627282930313233343536version: &apos;3&apos;services: db1: build: . volumes: - /var/lib/mysql:/var/lib/mysql environment: - MYSQL_ROOT_PASSWORD=password - MYSQL_USER=zabbix - MYSQL_PASSWORD=zabbix - MYSQL_DATABASE=zabbix zabbix-server: image: zabbix/zabbix-server-mysql:ubuntu-3.4.4 ports: - 10051:10051 volumes: - /var/lib/mysql:/var/lib/mysql environment: - DB_SERVER_HOST=db1 - MYSQL_USER=zabbix - MYSQL_PASSWORD=zabbix - MYSQL_DATABASE=zabbix zabbix-nginx-mysql: image: zabbix/zabbix-web-nginx-mysql:ubuntu-3.4.4 depends_on: - db1 ports: - 8000:80 environment: - DB_SERVER_HOST=db1 - MYSQL_USER=zabbix - MYSQL_PASSWORD=zabbix - ZBX_SERVER_HOST=zabbix-server - PHP_TZ:Asia/Shanghai 启动docker-compose up -d访问http://localhost:8000 参考链接http://wiki.jikexueyuan.com/project/docker-technology-and-combat/yaml_file.htmlhttps://www.hi-linux.com/posts/12554.htmlhttps://docs.docker.com/compose/install/#install-composehttp://www.ywnds.com/?p=7592]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[docker镜像仓库管理软件(habor）]]></title>
      <url>%2F2017%2F12%2F04%2Fdocker_habor%2F</url>
      <content type="text"><![CDATA[项目首页http://vmware.github.io/harbor/#gettingHarbor软件版本操作系统：centos7.2harbor：1.2.2docker：17.09-ce环境信息container-0 192.168.2.110container-1 192.168.2.112container-2 192.168.2.113 habor是什么？habor并不是镜像仓库，更确切的说是一个镜像仓库管理平台，原生的docker registry不支持用户权限管理，而habor镜像仓库也是用的原生的docker registry，只是又通过一些其他组件来进行用户和权限管理功能。habor默认镜像是存储在本地文件系统，可以的支持的第三方对象存储，如Amazon s3、openstack swift、ceph radosgw。 组件介绍habor安装默认会启动这些组件habor-administratorhabor的系统管理容器，可以进行habor-Server的一些系统信息的获取，如存储用量harbor-db负责存储habor的用户信息和项目信息harbor-jobservice负责habor与habor的之间项目的同步harbor-log负责统一管理habor日志habor-ui负责web端展示，token的验证和生成。nginx前端反向代理，registrydocker的镜像仓库 整体架构来源(http://blog.csdn.net/u010278923/article/details/77941995) 主要组件包括proxy，他是一个nginx前端代理，主要是分发前端页面ui访问和镜像上传和下载流量，上图中通过深蓝色先标识；ui提供了一个web管理页面，当然还包括了一个前端页面和后端API，底层使用mysql数据库；registry是镜像仓库，负责存储镜像文件，当镜像上传完毕后通过hook通知ui创建repository，上图通过红色线标识，当然registry的token认证也是通过ui组件完成；adminserver是系统的配置管理中心附带检查存储用量，ui和jobserver启动时候回需要加载adminserver的配置，通过灰色线标识；jobsevice是负责镜像复制工作的，他和registry通信，从一个registry pull镜像然后push到另一个registry，并记录job_log，上图通过紫色线标识；log是日志汇总组件，通过docker的log-driver把日志汇总到一起，通过浅蓝色线条标识。 docker login过程(http://www.sohu.com/a/67065472_115128) (a) 首先，这个请求会由监听80端口的proxy容器接收到。根据预先设置的匹配规则，容器中的Nginx会将请求转发给后端的registry 容器；(b) 在registry容器一方，由于配置了基于token的认证，registry会返回错误代码401，提示Docker客户端访问token服务绑定的URL。在Harbor中，这个URL指向Core Services；(c) Docker 客户端在接到这个错误代码后，会向token服务的URL发出请求，并根据HTTP协议的Basic Authentication规范，将用户名密码组合并编码，放在请求头部(header)；(d) 类似地，这个请求通过80端口发到proxy容器后，Nginx会根据规则把请求转发给ui容器，ui容器监听token服务网址的处理程序接收到请求后，会将请求头解码，得到用户名、密码；(e) 在得到用户名、密码后，ui容器中的代码会查询数据库，将用户名、密码与mysql容器中的数据进行比对（注：ui 容器还支持LDAP的认证方式，在那种情况下ui会试图和外部LDAP服务进行通信并校验用户名/密码)。比对成功，ui容器会返回表示成功的状态码，并用密钥生成token，放在响应体中返回给Docker 客户端。至此，一次docker login 成功地完成了，Docker客户端会把步骤(c)中编码后的用户名密码保存在本地的隐藏文件中。 docker push的流程(http://www.sohu.com/a/67065472_115128) 用户登录成功后用docker push命令向Harbor 推送一个Docker image：1# docker push 192.168.2.110/library/hello-world (a) 首先，docker 客户端会重复login的过程，首先发送请求到registry,之后得到token 服务的地址；(b) 之后，Docker 客户端在访问ui容器上的token服务时会提供额外信息，指明它要申请一个对imagelibrary/hello-world进行push操作的token；(c) token 服务在经过Nginx转发得到这个请求后，会访问数据库核实当前用户是否有权限对该image进行push。如果有权限，它会把image的信息以及push动作进行编码，并用私钥签名，生成token返回给Docker客户端；(d) 得到token之后Docker客户端会把token放在请求头部，向registry发出请求，试图开始推送image。Registry 收到请求后会用公钥解码token并进行核对，一切成功后，image的传输就开始了。 安装haborcontainer-0上安装habor安装是写好的docker-compose.yml文件，所以需要先安装docker-compose1easy_install pip 1pip install docker-compose 下载habor离线安装包1wget http://harbor.orientsoft.cn/harbor-1.2.2/harbor-offline-installer-v1.2.2.tgz md5d2af810be0554319181969835a462807解压1tar -xvf harbor-offline-installer-v1.2.2.tgz 修改配置文件1cd /root/habor vim /root/habor/harbor.cfg123hostname=192.168.2.110 //这里不能写localhost和127.0.0.1，ui_url_protocol = http //协议写http或httpsharbor_admin_password = 123456 //harbor admin的密码，默认为Harbor12345 执行安装1[root@container-0 harbor]# ./install.sh 它会将刚刚下载的镜像import进行，然后调用docker-compose，初始化，并启动。 查看集群状态 web访问和基本使用 我这里刚刚设置了用户名为admin密码为123456，如果默认密码为Harbor12345 用户管理创建用户 创建项目 将刚刚创建的wanshaoyuan添加成项目内，角色为项目管理员用户加入对应的项目分为三种角色项目管理员、开发人员、访客分别对应的权限为项目管理员：M(管理)、D(删除)、R(读取)、W(写入)、S(查询)开发人员：R(读取)、W(写入)、S(查询)访客：R(读取)、S(查询) 项目管理项目是一组镜像仓库的逻辑集合，habor对用户权限的控制，实上就是对这个用户在这个项目的权限控制，一个项目可以有多个项目管理员，下面的habor同步也是基于项目的。 验证docker-client配置去上传镜像啦，因为habor使用的是http协议，而默认docker push镜像默认是https的所以我们需要修改下docker-clientcontainer-1和container-2上操作vim /usr/lib/systemd/system/docker.service1ExecStart=/usr/bin/dockerd --insecure-registry=192.168.2.110 1systemctl daemon-reload 1systemctl restart docker 在container-1上，上传镜像导habor给镜像打taghabor格式为1[registry_host]:[port]/[project_name]/[封装的应用]:[版本] 原生的registry，是用户名，habor是写刚刚创建的项目名1[registry_host]:[port]/[username]/[封装的应用]:[版本] push镜像 在habor web端查看 日志可以看见刚刚的push操作 pull镜像在container-2上将刚刚push的镜像pull下来 多habor的同步有多个habor服务器时可以设置镜像的同步 在container-1上在启动一个habor 安装方法同上访问 配置项目复制点击进项目 添加复制规则container-0上查看日志，已经同步成功了 container-1上查看，项目已经创建好了 后面只要container-0上的镜像有更新，container-1也会自动同步 http://blog.csdn.net/u010278923/article/details/72468791http://www.sohu.com/a/67065472_115128https://github.com/vmware/harbor/blob/master/docs/installation_guide.mdhttp://www.cnblogs.com/huangjc/p/6270405.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Docker 管理平台(Rancher)]]></title>
      <url>%2F2017%2F12%2F01%2Fdocker-rancher%2F</url>
      <content type="text"><![CDATA[环境信息3台服务器192.168.2.110 rancher192.168.2.112 docker0192.168.2.113 docker1 软件版本操作系统：centos7.3rancher：v1.6.11在rancher上安装rancher 概念：http://rancher.com/docs/rancher/v1.6/zh/来自rancher官方文档Rancher是一个开源的企业级容器管理平台。通过Rancher，企业再也不必自己使用一系列的开源软件去从头搭建容器服务平台。Rancher提供了在生产环境中使用的管理Docker和Kubernetes的全栈化容器部署与管理平台。 Rancher由以下四个部分组成： 基础设施编排Rancher可以使用任何公有云或者私有云的Linux主机资源。Linux主机可以是虚拟机，也可以是物理机。Rancher仅需要主机有CPU，内存，本地磁盘和网络资源。从Rancher的角度来说，一台云厂商提供的云主机和一台自己的物理机是一样的。 Rancher为运行容器化的应用实现了一层灵活的基础设施服务。Rancher的基础设施服务包括网络， 存储， 负载均衡， DNS和安全模块。Rancher的基础设施服务也是通过容器部署的，所以同样Rancher的基础设施服务可以运行在任何Linux主机上。 容器编排与调度很多用户都会选择使用容器编排调度框架来运行容器化应用。Rancher包含了当前全部主流的编排调度引擎，例如Docker Swarm， Kubernetes， 和Mesos。同一个用户可以创建Swarm或者Kubernetes集群。并且可以使用原生的Swarm或者Kubernetes工具管理应用。 除了Swarm，Kubernetes和Mesos之外，Rancher还支持自己的Cattle容器编排调度引擎。Cattle被广泛用于编排Rancher自己的基础设施服务以及用于Swarm集群，Kubernetes集群和Mesos集群的配置，管理与升级。 应用商店Rancher的用户可以在应用商店里一键部署由多个容器组成的应用。用户可以管理这个部署的应用，并且可以在这个应用有新的可用版本时进行自动化的升级。Rancher提供了一个由Rancher社区维护的应用商店，其中包括了一系列的流行应用。Rancher的用户也可以创建自己的私有应用商店。 企业级权限管理Rancher支持灵活的插件式的用户认证。支持Active Directory，LDAP， Github等 认证方式。 Rancher支持在环境级别的基于角色的访问控制 (RBAC)，可以通过角色来配置某个用户或者用户组对开发环境或者生产环境的访问权限。 rancher本身也是个master/agent的架构模式，在server端安装的是rancher-server，被rancher纳管的机器，安装rancher-agent等其他rancher组件。 单节点安装container-0上通过容器的方式安装1docker run -d --restart=unless-stopped -p 8080:8080 rancher/server HA方式安装 组件分配vip 192.168.2.120192.168.2.110 rancher-server、nginx、mariadb、keepalived 192.168.2.112 rancher-server、nginx、mariadb、keepalived 192.168.2.113 rancher-server、nginx、mariadb、keepalived 建议采用3台host3台host上面都部署keepalive+rancher+nginx+mysqlnginx做反向代理keepalive用于管理vip落到健康的节点mysql做galera集群，保证3个节点数据库一致 配置环境绑定hosts 12scp /etc/hosts 192.168.2.112:/etc/hosts scp /etc/hosts 192.168.2.113:/etc/hosts 在三台服务器上安装nginx和keepalived1yum install nginx keepalived -y 三台服务器启动nginx1systemctl start nginx &amp;&amp; systemctl enable nginx 编写测试网页每个host上都不同，以此来验证。123[root@container-0 ~]# echo &quot;container-0&quot; &gt; /usr/share/nginx/html/index.html[root@container-1 ~]# echo &quot;container-1&quot; &gt; /usr/share/nginx/html/index.html[root@container-2 ~]# echo &quot;container-2&quot; &gt; /usr/share/nginx/html/index.html 重启nginx1systemctl restart nginx 验证 配置keepalived修改配置前先备份配置文件1cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak 编辑keepalived配置文件123456789101112131415161718192021222324! Configuration File for keepalivedvrrp_script check_running &#123; script &quot;/opt/check.sh&quot; interval 10 #执行的间隔时间 weight -20 # script执行失败vrrp_instance的优先级会减少20，比如master为100 slave为90，master故障后优先级掉到80，slave比master高，则vip到slave，若，master恢复优先级又会大于backup节点mater重新接管&#125;vrrp_instance VI_1 &#123; state MASTER #设置为主服务器 interface ens3 #vip放置的网卡 virtual_router_id 51 #主、备必须一样 ,这样加入一个集群 priority 100 #(主、备机取不同的优先级，主机值较大，备份机值较小,值越大优先级越高) advert_int 1 authentication &#123; auth_type PASS #VRRP认证方式，主备必须一致 auth_pass 1111 ##(密码) &#125; virtual_ipaddress &#123; 192.168.2.120 #vip地址 &#125; track_script &#123; #执行服务检查 check_running &#125;&#125; scp一份到另外两个备节点12scp /etc/keepalived/keepalived.conf container-1:/etc/keepalived/keepalived.confscp /etc/keepalived/keepalived.conf container-2:/etc/keepalived/keepalived.conf 两个备节点修改下配置文件123456789101112131415161718192021222324! Configuration File for keepalivedvrrp_script check_running &#123; script &quot;/opt/check.sh&quot; #检查脚本 interval 10 weight -20&#125;vrrp_instance VI_1 &#123; state BACKUP #节点类型为backup interface ens3 virtual_router_id 51 priority 90 #优先级要比master节点低 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.2.120 &#125; track_script &#123; check_running &#125;&#125; 编写进程检测脚本为防止脑裂。(container-1和cotainer-2上的脚本需要修改ping那hostname，别ping自己。)vim /opt/check.sh 12345678910111213141516171819202122232425#!/bin/bashfunction check_process &#123; ps -ef |grep nginx|grep -v grep &amp;&gt;/dev/null value=`echo $?` if [ $value -ne 0 ];then pkill keepalived fi&#125;#如果ping集群另外机器和网关都ping不通，则本身网络出问题，为防止脑裂，自杀。function check_network &#123; ping container-1 -c 5 &amp;&gt;/dev/null value1=`echo $?` ping container-2 -c 5 &amp;&gt;/dev/null value2=`echo $?` ping 192.168.2.1 -c 5 &amp;&gt;/dev/null value3=`echo $?` if [ $value1 -ne 0 -a $value2 -ne 0 -a $value3 -ne 0 ];then pkill keepalived fi&#125;main() &#123; check_process check_network&#125;main 添加执行权限1chmod a+x /opt/check.sh 脚本scp到container-1、container-2的/opt/目录，添加执行权限1234scp /opt/check.sh container-1:/opt/check.shscp /opt/check.sh container-2:/opt/check.shssh container-1 &quot;chmod a+x /opt/check.sh&quot;ssh container-2 &quot;chmod a+x /opt/check.sh&quot; container-1和container-2启动keepalived1systemctl start keepalived &amp;&amp; systemctl enable keepalived 验证keepalived验证nginx进程被kill掉时 vip迁移目前vip在master节点ens3上 将master节点的nginx kill掉去把container-2的nginx kill掉验证网络出问题时vip的迁移动将container-0的 ens3网卡关闭1ifdown ens3 配置mariadb galera集群三个节点操作配置repo文件vim /etc/yum.repo/mariadb.repo 12345[mariadb]name = MariaDBbaseurl = https://mirrors.ustc.edu.cn/mariadb/yum/10.2/centos7-amd64gpgkey=https://mirrors.ustc.edu.cn/mariadb/yum/RPM-GPG-KEY-MariaDBgpgcheck=1 1yum install MariaDB-server MariaDB-client galera 1systemctl start mariadb 一些初始化安全配置1/usr/bin/mysql_secure_installation 关闭数据库1systemctl stop mariadb 修改container-0上的/etc/my.cnf.d/server.cnf文件如下123456789[galera]wsrep_provider = /usr/lib64/galera/libgalera_smm.sowsrep_cluster_address = &quot;gcomm://192.168.2.110,192.168.2.112,192.168.2.113&quot;wsrep_node_name = container-0wsrep_node_address=192.168.2.110wsrep_on=ONbinlog_format=ROWdefault_storage_engine=InnoDBinnodb_autoinc_lock_mode=2 将此文件复制到container-1、container-2，注意要把 wsrep_node_name 和 wsrep_node_address 改成相应节点的 hostname 和 ip。 查看是否启用galera插件 连接mariadb,查看是否启用galera插件 配置nginx反向代理三个节点备份配置文件1cp /etc/nginx/nginx.conf /etc/nginx/nginx.conf.bak 修改配置文件123456789101112131415161718192021222324252627282930313233343536373839vim /etc/nginx/nginx.confuser nginx;worker_processes auto;error_log /var/log/nginx/error.log;pid /run/nginx.pid;# Load dynamic modules. See /usr/share/nginx/README.dynamic.include /usr/share/nginx/modules/*.conf;include /etc/nginx/conf.d/*.conf;events &#123; worker_connections 1024;&#125;创建rancher配置文件vim /etc/nginx/conf.d/rancher.confhttp &#123;upstream rancher &#123; server 192.168.2.110:8080; server 192.168.2.112:8080; server 192.168.2.113:8080;&#125;server &#123; listen 80; server_name 192.168.2.120; location / &#123; proxy_set_header Host $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://rancher; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection &quot;Upgrade&quot;; proxy_read_timeout 900s; &#125; &#125;&#125; 同步这两个配置文件到另外两个主机重启nginx1systemctl restart nginx 三个节点安装rancher-server在mariadb上创建库并授权用户123CREATE DATABASE IF NOT EXISTS cattle COLLATE = &apos;utf8_general_ci&apos; CHARACTER SET = &apos;utf8&apos;;GRANT ALL ON cattle.* TO &apos;cattle&apos;@&apos;%&apos; IDENTIFIED BY &apos;cattle&apos;;GRANT ALL ON cattle.* TO &apos;cattle&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;cattle&apos;; 使用外部数据库在三个host上执行1docker run -d --restart=unless-stopped -p 8080:8080 -p 9345:9345 rancher/server --db-host xxxx --db-user cattle --db-pass cattle --db-name cattle --advertise-address xxxx –advertise-address 为当前主机ip–db-host 指定MySQL服务器的连接地址(写本机ip)–db-port 连接端口–db-user 连接用户–db-pass 连接密码–db-name 连接库名 非HA打开浏览器访问http://localhost:8080 HA情况打开浏览器访问http://192.168.2.120在系统管理–&gt;高可用里面可以看见集群 测试高可用将container-0关机访问192.168.2.120 正常显示创建帐号系统配置—-&gt;访问控制—-&gt;本地帐号验证 以新帐号登录，并选择中文 添加主机进rancher管理将另外两台host给rancher管理，需要安装rancher-agent 复制图上第5步的命令在192.168.2.112上执行1sudo docker run -e CATTLE_AGENT_IP=&quot;192.168.2.112&quot; --rm --privileged -v /var/run/docker.sock:/var/run/docker.sock -v /var/lib/rancher:/var/lib/rancher rancher/agent:v1.2.7 http://192.168.2.110:8080/v1/scripts/282E41C46DCA4A2041AA:1483142400000:CPSeNWQ19B9bCfFAULmsX4TKY 在rancher上可以看见刚刚添加的主机已经进来了 在主机上查看发现rancher会在host上起这些容器 123456789r-network-services-metadata-dns-1-ffdc1af6r-ipsec-ipsec-router-1-eed9ad34r-scheduler-scheduler-1-9a4c9847r-ipsec-cni-driver-1-e5fd917dr-healthcheck-healthcheck-1-0363e64br-ipsec-ipsec-1-f3612834r-network-services-network-manager-1-9d604f7er-network-services-metadata-1-198285aerancher-agent 其中rancher-net：rancher网络的核心，它的作用是用strongSwan 和 charon创建IPSec网络;rancher-scheduler：负责容器的调度，选择合适的宿主机;rancer-dns：负责容器的主机名和ip的映射;rancher-metadata：为容器提供元数据的管理;rancher-healthcheck: 为容器提供健康状态检查，原理是通过haproxy进行检查，只有使用了rancher的托管网络才的容器才能被检查，其他网络检测不到; 基本使用点进host名字可以查看这个host的硬件信息和资源负载情况 按上面方法添加第二个host对容器的基本操作 启动、停止、查看日志、执行命令 构建单主机容器应用玩个简单的添加个http的container在对应的host下点击添加容器 输入应用的基本信息 可以看见httpd容器已经起来了 访问ip为宿主机的ip地址 构建多主机的容器集群应用两种方式一种是通过rancher本身自带的应用商店另外一种通过自己编写的docker-compose或rancher-compose，来进行应用编排 方式一通过应用商店构建grafana在应用商店搜索grafana 查看详情里面选择版本和设置密码 预览这可以看见对应的docker-compose和rancher-compose 点击部署后默认会生成1个容器访问进行容器伸缩添加负载均衡器配置负载均衡器 访问负载均衡器的地址 容器的伸缩点击容器名 进容器的管理页，通过修改容器数量进行增减 这样就做成了一个简单的高可用 方式二通过docker-compose进行构建应用用以下docker-compose构建个WordPressWordPress主要由三部分构建web Server+php+database这里是WordPress镜像里面本身包含了php和apache，只要安装WordPress和mysql就可以了 1234567891011121314151617181920212223version: &apos;2&apos;services: db1: image: mysql:5.7 volumes: - /var/lib/mysql:/var/lib/mysql environment: - MYSQL_ROOT_PASSWORD=password - MYSQL_DATABASE=wordpress - MYSQL_USER=wordpress - MYSQL_PASSWORD=wordpress wordpress: image: wordpress:latest depends_on: - db1 ports: - 8000:80 environment: - WORDPRESS_DB_HOST=db1 - WORDPRESS_DB_USER=wordpress - WORDPRESS_DB_PASSWORD=wordpress - WORDPRESS_DB_NAME=wordpress 构建完成 WordPress的数据主要存储在数据库中，如果没做data volume数据就存放在容器中，一旦这个容器remove了则数据就丢失了，做了data volume数据存放在host上，mysql的话可以通过主从和galera集群来使得多个host上的mysql容器数据保持一致，为了数据存放更加可靠，我们通常将数据存放在高可靠性的分布式存储和nfs上，如glusfs、ceph、nfs等上面。 rancher可以使用的持久化存储rancher-nfs http://blog.csdn.net/rancherlabs/article/details/52774702rancher ceph http://geek.csdn.net/news/detail/229747rancher本身的分布式块存储longhorn http://blog.csdn.net/rancherlabs/article/details/71080450 rancher-ebs http://rancher.com/docs/rancher/v1.6/zh/rancher-services/storage-service/rancher-ebs/ rancher网络rancher默认创建容器使用的是rancher本身的rancher-network，与其他基于vxlan和基于gre的overalay网络不一样，rancher-network是一种基于ipsec的隧道网络，使用ipsec数据报文都是经过加密算法进行加密了的，相比于其他overlay网络更加安全，但同时加密、解密对cpu资源消耗也比较多。 这就是创建容器时的托管网络 分别介绍rancher 5种网络类型和rancher-networkbridge：将容器的网卡桥接到docker0，根docker的桥接模式一样;container：两个容器共享一个网络栈他，共享网卡核配置信息。包括ip;host: 连接到host网络的容器，共享docker 宿主机的网络，并且连hostname也是宿主机的;manager:使用的就是rancher-network;none：none网络就是什么都没有的网络; rancher-network10.42.0.0/16IPAM相比V1.2版本，使用manger网络的容器都拥有两个ip地址，一个是docker0段的一个是manger段的，V1.6采用CNI接口，这样manger网络的容器只有一个manger段的ip了。 网络隔离性：目前使用manger网络的容器都在一个大二层，也就是没有网络的隔离。 rancher-agent将host的docker0也由之间的172.17.0.1改成了10.2.0.0/16段的地址，并且每个host的docker0的ip都不一样了 实现原理container-0 上的test-2 ping container-1上的test 通过路由追踪发现包先给了 host上的ipsec容器，10.42.190.140为container-0上的ipsec容器，通过他将包封装，加密，通过ipsec遂道传到contain-1上的ipsec容器 10.42.208.216，然后ipsec容器在转发到目标容器。 源节点包从ipsec容器到宿主机网卡，目标节点包从宿主机网卡到ipsec容器，都是Iptables 规则管理则是由 容器network-manager 组件来完成的。 查看ipsec 服务的 rancher-compose.yml 可以看到，type 使用 rancher-bridge，ipam 使用 rancher-cni-ipam，bridge 网桥则复用了 docker0，有了这个配置我们甚至可以随意定义 ipsec 网络的 CIDR，如下图所示： 12http://blog.csdn.net/rancherlabs/article/details/53835564 http://baijiahao.baidu.com/s?id=1561101273463943&amp;wfr=spider&amp;for=pc]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack使用ISO镜像]]></title>
      <url>%2F2017%2F11%2F28%2Fopenstack_iso%2F</url>
      <content type="text"><![CDATA[openstack默认使用ISO镜像创建虚机时，nova从glance那获取镜像，并将根磁盘设置的磁盘类型设置为cdrom类型，ide总线类型，导致到安装系统界面时，会发现找不到磁盘。 这种情况是正常的，因为你的根分区mout了这个系统的cdrom。当我们在直接使用kvm或vmware时，使用cdrom做为安装介质时，我们也是在选完安装介质后，还需要在创建硬盘进行安装，总好像pc的cdrom是 cd或dvd设备，创建的磁盘是pc的硬盘一样。 解决方法1、修改flavor，增加临时磁盘空间，这样安装时就可以看见硬盘了 弊端是openstack的快照功能只能针对根磁盘，所以想通过此方法创建主机，在做快照，通过快照做成镜像是不行的。可以通过修改libvirt代码将cdrom mount到临时磁盘，根分区用来安装系统就解决了这个问题。 2、给云主机挂载块云硬盘，将系统安装在云硬盘内 将此云硬盘做为镜像方法通过glance create创建一个空镜像，记录uuid占个坑glance image-create 在ceph底层将这个volume卷cp到glance读取image的pool rbd cp volume/volume-xxxxx images/image_uuid 创建snap打上protectrbd snap create images/image_uuid@snaprbd snap protet image/image_uuid@snap 修改刚刚创建的空的glance镜像1ceph_id=`ceph -s | grep cluster | awk &apos;&#123;print $2&#125;&apos;` 设置后端存储URL1glance location-add --url rbd://$&#123;ceph_id&#125;/images/$&#123;image_uuid&#125;/snap $image_uuid images为ceph中存放镜像的pool名，根据实际环境修改。更新镜像元数据1glance image-update --name=&quot;test_image&quot; --disk-format=raw --container-format=bare image_uuid 然后就可以基于新的镜像创建云主机了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Docker多主机网络(overlay)]]></title>
      <url>%2F2017%2F11%2F17%2Fdocker_network_overlay%2F</url>
      <content type="text"><![CDATA[此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记docker 版本：Docker version 17.10.0-ce, build操作系统版本：centos7.3 Docker Libnetwork Container Network Model（CNM）阵营 Docker Swarm overlayMacvlan &amp; IP network driversCalicoContiv（from Cisco） Docker Libnetwork的优势就是原生，而且和Docker容器生命周期结合紧密；缺点也可以理解为是原生，被Docker“绑架”。Container Network Interface（CNI）阵营 KubernetesWeaveMacvlanFlannelCalicoContivMesos CNI 其中overlay和macvlan是docker原生就支持的。weave、flannel、calico是需要安装额外的组件才可以的。 overlay 结构图 docker官方文档中，overlay网络是在swarm集群中配置的，但实际上，overlay网络可以独立于swarm集群实现，只需要满足以下前提条件即可。1、有consul或者etcd，zookeeper的集群key-value存储服务；2、组成集群的所有主机的主机名不允许重复，因为docker守护进程与consul通信时，以主机名相互区分；3、所有主机都可以访问集群key-value的服务端口，按具体类型需要打开进行配置。例如docker daemon启动时增加参数–cluster-store=consul://:8500 – -cluster-advertise=eth0:2376overlay网络依赖宿主机三层网络的组播实现，需要在所有宿主机的防火墙上打开下列端口; 协议 端口 说明启动后去访问8500端口 udp 4789 容器之间流量的vxlan端口 tcp/udp 7946 docker守护进程的控制端口 安装consul安装consul，这里通过容器的方式安装1docker run -d -p 8500:8500 -h consul --name consul progrium/consul -server -bootstrap 启动后去访问8500端口 修改container-1和container-2的docker-daemon的配置文件 vim /etc/systemd/system/docker.service.d/10-machine.conf 添加–cluster-store=consul://192.168.111.159:8500–cluster-advertise=eth0:2376其中–cluster-store：指定consul的地址–cluster-advertise：告知consul自己连接的地址打开consule可以看见已经连接进来了 创建overlay网络在container-1中创建ov_net1 docker network ls查看当前网络 默认是一个10.0.0.0/24的网段 在container-2中发现这个网络同步过来了 这是因为创建ovnet1时，container-1将overlay网络信息存在consul，container-2从consul读取了新网络的数据，后面ov_net的任何变化都会同步到container-1和container-2 在overlay中运行容器container-11docker run -itd --name overlay_1 --network ov_net1 busybox container-21docker run -itd --name overlay_2 --network ov_net1 busybox container-1上地址为 container-2上地址为 contaner-1 ping container-2 container-2 ping container-1 发现container-1和container-2都有两块网卡 eth0连接的是overlay ,另外docker会创建个docker_gwbridge为使用overlay网络的容器提供上外网的能力 inspect这个网络 1docker network inspect 07a9d7 发现地址分配记录overlay隔离不同overlay网络是相互隔离的在创建个overlay网络1docker network create -d overlay ov_net2 新创建的网络地址是10.0.1.0/24 创建主机contain-1上1docker run -itd --name over_lay3 --network ov_net2 busybox contain-2上1docker run -itd --name over_lay4 --network ov_net2 busybox ping ov_net1的机器，不通 因为他们vni不同，vlxan就是通过vni起到网络隔离效果。 通过docker_gwbridge也是一样的 overlay原理 1、docker会为每个overlay网络创建个单独的命名空间，在这个命名空间里创建了个br0的bridge。2、在这个命名空间内创建两张网卡并挂载到br0上，创建一对veth pair端口 和vxlan设备。3、veth pair一端接在namespace的br0上一端接在container上。4、vxlan设备用于建立vxlan tunnel，vxlan端口的vni由docker-daemon在创建时分配，具有相同vni的设备才能通信。5、docker主机集群通过key/value存储（我们这里用的是consul)共享数据，在7946端口上，相互之间通过gossip协议学习各个宿主机上运行了哪些容器。守护进程根据这些数据来在vxlan设备上生成静态MAC转发表。6、vxlan设备根据静态mac转发表，通过host上的4789端口将数据发到目标节点。7、根据流量包中的vxlan隧道ID，将流量转发到对端宿主机的overlay网络的网络命名空间中。8、对端宿主机的overlay网络的网络命名空间中br0网桥，起到虚拟交换机的作用，将流量根据MAC地址转发到对应容器内部。 查看namespace 由于容器和overlay的网络的网络命名空间文件不再操作系统默认的/var/run/netns下，只能手动通过软连接的方式查看。 ln -s/var/run/docker/netns /var/run/netns可以看见两个host上都有这个1-xx的namespace 查看vni 查看vxlan设备上生成的静态mac地址转发表 可以看见又192.168.111.162和192.168.111.163的mac地址 缺点1、由于vxlan网络与宿主机网络默认不再同一网络环境下，为了解决宿主机与容器的通信问题，docker为overlay网络中的容器额外增加了网卡eth1作为宿主机与容器通信的通道。这样在使用容器服务时，就必须根据访问性质的不同，选择不同的网卡地址，造成使用上的不便。2、容器对外暴露服务仍然只能使用端口绑定的方式，外界无法简单地直接使用容器IP访问容器服务。3、从上面的通信过程中来看，原生的overlay网络通信必须依赖docker守护进程及key/value存储来实现网络通信，约束较多，容器在启动后的一段时间内可能无法跨主机通信，这对一些比较敏感的应用来说是不可靠的。 container的ip都是自动分配的，如果需要静态的固定ip，怎么办？在创建网络的过程中有区别 12345docker network create -d overlay --subnet=192.168.2.0/24 multihostdocker run -d --name host1 --net=multihost --ip=192.168.2.2 centos7docker run -d --name host2 --net=multihost --ip=192.168.2.3 centos7 IPAMoverlay网络中所有主机共享一个大的subnet，容器启动时会顺序分配ip，可以通过–subnet指定ip地址。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Docker单主机网络]]></title>
      <url>%2F2017%2F11%2F03%2Fdocker_network%2F</url>
      <content type="text"><![CDATA[此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。docker网络（单host上容器网络)显示docker网络docker network list docker run时通过–network指定网络 none网络1docker run -it --network=none busybox none网络就是什么都没有的网络，挂载在这个网络的容器除了loopback没有其他任何网卡。 使用场景：对于一些安全性要求高且不需要联网的应用可以使用。 host网络连接到host网络的容器，共享docker 宿主机的网络，并且连hostname也是宿主机的。1docker run -itd --network=hhost busybox 使用场景：用host网络性能比较高，容器可以直接操作宿主机网络，缺点是牺牲了灵活性，需要考虑端口冲突问题。 bridge网络docker使用最广泛的方式，docker安装时会创建一个docker0的linux bridge，如果不指定容器网络默认都会挂到docker0上 启动个容器，查看docker0docker run -itd centos_10_23 docker 内的网卡eth0@if28和linux bridge上的vethfe788c9是一堆veth-pair 默认bridge配置的subnet就是172.17.0.3/16网关为172.17.0.1配在docker0上，容器创建时会自动从172.17.0.3/16里面抽出一个地址。 user-define(用户自定义网络)user-define网络支持：bridge，overlay和macvlan网络驱动，macvlan和overlay属于跨宿主机的容器的通信，这里先不讨论，这里讨论使用bridge驱动创建user-define网络。 创建一个bridge为driver的user-define网络 docker network create –driver bridge my_net 查看网络状态 inspect一下，发现新创建的网络默认subnet地址自动会从上一个开始， 创建好后，多了个网桥 指定网段,只需要加–subnet 和–gateway参数1docker network create --driver bridge --subnet 192.168.1.0/24 --gateway 192.168.1.1 my_net 容器指定ip，run时加上–ip，只能是使用的–subnet的网络的容器才能使用指定ip功能 1docker run -itd --network=my_net --ip 192.168.1.100 centos_10_23 定义另外一个network网络1docker network create --subnet 192.168.2.0/24 --gateway=192.168.2.1 my_net2 在mynet_2上创建容器1docker run -itd --network=my_net2 centos_10_23 宿主机上查看路由 开启路由转发sysctl net.ipv4.ip_forward 在容器内测试192.168.2.2 ping 192.168.1.100，发现网关都能通就是ping不通192.168.1.100 默认宿主机上的iptables DROP掉了这两个网桥的连接 给my_net2的容器加入my_net网络 docker network connect my_net 60cae2aee142 在去ping 192.168.1.100 就ok了 容器和容器间的通信3种方式：IP、Docker DNS server 、joined IP方式使用docker network connect将现有容器加入到指定网络实现容器间的通信。 Docker DNS serverDocker daemon内嵌了一个DNS Server通过Docker run –name定义的容器名进行通信，只能是user-define网络可以使用，默认的bridge不行 。 12docker run -itd --name=wan_1 --network=mynet centos_10_23 docker run -itd --name=wan_2 --network=my_net centos_10_23 1docker exec -it 8549430966f7 bash ping wan_1 joined方式两个容器共享一个网络栈他，共享网卡核配置信息。joined容器之间通过127.0.0.1直接通信 12docker run -itd --name=wan1 centos_10_23docker run -itd --name=wan2 --network=container:wan1 centos_10_23 joined合适以下场景1，不同容器中的程序希望通过loopback高效快速通信，比如web-Server和app-serve。2，独立的网络监控容器用来监控网络流量。 容器访问外网容器访问外网(SNAT)通过宿主机的iptables的snat，在宿主机查看 外界访问容器通过Docker-proxy每一个映射端口，host都会启动一个Docker-proxy进程来处理访问容器的流量]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Docker基础]]></title>
      <url>%2F2017%2F10%2F30%2Fdocker_base%2F</url>
      <content type="text"><![CDATA[此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。基础容器不关有docker还有core os的rkt、 容器runtime是真正运行容器的地方，runtime需要跟操作系统kernel紧密协作，为容器提供运行环境，docker曾经的runtime是linux上的lxc，后面docker自己开发了runc，目前runc是docker默认的runtime rkt是coreos开发的容器runtime. 容器管理工具lxd是lxc对应的管理工具runc的管理工具是docker engine，docker engine包含daemon和cli两个部分rkt的管理工具rkt cli 容器定义工具docker image是docker容器的模板,runtime依据docker image创建容器docker file是包含若干命令的文本文件，可以通过这些命令创建出docker image aci与docker image类似，它是coreos开发的rkt容器image格式。 容器仓库私有仓库：registry 公共仓库：docker hub，Quay.io 容器oscoreos 、atomic、ubuntu core 容器平台技术 容器编排引擎:docker swarm、kubernetes、mesos+mariathon 容器管理平台：rancher、containership、 基于容器的pass：deis、flynn、dokku 容器支持技术容器网络:docker network、flannel、weave、calico服务发现：etcd、consul、zookeeper数据管理：rex-ray日志管理：docker logs、logspout安全性：openscap docker镜像linux发行版都是由内核空间和用户空间组成，内核空间是kernel，用户空间的文件系统是rootfs，可以简单的说不同linux发行版的主要区别都是rootfs base镜像：1，不依赖其他镜像，其他应用镜像可以在base镜像的基础上进行扩展，base镜像通常是各linux发行版的docker镜像，如centos、ubuntu、Debian，大部分应用镜像都是在base镜像的基础上进行扩展的。 下载镜像docker pull centos 查看镜像docker images centos 正如上面所说base镜像这么小，因为base镜像只有一个rootfs，kernel它是和宿主机共享的，base镜像只是在用户空间和发行版是一致的，内核版本是共享宿主机的kernel的，ubuntu14.04 kernel版本是4.x，但centos7.3kernel版本是3.10，当在ubuntu14.04上跑centos7.3的docker镜像时，centos7.3的内核版本也会变成4.x因为它是共享宿主机内核。 容器只能使用宿主机kernel，不能更改，不能升级，如果应用要求高内核，建议使用虚拟机。 docker镜像采用分层的结构。基于docker file文件，你每做的操作其实都是在base镜像上加一层，每安装一个软件就是在现有镜像基础上增加一层，这样做的好处时，一个共享资源 比如：有多个镜像都从相同的base镜像构建出来，那宿主机只需要一个base_image,多个镜像共享一个base镜像当某个容器镜像修改基础镜像内容时，会将基础镜像的内容像copy一份到应用镜像内，然后在修改应用镜像内的数据，这根前面说的ceph的快照和clone是一样的，都是使用的cow(copy on write)技术 当容器启动时，会有一层可写层在容器顶部叫容器层，所有对容器的改动，无论添加、删除，修改文件都会发生的容器层中，容器层下面是镜像层，镜像层是只读的。 构建镜像docker commit 在对原镜像做修改后使用docker commit将生成一个新镜像。docker commit 原镜像名 新镜像名 Dockerfile Dockerfile是一个文本文件记录镜像构件的步骤 如Dockerfile为FROM centosRUN yum install vim -y docker build-t 为新镜像名字.为指定当前目录为build context，docker 会从build context中寻找Dockerfile-f可以手动指定Dockerfile的文件 如果Dockerfile不在本地 可以看见启动一个3d799f4的临时容器，然后在临时容器中安装vim，安装成功后将临时容器保存为镜像，最后删除临时容器。 查看容器分层 docker history 镜像名 镜像的缓存特性 docker会缓存已有的镜像的镜像层，如果镜像存在就直接使用，不用创建，如果不希望使用缓存镜像则在docker build时加上–no-cache参数 docker镜像的上层是依赖下层的，只要下层发生变化，上层也会根着变。 如果在build某一步中出错了,可以通过docker run -it 先进上一步生成临时镜像进行调试。 搭建docker私有镜像仓库 从docker hub上下载registryv2.3.1的私有仓库镜像docker pull registry:2.3.1 使用htpasswd配置docker私有仓库验证[root@wan_test /]# mkdir /root/auth/ 生成验证admin：admin为用户名和密码1docker run --entrypoint htpasswd registry:2.3.1 -Bbn admin admin &gt; /root/auth/htpasswd 启动私有仓库容器1docker run -d -p 5000:5000 -v /myregistry/:/var/lib/registry -v /root/auth/:/auth -e &quot;REGISTRY_AUTH=htpasswd&quot; -e &quot;REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm&quot; -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd registry:2.3.1 测试登录先用个错误的帐号密码测试 在用对的 认证以后无法直接在服务器查看curl 127.0.0.1:5000/v2/_catalog 仓库的镜像，会出现报错，但是可以用浏览器输入帐号密码访问。 测试上传镜像命名要规范格式为123[registry_host]:[port]/[username]/[封装的应用]:[版本]docker tag hello-world 10.211.55.5:5000/wan/hello-wolrd:v1 docker push上传镜像 默认client是打开ssl的，但Server端没有配置证书，所以先关闭/usr/lib/systemd/system/docker.service 关闭ssl安全认证1ExecStart=/usr/bin/dockerd --insecure-registry 10.211.55.5:5000 systemctl restart docker 测试下载将本地10.211.55.5:5000/wan/hello-wolrd:v1删除 重新从服务器上pull下来 查看镜像1curl http://10.211.55.5:5000/v2/_catalog web页面版的私有镜像管理软件Portusharborhttps://www.linuxea.com/1557.html 两种进入容器的方法docker attach image_iddocker exec -it image_id区别是，attach进去的还是之前的终端，不会启用新的终端，exec进去是启动一个新的终端。 运行容器的最佳实践服务类型的容器通过-d以后台的方式启动这类容器 工具类型的直接docker run -it的方式，用完就exit掉 容器的操作 容器在docker host中实际就是一个进程，docker stop命令实际就是向中国进程发送一个SIGKILL信号 docker run –restart=always 表示无论容器因何种原因退出（含正常退出)都立刻重启，–restart=on-failure:3意思是如果启动进程退出代码非0，则重启容器，最多重启3次。 pause/unpause 挂起容器/解除挂起 查看所有exited状态的容器docker ps -aq -f status=exited 容器资源的限制docker run时指定-m或–memory 设置内存的使用限额–memory-swap设置内存+swap的使用限额。1docker run -m 200M --memory-swap=300M centos 其含义是该容器最多使用200M内存和100M swap，因为–memory-swap是memory+swap所以前面指定的-m后面swap等于300-200， 默认情况下这两组参数都是-1，不是不限制。只指定定了-m没指定–memory-swap时默认–memory-swap是-m的两倍。 使用progrium/stress镜像测试，只要线分配给线程的内存大小在可分配范围(300M）内，镜像就会一直工作1docker run -it -m 200M --memory-swap=300M progrium/stress --vm 1 --vm-bytes 180M 如果超过了，容器马上回报错退出 cpu限额cpu限额限制的不是实际cpu的使用量，资源的优先级，也就是，在cpu资源紧张时，优先级高的获的的cpu时间片要长一些 docker cpu限制参数–cpuset-cpus=”” 允许使用的 CPU 集，值可以为 0-3,0,1-c,–cpu-shares=0 CPU 共享权值（相对权重）cpu-period=0 限制 CPU CFS 的周期，范围从 100ms~1s，即[1000, 1000000]–cpu-quota=0 限制 CPU CFS 配额，必须不小于1ms，即 &gt;= 1000–cpuset-mems=”” 允许在上执行的内存节点（MEMs），只对 NUMA 系统有效 cpu-shares通过–cpu-shares设置容器cpu配额，默认为一个core为1024，二个为2048，依次类推，设置为0，系统将会按默认值配置，假定一个1core的运行3个container，其中一个cpu-shares为1024，另外两个为512则，当cpu忙碌时，1024的占用50%cpu时间，512的分别占用25%，当增加一个1024的contain时，占用比为1024+1024+512+512 =3072 每个container占用cpu时间比为 1024/3072=33% 1024/3072=33% 512/3027=17% 512/3072=17% 比如在host上启用两个容器1docker run --name container_a -it --cpu-shares 2048 progrium/stress --cpu 2 1docker run --name container_b -it --cpu-shares 1024 progrium/stress --cpu 2 –cpu 有多少核，就写多少 ，这里设置为2可以看见container_a的cpu资源消耗是container_b的两倍 paush container_a,此时container_b又会将cpu整格cpu占满 block io限制block io是用来限制容器的磁盘的读写，针对bps（每秒读写的数据量)和iops(每秒IO的次数),控制容器的读写磁盘的带宽。 限制block io的参数 目前block io限额只对dicectIO（不使用文件缓存)有效。 测试先在容器内使用dd测试bps run一个centos的镜像 docker run -itd centos 限制前进入centos镜像docker exec -it 15643e5ec7d9 /bin/bash 限制io为30M后1docker run -itd --device-write-bps /dev/sda:300MB centos 实现容器的底层技术曾经docker的runtime是linux内核自带的lxc，后面docker重新开发了自己的runtime，runc，目前docker通过cgroup实现资源限制，如上面的限制cpu资源、限制memory资源、限制blockIO资源大都是通过linux底层的cgroup实现，namespace实现资源隔离，每个容器独立的文件系统，网卡资源等是通过linux底层的namespace，openstack neutron的 多租户隔离qroute和qdhcp也是通过namespace去实现的。 linux下6种namespace资源 docker存储容器镜像的分层结构容器镜像 1、镜像由一个base镜像+若干个只读镜像+一个可写的容器层，这种分层架构最大的特性建设copy-on-write。2、对数据的修改都是在容器层完成的，新修改的数据都会放在最上层容器层。3、修改数据时，如果容器层没有，会先从其他层将数据copy到容器层，然后在修改。此时镜像层数据保持不变。4、如果有多个层有命名相同的文件，用户只能看见最上层中的文件。 分层结构是由linux store driver提供，docker支持多种storage driver 如 AUFS、Device Mapper、Btrfs、OverlayFs、VFS、ZFS，ubuntu默认是AUFS、centos7默认是OverlayFs ， docker默认使用linux 发行版的storage driver docker info可以看见 有些容器创建和销毁，不依赖原有数据，有些容器创建时需要加载已有数据，销毁时希望保留之前产生的数据，这类容器就需要用到容器持久化存储 Data volumeData volume 本质是 Docker Host 文件系统中的目录或文件，能够直接被mount到容器的文件系统中 特点：1、Data volume是目录或文件，不是块设备2、容器可以读写volume中的数据3、volume的数据因为在host上，但容器被销毁时，不会影响数据，所以它是可以持久保存的。 在使用上，软件放在镜像层内，软件的数据如web Server的网页，数据库的数据这些放在Data volume上 因为data volume是宿主机的一部分，所以目前volume无法设置容量。 docker提供两种类型volumebind mount：宿主机上已经存在的目录或文件mount到容器。优点：使用起来直观、高效、可以控制目录的读写权限，支持单个文件的mount缺点：需要指定host 文件系统路径，限制了容器的可移值性，当需要将容器迁到其他host上，如果其他host上没有要mount的数据或数据不在相同目录时，操作会失败。 docker managed volume：与bind mount最大区别是不需要指定mount 源，只需要指定容器内mount的目录。优点：容器申请 mount docker manager volume时会自动在/var/lib/docker/volumes下生成一个目录，这样的话，容器移值性更好。缺点：不支持控制读写权限，不支持单个文件mount。 bind mount在宿主机上创建http目录 mkdir /root/httpecho “My test bind mount” &gt; /root/http/index.html1docker run -d -p 80:80 -v /root/http/:/usr/local/apache2/htdocs httpd 1-v &lt;host path&gt;:&lt;container path&gt; 1curl http://127.0.0.1 My test bind mount 将host的index.html更新看看，container内的是否也会根着改变1echo &quot;My test bind mount test_2 &quot; &gt; /root/http/index.html 1curl http://127.0.0.1 My test bind mount test_2 设置只读权限，这样就不能在container内对该目录进行修改了。 1docker run -d -p 80:80 -v /root/http/:/usr/local/apache2/htdocs:ro httpd docker manged volumemanged volume最大特点是不需要指定源，只需要指定mount point就可以了查看挂载的源1docker run -d -p 80:80 -v /usr/local/apache2/htdocs httpd 告诉docker需要一个volume并将其挂载到/usr/local/apache2/htdocs，这个但这个volume会从哪里来，inspect一下就知道了manged volume最大特点是不需要指定源，只需要指定mount point就可以了。1docker inspect 79af2997ca68 关注mounts信息 1当容器申请 manged volume时docker会在/var/lib/docker/volumes/下生成一个目录xxx/_data这个就是mount源。mount point指向以有的目录，容器内原有的数据会被复制到这个目录中 我们可以直接更新宿主机上的数据。 docker managed volume创建过程 1、容器启动时，简单告诉docker 我需要一个volume存放数据，帮我mount到容器xxx目录。2、docker在/var/lib/docker/volumes/中生成一个随机目录作为mount源。3、如果内mount的目录已经存在，则将数据复制到mount源。4、将host上生成的目录挂载的container中。 通过docker volume ls 查看 docker manged volume的容器 docker inspect 查看volume bind mount和docker managed volume比较 数据共享容器和宿主机共享数据1、两种类型的data volume它们均可以实现容器与host之间数据共享。2、docker cp 或linux的cp命令将host数据cp到容器中。 容器和容器间的数据共享方法一：将共享数据放到bind mount的源中然后mount到多个容器中。 如搭建web集群将宿主机目录共享到3个容器上，curl访问数据都一致 . 修改宿主机文件，3个容器会同时更新。方法二：volume container专门为其他容器提供volume的容器。专门create出一个容器用来提供volume，volume container的卷可以是bind mount 也可以是docker manged volume。如创建个容器 将/root/http通过bind mount或docker manged volume的方式挂上去，然后其他容器只需要挂这个容器即可。 创建 volume container1docker create --name vc_data -v /root/http/:/usr/lolca/apache2/htdocs httpd 因为volume container的作用只是提供数据，它本身不需要处于运行状态 其他容器 volume 使用volume container 查看 [root@wan_test http]# docker inspect web1 12345678910&quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;/root/http&quot;, &quot;estination&quot;: &quot;/usr/local/apache2/htdocs&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125; ], volume container的特点：(1)与bind mount相比，不必为每一个容器指定host path，所有path都在volume contain 中定义好了，容器只需要与volume container关联，实现了容器与host解偶。(2)使用 volume container的容器，其mount point是一致的，有利于配置规范的和标准，但也带来一定局限行，使用使要综合考虑。 data-packed volume container volume container，数据还是在host上 data-packed volume container数据，将数据完全放到volume container中的称为data-packed volume container 。 原理先将文件ADD 或cp到镜像中，然后在创建docker manged volume 使用Dockerfile build镜像vim Dokerfile123FROM centosADD http /usr/local/apache2/htdocsVOLUME /usr/local/apache2/htdocs 查看volume 在dockerfile中指定了读取volume的数据，所以这里就不需要指定volume的mount point了 启动http容器使用 data-packed volume container docker run -d -p 80:80 –volumes-from vc_data httpd 容器能正确读取volume中的数据，data-packed volume container是自包含的不依赖host提供数据，具有很强移值性，非常合适静态数据的场景，比如应用的配置信息，web Server的静态文件等. data volume 生命周期 备份1、备份host上对应的目录。2、使用一个临时容器挂载目标容器备份。如创建一个managerd-volume容器1docker run -itd --name App-container1 -v /var/volume1 -v /var/volume2 ubuntu 编辑些数据1docker exec -it data_volume bash 12cd /var/volume1/ &amp;&amp; touch &#123;1..100&#125;cd /var/volume2/ &amp;&amp; touch &#123;200..300&#125; 在宿主机上创建个目录用于存储备份的数据123mkdir /backupdocker run -it --rm --volumes-from App-container1 -v /backup:/backup ubuntu tar -cvf /backup/data_volume_2017_10_01.tar /var/volume1 /var/volume2 这条命令启动一个临时容器，这个容器有挂载两个volume一个的宿主机的/backup一个是映射App-container1的目录，这样这个临时容器就有了App-container1和宿主机backup目录，然后将App-container1目录的/var/volume1和/var/volume2目录备份到backup目录。因为backup目录和宿主机是映射的，这样宿主机就有了备份文件。 将本地backup目录和容器内backup目录进行映射然后将/var/volume1和/var/volume2打包备份到backup目录。可以查看宿主机上目录[root@wan_test /]# ll /backup/总用量 112-rw-r–r– 1 root root 112640 10月 6 19:34 data_volume_2017_10_01.tar 恢复启动个App-container2将App-container-1的数据恢复 到App-container2中1docker run -itd --name App-container2 -v /var/volume1 -v /var/volume2 ubuntu 此时App-container2中的/var/volume1和/var/volume2目录是空的开始恢复1docker run -it --rm --volumes-from App-container2 -v /backup:/backup ubuntu tar -xvf /backup/data_volume_2017_10_01.tar -C / 查看App-container2的/var/volume1和/var/volume2，数据恢复。迁移1、stop原用容器2、将原目录mount到新容器 销毁销毁容器不会销毁bind mount data managed volume销毁需要docker rm -v 或docker volume rm xxx 批量删除docker volume rm $(docker volume ls -q) https://yeasy.gitbooks.io/docker_practice/content/network/linking.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Docker machine(docker 多主机部署)]]></title>
      <url>%2F2017%2F10%2F30%2Fdocker_machine%2F</url>
      <content type="text"><![CDATA[此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。Docker machine概念Docker machine：在一个multi-host的环境中，手动去将安装和配置docker非常繁琐，用户能够使用docker machine能够快速的安装docker环境并配置安全配置。docker machine支持在不同的环境下安装配置docker host，同时还能对构建好的host进行一些简单的管理操作。(1)常规linux操作系统(2)虚拟化平台 ：virtualbox、vmware、kvm、hyperV等(3)公有云：aws、azure等 实际体验：可能因为网络原因，经常卡住不动，不如写shel脚本l、使用puppet、和ansible这些工具，可控程度高。 实验环境3个host 安装 docker machine123curl -L https://github.com/docker/machine/releases/download/v0.13.0/docker-machine-`uname -s`-`uname -m` &gt;/tmp/docker-machine &amp;&amp;chmod +x /tmp/docker-machine &amp;&amp;sudo cp /tmp/docker-machine /usr/local/bin/docker-machine 执行docker-machine –version验证安装 安装 bash completion script，这样在 docker-manchine 管理对应的host时，命令提示符会变成对应host的name cd /etc/bash_completion.d执行以下命令1scripts=( docker-machine-prompt.bash docker-machine-wrapper.bash docker-machine.bash ); for i in &quot;$&#123;scripts[@]&#125;&quot;; do sudo wget https://raw.githubusercontent.com/docker/machine/v0.13.0/contrib/completion/bash/$&#123;i&#125; -P /etc/bash_completion.d; done 修改bashrcvim ~/.bashrc12source /etc/bash_completion.d/docker-machine-prompt.bashPS1=&apos;[\u@\h \W$(__docker_machine_ps1)]\$ &apos; 创建machine创建machine就是在host上自动安装和配置dockerdocker-machine ls #显示当前通过docker-machine创建的host 创建前需要进行互信操作，实现免密码登录，docker-machine生成公钥 12ssh-copy-id root@192.168.111.162ssh-copy-id root@192.168.111.163 开始创建执行1docker-machine create --driver generic --generic-ip-address=192.168.111.162 container-1 卡住的话 docker-machine rm xxx 然后做试几次create 123--driver 调用哪个driver，使用普通Linux就写generic ，--generic-ip-address 指定目标系统的ipcontainer-1表示命名为contaner-1 添加container-21docker-machine create --driver generic --generic-ip-address=192.168.111.163 container-2 执行过程 1、通过ssh登录到远程主机2、安装docker3、复制证书4、配置docker-daemon5、启动docker docker-machine ls 可以看见已经安装好的docker host了 docker-machine 执行的步奏docker-machine creat会做两步操作。关于docker-machine做的安全配置参考http://www.cnblogs.com/sparkdev/p/7066789.html1，安装docker，并进行配置。2，生成证书保证docker服务安全。我们手动按装的docker，docker daemon监听tcp端口，但没有做任何安全限制，也就是任何人想连都可以连接进来。使用docker-machine create创建的docker默认是做的tls证书加密验证的，只有安装了特定证书的client才能与docker-daemon交互。 /etc/systemd/system/docker.service.d/10-machine.conf 在 Docker daemon 的配置文件中看到四个以 –tls 开头的参数，分别是 –tlsverify、–tlscacert、–tlscert和 –tlskey。其中的 –tlsverify 告诉 Docker daemon 需要通过 TLS 来验证远程客户端。其它三个参数分别指定了一个 pem 格式文件的路径，按照它们指定的文件路径去查看一下： 回到安装docker-machine的机器上ls /root/.docker/machine/machines/container-1/ 和docker-daemon上的是一样的 Docker Machine 在执行 create 命令的过程中，生成了一系列保证安全性的秘钥和数字证书*.pem文件。这些文件在本地和远程 Docker 主机上各存一份，本地的用于配置 Docker 客户端，远程主机上的用于配置 Docker daemon，让两边都设置 TLS 验证的标记，依此实现安全的通信。 管理machine显示container-1的环境变量 执行1eval $(docker-machine env container-1) 可以发现命令提示符发生了改变，因为我们前面在bashrc配置了在此状态下执行所有的docker命令都相当于在container-1上执行 切换到container-2eval $(docker-machine env container-2) docker-machine 命令 upgrad ：更新machine的docker到最新版本 docker-machine upgrade container-1 container-2config：查看machine的docker daemon配置stop/restart/restart :对host操作系统进行操作scp：可以在不同machine中scp文件。 http://www.cnblogs.com/sparkdev/p/7066789.htmlhttp://www.cnblogs.com/CloudMan6/p/7237420.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Docker安装]]></title>
      <url>%2F2017%2F10%2F23%2Finstall_docker%2F</url>
      <content type="text"><![CDATA[centos7安装docker官方推荐centos7.3系统安装依赖yum install -y yum-utils device-mapper-persistent-data lvm2 添加docker yum源1yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 版本区别：docker-ce-edge：类似开发测试版，功能比较新，但不一定稳定，edge是每月更新一次。 docker-ce-stable：稳定版，每3个月更新一次。 开启docker-ce-edge更新(默认是关闭的)yum-config-manager –enable docker-ce-edgeyum-config-manager –disable docker-ce-edge 安装docker yum install docker-ce -y 启动docker systemctl start docker 测试docker是否安装成功,运行个httpd 镜像 docker run -d -p 80:80 httpd -d:以后台方式运行-p：端口映射，源端口:目标端口 docker hub在国外，速度太慢了，使用国内的镜像源daocloud1curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://a58c8480.m.daocloud.io 重启docker 进程systemctl restart docker 运行mysql1docker run --name first-mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=4435448 -d mysql -e设置环境变量这里使用-e设置环境变量MYSQL_ROOT_PASSWORD 连接方法mysql -h 10.211.55.5 -P3306 -uroot -p4435448]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[从零开始搭建云平台监控(五)监控自动化部署]]></title>
      <url>%2F2017%2F10%2F16%2Fmonitor_5%2F</url>
      <content type="text"><![CDATA[前面四篇文章讲的更多是说这套监控平台是如何一步搭建起来的，但在实际生产中，我们要给客户搭建很多套私有云环境，不可能每个环境手动去装一边，因为zabbix的监控项和grafana的dashboard都可以做成模板导入。剩下就是一些软件安装和zabbix-agent的配置问题了，目前我采用的做法是将monitor-server封装成一个镜像，镜像里面将软件都安装好了monitor-server上有写好的ansible的playbook，agent端通，过monitor-server上的ansible的playbook去推送安装。 ansible是一个python写的基于ssh的轻量级的自动化运维工具，它有以下优点优点：(1)、轻量级，无需在客户端安装agent，更新时，只需在操作机上进行一次更新即可；(2)、批量任务执行可以写成脚本，而且不用分发到远程就可以执行；(3)、使用python编写，维护更简单，ruby语法过于复杂；(4)、支持sudo；(5)、基于ssh无需agent端就可以实现自动化配置。 缺点：(1)、ansible默认才用轮询的方式，如果节点数量一多，撑不住，相比较基于c/s架构的saltstack 中间还有消息队列辅助在大规模集群中性能确实弱。 下面主要介绍自己写的playbook，目录结构如下1234567891011121314151617181920212223242526272829303132333435363738/etc/ansible/|-- ansible.cfg|-- hosts|-- readme|-- roles| |-- ceph| | |-- handlers| | | `-- main.yml| | |-- tasks| | | `-- main.yml| | |-- templates| | | `-- zabbix_agentd.conf.j2| | `-- vars| | `-- main.yml| |-- common| | |-- 1| | |-- handlers| | | `-- main.yml| | |-- tasks| | | |-- 1| | | `-- main.yml| | |-- templates| | | `-- zabbix_agentd.conf.j2| | `-- vars| | `-- main.yml| `-- telegraf| |-- handlers| | `-- main.yml| |-- tasks| | `-- main.yml| |-- templates| | `-- telegraf.conf.j2| `-- vars| `-- main.yml|-- server_ip.sh|-- site.retry|-- site.yml`-- zabbix_agent.tar 我这里分为3个roles ，ceph、common、telegrafceph主要做的操作有1、拷贝配置文件 common 主要做的操作有1、环境初始化2、拷贝软件包到对应节点3、解压tar包4、安装软件5、修改对应的配置文件6、修改执行权限7、修改防火墙开放端口8、启动服务、设置开机自启 telegraf主要做的操作有1、安装telegraf2、修改telegraf systemd的启动用户3、拷贝telegraf配置文件4、重启telegraf并设置开机启动 因为每个节点所对应的角色不一样，ansible所跑的脚本也是不一样的，所以定义这3个roles，服务器的角色通过hosts文件控制 定义了三组角色，控制节点、计算节点、存储节点、融合节点、每组角色设置了不同的metadata，这个metadata是通过templates/zabbix_agentd.conf.j2传给对应节点zabbix-agent里面的HostMetadata参数，然后zabbix-server根据不同的metadata去调用不同的模板。定义group是为了在task里面调用不同的命令。 通过site.yml来控制不同的host调用不同的role 可以看见common的hosts是all表示在hosts定义的所有组都会调用common， telegraf roles是只有control的host会去调用 ceph roles是storage的 hosts去调用 每个roles下面分handlers、tasks、templates、vars 其中，handlers：也是task，但只有其关注的条件满足时，才会被触发执行 ， templates：定义的是配置文件模板，比如我们这里定义好了zabbix_agent、和telegraf的配置文件模板 vars：定义变量，我在templates里面定义了zabbix_agent变量，里面的server 和server_active我都是以变量方式存储的，然后在执行ansible前先把变量改好，这样在不同的环境配置文件里的ip也会根着变。 项目代码先以开源：https://github.com/wanshaoyuan/ansible_monitor]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[从零开始构建云平台监控(四)图形化展示监控效果]]></title>
      <url>%2F2017%2F10%2F13%2Fmonitor_4%2F</url>
      <content type="text"><![CDATA[组件安装和配置zabbix本功能非常强大，自定义监控项，自动发现、自动注册等，但zabbix-server获取的zabbix数据看起来不是特别直观。 grafana的出现正好弥补了zabbix绘图能力上的不足，grafana是基于js开发的图形编辑器。telegraftelegraf是一个go语言编写的收集监控项的agentTelegraf内存占用小的特点，通过插件来实现不同的监控需求，这里我使用ceph插件。 influxdbInfluxDB 是一个开源分布式时序|、事件和指标数据库。使用 Go 语言编写，无需外部依赖。其设计目标是实现分布式和水平伸缩扩展。它有三大特性： Time Series （时间序列）：你可以使用与时间有关的相关函数（如最大，最小，求和等） Metrics（度量）：你可以实时对大量数据进行计算 Eevents（事件）：它支持任意的事件数据特点schemaless(无结构)，可以是任意数量的列Scalablemin, max, sum, count, mean, median 一系列函数，方便统计Native HTTP API, 内置http支持，使用http读写Powerful Query Language 类似sqlBuilt-in Explorer 自带管理工具 通过grafana调用zabbix的接口的实现，通过自定义模板和key去拉去数据，在grafana上进行展示，grafana只做统一的监控展示平台ceph的一些监控数据由telegraf收集存到influxdb，grafana去读取。 为什么选择telegraf而不直接通过zabbix写item获取这些数据？1，telegraf是一个go语言写的小程序，占用资源非常小，并且本身有监控ceph的插件2，telegraf监控ceph的插件，监控的数据非常全，osd数mon数，细致点，journal盘的速率、pgmap的数率，cluster的iops、pool池的使用趋势等等，如果用zabbix的话我们想获取这些数据要写非常多的item。 monitor-server端操作安装influxdbinfluxdb的安装配置yum源12345678cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/influxdb.repo[influxdb]name = InfluxDB Repository - RHEL \$releaseverbaseurl = https://repos.influxdata.com/rhel/\$releasever/\$basearch/stableenabled = 1gpgcheck = 1gpgkey = https://repos.influxdata.com/influxdb.keyEOF yum install influxdbsystemctl enable influxdbsystemctl restart influxdb 默认influxdb的web管理界面是关闭的开启方法 vim /etc/influxdb/influxdb.conf 123[admin] enabled = true bind-address = &quot;:8083&quot; 重启influxdb浏览器服务http://ip:8083 在ceph-mon节点上安装telegraftelegraf通过读取ceph的asok文件获取里面的信息来达到监控目的。1wget https://dl.influxdata.com/telegraf/releases/telegraf-1.2.1.x86_64.rpm yum localinstall telegraf-1.2.1.x86_64.rpm 修改telegraf的启动用户，不然会读取ceph asok文件权限不足vim /lib/systemd/system/telegraf.service将User=telegraf改为User=rootsystemctl daemon-reloadsystemctl restart telegrafsystemctl enable telegraf 配置telegrafvim /etc/telegraf/telegraf.conf1234567891011121314[log]logfile = &quot;/var/log/telegraf/telegraf.log&quot;[[outputs.influxdb]]urls = [&quot;http://10.10.1.100:8086&quot;] # required #填写对应的influxdb的地址[[inputs.ceph]]interval = &apos;1m&apos;ceph_binary = &quot;/usr/bin/ceph&quot;socket_dir = &quot;/var/run/ceph&quot;mon_prefix = &quot;ceph-mon&quot;osd_prefix = &quot;ceph-osd&quot;ceph_user = &quot;client.admin&quot;ceph_config = &quot;/etc/ceph/ceph.conf&quot;gather_admin_socket_stats = truegather_cluster_stats = true 另外3个控制节点同步配置重启telegraf测试数据是否拿到1telegraf -test -config /etc/telegraf/telegraf.conf -config-directory /etc/telegraf/telegraf.d -input-filter ceph 在monitor-server的influxdb上可以看见创建好的库 进入influxdbinfluxd列出全部库show databases;进入指定的库use telegraf列出库中全部表show measurements;查询表select * from ceph; 默认influxdb的数据存储时间为168小时也就是7天如果要修改方法为123456create retention policy &quot;rp_name&quot; on &quot;db_name&quot; duration 3w replication 1 defaultrp_name：策略名db_name：具体的数据库名3w：保存3周，3周之前的数据将被删除，influxdb具有各种事件参数，比如：h（小时），d（天），w（星期）replication 1：副本个数，一般为1就可以了default：设置为默认策略 对已有的策略修改1alter retention policy &quot;rp_name&quot; on &quot;db_name&quot; duration 30d default 删除已有策略1drop retention policy &quot;rp_name&quot; 安装grafana配置安装grafana下载软件包1wget https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana-4.1.2-1486989747.x86_64.rpm 安装软件1yum localinstall /root/grafana-4.1.2-1486989747.x86_64.rpm 启动grafanasystemctl start grafana-server查看端口lsof -i:3000 安装pie chart插件 ceph这一块用饼图描述比较直观点，需要安装grafana Pie Chart插件 grafana-cli plugins install grafana-piechart-panel 在线安装方式grafana-cli plugins install grafana-piechart-panel 离线安装方式将下载好的plugins解压到/var/lib/grafana/pluginsgrafana-cli plugins install grafana-piechart-pane 重启grafanasystemctl restart grafana-server登录grafanahttp://localhost:3000默认帐号密码为admin/admin将插件激活 enable zabbix 右侧可以看见已经安装的插件 添加Datasource选zabbix url输入http://192.168.122.100/zabbix/api_jsonrpc.php 测试api连接 然后就可以在dashboard里面看见两个新模板了 将Template Linux Server删除 查看zabbix-server模板 添加influxdb DataSource 创建以下dashboard 创建好后，我们从一个个dashboard里面添加模块 openstack物理节点性能监控配置template因为这是一个集群，有很多台机器，每台机器所对应的角色是不一样的，比如node-1到node-3是控制节点node-4到node6是存储节点 node-7到node-10是计算节点，不同角色的监控项也有略微不同。但我们想在一个dashboard展示多台机器的监控值 所以我们需要定义Templating 定义group的templating 然后在zabbix里面定义的group就出来了 selection options host可以选多个，然后在一个图上绘出，我建议，这里还是一一对应的显示吧定义host的templating 匹配完的显示 创建rowrow是一组panel的集合 这里面的uptime、memeory free、free disk space on / 、system load(1min）这些panel就是属于主机情况这个row的。以创建主机情况row为例dashboard下面有一个add row的按钮 编辑row 编辑名称显示名字 按照上述方法创建另外3个row，资源使用情况，网卡信息，然后ctrl+s保存这个dashboard的改变。创建panel将panel创建到对应的row中创建uptime时间panel点击创建panel 类型为Singlestat panelgeneral配置 标题大小 metrics配置数据来源 配置字体颜色 ctrl+s保存，uptime 的panel就完成了。 定义memory free panel在主机情况row创建Singlestat类型的panel options定义背景色，和定义值范围，根据值的不同范围，背景色会自动调整 这里需要注意下Thresholds这个参数，这个参数定义值的范围，然后granafa会根据这个范围自动调整颜色，如7,9如果值小于7则显示红色，在7~9之间则显示橙色，大于9则显示绿色，但这个值不支持单位换算后的值，所以这也是为什么我这里是10737418240,21474836480，是10,20 G内存空闲小于10G变红色，10G~20G之间橙色，大于20G绿色。 ctrl+s保存 定义根分区剩余空间panel在主机情况row创建Singlestat类型的panel 配置metrics options定义背景色，和定义值范围，根据值的不同范围，背景色会自动调整 这里也是定义小于10G时背景色变红，10G~20G范围内为橙色，大于20G为绿色。ctrl+s保存定义cpu负载panel不过这里首先需要修改我们zabbix的 item，zabbix item里面定义的 per cpu而不是all cpu，所以得出来的值会比操作系统里面直接uptime看起来小 ，需要修改zabbix 里面Template OpenStack Compute、和Template OpenStack Controller里面，找到这三个健值修改成all 测试zabbix_agentd -t “system.cpu.load[all,avg1]” 在主机情况row创建Singlestat类型的panel这里我选择15分钟负载情况 ctrl+s保存 定义cpu空闲率panel在资源使用情况row创建Graph类型的panel 单位为percent 保存ctrl+s 定义CPU时间片使用情况在资源使用情况rom创建Table panelGenterl配置panel 名字 配置Metrics 配置Options ctrl+s保存 定义可用内存panel在资源使用情况rom创建Graph panel 定义网桥流量监控panel这里需要注意，我们直接对ovs桥进行监控，因为通过ovs桥我们可以确认这部分流量的用途，直接通过物理网卡的话不直观，所以这里需要在zabbix里面将对应的item加上。 这里将Incoming和Outcoming拆分了开来用两张图显示，并且只监控traffic流量。在网卡信息row创建Graph panel先配置Incoming_Network_traffic 注意item为&quot;/Incoming network traffic on br-.*/&quot; 在配置Outcoming_Network_traffic item写这个 /Outgoing network traffic on br-.*/ ctrl+s 最终效果 添服务状态dashboard，这里我拆分为3个dashboard，分别是OpenStack控制节点服务状态、OpenStack计算节点服务状态、OpenStack储存节点服务状态 每个dashboard设置不同的Templating以OpenStack控制节点服务状态为例，后面都一样创建group，指定为controller这样就不会选别的了，计算节点就指定为computer、储存节点就指定为ceph 创建host ctrl+s保存 OpenStack控制节点服务状态定义nova-api panel 配置 value mapping，因为默认抓取的值为数字我们需要将数字映射为更好看的值，比如大于1映射为running，其他映射为down，这里测试发现down后值经常为0.0x所以这里设置映射为值的范围，比如0~0.9为down，1为up 效果如下我把nova-api关掉 其他控制节点和计算节点监控项目可以直接duplicate了，只需要修改下panel和metrics控制节点需要显示的panel12345678910111213141516171819202122nova-apinova-schedulernova-conductorNeutron-serverNeutron-l3-agentNeutron-ovs-agentNeutron-metadataNeutron-DHCPGlance-apiGlance-RegistryKeystoneRabbitmq_clusterRabbitmq_liste_portRabbitmq_BEAM_processRabbitmq_EPMD_listen_portRabbitmq_EPMD_processMysql_clusterCinder_apiCinder_schedulerCinder_volumerCeilometer_processCeilometer_API 计算节点需要显示的panel1234openstack-computelibvirtdopenvswitch-agentceilometer-agent-compute zabbix-altert dashboardalter-dashboard主要显示整个集群的问题 创件panel，类型为zabbix-Triggers 添加后，它自动会将zabbix的Triggers同步过来 ceph dashboard创建template 这里需要注意把group中固定住控制节点 host配置 添加ceph健康状态监控panel在状态row中创建 Singlestat panel 添加ceph mon总数监控panel在状态row中创建一个singlestat的panel，General修改名字，metrics配置数据源 添加ceph mon up数panel在状态row中创建一个singlestat的panel，General修改名字，metrics配置数据源 thresholds要根据实际集群mon数写，ceph-mon不能挂掉总个数的一半，3个mon只能挂1个，所以这里写2,3小于2时就变红。 添加ceph osd总数监控panel在状态row中创建一个singlestat的panel，General修改名字，metrics配置数据源 添加ceph osd up数监控panel在状态row中创建一个singlestat的panel，General修改名字，metrics配置数据源 添加ceph总容量panel在容量row中创建一个singlestat的panel，General修改名字，metrics配置数据源 注意单位 添加ceph剩余容量panel在容量row中创建一个singlestat的panel，General修改名字，metrics配置数据源 添加ceph已用容量panel在容量row中创建一个singlestat的panel，General修改名字，metrics配置数据源 注意单位 添加ceph容量使用率(饼图)panel在容量row中创建一个 Pie Chart 的panel，General修改名字，metrics配置数据源已经使用和剩余可用百分比利用饼图展示已用空间占总空间的比例新建个panel，类型选择pie chartgeneral设置名字 添加pool容量使用率(已compute pool为例)panel在容量row中创建一个 Pie Chart 的panel，General修改名字，metrics配置数据源 option这里根刚才一样 另外几个pool根这个一样，需要注意的是 where name里面的pool name要更改。 添加pool池的使用趋势图(以compute pool为例)panel在pool池中的使用情况row中创建Graph panel 显示30天的数据剩下pool的使用情况图根上面有一，只需要修改metrics里面的pool名就可以了 添加性能panel在性能row中创将graph panel 注意这里的datasource是 zabbix 单位选择IOPS 添加ceph pg read rate panel在性能row中创将graph panel 单位为bites 显示一天的 添加ceph pg write rate panel在性能row中创将graph panel 最终效果]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[从零开始构建云平台监控(三)配置基础监控平台]]></title>
      <url>%2F2017%2F10%2F11%2Fmonitor_3%2F</url>
      <content type="text"><![CDATA[环境：3台控制节点+2台融合节点 2个osd 给zabbix-server导入模板模板为分别对应为control、compute、ceph 导入这3个模板 配置自动注册自动注册就是zabbix-server根据zabbix-agent里面配置的HostMetadata参数的值去进行一系列的操作，和自动发现不一样的是，自动发现只能跟据zabbix-server配置的扫描的网段去添加机器，非常不灵活，并且这个操作是zabbix-server发起的机器规模一大，对zabbix-server有很大的负载压力。自动注册是agent端主动将HostMetadata参数的值给server，server根据管理员的配置在做出相应的操作，在这里比如我openstack的控制节点我就在agent端的HostMetadata配置openstack_controler、然后计算节点配置openstack_computer、存储节点openstack_storager、计算和存储融合节点为 openstack_computer&amp;storager,然后在zabbix-server端创建相应的动作如下图(以添加control为例)其他类似。 操作都填写这些 最后 配置zabbix-agent在agent端安装zabbix-agent，并导入脚本下载zabbix-agent，agent端就不用源码安装了，直接下载rpm包。1wget http://repo.zabbix.com/zabbix/3.4/rhel/7/x86_64/zabbix-agent-3.4.2-1.el7.x86_64.rpm 安装1234567891011[root@node-6 ~]# rpm -ivh zabbix-agent-3.4.2-1.el7.x86_64.rpmvim /etc/zabbix/zabbix.confPidFile=/var/run/zabbix/zabbix_agentd.pidLogFile=/var/log/zabbix/zabbix_agentd.logLogFileSize=0Server=10.10.1.100ListenPort=10050ServerActive=10.10.1.100HostMetadata=openstack_controlerTimeout=15Include=/etc/zabbix/zabbix_agentd.d/*.conf 解压123tar -xvf controller.agent.tar.gz -C /etc/zabbix/[root@node-6 zabbix_agentd.d]# chown zabbix:zabbix *[root@node-6 zabbix_agentd.d]# chmod a+x * cd /etc/zabbix/script[root@node-6 script]# chown zabbix:zabbix *其他几个节点一样 注意两个融合节点metadata配置HostMetadata=openstack_computer&amp;storager重启zabbix-agent，等待自动注册。12systemctl restart zabbix-agentsystemctl enable zabbix-agent 可以看见机器都自动注册进来了。 配置邮件告警脚本配置将发送邮件脚本放置到/usr/local/etc/zabbix/alertscripts vim sendmail.py chmod a+x sendmail.py zabbix-server web端配置创建报警媒介类型 名字、类型、注意下面三个参数要传递到脚本里面。 配置用户 报警媒介 输入联系人更新 创建动作，zabbix在3.4.2版本中对创建动作这块有较大改动，多了确认操作，以前只有故障和恢复时发邮件，触发动作的条件可以自己配置。 添加新的动作 添加用户、添加组、选择刚刚创建的发送媒介 恢复操作通知。 测试将zabbix-agent关闭等待5分钟。动作日志。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[从零开始构建云平台监控(二)安装zabbix]]></title>
      <url>%2F2017%2F10%2F10%2Fmonitor_2%2F</url>
      <content type="text"><![CDATA[安装zabbix-server这里我选择zabbix3.4.2正式版,zabbix3.4.2修复前期非常多bug，release文档如下:https://www.zabbix.com/rn3.2.41wget https://ncu.dl.sourceforge.net/project/zabbix/ZABBIX%20Latest%20Stable/3.4.2/zabbix-3.4.2.tar.gz 创建zabbix用户和组groupadd zabbixuseradd -g zabbix zabbix -s /sbin/nologin 创建库并授权12345678MariaDB [(none)]&gt; create database zabbix character set utf8 collate utf8_bin;Query OK, 1 row affected (0.00 sec)MariaDB [(none)]&gt; grant all privileges on zabbix.* to zabbix@localhost identified by &apos;passw0rd&apos;;Query OK, 0 rows affected (0.01 sec)MariaDB [(none)]&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) 解压zabbix tar包tar -xvf zabbix-3.4.2.tar.gz -C /lnmp/ 导入数据库123456[root@localhost zabbix-3.4.2]# cd /lnmp/zabbix-3.4.2/database/mysql/注意顺序[root@localhost mysql]# mysql -uzabbix -ppassw0rd zabbix &lt; schema.sql[root@localhost mysql]# mysql -uzabbix -ppassw0rd zabbix &lt; images.sql[root@localhost mysql]# mysql -uzabbix -ppassw0rd zabbix &lt; data.sql 安装需要依赖1yum install -y net-snmp net-snmp-devel OpenIPMI OpenIPMI-devel libevent libevent-devel unixODBC-devel 编译注意这里不安装zabbix-java-gateway， zabbix-java-gateway是用来监控tomcat的需要本机有java环境，这里先不装，如果需要，安装java环境后重新编译加上–enable-java就可以了123./configure --enable-server --enable-agent --with-mysql --with-net-snmp --with-libcurl --with-libxml2 --with-openipmi --with-unixodbc --with-opensslmake install 配置monitor-server创建zabbix日志目录12[root@localhost yum.repos.d]# mkdir /var/log/zabbix[root@localhost yum.repos.d]# chown -R zabbix:zabbix /var/log/zabbix/ 创建自定义脚本目录12[root@localhost tru64]# mkdir /usr/local/etc/zabbix/alertscripts -pv[root@localhost yum.repos.d]# chown -R zabbix:zabbix /usr/local/etc/zabbix/alertscripts 配置文件目录/usr/local/etc/编辑zabbix-server.conf1234567891011LogFile=/var/log/zabbix/zabbix_server.logDBHost=localhostDBName=zabbixDBUser=zabbixDBPassword=passw0rdListenIP=0.0.0.0FpingLocation=/usr/sbin/fpingTimeout=20CacheSize=1024AlertScriptsPath=/usr/lib/zabbix/alertscriptsExternalScripts=/usr/lib/zabbix/externalscript 编辑zabbix-agent.conf1234LogFile=/var/log/zabbix/zabbix_agentd.logServer=127.0.0.1ServerActive=127.0.0.1Hostname=Zabbix server 拷贝启动脚本12[root@localhost tru64]# cp /lnmp/zabbix-3.4.2/misc/init.d/tru64/zabbix_server /etc/init.d/[root@localhost tru64]# cp /lnmp/zabbix-3.4.2/misc/init.d/tru64/zabbix_agentd /etc/init.d/ 添加执行权限12[root@localhost tru64]# chmod a+x /etc/init.d/zabbix_server[root@localhost tru64]# chmod a+x /etc/init.d/zabbix_agentd 编辑启动脚本vim /etc/init.d/zabbix_server123#!/bin/sh#chkconfig: 345 95 95#description: Zabbix_Server vim /etc/init.d/zabbix_agent123#!/bin/sh#chkconfig: 345 95 95#description: Zabbix_agentd 添加服务12[root@localhost tru64]# /sbin/chkconfig --add zabbix_agentd[root@localhost tru64]# /sbin/chkconfig --add zabbix_server 开机自启12[root@localhost tru64]# /sbin/chkconfig zabbix_server on[root@localhost tru64]# /sbin/chkconfig zabbix_agentd on 创建目录1[root@localhost /]# mkdir /var/www/html/zabbix 拷贝安装页1cp -rf /lnmp/zabbix-3.4.2/frontends/php/* /var/www/html/zabbix/ 重启nginx /etc/init.d/nginx restart打开浏览器 按提示操作，下载文件放到指定目录设置中文安装完成]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[从零开始构建云平台监控(一)搭建LNMP]]></title>
      <url>%2F2017%2F10%2F04%2Fmonitor_1%2F</url>
      <content type="text"><![CDATA[搭建LNMP先安装mysql—-&gt;安装php—–&gt;安装nginx版本:mysql 5.7.19php 7.1.7nginx 1.12.1 本次搭建除mysql外全部采用源码方式安装，因为采用源码的方式安装可以更好的对功能模块是否需要做定制。 安装依赖1yum -y install gcc gcc-devel gcc-c++ gcc-c++-devel libaio-devel boost boost-devel autoconf* automake* zlib* libxml* ncurses-devel ncurses libgcrypt* libtool* cmake openssl openssl-devel bison bison-devel unzip numactl-devel 创建mysql软件源vim /etc/yum.repo.d/mysql.repo1234[mysql]name = mysqlbaseurl = https://mirrors.ustc.edu.cn/mysql-repo/yum/mysql-5.7-community/el/7/x86_64/gpgcheck=0 1yum install mysql-community-libs mysql-community-common mysql-community-client mysql-community-devel mysql-community-server 启动mysqlsystemctl start mysqld mysql初始化mysqld –initialize –user=mysql 查找默认密码grep “temporary password” /var/log/mysqld.log 默认密码策略设置密码需要大写、小写、特殊符号。关闭默认密码策略 编辑/etc/my.cnfvalidate_password=off 重启mysqlsystemctl restart mysqld 用初始密码登录mysql重启密码1alter user root@localhost identified by &apos;passw0rd&apos;; 修改默认编码由下图可见database和server的字符集使用了latin1编码方式，不支持中文，即存储中文时会出现乱码。以下是命令行修改为utf-8编码的过程，以支持中文 修改为utf-8打开，编辑/etc/my.cnf.d/server.cnf在mysqld里面添加character_set_server = utf8 重启mysqlsystemctl restart mysqld 再次查看 安装php7php7在性能上比php5.x有2倍多的提升，同时兼容性也特别好。同时php5.x很多openssl和openssh的漏洞 安装依赖包1yum -y install libmcrypt-devel mcrypt mhash gd-devel ncurses-devel libxml2-devel bzip2-devel libcurl-devel curl-devel libjpeg-devel libpng-devel freetype-devel net-snmp-devel openssl-devel 安装libconv创建目录mkdir /lnmp进入目录cd /lnmpwget http://ftp.gnu.org/pub/gnu/libiconv/libiconv-1.14.tar.gz解压包12345tar xvf libiconv-1.14.tar.gzcd libiconv-1.14./configure --prefix=/usr/local/libiconv1.14 make时包此错 解决方法 123456vi libiconv-1.14/srclib/stdio.in.h将698行的代码：_GL_WARN_ON_USE (gets, &quot;gets is a security hole - use fgets instead&quot;);替换为：#if defined(__GLIBC__) &amp;&amp; !defined(__UCLIBC__) &amp;&amp; !__GLIBC_PREREQ(2, 16) _GL_WARN_ON_USE (gets, &quot;gets is a security hole - use fgets instead&quot;);#endif 重新make &amp;&amp; make install保存动态链接库12echo &quot;/usr/local/lib64/&quot; &gt; /etc/ld.so.conf.d/lnmp.confecho &quot;/usr/local/lib/&quot; &gt;&gt; /etc/ld.so.conf.d/lnmp.conf 刷新ldconfig 下载php7.1.7源码包cd /lnmpwget http://cn2.php.net/distributions/php-7.1.7.tar.gz 解压tar -xvf php-7.1.7.tar.gz 配置和检查依赖php7.1.71./configure --prefix=/usr/local/php7.1.7 --with-config-file-path=/usr/local/php7.1.7/etc --enable-mysqlnd --with-mysqli=mysqlnd --with-pdo-mysql=mysqlnd --with-iconv-dir=/usr/local/libiconv1.14 --with-pcre-regex --with-zlib --with-bz2 --enable-calendar --with-curl --enable-dba --with-libxml-dir --enable-ftp --with-gd --with-jpeg-dir --with-png-dir --with-zlib-dir --with-freetype-dir --enable-gd-native-ttf --with-mhash --enable-mbstring --with-mcrypt --enable-pcntl --enable-xml --disable-rpath --enable-shmop --enable-sockets --enable-zip --enable-bcmath --with-snmp --disable-ipv6 --with-gettext --enable-fpm --with-fpm-user=www --with-fpm-group=www --with-openssl make &amp;&amp; make install复制php配置文件1cp /lnmp/php-7.1.7/php.ini-production /usr/local/php7.1.7/etc/php.ini 配置php修改时区1sed -i &apos;s#;date.timezone =#date.timezone = Asia/Shanghai#g&apos; /usr/local/php7.1.7/etc/php.ini 隐藏php版本号1sed -i &apos;s#expose_php = On#expose_php = Off#g&apos; /usr/local/php7.1.7/etc/php.ini 配置启动脚本12[root@localhost php-7.1.7]# cp /lnmp/php-7.1.7/sapi/fpm/init.d.php-fpm /etc/init.d/php-fpm[root@localhost php-7.1.7]# chmod +x /etc/init.d/php-fpm 配置fast-cgi1cp /usr/local/php7.1.7/etc/php-fpm.d/www.conf /usr/local/php7.1.7/etc/php-fpm.conf 修改fast-cgivim /usr/local/php7.1.7/etc/php-fpm.conf修改已下参数rlimit_files = 65535 创建www用户和组1useradd www -s /sbin/nologin 启动php-fpm /etc/init.d/php-fpm start设置开机自启/sbin/chkconfig –add php-fpm修改php.ini以满足zabbix要求vim /usr/local/php7.1.7/etc/php.ini限制执行目录,nginx网页放在/var/www/html下12345open_basedir = &quot;/var/www/html/:/tmp&quot; max_execution_time = 300 max_input_time = 300 post_max_size = 24M upload_max_filesize = 4M 修改php连接mysqlpdo_mysql.default_socket= /var/lib/mysql/mysql.sockmysqli.default_socket = /var/lib/mysql/mysql.sock 安装nginxwget http://nginx.org/download/nginx-1.12.1.tar.gz解压 [root@localhost lnmp]# tar -xvf nginx-1.12.1.tar.gz [root@localhost lnmp]# cd nginx-1.12.11[root@localhost lnmp]#./configure --prefix=/usr/local/nginx1.12.1 --user=www --group=www --with-http_stub_status_module --with-http_gzip_static_module --with-http_ssl_module [root@localhost lnmp]#make &amp;&amp; make install 配置nginx1cp /usr/local/nginx1.12.1/conf/nginx.conf /usr/local/nginx1.12.1/conf/nginx.conf.bak vim /usr/local/nginx1.12.1/conf/nginx.conf123456789101112131415161718192021222324252627282930313233343536373839404142434445user www www;worker_processes 4;error_log logs/error.log info;pid logs/nginx.pid;events &#123; worker_connections 65535; use epoll;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; tcp_nopush on; keepalive_timeout 65; gzip on; log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; access_log logs/access.log main; server &#123; listen 80; server_name localhost; charset utf8; location / &#123; root /var/www/html; index index.php index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /var/www/html; &#125; location ~ \.php$ &#123; root /var/www/html; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125; &#125;&#125; 修改ulimitulimit -n 65535编辑/etc/systemd/system.conf设置DefaultLimitNOFILE=65535 重启系统 启动nginx/usr/local/nginx1.12.1/sbin/nginx重载nginx/usr/local/nginx1.12.1/sbin/nginx -s reload关闭nginx/usr/local/nginx1.12.1/sbin/nginx -s stop重启nginx/usr/local/nginx1.12.1/sbin/nginx -s reopen 设置nginx启动脚本编辑/etc/init.d/nginx注意PATH和NAME变量123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#!/bin/bash# chkconfig: - 85 15PATH=/usr/local/nginx1.12.1DESC=&quot;nginx daemon&quot;NAME=nginxDAEMON=$PATH/sbin/$NAMECONFIGFILE=$PATH/conf/$NAME.confPIDFILE=$PATH/logs/$NAME.pidSCRIPTNAME=/etc/init.d/$NAMEset -e[ -x &quot;$DAEMON&quot; ] || exit 0do_start() &#123;$DAEMON -c $CONFIGFILE || echo -n &quot;nginx already running&quot;&#125;do_stop() &#123;$DAEMON -s stop || echo -n &quot;nginx not running&quot;&#125;do_reload() &#123;$DAEMON -s reload || echo -n &quot;nginx can&apos;t reload&quot;&#125;case &quot;$1&quot; instart)echo -n &quot;Starting $DESC: $NAME&quot;do_startecho &quot;.&quot;;;stop)echo -n &quot;Stopping $DESC: $NAME&quot;do_stopecho &quot;.&quot;;;reload|graceful)echo -n &quot;Reloading $DESC configuration...&quot;do_reloadecho &quot;.&quot;;;restart)echo -n &quot;Restarting $DESC: $NAME&quot;do_stopdo_startecho &quot;.&quot;;;*)echo &quot;Usage: $SCRIPTNAME &#123;start|stop|reload|restart&#125;&quot; &gt;&amp;2exit 3;;esacexit 0 添加执行权限chmod a+x /etc/init.d/nginx注册成服务/sbin/chkconfig –add nginx 添加开机自启/sbin/chkconfig nginx on 查看创建目录mkdir -p /var/www/html修改用户和属组chown www:www /var/www/html重启测试启动nginxsystemctl start nginx重启systemctl restart nginx关闭systemctl stop nginx 测试php在/var/www/html下编写index.php文件 123&lt;?php phpinfo();?&gt; 修改权限chown www:www /var/www/html/index.php重启nginx和php-fpm 打开浏览器输入地址 至此LNMP搭建完毕。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[从零开始构建云平台监控(写在前面)]]></title>
      <url>%2F2017%2F10%2F03%2Fmonitor%2F</url>
      <content type="text"><![CDATA[监控的重要性在整个it系统中，监控是最为重要的一个环节，监控是发现集群问题的关键，没有监控的话，只能等到出了问题才能发现集群的问题，那时为时以晚。 监控的范围在一套完整的openstack私有云中，组件非常复杂因为它不仅仅有openstack的东西还有分布式存储ceph的东西。计算组件：nova-api、nova-scheduler、nova-condutor、nova-novncproxy、nova-compute……网络组件：neutron-server、neutron-dhcp-agent、neutron-metadata-agent、neutron-openvswitch-agent、openvswitch……存储组件：cinder-api、cinder-scheduler 、cinder-volume 其他：等还有认证组件keystone、消息队列、镜像组件、还有底层分布式存储ceph、一个这么庞大的系统可想而知如果没有一个比较完善的监控很多并且在不同的角色上所监控的组件也是不一样的，比如控制节点没有nova-compute服务，计算节点没有控制节点服务，所以监控项还得针对不同的角色做区分。 除了组件上的监控外，其实少不了还有硬件上的监控，比如说，在私有云生产环境中，网卡都是会做bond的，做了bond的好处是当其中一个网卡出现故障时并并不是影响到整个集群，但你也得通过监控发现这个挂了的网卡，然后准备更换，ceph的osd down了，有可能是osd进程被kill掉了，也有可能是硬盘坏了导致osd进程挂了，这也都通过监控去发现。 组件构成 监控节点：nginx、php、mysql、influxdb、grafana、zabbix-server、zabbix-agent控制节点：telegraf、zabbix-agent融合节点：zabbix-agent 组件作用nginx+php+mysql提供基础的lnmp平台让zabbix可以运行，mysql存储zabbix监控数据zabbix-server：用于接收各个机器agent端发过来的数据zabbix-agent：采集数据发给zabbix-servertelegraf：用于采集ceph的监控数据influxdb：存储telegraf的数据grafana：用于对采集数据图形化展示 其中需要注意的是因为教程所以监控节点组件都放一起了，并且还是单节点，在生产环境中监控节点要做高可用，根据监控的机器数决定是否将数据库能独立出来。 以下为最终效果。 效果展示ceph监控界面 openstack节点性能状态 控制节点服务状态展示 计算节点服务状态展示 存储节点osd监控 zabbix-server监控]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Dockerfile]]></title>
      <url>%2F2017%2F10%2F03%2FDockerfile%2F</url>
      <content type="text"><![CDATA[本文为Dockerfile的语法简单介绍使用Dockerfile构建镜像比docker commit的优势 ：1、首先dockerfile是一个文本文件，可控性好，后面要进行翻看或check时更加简单。2、在使用CI/CD等devops工具时，我们一般都是将写好的Dockerfile push到gitlab中去，然后Jenkins监测到对应的project有变动，然后执行自动构建，基于Dockerfile去构建docker image，然后去部署到kubernetes集群中。 Dockerfile常用指令FROM #指定base镜像MAINTAINER #设置镜像作者COPY #将文件从host上的build context拷贝到镜像目录，注意src文件必须是build context内的 支持的格式COPY src dest 和COPY [“src”,”dest”] ADD #与copy类似，从build context复制文件到镜像，不同的是如果src是归档类文件（tar,zip,tgz,xz等),文件会自动解压到dest ENV #设置环境变量，环境变量可被后面的指令使用。 EXPOSE#指定容器中进程会监听的端口，Docker可以将端口暴露出来。 VOLUME #将文件或目录声明为volume WORKDIR #build时会直接切换到此目录，为后面的RUN、CMD、ENTRYPOINT、ADD、COPY等指令设置镜像当前工作目录 RUN #容器中运行指定的命令 CMD #容器启动是运行指定的命令，可以由多个CMD命令，但只有最后一个会生效，CMD命令会被docker run之后根的参数替换掉。 ENTRYPOINT #设置容器启动时命令，Dockerfile中有多个ENTRYPOINT命令但只有最后一个生效，cmd或docker run之后的参数会被当做参数传递给ENTRYPOINT。 指令有两中格式shell和exec格式 shell格式 如RUN apt get install python3CMD echo “Hello world” exec格式 [“executable”,”param1”,”param2”,…]如RUN [“apt-get”, “install”,”python3”]CMD[“/bin/echo”, “hello world”] exec执行指令时，会直接调用[command]命令，不会被shell解析， 当有环境变量时使用exec格式不会被解析。如ENV name wan ENTRYPOINT [“/bin/echo”,”hello”,$name”]运行容器将输出hello $name 如果要输出变量ENV name wan ENTRYPOINT [“/bin/sh”,”-c”,”echo hello,$name”] 运行容器将输出hello ，wan cmd和entrypoint的区别如果docker run指定了其他命令，cmd指定的默认命令将被忽略，当entrypoint一定会执行。并将docker run后面的参数做为entrypoint后面的参数。 cmd可以做entrypoint的参数 ENTRYPOINT [“/bin/echo”,”hello”]CMD [“world”] 当容器通过docker run -it [image]启动时输出hello world entrypoint使用shell格式是，会忽略cmd或docker run提供的参数。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python socket模块]]></title>
      <url>%2F2017%2F10%2F01%2Fpython_socket%2F</url>
      <content type="text"><![CDATA[socket俗称套接字，应用程序的进程和进程的之间的沟通是通过套接字来进行的。在python中，socket模块来创建套节字。 在同一台机器上，不同进程之间，通过进程号进行区分，但在不同的机器上，会存在相同的进程号，比如cenos7的PID为1的进程为systemd，在另外一个机器的centos7也是同样的，所以在网络环境下PID并不能唯一标识一个进程，比如主机A也有PID为1的进程，主机B也有PID为1的进程，但此问题，tcp/ip协议族已经帮我们解决了，网络层的ip地址可以唯一的确定一台主机，传输层的协议和端口可以唯一确定这台主机上的进程。这样利用三元组(ip+协议+端口）可以与其进程进行交互。 python socket编程思路tcp服务端12345678910111213141516创建socket [socket.socket(socket.AF_INET,socket.SOCK_STREAM)] | |将ip和端口与socket绑定 [socket.bind((host,port))] | |监听端口 [socket.listen()] | |接收连接，建立连接请求[socket.accept()] | |接收数据，并发送数据[socket.recv(),socket.send()] | |连接结束，关闭连接。[socket.close()] tcp客户端 12345678910创建socket | |建立连接[socket.socket(socket.AF_INET,socket.SOCK_STREAM)] | |接收数据，同时也发送数据。[socket.recv(),socket.send()] | |关闭连接[socket.close()] socket功能列表socket(family,type[,protocal]) 使用给定的地址族、套接字类型、协议编号（默认为0）来创建套接字。 socket类型 描述 socket.AF_UNIX 只能够用于单一的Unix系统进程间通信 socket.AF_INET 服务器之间网络通信 socket.AF_INET6 IPv6 socket.SOCK_STREAM 流式socket , for TCP socket.SOCK_DGRAM 数据报式socket , for UDP socket.SOCK_RAW 原始套接字，普通的套接字无法处理ICMP、IGMP等网络报文，而SOCK_RAW可以；其次，SOCK_RAW也可以处理特殊的IPv4报文；此外，利用原始套接字，可以通过IP_HDRINCL套接字选项由用户构造IP头 socket.SOCK_SEQPACKET 可靠的连续数据包服务 创建TCP Socket： s=socket.socket(socket.AF_INET,socket.SOCK_STREAM) 创建UDP Socket： s=socket.socket(socket.AF_INET,socket.SOCK_DGRAM) socket函数服务端函数 socket函数 描述 s.bind() 将套接字绑定到地址, 在AF_INET下,以元组（host,port）的形式表示地址 s.listen(backlog) 开始监听TCP传入连接。backlog指定在拒绝连接之前，操作系统可以挂起的最大连接数量。该值至少为1，大部分应用程序设为5就可以了。 s.accept() 接受TCP连接并返回（conn,address）,中conn是新的套接字对象，可以用来接收和发送数据。address是连接客户端的地址。 accept默认是阻塞，当有connect过来时才会打开 客户端socket函数 socket函数 描述 s.connect(address) 连接到address处的套接字。一般address的格式为元组（hostname,port），如果连接出错，返回socket.error错误。 s.connect_ex(adddress) 功能与connect(address)相同，但是成功返回0，失败返回errno的值。 公共socket函数 socket函数 描述 s.recv(bufsize[,flag]) 接受TCP套接字的数据。数据以字符串形式返回，bufsize指定要接收的最大数据量。flag提供有关消息的其他信息，通常可以忽略。 s.send(string[,flag]) 发送TCP数据。将string中的数据发送到连接的套接字。返回值是要发送的字节数量，该数量可能小于string的字节大小。 s.sendall(string[,flag]) 完整发送TCP数据。将string中的数据发送到连接的套接字，但在返回之前会尝试发送所有数据。成 功返回None，失败则抛出异常。 s.recvfrom(bufsize[.flag]) 接受UDP套接字的数据。与recv()类似，但返回值是（data,address）。其中data是包含接收数据的字符串，address是发送数据的套接字地址。 s.sendto(string[,flag],address) 发送UDP数据。将数据发送到套接字，address是形式为（ipaddr，port）的元组，指定远程地址。返回值是发送的字节数。 s.close() 关闭套接字。 s.getpeername() 返回连接套接字的远程地址。返回值通常是元组（ipaddr,port）。 s.getsockname() 返回套接字自己的地址。通常是一个元组(ipaddr,port) s.setsockopt(level,optname,value) 设置给定套接字选项的值。 s.getsockopt(level,optname[.buflen]) 返回套接字选项的值。 s.settimeout(timeout) 设置套接字操作的超时期，timeout是一个浮点数，单位是秒值为None表示没有超时期。一般，超时期应该在刚创建套接字时设置，因为它们可能用于连接的操作（如connect()） s.fileno() 返回套接字的文件描述符。 s.setblocking(flag) 如果flag为0，则将套接字设为非阻塞模式，否则将套接字设为阻塞模式（默认值）。非阻塞模式下，如果调用recv()没有发现任何数据，或send()调用无法立即发送数据，那么将引起socket.error异常。 s.makefile() 创建一个与该套接字相关连的文件 简单例子server端 1、使用一个死循环while True：将会使server端一直处于监听状态。2、children,addr=s.accept 将创建一个新的socket，这样原先的socket继续侦听，而新的socket将接收client端的数据,addr返回的是客户端的ip。那么问题来了，当客户端发送数据关联时是与哪个socket进行连接呢，首先我们需要知道的是客户端发送的数据有两种，一种是请求建立连接的，一种是已经建立好连接后的数据传输，就如上所说tcp/ip有接收缓存和发送缓存，当收到建立连接的请求时，则传给正在监听端口的socket调用accept，当收到连接好连接后的数据传输时，将输据放入接收缓冲区，这样当服务器需要读取数据时调用accept建立的新socket，的recv函数从接收缓冲区读取。3、将socket.accept写在循环里面client新的连接一次将重新创建一个新的socket。 client端 1，s.recv(1024)一次最大接收1024字节2, 当收到连接好连接后的数据传输时，将输据放入接收缓冲区，这样当服务器需要读取数据时调用accept建立的新socket，的recv函数从接收缓冲区读取。输出server端 client端socket.recv()tcp/ip socket在内核中都有一个接收缓冲区和发送缓冲区，当socket接收到数据时，并不是马上调用socket.recv(),而是将数据拷贝到socket中的接收缓冲区中，调用socket.recv()后就是将接收缓冲区的数据，移动到应用层的buff中，并返回。当接收窗口满了后发生的操作是，收端通知发端，停止发送。socket.send()socket.send()是将应用层的buff拷贝到tcp-socket的发送缓冲区中 UDPserver端 client 输出server端 client端 socket实现tcp简单聊天server端 需要注意的是这里的socket.accept()是写在while循怀外的，因为写在死循怀里，脚本一直在执行，client调用一次accept后就进入阻塞状态了，而client端用的还是旧的connect所以，如果写在循怀里，就是执行client后建立一个连接后只能进行一次对话，因为server端的accept又重新回到阻塞状态了,重新执行client生成一个新的connec又可以一次对话。 client端 结果server端 client端 优化点：1，目前程序没有多线程，IO复用，还是半双工状态，一次只能一个用户说话，一个发时，另一个只能收，server在说话，client就不能说。 参考链接123456http://www.oschina.net/question/12_76126?sort=default&amp;p=1https://segmentfault.com/q/1010000000591930http://www.cnblogs.com/aylin/p/5572104.htmlhttp://blog.csdn.net/rebelqsp/article/details/22109925http://blog.csdn.net/hguisu/article/details/7445768/http://blog.csdn.net/rebelqsp/article/details/22191409]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python logging模块]]></title>
      <url>%2F2017%2F09%2F27%2Fpython_logging%2F</url>
      <content type="text"><![CDATA[默认logging函数输出的warning级别的日志。 日志级别日志级别大小关系为：CRITICAL &gt; ERROR &gt; WARNING &gt; INFO &gt; DEBUG &gt; NOTSET logging库提供了多个组件：Logger、Handler、Filter、Formatter：1234Logger 对象提供应用程序可直接使用的接口，供应用代码使用；Handler 发送日志到适当的目的地，比如socket和文件等Filter 提供了过滤日志信息的方法，控制输出；Formatter 指定日志输出和显示的具体格式。 通过logging.basicConfig对日志的输出格式配置 cat test.log 需要注意的是只有等级大于等于basicConfig定义的level的log才会被输出，比如这里定义的等级为DEBUG、debug、info、warning、error日志等级都大于等于debug logging.basicConfig各参数level：日志等级format格式123456789101112131415161718192021222324252627282930313233343536373839format: 指定输出的格式和内容，format可以输出很多有用信息，如上例所示: %(levelno)s: 打印日志级别的数值 %(levelname)s: 打印日志级别名称 %(pathname)s: 打印当前执行程序的路径，其实就是sys.argv[0] %(filename)s: 打印当前执行程序名 %(funcName)s: 打印日志的当前函数 %(lineno)d: 打印日志的当前行号 %(asctime)s: 打印日志的时间 %(thread)d: 打印线程ID %(threadName)s: 打印线程名称 %(process)d: 打印进程ID %(message)s: 打印日志信息 filename：输出文件名filemode：写入模式w为直接写入，a为追加datafmt：输出的时间格式 这里用%Y-python中时间日期格式化符号：%y 两位数的年份表示（00-99）%Y 四位数的年份表示（000-9999）%m 月份（01-12）%d 月内中的一天（0-31）%H 24小时制小时数（0-23）%I 12小时制小时数（01-12）%M 分钟数（00=59）%S 秒（00-59）%a 本地简化星期名称%A 本地完整星期名称%b 本地简化的月份名称%B 本地完整的月份名称%c 本地相应的日期表示和时间表示%j 年内的一天（001-366）%p 本地A.M.或P.M.的等价符%U 一年中的星期数（00-53）星期天为星期的开始%w 星期（0-6），星期天为星期的开始%W 一年中的星期数（00-53）星期一为星期的开始%x 本地相应的日期表示%X 本地相应的时间表示%Z 当前时区的名称 将日志同时输出到屏幕和文件 输出 logging 日志轮询使用TimedRotatingFileHandler设置日志轮转，轮转的方式有两种一种是基于时间的轮转，一种是基于日志文件大小的轮转TimedRotatingFileHandler函数参数说明1logging.handlers.TimedRotatingFileHandler(file_name,when=时间单位, interval=&apos;时间间隔&apos;,backupCount=&quot;保留的文件个数&quot;) interval:表示等待多长时间文件重建，重建的文件名等于file_name+suffix 以下面例子说明 myadd.addHandler(filehandler)的意思是给myapp这个logger添加filehandler这个handler。 执行脚本可以看见每隔一秒会自动生成一个新的日志文件，到满3个时会自动进行一次新的轮转 TimedRotatingFileHandler设置的重建时间间隔后，suffix就需要按装下面的进行配置不然删除不了，比如设置的为S则suffix为%Y-%m-%d_%H-%M-%S RotatingFileHandler安文件大小切分 logger实例的父子关系通过前面几个例子对比你应该发现了前面我用logging.basicConfig()去设置format，后面我是通过getlogger创建一个logger后，通过setformat方法去给他对应的handler设置format。 root logger是处于最顶层的logger，同时也是默认的logger，如果不创建logger实例默认调用logger.info(),logger.debug(),logger.error()使用 如何得到root logger通过logging.getLogger()和logging.getLogger(“”)得到root logger实例。logger的父子关系logger以name命名方式来表达父子关系比如父logging.getLogger(foo)子logging.getLogger(foo.tar) effective level一个looger如果没有指定level，就继承父level，如果父logger也没有就直接继承root的level。handler同样，子没有就继承父的，父也没有的话就继承root的例 root logger这里没设置logger的setLevel默认是warning，但父logger设置了，所以父logger会将自己的logger setlevel传递给root logger 调用配置好的logging正常写程序中只要配置好一个logging，其他程序只要调用他就可以了一种是通过logging.config，一种是通过模块导入 介绍方法二：比如我将配置好的logging写在test.py里面在另外一个程序中调用它12import testtest.myapp.info(&quot;test&quot;) 这样输出的就是按test.py里面myapp这个logger配置好的log了 12345http://blog.csdn.net/lizhe_dashuju/article/details/72579705http://blog.csdn.net/z_johnny/article/details/50812878http://kenby.iteye.com/blog/1162698http://blog.csdn.net/chosen0ne/article/details/7319306]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[kdump]]></title>
      <url>%2F2017%2F09%2F21%2Fkdump%2F</url>
      <content type="text"><![CDATA[什么是kdump?kdump 是一种的基于 kexec 的内核崩溃转储机制，类似飞机的黑匣子，系统一但崩溃，内核无法正常记录信息了，这时kdump将转入带第二个捕获内核，将第二个内核加载的内存中，对第一个内核的信息进行捕获。由于 kdump 利用 kexec 启动捕获内核，绕过了 BIOS，所以第一个内核的内存得以保留。这是内核崩溃转储的本质。kexec是一个快速启动机制，可以通过已经运行的内核，启动另外一个内核不需要经过bios kdump原理？kdump 需要两个不同目的的内核，生产内核和捕获内核。生产内核是捕获内核服务的对像。捕获内核会在生产内核崩溃时启动起来，与相应的 ramdisk 一起组建一个微环境，用以对生产内核下的内存进行收集和转存。 kdump配置和使用操作系统:centos7.2 安装配置分析工具crashyum install crash 安装kernel-debuginfo(需要根内核版本一一对应)1wget http://debuginfo.centos.org/7/x86_64/kernel-debuginfo-common-x86_64-3.10.0-327.el7.x86_64.rpm 12wget http://debuginfo.centos.org/7/x86_64/kernel-debuginfo-3.10.0-327.el7.x86_64.rpm` 下载完成后安装1rpm -ivh kernel-debuginfo-common-x86_64-3.10.0-327.el7.x86_64.rpm kernel-debuginfo-3.10.0-327.el7.x86_64.rpm 安装完后确认是否有1/usr/lib/debug/lib/modules/3.10.0-327.el7.x86_64/vmlinux此文件 配置kdump可以配置内核崩溃后崩溃日志存到何地 默认是放在本地/var/crash下图为配置scp，表示将log文件放到192.168.2.100/log 目录，同时key文件目录需要指定 测试kdump配置捕获占用的内存使用grubby修改grub2.cfg文件1grubby --update-kernel=DEFAULT --args=crashkernel=128M 重启服务器reboot 启动kdump1systemctl start kdump &amp;&amp; systemctl enable kdump ls /boot 会发现生成了一个kdump结尾的文件 执行以下命令让内核crash12echo 1 &gt; /proc/sys/kernel/sysrqecho c &gt; /proc/sysrq-trigger 此时系统会立刻失去连接进入捕获内核 开机后 以下参考IBM文档链接见文尾 crash解析崩溃日志vmcore-dmesg.txt可以查看错误信息(1) 错误类型首先可以在vmcore-dmesg.txt中先查看错误类型，如：divide error: 0000 [#1] SMP，除数为0造成内核崩溃，由1号CPU触发。 BUG: unable to handle kernel NULL pointer dereference at 000000000000012c，引用空指针。这样一来就能知道引发内核崩溃的错误类型。 (2) 错误地点RIP为造成内核崩溃的指令，Call Trace为函数调用栈，通过RIP和Call Trace可以确定函数的调用路径，以及在哪个函数中的哪条指令引发了错误。 例如RIP为：[&lt;ffffffff812cdb54&gt;] ? tcp_enter_loss+0x1d3/0x23b [&lt;ffffffff812cdb54&gt;]是指令在内存中的虚拟地址。tcp_enter_loss是函数名(symbol)。0x1d3是这条指令相对于tcp_enter_loss入口的偏移，0x23b是函数编译成机器码后的长度。这样一来就能确定在哪个函数中引发了错误，以及错误的大概位置。 Call Trace为函数的调用栈，是从下往上看的。可以用来分析函数的调用关系。 vmcore信息1crash /usr/lib/debug/lib/modules/3.10.0-327.el7.x86_64/vmlinux /var/crash/127.0.0.1-2017-09-20-13\:09\:30/vmcore 12345678910111213KERNEL: 系统崩溃时运行的 kernel 文件DUMPFILE: 内核转储文件CPUS: 所在机器的 CPU 数量DATE: 系统崩溃的时间TASKS: 系统崩溃时内存中的任务数NODENAME: 崩溃的系统主机名RELEASE: 和 VERSION: 内核版本号MACHINE: CPU 架构MEMORY: 崩溃主机的物理内存PID:3507表示当崩溃时3507这个bash的进程在操作PANIC: 崩溃类型，常见的崩溃类型包括：SysRq (System Request)：通常是测试使用。通过 echo c &gt; /proc/sysrq-trigger，就可以触发系统崩溃。oops：可以看成是内核级的 Segmentation Fault。应用程序如果进行了非法内存访问或执行了非法指令，会得到 Segfault 信号，一般行为是 coredump，应用程序也可以自己截获 Segfault 信号，自行处理。如果内核自己犯了这样的错误，则会弹出 oops 信息。 bt(backtrace)显示内核堆栈信息 如上输出中，以“# 数字”开头的行为调用堆栈，即系统崩溃前内核依次调用的一系列函数，通过这个可以迅速推断内核在何处崩溃。 log - dump system message bufferlog 命令可以打印系统消息缓冲区，从而可能找到系统崩溃的线索。log 命令的截图如下（为节省篇幅，已将部分行省略）： ps 命令用于显示进程的状态，（如图）带 &gt; 标识代表是活跃的进程。ps 命令的截图如下（省略部分行）： dis - disassembling instructiondis 命令用于对给定地址的内容进行反汇编。dis 命令的截图如下： 1234参考链接http://www.361way.com/centos-kdump/3751.htmlhttp://blog.csdn.net/zhangskd/article/details/38084337https://www.ibm.com/developerworks/cn/linux/l-cn-kdump4/index.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[OpenStack vlan模式下网络流量走向]]></title>
      <url>%2F2017%2F09%2F17%2Fneutron_vlan%2F</url>
      <content type="text"><![CDATA[使用openvswitch+vlan的组网以生产网vlan为101和102为例在openstack上创建个私有网络，创建了 OVS Integration bridge br-int。它的四个 Access 端口中，两个打上了内部 Tag 1，连接接入 network 1 的两个网卡；另两个端口的 VLAN Tag 为 2。创建了一对 patch port，连接 br-int 和 br-eth0。设置 br-int 中的 flow rules。对从 access ports 进入的数据帧，加上相应的 VLAN Tag，转发到 patch port；从 patch port 进入的数据帧，将 VLAN ID 101 修改为 1, 102 修改为 2，再转发到相应的 Access ports。设置 br-eth0 中的 flow rules。从 patch port 进入的数据帧，将内部 VLAN ID 1 修改为 101，内部 VLAN ID 2 修改为 102，再从 eth1 端口发出。对从 eth1 进入的数据帧做相反的处理。 br-int上的local_id和vlan_id的转换是实现多租户的主要技术。 以下介绍在云平台三中情况数据流量的走向 相同物理机上的虚拟机vm1和vm2在同一个宿主机上同一个项目下同一个子网时，两个虚机之间的流量是不需要经过交换机的，直接通过ovs的br-int桥就可以做转发了,不同项目是不通的因为br-int上有local_id用于多租户隔离。 不同物理机上的虚拟机vm1和vm2在同一个宿主机上同一个项目下同一个子网时，两个虚拟机之间的流量是需要通过物理交换机进行转发的 vm的流量首先经过tap网卡到qbr桥上，qbr桥是linux-bridge桥，曾经的ovs不支持安全组的实现所以openstack只能加一个qbr桥在上面通过iptables来实现安全组，(mitaka版本中已经没有了因为ovs已经可以通过openflow来实现了),通过veth口将br-int和qbr桥连接起来，流量到br-int上，br-int上有对应的ovs规则转发到br-eth0上，，r-eth0 中的 flow rules。从 patch port 进入的数据帧，将内部 ocal_id 修改为vlan_id，再从 eth0 端口发出。通过交换机到达另外一个计算机节点。 不同租户的不同子网的通信或虚机根外部通信需要经过网络节点，由网络节点的qroute做三层转发，也可以直接使用物理交换机。不同子网的通信，虚机会将流量丢给默认网关然后到达网络节点的qrouter通过qrouter的三层转发通讯。 使用内部vlan号是为了实现多租户网络隔离，也就是说计算节点上将数据包隔离通过br-int的loca_id，假设没有这个local_id的话，租户a的192.168.1.0/24和租户b的192.168.1.0/24在同一个计算节点的同一个br-int的网桥上，如果没有local_id隔离，网络就串了。这种是openvswitch的实现方式， linuxbridge的实现方式比如租户a的私有网络是vlan100，租户b的私户网络是vlan200，然后linuxbridge会将在这个计算节点上创建两个网桥。然后用vconfig将对应的网卡创建出子接口，挂到刚刚创的桥上，在将对应的虚机tap网卡挂载到对应的桥上。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack添加多个不同物理出口的private网络]]></title>
      <url>%2F2017%2F09%2F13%2Fopenstack_multi_private%2F</url>
      <content type="text"><![CDATA[在生产环境中我们有些虚拟机需要同时需要接入两个二层网络，一个生产网络，一个DMZ网络，通常情况下这两个网络也是对应两个不同的物理交换机的，所以对应的计算节点也应该是两个不同的物理出口。 以上图的环境为例compute-2上的虚拟机需要同时接入两个网络，生产网和DMZ网络，所以在对compute-2接完线后，还需要配置neutron，让br-int根对应的br-dmz桥通过ovs的patch口连接起来，同时生成对应的流表控制，修改openvswitch配置文件使得physnetX根br-dmz mapping。需要注意的是这里control-1接dmz的网络主要是为了dhcp 地址、metadata注入、和根三层网络的NAT。你可以通过neutron availability zone的方式把把dhcp和metadata和l3部署在网络能通的计算节点，这样就不需要控制节点接线了。 配置方法（组件为openvswitch)这里假设compute-2的br-dmz物理出口为br-ovs-bond2控制节点和计算节点都执行创建ovs桥1ovs-vsctl add-br br-dmz 添加端口1ovs-ovsctl add-port br-dmz br-dmz--br-ovs-bond2 设置端口类型为patch1ovs-vsctl set interface br-dmz--br-ovs-bond2 type=patch 设置patch口的peer端1ovs-vsctl set interface br-dmz--br-ovs-bond2 options:peer=br-ovs-bond2--br-dmz 在br-ovs-bond2上配置另外一个path口添加端口1ovs-vsctl add-port br-ovs-bond2 br-ovs-bond2--br-dmz 设置端口类型为patch1ovs-vsctl set interface br-ovs-bond2--br-dmz type=patch 设置patch口的peer端1ovs-vsctl set interface br-ovs-bond2--br-dmz options:peer=br-dmz--br-ovs-bond2 控制节点配置vim /etc/neutron/plugins/ml2/ml2_conf.ini1network_vlan_ranges =physnet2:1000:2000,physnet3 #在原来基础上新增个physnet3为我新增的，没有明确vlan-id范围可以不写 vim /etc/neutron/plugins/ml2/openvswitch_agent.ini1bridge_mappings=physnet2:br-prv,physnet3:br-dmz #在原来基础上新增个physnet3:br-dmz 重启neutron-server和neutron-openvswitch-agent1systemctl restart neutron-server 1systemctl restart neutron-openvswitch-agent 计算节点配置vim /etc/neutron/plugins/ml2/openvswitch_agent.ini 1bridge_mappings=physnet2:br-prv,physnet3:br-dmz #在原来基础上新增个physnet3:br-dmz 重启neutron-openvswitch-agent1systemctl restart neutron-openvswitch-agent 创建网络，输入对应的physnet1neutron net-create dmz_net --router:external=true --provider:network_type=vlan --provider:physical_network=physnet3 --provider:segmentation_id=108 1neutron subnet-create --gateway 20.52.15.254 --allocation-pool start=20.52.15.30,end=20.52.15.40 --disable-dhcp dmz_net 20.52.15.0/24]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[OpenStack对接vmware]]></title>
      <url>%2F2017%2F09%2F08%2Fvmware%26openstack%2F</url>
      <content type="text"><![CDATA[openstack版本：libertyvsphere版本：6.0 拓扑图 架构图 vmware对接openstack有两种驱动1，VCDriver，以nova driver的方式通过vcenter来控制计算和网络2，VMDKDriver，以cinder driver的方式通过vcenter来控制datastore。 需要注意的是本文档的对接都是对接vmware的标准交换机进行对接的，如果需要对接vmware的dvs的话需要安装另外的包并配置neutron。 这里采用vcdriver的方式这种方式目前有几个问题1，就是已经存在vmware集群的vm是不能被openstack管理了，只有新创建的机器可以被openstack管理。 2，vlan模式下，VCDriver要求vCenter里被使用的VLAN port group的名字必须和OpenStack里integration bridge（默认是br-int）的名字一样，而且vlan 号也得一样，这里我配置port group为0，表示不做限制。 3，首次创建虚机会花费很长时间，因为需要将镜像从glance通过vcenter 拷贝到cluster后端的datasotre，后面再创建速度就会很快。 要点：一台计算节点对接vsphere，需要这台计算节点能根vcenter通信。 Vsphere上的配置配置好对应的openstack计算节点对应的cluster如这个vsphere的cluster_name为UAT_Lenovo_6.0 cluster对应的storage名字 创建对应的port_group nova配置需要注意对应的物理适配器，待会需要写到配置项内。12345678910openstack平台对接vcenter配置参考compute_driver #对应的驱动host_ip #连接vsphere的ip地址host_username #管理员帐户host_password #管理员密码datastore_rege #cluster对应的后端存储cluster_name #对应的cluster 名字vlan_interface #port group对应的物理网卡vmware.integration_bridge #对应的port_groupinsecure=True #关闭ssl连接不然报下面错 修改/etc/nova/nova.conf配置文件：123456789101112[default]compute_driver = vmwareapi.VMwareVCDriver[vmware]host_ip=10.3.1.11host_username=administrator@UAT.comhost_password=1qaz@WSXdatastore_regex=Huawei_S5500_01cluster_name=UAT_Lenovo_6.0api_retry_count=10vlan_interface=vmnic0vmware.integration_bridge=br-intinsecure=True 需要注意这些参数仔细核对。 启动openstack-nova-compute服务：systemctl restart openstack-nova-compute 此时应该可以看见vmware 对应的hypervisor 转换镜像将一个qcow2的镜像转换成vmware用的vmdk格式 glance配置转换镜像格式为vmdk：qemu-img convert -f raw -O vmdk centos_6.5.raw centos_6.5.vmdk 上传镜像：1glance image-create --name=centos-6.5.vmdk --disk-format=vmdk --property hypervisor_type=&quot;vmware&quot; --property vmware_adaptertype=&quot;ide&quot; --property vmware_disktype=&quot;sparse&quot; --is-public=True --property vmware_ostype=&quot;otherLinuxGuest&quot; --container-format=bare &lt; centos_6.5.vmdk 创建虚拟机进行测试 第一次创建时间比较久因为要将镜像从glance 拷贝到vmware 的storage内 可以看见vsphere内已经有以uuid命名的vm了，表示创建成功 后面为可选配置，配置cinder对接vmware存储 cinder配置编辑/etc/cinder/cinder.conf配置文件，修改如下参数：12345volume_driver=cinder.volume.drivers.vmware.vmdk.VMwareVcVmdkDrivervolume_backend_name=vmwarevmware_host_ip=10.68.35.24vmware_host_username=administratorvmware_host_password=admin123 重启cinder-volume服务：/etc/init.d/openstack-cinder-volume restart创建多类型存储：cinder type-create vmwarecinder type-key vmware set volume_backend_name=vmwarecinder type-create cephcinder type-key ceph set volume_backend_name=DEFAULT 创建云硬盘，挂在云主机进行测试 mitaka版本对接vsphere6.5mitaka对接vsphere有个bug就是在https的连接情况下会连接失败https://review.openstack.org/#/c/341816/替换/usr/lib/python2.7/site-packages/nova/virt/vmwareapi/vmops.py 重启nova-computesystemctl restart openstack-nova-compute编辑nova.conf1234567891011[vmware]host_port=443host_ip=vcenter.xjrccu.comhost_username=administrator@xjrccu.comhost_password=Rayking,135datastore_regex=Datastore2cluster_name=LenovoServer-Clusterapi_retry_count=10vlan_interface=vmnic4vmware.integration_bridge=br-intinsecure=True 重启nova-computesystemctl restart openstack-nova输入nova hypervisor-list可以看见vmware这个hypervisor cinder对接vmware datastore1cp /usr/lib/python2.7/site-packages/oslo_vmware/wsdl/6.0 /usr/lib/python2.7/site-packages/oslo_vmware/wsdl/6.5 2、修改 enabled_backends = ceph为 enabled_backends = ceph,vmware 3、创建type-key，其中ceph的也需要创建（web界面也可以做，在全局管理里——云硬盘类型）12345cinder type-create vmwarecinder type-key vmware set volume_backend_name=vmwarecinder type-create cephcinder type-key ceph set volume_backend_name=ceph` 4、给vmware type-key添加扩展属性，vmware:storage_profile（可以在web页面里添加），但前提条件需要在vmware那边先创建好“虚机存储规则策略” 5、登录vsphere web client，先创建一个标签，在主页里，名字和类别随便起。 6、给要使用datastore分配新建标签 7、回到主页，点击策略和配置文件，点击虚拟机存储策略，点击创建一个，按照向导做。名字很重要！！ 8、对到云平台1cinder type-key vmware set vmware:storage_profile=cinder-storage-policy 即可。=号后是之前创建的策略名字！！另外，M版对接6.5，创建云硬盘，不会出现volume开头的虚机，存储里也看不到volume-id文件夹，只有挂载后才会出现，注意！！ 参考链接12345https://docs.openstack.org/mitaka/config-reference/compute/hypervisor-vmware.htmlhttp://blog.csdn.net/jmilk/article/details/52102020http://blog.csdn.net/halcyonbaby/article/details/37818789http://www.cnblogs.com/zhoumingang/p/5514556.htmlhttp://www.iyunv.com/forum.php?mod=viewthread&amp;tid=86702]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack nova server_group]]></title>
      <url>%2F2017%2F08%2F12%2Fopenstack_nova_server_group%2F</url>
      <content type="text"><![CDATA[前言：openstack在I版中增加了server group的概念，设置server group策略可以做为虚机调度的策略，目前支持的策略有”affinity”和”Anti-Affinity” 意思就是亲和性和非亲和性。 使用场景：我们知道nova创建虚机调度是通过nova-schedule的算法去筛选出对应的计算节点，然后调度到上面。在openstack上创建个app高可用集群，需要虚机分布在不同的本nova availability zone的不同的计算节点内，那么我们只需要创建个server group然后配置策略为anti-affinity，创建虚机时选择这个策略，这样创建出来的虚机就会落到不同的计算节点上。 虽然默认的nova-schedule也支持affinity和anti-affinity但nova-schedule并不能支持持久化，只在虚机创建时生效，如果后面执行migration、evacute、resize那高可用就被破坏了。 server group会将group的信息持久化，每次重新调度都能获得正确的虚机。 实现原理在nova库中增加了以下三张表instance_group_member、instance_group_policy instance_groups。 1，在创建云主机时如果加入了server-group这将instances加入到server_group_member中；2，conductor 从数据库中取得group信息，解析 group 的 policy 并设置调度参数，通过RPC让 scheduler 选择合适的宿主机；3，nova-scheduler将创建请求通过rpc传递给对应的nova-compute；4，nova-comput去访问nova-conductor去获取创建虚机的一些信息； 启用server_group修改控制节点nova.conf将scheduler_default_filters改成如下。1scheduler_default_filters=RetryFilter,AvailabilityZoneFilter,RamFilter,CoreFilter,DiskFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter 目前dashboard没有Server group接口，只能通过cli的方式。CLI命令创建server_group 创建server-group 其中test-2为group的名字 affinity为policy 查看哪些云主机使用了此server_group 创建云主机时要指定server-group的方法nova boot时加上–hint group=server_group_id如1nova boot --image IMAGE_ID --flavor flavor_id --hint group=group-1 test3]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ceph crushmap]]></title>
      <url>%2F2017%2F08%2F05%2Fcrush%2F</url>
      <content type="text"><![CDATA[前言通过crushmap划分性能型池和容量型池。在实际生产环境中，考虑到成本因素，很少土豪公司会将osd全部用ssd，但私有云上有部分业务需要高性能存储，部分业务只需要普通的sas盘做容量性存储，在公有云中也经常有，不同性能的存储卖不同的价格。在ceph中的解决方法就是通过修改crushmap，创建多种host，将osd加入到host中，在创建多个pool，每个pool对应不同的rule 本例中两个存储节点的前两个osd为ssd，后面两个osd为sas。需要划分ssd pool和sas pool，其中云主机和性能型存储用sas pool， 性能型存储为ssd pool。环境两台存储节点node-4、node-5，每个存储节点4个osd，将每个存储节点的前两个osd是ssd盘，后两个osd是sas盘 层级结构:中host bucket高于osd bucket，root bucket高于host bucket，划分步骤为：1、创建对应的host bucket 如node-4-sas、node-4-ssd、node-5-sas、node-5-ssd。2、将对应的osd划到对应的host中。3、创建root，如ssd、sas，将对应的host加到对应的root中。4、创建rules将root加入到对应的rule中。5、pool调用ruleset。 权重：修改crushmap时需要特别注意osd的权重问题,1TB OSD为1.00，500G为0.50，3TB位3.00 rules：pool所使用的规则，在crushmap中有一个对应的id，pool直接使用这个id表示这个pool的pg按这个规则进行分布。修改方法有两个 先修改ceph.conf禁止osd启动时自动修改crushmapecho ‘osd_crush_update_on_start = false’ &gt;&gt; ceph.conf 第一直接直接使用ceph命令创建bucket，move bucket，在修改rules。第二通过将crushmap导出，修改crushmap的方式。 方法1(直接通过ceph命令)：12345678910111213141516171819202122231、创建对应的root ceph osd crush add-bucket ssd root ceph osd crush add-bucket sas root2、创建对应的host ceph osd crush add-bucket node-4-sata host ceph osd crush add-bucket node-5-sata host ceph osd crush add-bucket node-4-ssd host ceph osd crush add-bucket node-5-ssd host3、移动host到对应的root下 ceph osd crush move node-4-sas root=sas ceph osd crush move node-5-sas root=sas ceph osd crush move node-4-ssd root=ssd ceph osd crush move node-5-ssd root=ssd4、将osd移到host下 ceph osd crush move osd.3 0.88 host=node-4-sas ceph osd crush move osd.4 0.88 host=node-4-sas ceph osd crush move osd.6 0.88 host=node-5-sas ceph osd crush move osd.7 0.88 host=node-5-sas ceph osd crush move osd.1 0.97 host=node-4-ssd ceph osd crush move osd.2 0.88 host=node-4-ssd ceph osd crush move osd.0 0.97 host=node-5-ssd ceph osd crush move osd.5 0.88 host=node-5-ssd 导出crushceph osd getcrushmap -o crushmap.txt反编译crushtool -d crushmap.txt -o crushmap-decompile 打开反编译后的文件修改rule（修改ruleset、和step take) 123456789101112131415161718rule ssd &#123; ruleset 1 type replicated min_size 1 max_size 10 step take ssd step chooseleaf firstn 0 type host step emit&#125;rule sas &#123; ruleset 0 type replicated min_size 1 max_size 10 step take ssd step chooseleaf firstn 0 type host step emit&#125; 重新编译crushtool -c crushmap-decompile -o crushmap-compiled 应用到集群ceph osd setcrushmap -i crushmap-compiled 创建一个新的poolceph osd pool create ssd 1024 设置ssd pool使用rules 1ceph osd pool set ssd crush_ruleset 1 校验object的pg的散落方法参考方法2 方法2(直接修改crushmap)提取现集群中使用的crushmap保存到一个文件ceph osd getcrushmap -o crushmap.txt默认导出来的crushmap打开是乱码的，需要进行反编译才能修改crushtool -d crushmap.txt -o crushmap-decompile重新编译这个crushmapcrushtool -c crushmap-decompile -o crushmap-compiled 将新的CRUSH map 应用到ceph 集群中ceph osd setcrushmap -i crushmap-compiled修改crushmap，需要注意的是bucket ID不要重复了，还有osd的weigh，我这一个osd是90G所以为0.088 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768host node-5-sas &#123; id -2 # do not change unnecessarily # weight 0.176 alg straw hash 0 # rjenkins1 item osd.6 weight 0.088 item osd.7 weight 0.088&#125;host node-4-sas &#123; id -3 # do not change unnecessarily # weight 0.176 alg straw hash 0 # rjenkins1 item osd.3 weight 0.088 item osd.4 weight 0.088&#125;root sas &#123; id -1 # do not change unnecessarily # weight 0.720 alg straw hash 0 # rjenkins1 item node-5-sas weight 0.360 item node-4-sas weight 0.360&#125;host node-5-ssd &#123; id -6 # do not change unnecessarily # weight 0.185 alg straw hash 0 # rjenkins1 item osd.0 weight 0.097 item osd.5 weight 0.088&#125;host node-4-ssd &#123; id -5 # do not change unnecessarily # weight 0.185 alg straw hash 0 # rjenkins1 item osd.1 weight 0.097 item osd.2 weight 0.088&#125;root ssd &#123; id -4 # do not change unnecessarily # weight 0.720 alg straw hash 0 # rjenkins1 item node-5-ssd weight 0.360 item node-4-ssd weight 0.360&#125;# rulesrule ssd &#123; ruleset 1 type replicated min_size 1 max_size 10 step take ssd step chooseleaf firstn 0 type host step emit&#125;rule sas &#123; ruleset 0 type replicated min_size 1 max_size 10 step take sas step chooseleaf firstn 0 type host step emit&#125; 重新编译这个crushmapcrushtool -c crushmap-decompile -o crushmap-compiled将新的CRUSH map 应用到ceph 集群中ceph osd setcrushmap -i crushmap-compiled 创建一个新的poolceph osd pool create ssd 1024 设置ssd pool使用rules 1ceph osd pool set ssd crush_ruleset 1 检查下ceph osd pool get ssd crush_ruleset 测试在ssd中写入个数据是否都落到osd.0、osd.5、osd.1、osd.2rbd create ssd/testimg -s 10240 #在ssd pool中创建块 查看pg的散落情况因为我这是两副本所以只会落到两个osd上，分别落在osd.0和osd.1上。 cinder多后端修改cinder.conf1234567891011enabled_backends=sata,ssd[ssd]volume_backend_name=ssdvolume_driver=cinder.volume.drivers.rbd.RBDDriverrbd_pool=ssdrbd_user=volumesrbd_ceph_conf=/etc/ceph/ceph.confsrbd_secret_uuid=a5d0dd94-57c4-ae55-ffe0-7e3732a24455rbd_max_clone_depth=5 secret_uuid就是对接ceph时导入的secret 创建typecinder type-create ssdcinder type-key ssd set volume_backend_name=ssd 重启cinder服务systemctl restart openstack-cinder-apisystemctl restart openstack-cinder-schedulersystemctl restart openstack-cinder-volume 通过crushmap隔离故障域，让pg分布在不同机柜上主机上但这样故域还是host，pg的分布还是比较散乱的，但集群规模大时，如果按照默认的host为故障域的话副本pg很有可能分布同一机架相邻的host的osd上，这样如果你一但此机架断电很有可能导致集群出现ERROR 但我们可以通过修改crushmap 让副本pg分布到不同机架的服务器上去，来达到隔离故障域的目的。 rack 底层的分支 下面方案如下有4个rack，每个rack有一个host ,通过修改crushmap将pg分布到不同列的rack的host上，比如可以指定compute pool的第一个副本放rackB01的node4上，第二个副本放rackC01的node-5上， 123456789node-5上，rackB01 node-4rackC01 node-5rackB02 node-6rackC02 node-7 这样做的好处就是将副本放在不同列的不同机柜上，来提高可靠性12345rack方案node-4 ---B01柜node-5 ---B02柜node-6 ---C01柜node-7 ---C02柜 将compute pool的副本都放到B、C 01柜里面讲test_pool的副本都放到B、C 02柜里面 修改后 方法一(直接通过ceph命令)：123456789101112131415161718192021222324251、添加racks: ceph osd crush add-bucket rackB01 rack ceph osd crush add-bucket rackB02 rack ceph osd crush add-bucket rackC01 rack ceph osd crush add-bucket rackC02 rack3、把每一个host移动到相应的rack下面： ceph osd crush move node-4 rack=rackB01 ceph osd crush move node-5 rack=rackB02 ceph osd crush move node-6 rack=rackC01 ceph osd crush move node-7 rack=rackC024、添加root ceph osd crush add-bucket rackB_C01 root ceph osd crush add-bucket rackB_C02 root4、把所有rack移动到对应 root 下面： ceph osd crush move rackB01 root=rackB_C01 ceph osd crush move rackB02 root=rackB_C02 ceph osd crush move rackC01 root=rackB_C01 ceph osd crush move rackC02 root=rackB_C02 导出crushmap 添加rulesceph osd getcrushmap -o crushmap.txt 反编译crushtool -d crushmap.txt -o crushmap-decompile 修改 添加如下(注意ruleset、和step take) 123456789101112131415161718rule rackB_C01 &#123; ruleset 0 type replicated min_size 1 max_size 10 step take rackB_C01 step chooseleaf firstn 0 type rack step emit&#125;rule rackB_C02 &#123; ruleset 1 type replicated min_size 1 max_size 10 step take rackB_C02 step chooseleaf firstn 0 type rack step emit&#125; 编译crushtool -c crushmap-decompile -o crushmap-compiled 将新的CRUSH map 应用到ceph 集群中ceph osd setcrushmap -i crushmap-compiled 设置test_pool套用rule rackB_C02 ceph osd pool set test_pool crush_ruleset 1 查看是否应用成功 测试在test_pool里面创建个对象应该分到B02和C02柜的host上面的osd上,rbd create test_pool/testimg -s 1024在test_pool里面创建个对象应该分到B01和C01柜的host上面的osd上,rbd create compute/test_2.img -s 1024 查看 这样副本pg就会分布在不同机柜的不同host上的osd。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack秒级创建云主级秒级创建快照原理]]></title>
      <url>%2F2017%2F08%2F05%2Fceph_clone%2F</url>
      <content type="text"><![CDATA[概念thin-provisioned当分配一个100G的空间时，并不会立刻占满这100，只是占用了一些文件的元数据，当写入数据时会根据实际的大小动态的分配,类似linux中的稀疏文件。 cow （copy on write)写时复制，也就是做快照时，先圈好个位置但这里面是空的，只有当父镜像有数据有变化时，这时会先将变化前的数据cp到快照的空间，然后继续修改父镜象，这样做的好处是可以节省大量做快照的时间和减少存储空间，因为snapshot存储的都是发生改变前的区域，其它区域都是与父镜像共享的。 优点：cow快照，拷贝只是拷贝一些元数据，所以拷贝速度特别快，同时相比全量快照，占用的空间也要少很多 缺点：cow快照后的第一次数据更新时父镜像每次要写数据，要先将原始数据读出来然后在拷贝到快照卷中，然后在写父镜像，这样进行一次更新操作就需要一次读+两次写，会降低父镜像的写性能，如果父镜像链接更多的快照，那性能会更低。 ceph快照是基于cow（copy on write）ceph 使用 COW （copy on write）方式实现 snapshot：在写入object 之前，将其拷贝出来，作为 snapshot 的 data object，然后继续修改原始数据。rbd在创建快照时，并不会向pool中创建对象，也就是说并不会占用实际的存储空间,只是增加了一些对象信息 ceph cloneclone：clone是将一个snapshot变成一个image，它是基于snapshot 创建 Clone 是将 image 的某一个 Snapshot 的状态复制变成一个 image。如 imageA 有一个 Snapshot-1，clone 是根据 ImageA 的 Snapshot-1 克隆得到 imageB。imageB 此时的状态与Snapshot-1完全一致，并且拥有 image 的相应能力，其区别在于 ImageB 此时可写。 从用户角度来看，一个 clone 和别的 RBD image 完全一样。你可以对它做 snapshot、读/写、改变大小 等等，总之从用户角度来说没什么限制。同时，创建速度很快，这是因为 Ceph 只允许从 snapshot 创建 clone，而 snapshot 需要是只读（protect）的。 向 clone 的 instance 的object 写数据ceph的克隆也是采用cow技术， 从本质上是 clone 的 RBD image 中读数据，对于不是它自己的 data objects，ceph 会从它的 parent snapshot 上读，如果它也没有，继续找它的parent image，直到一个 data object 存在。从这个过程也看得出来，该过程是缺乏效率的。 向 clone 的 instance object 写数据 Ceph 会首先检查该 clone image 上的 data object 是否存在。如果不存在，则从 parent snapshot 或者 image 上拷贝该 data object，然后执行数据写入操作。这时候，clone 就有自己的 data object 了。 flatten克隆操作本质上复制了一个 metadata object，而 data objects 是不存在的。因此在每次读操作时会先向本卷可能的 data object 访问。在返回对象不存在错误后会向父卷访问对应的对象最终决定这块数据是否存在。因此当存在多个层级的克隆链后，读操作需要更多的损耗去读上级卷的 data objects。只有当本卷的 data object 存在后(也就是写操作后)，才不需要访问上级卷。 为了防止父子层数过多，Ceph 提供了 flattern 函数将 clone 与 parent snapshot 共享的 data objects 复制到 clone，并删除父子关系。 flatten就是将 与父镜像共享的镜像都copy到clone中一层层递归，然后clone 与原来的父 snapshot 之间也不再有关系了，真正成为一个独立的 image，断绝父子关系，然后将那个snapshot删除但flatten是极其消耗网络IO的，非常耗时间 flatten有什么好处呢1，flatten后与父镜像和snapshot已经脱离关系了，可以任意删除。不然不能删除。2，clone的instance 当访问某个不存在data object，需要向上级一级查找，这是非常影响效率，而flatten后改instance这些data object都从上级copy过来，速率会更快。 使用ceph做后端存储，创建虚机如果没有使用ceph做后端存储，openstack创建虚机的流程是，先探测本地是否已经有镜像存在了，如果没有，则需要从glance仓库拷贝到对应的计算节点启动，如果有则直接启动，因此网络IO开销是非常大的如果使用Qcow2镜像格式，创建快照时需要commit当前镜像与base镜像合并并且上传到Glance中，这个过程也通常需要花费数分钟的时间。 当使用ceph做后端存储时，由于ceph是分布式存储，虚拟机镜像和根磁盘都是ceph的rbd image，所以就不需要copy到对应的计算节点，直接从原来的镜像中clone一个新镜像RBD image clone使用了COW技术，即写时拷贝，克隆操作并不会立即复制所有的对象，而只有当需要写入对象时才从parent image中拷贝对象到当前image中。因此，创建虚拟机几乎能够在秒级完成。 注意Glance使用Ceph RBD做存储后端时，镜像必须为raw格式，否则启动虚拟机时需要先在计算节点下载镜像到本地，并转为为raw格式，这开销非常大。 步骤如下1，先基于镜像 做个snapshot、并添加protect、因为clone操作只能针对snapshot、 2， 创建虚拟机根磁盘创建虚拟机时，直接从glance镜像的快照中clone一个新的RBD image作为虚拟机根磁盘: rbd clone 1b364055-e323-4785-8e94-ebef1553a33b@snap fe4c108a-7ba0-4238-9953-15a7b389e43a_disk 3，启动虚拟机启动虚拟机时指定刚刚创建的根磁盘，由于libvirt支持直接读写rbd镜像，因此不需要任何下载、导出工作。 openstack给云主机做快照openstack给云主机做快照，因为还原是基于快照重新创建个云主机，所以本质上是做一个clone操作 具体流程1基于云主机创建个snapshot---&gt;给snapshot设置只读权限（protect）-----&gt;基于该snapshot clone一个image出来----&gt;flatten操作----&gt;删除snapshot 为了秒级快照，这导致的后果就是，1，做了快照的云主机在控制台删除后在ceph存储的pool中仍然还会存在，因为你的快照根主机的images还是存在父子关系，数据还是共享的，2，云主机 xxxx_disk 存在snapshot，因为做clone是基于protect的snapshot，没flatten的话，snapshot自然没删除。3，残留云主机和snapshot没清理的话会导致上传在glance的镜像也无法删除，因为你的云主机的xxx_disk是基于你glance的镜像clone的存在父子关系不能删除。。 解决办法，后台定期flatten然后rm那些客户已经删了云主机的残留数据。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[neutron-高可用(1)--DVR]]></title>
      <url>%2F2017%2F07%2F23%2Fdvr%2F</url>
      <content type="text"><![CDATA[环境 fuel8.0 介绍DVR简称分布式路由，M版前的neutron网络结构。 无论是南北流量中的1:N还是1:1还是东西流量也好都经过网络节点，这就带来一个问题了，网络节点负载很大，往往成为整个架构的性能短板，同时还很容易出现单点故障。在Juno版本后可以配置DVR模式，每个compute节点上都配置了一个L3 agent，集群内的流量（东西流量）这里有个前提条件，同网段内主机在不同compute节点，在同一个compute节点就直接通过本机就能直接转发了。直接就通过tunnel到达另外一个compute节点，不在需要通过网络节点去做转发了，而配置了floatting ip的主机，直接通过本compute节点的L3 agent 通过br-ex连接外网。 进行深入前的一些基本概念的讲解：路由策略引用百科里的一句话路由策略是一种基于目标网络进行路由更加灵活的数据包路由转发机制。应用了路由策略，路由器将通过路由图决定如何对需要路由的数据包进行处理，路由图决定了一个数据包的下一跳转发路由器。 路由策略的种类 大体上分为两种：一种是根据路由的目的地址来进行的策略称为：目的地址路由； 另一种是根据路由源地址来进行策略实施的称为：源地址路由！ 随着路由策略的发展现在有了第三种路由方式：智能均衡的策略方式！ 使用 ip rule 操作策略路由 基于策略的路由比传统路由在功能上更强大，使用更灵活，它使网络管理员不仅能够根据目的地址而且能够根据报文大小、应用或IP源地址等属性来选择转发路径。ip rule 查看策略路由表 数字越小，优先级越高 规则0，它是优先级别最高的规则，规则规定，所有的包，都必须首先使用local表（254）进行路由。本规则不能被更改和删除。 规则32766，规定所有的包，使用表main进行路由。本规则可以被更改和删除。 规则32767，规定所有的包，使用表default进行路由。本规则可以被更改和删除。 在默认情况下进行路由时，首先会根据规则0在本地路由表里寻找路由，如果目的地址是本网络，或是广播地址的话，在这里就可以找到合适的路由；如果路由失败，就会匹配下一个不空的规则，在这里只有32766规则，在这里将会在主路由表里寻找路由;如果失败，就会匹配32767规则，即寻找默认路由表。如果失败，路由将失败。重这里可以看出，策略性路由是往前兼容的。 3232243201: from 192.168.30.1/24 lookup 3232243201 #这条规则的含义是规则3232243201 源ip为192.168.30.1/24的包，使用local表3232243201 进行路由 linux二层网络中一些基本名词 TAP设备：如kvm创建虚拟机后，宿主机上的vnet0、vnet1等，是操作系统内核中的虚拟网络设备。veth配对设备（veth pair）或ovs配对设备，它其实是一对网卡把一块叫veth，另外一块叫peer，当一个数据真被发送到其中一端中，veth的另外一端也会收到此帧。在虚拟化中常用veth pair来连接两个bridge，比如qbr网桥跟br-int网桥上的qvb-xxx和qvo-xxx就是一对veth pair。 启用DVR以下配置的前提是开启了l2_populationl2population原理介绍如下http://blog.csdn.net/cloudman6/article/details/53167522 1，安装compute节点(1)安装l3-agent(2)修改的内核参数123456vi /etc/sysctl.confnet.ipv4.ip_forward = 1net.ipv4.conf.all.rp_filter = 0net.ipv4.conf.default.rp_filter = 0sysctl -p （3）增加一块能上外网的网卡，并创建个br-ex网桥把这块网卡桥接到br-ex上12ovs-vsctl add-br br-exovs-vsctl add-port br-ex ethx (4)配置l3agent.ini123456vim /etc/neutron/l3_agent.iniinterface_driver =neutron.agent.linux.interface.OVSInterfaceDriveruse_namespaces = Trueagent_mode = dvr` vim /etc/neutron/plugins/ml2/ml2_conf.ini12[agent]enable_distributed_routing=True 重新启动 neutron-openvswitch-agent, netron-l3-agent ,openvswitch-switch 服务。 网络节点12vim /etc/neutron/l3_agent.iniagent_mode = dvr_snat 123vim /etc/neutron/plugins/ml2/ml2_conf.ini[agent]enable_distributed_routing=True 1重启neutron-server、netron-l3-agent ,openvswitch-switch 服务。 配置成功后可以neutron agent-list 看见compute的l3-agent。 原理解释1台control节点（server-4），2台compute（server-5、server-6）、外网为192.168.122.0/24在control节点上在控制台创建个网络，创建个路由器，设置网关，在路由器上将网络端口添加进去。在基于此网络创建个主机。可以发现在control节点上创建路由器时，会同时在namespace里面创建qroute-xxx和snat-xxx两个命名空间，snat-xxx命名空间会根据创建的子网来添加接口，qg-xxx和sg-xxx，其中qg-xxx上面配置了外网ip，sg-xxx配置的是本子网可用的第一个ip（前两个为网关和dhcp） 当该网络内其他主机需要上外网时，源ip会已postrouting形式，snat成路由器的网关，这步操作会在snat-xx命名空间内做 在传统网络架构下，snat操作本来是应该在qroute下进行，而在DVR模式下qroute-xxx完全是连接内网的网关。 当创建一个实例时，会自动在实例所在的compute节点创建一个qroute-xxx 此qrouter-xxx的ip和mac都与control节点的一样。 当给实例分配一个floating_ip时，在自动在实例所在的计算节点namespace里面生成fip-xxx命名空间 需要注意的是fip-xxx这个id是与你的public网络id是一致 也就是说一个public网络对应一个fip-xxx namespace,在下面会解释为什么这样做节点fg-xxx对应的是外网接口，fpr接口与qrouter接口一对veth pair DVR模式下流量的走向南北流量（1：N）所谓南北流量（1：N）实际上就是指SNAT就是子网内的主机通过路由器的floatingIP去上外网1，首先需要确认router已经发到compute节点，我这router叫test2可以看见已经在server-6上有了 我在vm（192.168.50.4）上面ping百度 因为是跨网段的，所以发送给默认路由，就是网关。在网关上路由策略可以看见发现，发给默认路由192.168.50.1的数据，最终会发给192.168.50.3 那192.168.50.3在哪呢，通过前面分析，我们知道，192.168.50.3在control节点snat-xx命名空间里面 然后在iptables里面做一次snat 就出外网了，因为我这是KVM虚拟机里面所以还要nat一层。 ping public网络 192.168.122.1 然后抓包分析 所以SNAT流程12 （compute节点） vxlan （network节点）vm---&gt;qr-xxx---&gt;br-int----&gt;br-tun---&gt; br-tun-----&gt;br-int-----&gt;snat-xxx----br-ex 南北流量（1：1）给主机192.168.50.4绑定一个floating_ip 观察control节点和compute节点的namespace发现control节点没有发生变化，而vm所在的compute的节点的namespace发现多了一个fip命名空间 注意看这个fip的ip是我们创建的外网网络id，这个在上面已经说过了。查看一个这个fip-xxx的内容 vm绑定floating_ip后如何跟外网进行通信 1，vm ping外网，首先数据包丢给默认路由，qroute，2，查找路由表 命中table 16 走rfp端口 3，然后进行snat 4，然后发送给fip-xxx命名空间 因为已经进行snat了地址变成外网的，所以这里都default route 直接给外网网关，完成 如下图来源http://www.cnblogs.com/sammyliu/p/4713562.html vm绑定vip后外网DNAT进入内网主机1，arp-proxy外网ip要ping通内网，首先需要知道内网ip的mac，fip，并没有直接绑定在一个特定的端口是怎么知道mac的呢 进行路由追踪发现 arping发现mac地址为 尾数为C7:C3的 那这个mac地址为谁的 竟然是fip-xxx的fg端口 原来是fip-xxx在fg这个端口上做了arp-proxy，这样，fg既可以响应发给它自己的arp请求，也可以响应发给经过它路由的端的arp请求 可以看见内核参数里面已经开启了 2，将包丢给路由器 我们从fip-xxx ip route 可以看见包被丢给169.254.93.254这个接口了，这个接口是qroute-xxx的一个接口 3，qroute–xxx做DNAT 1234流程如下 arp欺骗 通过fpr DNATextra-----&gt;fip--xxx----&gt;qroute-----&gt; VM 为什么要独立出一个snat namespace来做SNAT，而不继续之前的qrouter多SNAT方案？ 因为 当创建子网的第一台主机时，会同时在compute节点上也创建个qrouter-xxx的namespace，但computer节点上的qrouter并不做snat功能，只做当有绑定float_ip时 连接FIPnamespace中转，所以需要将SNAT功能独立出来，于是就有了SNATnamespace当然这个也只会在网络节点的namespace里面创建。 当虚拟机绑定fip时，为什么compute要多出一个fip namespace，不直接在qrouter上的qg直接连接br-ex？是为了减少暴露在外部网络上的mac和ip地址，所以才需要有fip的出现。由于传统router只在网络节点上，数量很少，直接在传统router的qg上配floating ip和用其mac地址应答arp请求，所以没这个问题。但是dvr router由于分布到大量的compute node（每个私有网络都可能有一个router），如果再这么做，会导致有大量router的qg接口暴露在外部网络上，因此分离出fip，而fip是按照（compute node，external network）分配的，数量少很多。一个external_network就一个另外fip也可以做集中的arp proxy，不需要把floating ip真正绑到fip的接口上。同时也可以减少交换机MAC地址表的占用，减轻交换机的负担。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux下单网卡配置属于不同vlan的ip(vlan子接口)]]></title>
      <url>%2F2017%2F05%2F23%2Flinux_vconfig%2F</url>
      <content type="text"><![CDATA[一台服务器上一块网卡想同时划到两个不同vlan，并且让他们之间可以互相通信。 1、首先确认Linux系统内核是否已经支持VLAN功能，加载了8021q模块，lsmod |grep 8021q。2、关于网卡的解释，好多人不知道网卡接口上的冒号和点号的区别，以下是一些解释（我也是从网上查的，仅供参考） a、物理网卡：物理网卡指的是服务器上实际的网络接口设备，如在系统中看到的2个物理网卡分别对应是eth0和eth1这两个网络接口。 b、子网卡：子网卡并不是实际上的网络接口设备，但是可以作为网络接口在系统中出现，如eth0:1、eth1:2这种网络接口。它们必须要依赖 于物理网卡，虽然可以与物理网卡的网络接口同时在系统中存在并使用不同的IP地址，而且也拥有它们自己的网络接口配置文件。但是当所依赖的物理网卡不启用 时（Down状态）这些子网卡也将一同不能工作。 c、虚拟VLAN网卡：这些虚拟VLAN网卡也不是实际上的网络接口设备，也可以作为网络接口在系统中出现，但是与子网卡不同的是，他们没有自己 的配置文件。他们只是通过将物理网加入不同的VLAN而生成的VLAN虚拟网卡。如果将一个物理网卡添加到多个VLAN当中去的话，就会有多个VLAN虚 拟网卡出现，他们的信息以及相关的VLAN信息都是保存在/proc/net /vlan/config这个临时文件中的，而没有独自的配置文件。它们的网络接口名是eth0.1、eth1.2这种名字。 下面为实际操作，我的环境是一台H3C S5120三层交换机，服务器有4个网卡。 交换机配置配置trunk，并且配置允许vlan id，我这里端口是g1/0/11 到g1/0/15 设置该端口为trunk模式123[H3C-Git1/0/11]port link-type trunk //vlan端口默认为access模式``` 设置允许vlan 10和20的tag通过该端口 [H3c-Git1/0/11]port trunk permit vlan 40 50 //替换后面的数字为all则是允许所有vlan tag通过该端口12345其他12，13，14，15口一样配置接下来给vlan配置ip(配置网关) [H3C]interface Vlan-interface 40[H3C-Vlan-interface1]ip address 192.168.4.254 255.255.255.0 [H3C]interface Vlan-interface 50[H3C-Vlan-interface1]ip address 192.168.5.254 255.255.255.0123456789101112交换机配置到此结束，接下来是Linux网卡配置Linux要配置多vlan口。注：当需要启用VLAN虚拟网卡工作的时候，关联的物理网卡网络接口上必须没有IP地址的配置信息。首先确认是否安装vconfig然后查看核心是否提供VLAN 功能，执行 dmesg | grep -i 802 [root@test]# dmesg | grep -i 802 802.1Q VLAN Support v1.8 Ben Greear&#x67;&#x72;&#101;&#101;&#x61;&#114;&#x62;&#64;&#x63;&#97;&#x6e;&#100;&#x65;&#x6c;&#97;&#x74;&#101;&#99;&#x68;&#x2e;&#x63;&#111;&#109; [root@test]# modprobe 8021q [root@test~]#lsmod |grep 8021q //查看系统内核是否支持802.1q协议 8021q 18633 012 vconfig add eno3 300 // eno3为物理网络接口名称，300为 802.1q tag id 也即 vlan IDvconfig add eno3 500 ifconfig eno3.300 192.168.4.5/24 upifconfig eno3.500 172.31.5.5/24 up 1234567891011最终可以在可以创建成功的vlan接口 root@node-78:~# ls /proc/net/vlan/然后把这些写进配置文件不然重启就没有了vim /etc/network/interfaces auto lo eno3iface lo inet loopbackiface eno3.300 inet static address 192.168.4.5 netmask 255.255.255.0iface eno3.500 inet static address 172.31.5.5 netmask 255.255.255.01234567891011测试在服务器上分别ping 192.168.4.254和172.31.5.254可以ping通就行。不能通的话， router -n 查看路由，是不是还是以前的老路由，是的话要删除ip route del xxxx via xxx.xxx.xxx.xxx dev 接口5,删除VLAN命令 [root@test0001~]#vconfig rem eth0.100Removed VLAN -:eth0.100:-[root@test0001~]#vconfig rem eth0.200Removed VLAN -:eth0.200:-```在openstack虚拟机里不能做操作，不过newton版开发了这个plugin叫VLAN-aware-VMshttp://blog.csdn.net/bc_vnetwork/article/details/53927687]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[boot from volume的两种方式]]></title>
      <url>%2F2017%2F05%2F08%2Fboot_from_volume%2F</url>
      <content type="text"><![CDATA[boot for volume的两种方式简介： openstack对接商业存储一般直接用cinder直接对接商业存储，但想将nova创建的云主机放商业存储上，此时需要使用boot from volume 方式一直接选择镜像，dest选择云硬盘，需要注意的是云硬盘的大小必须大于等于镜像大小。 方式二创建空云硬盘，创建主机时将选择的镜像，加载到这个空硬盘。（需要注意的是，创建的空盘大小必续要大于你选择的镜像的大小）。 方式一1nova boot --flavor 1 --nic net-id=48baa60d-b785-45c1-8f90-8467f56abb5b --block-device source=image,id=d3a0512b-8cfc-4ad4-9fd4-7b38c9a44a32,dest=volume,size=10,shutdown=preserve,bootindex=0 test 流程：先是基于image创建block volume，然后从这个volume中boot instance 。shutdown选项选为preserve, 而不要选为remove， 这样在instance关闭时， volume会被save下来；其中的size选项要求大于等于flavor中的disk大小，同时要求我们的后端存储要有大于此size大小的空间。 方法二openstack volume create –image IMAGE_ID –size SIZE_IN_GB –name CINDER_NAME 获取云硬盘id 基于云硬盘创建云主机 1nova boot --flavor 1 --nic net-id=48baa60d-b785-45c1-8f90-8467f56abb5b --boot-volume f9af8677-28cf-4dfb-8dbc-b722025f9fa0 --security-group default --admin-pass test test]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ironic-简介]]></title>
      <url>%2F2017%2F05%2F08%2Fironic%2F</url>
      <content type="text"><![CDATA[ironic简介介绍openstack中对物理机管理的组件，通过ironic可以给物理机，上电、下电、重启，自动安装操作系统，根据设定的规则模板，进行自动化配置。ironic是I版开始进入孵化，j版与nova进行集成、K版正式release,ironic原本是mirantis开发的，后面贡献到社区。部署虚机和部署物理机对底层nova调用来创建虚机，但对nova-compute的底层的hypevisor是不一样的调用虚机底层调用的驱动可能是libvirtd、vmwareapi.VMwareVCDriver等，而物理机底层驱动是ironic。 用途目前有些服务在虚机上运行达不到理想的性能，仅管openstack拥有trove、sahara、magnum等组件用于支持数据库平台、大数据平台、容器平台、但毕竟是在虚拟机里面部署服务，性能远远不够。但直接部署在物理机上又不好管理，此时ironic应运而生了。ironic可以解决物理机的添加、删除、电源管理、安装部署、未来可能能支持自动部署数据库、大数据平台、容器平台。 项目构成ironic： 包含ironic-api 和ironic-conductor进程python-ironicclinet: python clinet and CLIironic-python-agent: 一个运行在deployment ramdisk中的Python程序，用于执行一系列部署动作pyghmi: 一个python的IPMI库，可以代替IPMItoolironic-inspector: 硬件自检工具ironic-lib： ironic的通用库函数ironic-webclinet ：web consoleironic-ui：ironic的horizon插件bifrost：一套只运行Ironic的Ansible脚本 ironic组件ironic-api：负责接收请求，并且将请求传递给ironic-conductorironic-conductor：ironic中唯一一个能根数据库进行交互的组件，负责接收ironic-api的请求，然后根据请求执行相应的，创建、开机、关机、删除操作ironic-python-agent：部署裸机时，pxe启动进入的一个bootstrap镜像，此镜像是用社区diskimage-builder工具制做，镜像内安装了ironic-python-agent，进入系统后与ironic-conductor进行交互， 将本地磁盘以iscsi方式挂载到控制节点，控制节点将系统dd到裸机磁盘上 ironic用到的技术PXE+tftp+dhcp IPMI iscsi 裸机部署原理1，nova boot启动一个实例，nova-api将请求通过rabbitmq，传给nova-conductor，在传给nova-scheduler。2，nova-scheduler根据传递进来的flavor和image筛选出合适的计算节点，然后将请求传递到对应的计算节点。3，nova-compute与ironic通信，调用ironic-api。4，ironic-api将请求下发给ironic-conductor，ironic-ductor调用glance下载bootstrap镜像。在tftp-serer目录生成pxelinux配置文件，并将bootstrap镜像放到tftp目录。5，ironic-conductor通过调用ipmi driver将物理机开机，并设置以pxe的方式启动。6，物理服务器启动后，通过pxe加载了bootstrap镜像启动后，镜像内ironic-python-agent 启动，与ironic-ductor进行交互，将本地磁盘通过iscsi方式挂载到控制节点，控制节点通过dd的方式将用户选择的系统写入到磁盘中。7，写入完毕后ironic将调用ipmi driver对物理服务器进行重启操作。8，重新启动后主机，初次进入系统时，运行镜像内的cloud-init进行初始化，配置hostname和密码。这里需要注意，使用的镜像，需要提前在 /etc/sysconfig/network-scripts/里面将网卡的配置文件提交准备好不然开机，网卡会up不起来。centos7需要修改grub参数，将网卡名以ethx显示，不然以设备名显示，配置文件不好写。 流程图如下 需要注意的是镜像ironic有两种镜像一种是部署时的bootstrap镜像，这种镜像是内含了，ironic-python-agent，启动后会主动与ironic-conductor进行交互。 另外一种是用户需要安装到裸机的镜像，这种镜像是通过bootstrap，通过iscsi将磁盘挂载到控制节点dd写入的。 网络在Newton版前，ironic都是不支持多租户，都是在一个flat网络上，互相之间是没有隔离关系的。 https://www.ibm.com/developerworks/cn/cloud/library/cl-cn-virtualboxironic/index.htmlhttp://www.cnblogs.com/menkeyi/p/6063551.htmlhttps://docs.openstack.org/developer/ironic/liberty/deploy/user-guide.htmlhttps://wiki.openstack.org/wiki/Ironic]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[trove原理]]></title>
      <url>%2F2017%2F04%2F17%2Ftrove%2F</url>
      <content type="text"><![CDATA[版本：trove4.0openstack版本：liberty DBaaS是什么？字面上理解数据库即是服务，简单来说就是以服务的形式为用户提供数据库服务 在云平台上使用trove有什么优势？1，简化IT操作流程，降低使用数据库使用门槛举个例子，曾经我搭建一个LAMP网站，数据库要自己安装，创建，授权，必要的话，还要自己做主从很繁琐，而且不是专业人员也搞不定，有了Dbaas后，我只需要在控制台点几下就弄好。 2，自动化操作，自动的增、删、改、备。 3，更好的资源利用，你可以根据业务量，自由的对数据库实例进行伸缩。 架构解析 trove根其他一些openstak组件一样，它暴露一个public-api，通过这个api访问trove-service，同时也保存着一些数据库实例状态到数据库中。 组件功能trove-api用于操作请求的接收和分发操作提供REST风格的API，同时与trove-conductor和trove-taskmanager通信，一些轻量级的请求比如获取实例状态，实例数量等操作都是自身直接处理或访问conductor和trove-taskmanager处理，比较重量级的操作比如创建数据库，创建备份等操作都是通过rpc传递给trove-taskmanager，taskmanager，然后在通过调用nova、swift、neutron、cinder等组件来完成操作。 trove-conductor将vm内trove-guestagent发送的状态信息保存到数据库，与trove-guestagent的通信是通过rpc来实现的，trove-conductor这个组件的目的是为了避免创建的数据库的实例直接访问数据库，它是做为一个trove-guestagent将昨天写入数据库的中间件。 trove-taskmanager执行trove中大部分复杂的操作，请求者发送消息到task manager，task manager在请求者的上下文中调用相应的程序执行这些请求。task manager处理一些操作，包括实例的创建、删除，与其他服务如Nova、Cinder、Swift等的交互，一些更复杂的Trove操作如复制和集群，以及对实例的整个生命周期的管理。trov-taskmanager就像是其他openstak服务的客户端，如nova，swift，cinder等，当要创建数据库实例时就将请求发送给nova，让nova去创建个实例，要备份的话就调用swift接口上传备份。 trove-guestagenttrove-guestagent集成在vm镜像里面，通过监听rpc里面task manager发过来的指令，并在本地执行代码完成数据库任务，taskmanager将消息发送到guest agent，guest agent通过调用相应的程序执行这些请求。 功能原理介绍(这里只介绍对mysql数据库的功能实现，因为trove对mysql支持比较成熟）这里分别介绍三个功能的原理1，创建数据库实例2，创建数据库备份3，mysql的集群 创建数据库实例 创建数据库实例时，实际上就是通过trove-taskmanager create_instance()方法去调用nova-api 然后调用 _get_injected_files方法将guet_info和trove-guetagent.conf信息注入到 数据库实例/etc/trove/conf.d/里面，提供给guest-agent进行后续的操作。 所以4.0版本的trove并不需要一开始就将trove-guestagent.conf这个配置文件封装在镜像里面，这个配置文件是通过nova注入的，所以镜像只需要配置好guest-agent从哪里读这个配置文件。剩下的就交给trove-guestagent guest_info_file这个配置文件1234[DEFAULT]guest_id=7ec35639-5139-4ae4-8388-8101e41cc0f7 #这个ID是trove 分配给这个实例的IDdatastore_manager=mysql #采用的是哪个datastoretenant_id=f2f0e038ff0342a3bc99d8971f829ac2 #是哪个租户的 当你在控制台输入需要创建的云硬盘的大小时，实际上是通过调用taskmanager里面的_create_volume方法 收集齐上面那些信息后，然后调用nova来创建数据库实例 然后数据库实例里面的guest-agent会去读取通过nova注入的trove-guestagent.conf 去连接rpc读取taskmanager发送过来的操作请求。 剩下的一些操作比如创建数据库、创建用户这些都是taskmanager调用数据库实例里面的guest-agent去实现 guest-agent对mysql的一些操作实现是在123/usr/lib/python2.7/dist-packages/trove/guestagent/datastore/mysql/service_base.py 他里面包含了def _get_actual_db_status() #获取数据库实例状态方法 主要是通过调用/usr/bin/mysqladmin ping” 和ps -C mysqld h 去获取数据库实例状态 通过判断pid文件是否存在来判断mysql是否shutdown。 def create_database #创建数据库实例方法 def create_user #创建用户并且授权方法 后面还有删除数据库，删除用户，获取binlog，开始slave、关闭slave等方法同时需要注意的是trove创建数据库实例时，会默认为每个数据库实例同时创建一个 SecGroup-xxx xxx为主机ID的安全组。trove默认是不启动root用户的所以在控制台用户选项卡里面用户名称是不能填root的。 需要注意的是上述所有操作都是由trove用户来执行，所以必须要确认的是trove用户拥有sudo权限，否则会失败。 在执行完上述操作前此时数据库状态还是building状态的， 那就是 vm 正在启动，创建数据库，创建用户，对用户授权，同步my.cnf配置文件到数据库实例内，重启mysql ，trove-guestagent 发送 rpc 给trove-taskmanager，最后检测数据库成功运行后发送 Active 状态消息给 rpc，trove-taskmanager 收到 Active 消息后，不再发送创建数据库消息，而 trove-conductor 同时收到 trove-guestagent Active 消息后，去数据库里更新 trove instance 的状态，在trove list 就可以看见instance Active 的状态了。 备份还原 目前trove-guestagent只支持mysql的三种备份方式，一种是传统的mysql Dump方式一种是InnoBackupEx 还有增是InnoBackup的增量备份方式InnoBackupExIncremental。 备份的程序放在1/usr/lib/python2.7/dist-packages/trove/guestagent/strategies/backup 其调用方式也比较简单，就是trove-guestagent.conf里面配置了什么备份方式就调用指定类执行里面的方法，方法内也都是一些软件的命令。 需要注意的是默认不配置是调用Innobackup备份的日志会存在tmp目录下 备份完成后默认是会存储到swift内 默认备份在swift内的备份文件夹为database_backups 、开启压缩、ssl加密，分片等调用SwiftStorage类里面的save方法上传到Swift中 其中会进行文件的校验是备份上去的实际上有两个文件，第一个enc文件主要是用来分片使用，第二个文件才是主要的备份文件。 mysql主从 trove-master端先将当前数据备份到Swift—&gt;然后taskmanager重新创建个数据库实例——&gt;新创建的数据库实例将刚刚的备份从Swift拉下来根据里面的bin-log里面的GTID进行还原—-&gt;建立主从关系—检测创建成功taskmanager删除上传到Swift的备份。 备份前会做个检测，发现以前有备份就调用增量备份的方法节省空间，检测到没有就调用全备的方法这里先做个变量定义，定义好增量备份和全备的变量 if判断调用全备还是增量备份。 目前trove只支持mysql的主从不支持主主并且还是异步的主从。创建主从时，创建从同样是调用create_instance()方法 只是这里做了个判断，如果传过来了slave_of_id就调用__create_replication_slave()方法 __create_replication_slave()方法会去获去备份的ID ,然后继续调用nova创建主机 接下来操作会交给数据库实例里面的guest-agent进行操作。 guest-agent会先将备份文件从来Swift 下载下来。然后还原。接下来建立主从关系，这里要说明的是trove建立主从关系的方式有两种一种是传统的bin-log的形式，一种的用GTID的形式。 1在/usr/lib/python2.7/dist-packages/trove/common/cfg.py 这个是定义的两个不同的策略 同时也会调用不同的方法去执行当你的配置文件为12replication_strategy = MysqlBinlogReplicationreplication_namespace = trove.guestagent.strategies.replication.mysql_binlog 调用的是1/usr/lib/python2.7/dist-packages/trove/guestagent/strategies/replication/mysql_binlog.py 当你的配置文件为12replication_strategy = MysqlGTIDReplicationreplication_namespace = trove.guestagent.strategies.replication.mysql_gtid 调用的是1/usr/lib/python2.7/dist-packages/trove/guestagent/strategies/replication/mysql_gtid.py 这两个文件有何不同，方法内定义的命令不同 GTID的概述： 全局事物标识：global transaction identifieds。 GTID事物是全局唯一性的，且一个事务对应一个GTID。 一个GTID在一个服务器上只执行一次，避免重复执行导致数据混乱或者主从不一致。 GTID用来代替classic的复制方法，不在使用binlog+pos开启复制。而是使用master_auto_postion=1的方式自动匹配GTID断点进行复制。 MySQL-5.6.5开始支持的，MySQL-5.6.10后开始完善。 在传统的slave端，binlog是不用开启的，但是在GTID中，slave端的binlog是必须开启的，目的是记录执行过的GTID（强制）。 下面介绍一下mysql GTID（忘记了从哪个网上摘录的) GTID的组成部分： 前面是server_uuid：后面是一个序列号 例如：server_uuid：sequence number 7800a22c-95ae-11e4-983d-080027de205a:10 UUID：每个mysql实例的唯一ID，由于会传递到slave，所以也可以理解为源ID。 Sequence number：在每台MySQL服务器上都是从1开始自增长的序列，一个数值对应一个事务。 GTID比传统复制的优势： 更简单的实现failover，不用以前那样在需要找log_file和log_Pos。 更简单的搭建主从复制。 比传统复制更加安全。 GTID是连续没有空洞的，因此主从库出现数据冲突时，可以用添加空事物的方式进行跳过。 GTID的工作原理： master更新数据时，会在事务前产生GTID，一同记录到binlog日志中。 slave端的i/o 线程将变更的binlog，写入到本地的relay log中。 sql线程从relay log中获取GTID，然后对比slave端的binlog是否有记录。 如果有记录，说明该GTID的事务已经执行，slave会忽略。 如果没有记录，slave就会从relay log中执行该GTID的事务，并记录到binlog。 在解析过程中会判断是否有主键，如果没有就用二级索引，如果没有就用全部扫描。 要点： 1、slave在接受master的binlog时，会校验master的GTID是否已经执行过（一个服务器只能执行一次）。, 2、为了保证主从数据的一致性，多线程只能同时执行一个GTID。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[vxlan]]></title>
      <url>%2F2017%2F02%2F24%2Fvxlan%2F</url>
      <content type="text"><![CDATA[什么是vxlan？VXLAN（Virtual Extensible LAN）虚拟可扩展局域网是目前NVO3(Network Virtualization Over Layer 3 基于三层IP overlay网络构建虚拟网络技术统称NVO3)，它是目前NVO3中影响力最为广泛的一种，它通过L2 over L4 (MAC in UDP)的报文封装方式，实现基于IP overlay的虚拟局域网。 传统网络存在的一些问题1，传统的二层网络，交换机通过mac地址进行数据转发，mac地址一多会造成交换机的转发速率变慢，同时交换机的mac地址表的大小是有限制的，在云计算中，mac表容量限制了虚拟机的数量。2，VLAN的vlan id是一个12bit，最大只支持4096个vlan，这在云计算中是远远不够的。 vxlan的提出很好的解决了上述问题1，vxlan采用MAC in UDP的方式将vm主机的数据报文封装在UDP中，并使用物理网络VTEP的IP和MAC地址在外层包头封装进行数据传输，对外表现为VTEP之间的mac地址，极大的降低了交换机mac地址使用。 2，VXLAN报文中拥有一个24bit vni段，vxlan隔离不同租户就是通过vni来进行，一个vni表示一个租户，不同vni之间二层不能直接通信，但可以通过vxlan三层网关进行通信。即使多个终端用户属于同一个vni也属于同一个租户。 VXLAN的优点1，基于ip的overlay网络，仅需要边界VTEP设备间可通信2，ip overlay TTL避免环路。3，overlay+vni构建虚拟网络支持多达16M的虚拟网络，充分满足多租户的需求4，接入交换机只需要学习物理服务器的mac地址，不需要学习每台VM的mac地址减轻了交换机的负担 概念NVE(Network virtrualization Edge网络虚拟边缘节点）是实现网络虚拟化功能的实体。VTEP(vxlan tunnel end point vxlan 隧道端点）vxlan网络中的NVE以VTEP为标识；每个NVE至少得有一个VTEP；VTEP使用NVE的IP地址表示；两个VTEP之间可以确立一条VXLAN隧道，VTEP间的这条VXLAN隧道将两个NVE之间的所有的VNI所公用。VTEP可以由专有硬件来实现，也可以使用纯软件实现，硬件的实现是通过一些SDN交换机，软件的实现主要有 1，带vxlan内核模块的Linux2，openvswitch vxlan报文格式 端口默认使用4789端口报文中源IP为报文的虚拟机所属的VTEP的IP地址，目的IP为目的虚拟机所属的VTEP的IP，目标ip地址可以为单播地址也可以为组播地址 vxlan网络架构图 vxlan使用 MAC IN UDP的方式来延伸二层网络，是一种扩展大二层网络的隧道封装技术。 NVE负责将vm的报文封装，建立隧道网络。如服务器上的openvswitch就是一个NVE。 VXLAN报文的转发一种是BUM(broadcast&amp;unknown-unicast&amp;multicast)报文转发，一种是已知的单播报文转发 BUM报文转发：采用头端复制的方式(vm上层VTEP接口收到BUM报文后本地VTEP通过控制平面获取同一VNI的VTEP列表，将收到的BUM报文根据VTEP列表进行复制并发送给属于同一VNI的所有VTEP)并进行报文封装。 1，switch_1收到来终端A发出的报文后，提取报文信息，并判断报文的目的MAC地址是否为BUM MAC. 是，在对应的二层广播域内广播，并跳转2。 不是，走已知单播报文转发流程。2，switch_1根据报文中的VNI信息获取同一VNI的VTEP列表获取对应的二层广播域，进行vxlan封装，基于出端口和vxlan封装信息封装vxlan头和外层IP信息，进行二层广播。3，switch_2和switch_3上VTEP收到VXLAN报文后，根据UDP目的端口号、源和目的IP地址VNI判断VXLAN报文的合法有效性，依据VNI获取对应的二层广播域，然后进行VXLAN解封装，获取内层二层报文，判断报文的目的MAC是否为BUM MAC 是，在对应的二层广播域内非VXLAN侧进行广播处理。 不是，再判断是否是本机的MAC地址。 是，本机MAC，上送主机处理。 不是，在对应的二层广播域内查找出接口和封装信息并跳转4.4，switch_2和switch_3根据查找到的出接口和封装信息，为报文添加VLAN tag，转发给对应的终端B/C. 已知单播报文转发 1，switch_1收到来自终端A的报文，根据报文信息接入端口和VLAN信息获取对应的二层广播域，并判断报文的目的MAC是否为已知的单播MAC 是，在判断是否为本机MAC. 是，上送主机处理。 不是，在对应的二层广播域内查找出接口和封装信息，并跳装到2. 2， switch_1上VTEP根据查找到的出接口和封装信息进行VXLAN封装和报文转发。3，switch_2上VTEP收到VXLAN报文后，根据UDP目的端口号，源/目的IP地址，VNI判断VXLAN报文的合法有效性，依据VNI获取对应的二层广播域，然后进行VXLAN解封装、获取内层二层报文，判断报文的目的MAC是否为已知单播报文MAC。 是，在对应的二层广播域内查找出接口和封装信息，并跳转到4. 不是，在判断是否是本机的mac。 是，上送主机处理。 不是，走BUM报文转发流程。4，switch_2根据查找的出接口和封装信息，为报文添加VLAN tag，转发给对应的终端B。 为了防止不同vm之间通信发送arp广播引发广播风暴，vtep有一个arp proxy的功能也就是说在请求这个VTEP节点上的虚拟机的mac地址都是由VTEP的mac地址应答。也就是l2population。 vxlan网关vxlan下vm之间的通信方式有3种，同VNI下不同VM，不同VNI下跨网访问，vxlan和非vxlan之间访问。 vxlan网关分为二层网关、三层网关。二层网关：主要用于解决同VNI下不同VM之间通信，一般为vetp IP三层网关：用于解决不同VNI下跨网访问，和vxlan和非vxlan之间访问。 通俗理解就是在一台实体服务器上可以虚拟出一个交换机来，这个交换机就是VSwitch，而这个VSwitch下挂的不再是实体服务器，而是一个个VM，一个VM其实就是一个租户租用的服务器，不同租户之间肯定是不能互访的，要不然租户数据的安全性如何保障，这个隔离就是靠的VNI这个ID，其实这个你可以向下VLAN是如何隔离的，目的就是为了隔离租户。我一个租户有2个VM的话，那么我这2个之间应该可以互访吧。所以说基于VNI定义的租户，而非基于VM。内部的结构说清楚了再来说上行如何访问，在一个L2交换机你要跨网访问必然要经过网关，这个网关的IP地址就是VTEP IP，在网络上有个概念叫arp-proxy，一般用途是为了保护内部私有网络，外界的所有应答都有网关来代替回答（可以理解为门卫）。在这里外界只需要你的VTEP IP即可，对端报文到达VTEP这个网关后自己在内部走L2进行转发。因此VXLAN报文中的目的IP就是对端的网关（VTEP IP），而源地址自然也是自己的网关（VTEP IP）。而对于不同leaf上的同一VNI的VM来说，他们的VTEP IP肯定要配置相同，想下同一vlan下的服务器的网关是如何配置的就明白了 http://www.cnblogs.com/hbgzy/p/5279269.htmlhttp://blog.csdn.net/zztflyer/article/details/51883523 vxlan实验环境操作系统：centos7.2内核版本：3.10openvswitch版本：2.5 在两台虚拟机上安装好ovs，并启动这个实验目的，就是HOST 1的 br1，没有根物理网卡绑定，但可以通过建利vxlan隧道与HOST2的br1通信。 配置host1创建两个网桥br0、br1ovs-vsctl add-br br0ovs-vsctl add-br br1将eth0挂载到br0上ovs-vsctl add-port br0 eth0 将eth0的ip分配到br0上ifconfig eth0 0 &amp;&amp; ifconfig br0 192.168.1.2/24 给br1分配一个ipifconfig br1 192.168.2.2/24 配置host2创建两个网桥br0、br1ovs-vsctl add-br br0ovs-vsctl add-br br1将eth0挂载到br0上 ovs-vsctl add-port br0 eth0 将eth0的ip分配到br0上ifconfig eth0 0 &amp;&amp; ifconfig br0 192.168.1.3/24 给br1分配一个ipifconfig br1 192.168.2.3/24 此时在hots1上ping host2上的br0的ip是可以通的 ping br1的ip是pint不通的 通过建立vxlan隧道来实现br1之间通信在host1上执行给br1上增加一个vxlan0接口类型为vxlan，远程ip为 host2上的br0 ip， vni为100在host2上执行远端ip为host1上br0的ip 此时在host1上ping host2的br1的ip 就可以ping通了 在host2上的 eth0上抓包 有两层报文封装第一层源ip为192.168.2.2 目标ip为192.168.2.3，然后被udp封装，走vxlan隧道，第二层源ip为192.168.1.2 目标ip为192.168.1.3 网络包经过 vxlan interface 到达 eth1 的过程中，Linux vxlan 内核模块会将网络包二层帧封装成 UDP 包，因此，vxlan interface 必须设置适当的 MTU 来限制通过它的网络包的大小（vxlan interface 的 MTU 需要比它所绑定的物理网卡的 MTU 小 50），否则，封装后的包会被 eth1 丢弃。 VTEP (vxlan 随道端点）vswitch生成br-tun连接各个节点 像这些 为什么vlan只支持4096个因为vid只有12bit 2的12次方=4096 为什么用vxlan不用vlan？因为vlan最大只有4096个虚拟化技术的话用vlan运行vm一多，交换机上的mac地址会很多，影响交换机性能。stp算法会产生大量多路路径冗余 vxlan：建立在物理网络上的虚拟以太网vlxan是一种将二层报文用三层协议进行封装的技术，它进行传输的标识是通过VNI vni包含24bit所以vxlan最大支持2的24次方约16m个，会将二层数据包封装成udp报文通过隧道组播传输，一般配置的组播地址224.0.0.1发送arp，udp端口是4789，共50字节封装报头 UDP校验和：一般为0，非0则此包将会被丢弃。 数据包都是通过vtep进行封装传输在 OVS 中, 有几个非常重要的概念： Bridge: Bridge 代表一个以太网交换机（Switch），一个主机中可以创建一个或者多个 Bridge 设备。 Port: 端口与物理交换机的端口概念类似，每个 Port 都隶属于一个 Bridge。 Interface: 连接到 Port 的网络接口设备。在通常情况下，Port 和 Interface 是一对一的关系, 只有在配置 Port 为 bond 模式后，Port 和 Interface 是一对多的关系。 Controller: OpenFlow 控制器。OVS 可以同时接受一个或者多个 OpenFlow 控制器的管理。 datapath: 在 OVS 中，datapath 负责执行数据交换，也就是把从接收端口收到的数据包在流表中进行匹配，并执行匹配到的动作。 Flow table: 每个 datapath 都和一个“flow table”关联，当 datapath 接收到数据之后， OVS 会在flow table 中查找可以匹配的 flow，执行对应的操作, 例如转发数据到另外的端口。 veth-pair：一对接口，一个接口收另外一个接口同时也能收到，用于连接两个Brigge 设置网络接口设备的类型为“internal”。对于 internal 类型的的网络接口，OVS 会同时在 Linux 系统中创建一个可以用来收发数据的模拟网络设备。我们可以为这个网络设备配置 IP 地址、进行数据监听等等。 参考链接http://www.360doc.com/content/16/0227/12/3038654_537760576.shtmlhttp://blog.csdn.net/xjtuse2014/article/details/51376123?locationNum=7http://www.cnblogs.com/hbgzy/p/5279269.htmlhttp://blog.csdn.net/zztflyer/article/details/51883523]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[kolla 3.0.3部署openstack newton]]></title>
      <url>%2F2017%2F02%2F09%2Fkillo_newton%2F</url>
      <content type="text"><![CDATA[kolla是什么？容器化部署openstack，简而言之就是openstack里面每个组件都是用docker封装好了，对应docker的一个image。 容器化好处是什么?通过docker，降低openstack升级的难度，使升级对openstack影响最小，一旦升级失败，直接回滚旧的docker image，升级只需要三步：Pull新版本的容器镜像，停止老版本的容器服务，然后启动新版本容器。回滚也不需要重新安装包了，直接启动老版本容器服务就行，非常方便。对比过目前生产环境主流的部署方式，fuel、puppet、ROD， 我个人认为容器化部署将是未来的趋势。kolla底层通过ansoble去启动配置 已经封装好了的docker image。 缺点：要熟悉kolla，不仅仅是要熟悉openstack那么简单了，还要熟悉docker、和ansiable。 环境cpu：8核内存：16G根分区大小：100G至少需要2个网络接口（一个管理口，一个外网口）管理地址 192.168.122.77haproxy vip：192.168.122.76 安装时注意审查软件版本，我这里是安装newton版对应的是koll 3 ，2月24日发布的ocata版对应的是kolla 4 搭建docker本地镜像库 安装epel源和python-pip 安装dockercurl -sSL https://get.docker.io | bash 查看docker版本，确认docker是否安装成功。 修改docker参数，如果没有修改的话，会造成部署 neutron-dhcp-agent container 和访问APIError/HTTPError mkdir -p /etc/systemd/system/docker.service.d vi /etc/systemd/system/docker.service.d/koll.conf12345[Service]MountFlags=sharedEnvironmentFile=/etc/sysconfig/dockerExecStart=ExecStart=/usr/bin/docker daemon $other_args 保存退出修改/etc/sysconfig/docker参数并添加下面一行，目的是为了配置本地镜像仓库。other_args=&quot;--insecure-registry 192.168.122.77:4000&quot; 重启docker进程systemctl daemon-reloadsystemctl enable dockersystemctl restart docker 制做docker本地镜像仓库链接：http://pan.baidu.com/s/1gf5LTV9 密码：aysu下载解压将已经build好的openstack镜像解压到本地tar -xvf kolla-image-newton-latest.tgz 加载下载好的docker registry，docker搭建私有镜像仓库使用registry这个软件docker load &lt; ./registry-server.tar将镜像文件放到/home/下docker run -d -p 4000:5000 --restart=always -e REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY=/tmp/registry -v /home/tmp/registry/:/tmp/registry --name registry registry:2 测试是否搭建本地镜像仓库成功curl -XGET http://127.0.0.1:4000/v2/_catalog #正常情况会输出很多image name。仓库里面存在的镜像查看该镜像的tagcurl -XGET http://127.0.0.1:4000/v2/kolla/centos-binary-nova-compute/tags/list 3.0.3是版本号查看仓库数据 安装ansibleyum install ansibleclone newton分支git clone -b stable/newton https://github.com/openstack/kollacd kollacp -r etc/kolla /etc/ 安装kollapip install kolla vim /etc/kolla/globals.yml12345678openstack_release: &quot;3.0.3&quot; #上面搭建的本地仓库里面curl出来的tag号，写错了的话会导致找不到image。network_interface: &quot;ens3&quot;kolla_internal_vip_address: &quot;192.168.123.76&quot;#配置给高可用的vipneutron_external_interface: &quot;eth0&quot;docker_registry: &quot;192.168.122.77:4000&quot; #本地仓库地址docker_namespace: &quot;kolla&quot; 生成密码文件 kolla-genpwd路径/etc/kolla/password.yml 执行部署cd /root/kolla/tools/./kolla-ansible deploy -i /root/kolla/ansible/inventory/all-in-one这里需要注意的是，我是部署all-in-one，如果需要部署多节点的用multinode 修改一下里面的hostname部署完成查看容器pip install python-openstackclientpip install python-neutronclient 输入以下命令生成一个环境变量文件kolla-ansible post-deploy 文件路径为/etc/kolla/admin-openrc.sh cp /etc/kolla/admin-openrc.sh /root/ source /root/admin-openrc.sh 查看nova service查看neutron agent 打开控制台访问 帐户admin，密码通过刚刚生成的admin-openrc.sh获取 执行cd /usr/share/kolla ./init-runonce #一个测试脚本，自动下载镜像，上传，创建网络，创建路由器…… 最后创建虚拟机 需要注意的是如果是在虚拟机中测试kolla需要在宿主机上修改nova-compute的配置文件 为virt_type=qemu不然默认用的是kvm，会造成创建云主机失败。vim /etc/kolla/nova-compute/nova.conf 重启这个容器。docker restart nova_compute创建云主机测试 最后 Docker使用heka来展现收集到的日志信息。这些openstack容器的log都在heka 容器内展现默认是没有安装cinder和其他一些软件的，如果需要安装在部署时可以修改/etc/kolla/globals.yml 参考链接http://blog.csdn.net/u011211976/article/details/52085891http://docs.openstack.org/developer/kolla-ansible/quickstart.htmlhttp://geek.csdn.net/news/detail/60805?utm_source=tuicool&amp;utm_medium=referral]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack N版对接ceph jewel版]]></title>
      <url>%2F2017%2F02%2F06%2Fopenstac%26ceph%2F</url>
      <content type="text"><![CDATA[环境网络配置： public_network cluster_network 控制节点/ceph-mon 192.168.4.6 192.168.3.5 ceph-osd1 192.168.4.12 192.168.3.9 ceph-osd2 192.168.4.8 192.168.3.7 硬件配置：ceph-moncpu：4核内存：4G ceph-osd1cpu：4核内存：4G硬盘：3块100G磁盘 ceph-osd2cpu：4核内存：4G硬盘：3块100G磁盘将selinux和firewalld关闭，或配置防火墙规则 配置软件源：163 yum源wget -O /etc/yum.repo/ CentOS7-Base-163.repo http://mirrors.163.com/.help/CentOS7-Base-163.repo 配置epel源wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo 增加ceph源vim /etc/yum.repos.d/ceph.repo12345678[ceph]name=cephbaseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/x86_64/gpgcheck=0[ceph-noarch]name=cephnoarchbaseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/noarch/gpgcheck=0 清理yum缓存yum clean all创建缓存yum makecache所有节点安装cephyum install ceph –y 开始部署在部署节点安装我这里的是openstack的controller节点安装ceph-deploy，手动部署参考上一篇文章 http://www.bladewan.com/2017/01/01/manual_ceph/#more yum install ceph-deploy –y 在部署节点创建部署目录mkdir /etc/cephcd /etc/ceph/ceph-deploy new control-node1没有erro继续向下 此时目录下有ceph.conf、ceph-deploy-ceph.log、ceph.mon.keyring 修改ceph.conf添加public_network和cluster_network，同时增加允许时钟偏移vim /etc/ceph/ceph.conf 开始monitor在controller上执行ceph-deploy mon create-initial …… 部署目录多了以下文件 查看ceph状态ceph -s此时ceph状态应该是ERROR的health HEALTH_ERRno osdsMonitor clock skew detected 部署osd ceph-deploy --overwrite-conf osd prepare ceph-osd1:/dev/vdb /dev/vdc /dev/vdd ceph-osd2:/dev/vdb /dev/vdbc /dev/vdd --zap-disk部署完后查看ceph状态 查看osd tree 推送配置ceph-deploy --overwrite-conf config push ceph-osd1 ceph-osd2 重启ceph进程mon节点systemctl restart ceph-mon@control-node1.service osd节点重启systemctl restart ceph-osd@x 查看public_network和cluster_network配置是否生效 根openstack对接Ceph创建pool根据公式计算出每个pool的合适pg数 PG NumberPG和PGP数量一定要根据OSD的数量进行调整，计算公式如下，但是最后算出的结果一定要接近或者等于一个2的指数。Total PGs = (Total_number_of_OSD * 100) / max_replication_count 例：有6个osd，2副本，3个pool Total PGs =6*100/2=300每个pool 的PG=300/3=100，那么创建pool的时候就指定pg为128ceph osd pool create pool_name 128ceph osd pool create pool_name 128 创建3个poolceph osd pool create volumes 128ceph osd pool create images 128ceph osd pool create vms 128 创建nova、cinder、glance、backup用户并授权12345ceph auth get-or-create client.cinder mon &apos;allow r&apos; osd &apos;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rwx pool=images&apos;ceph auth get-or-create client.glance mon &apos;allow r&apos; osd &apos;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rwx pool=images&apos;ceph auth get-or-create client.nova mon &apos;allow r&apos; osd &apos;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rwx pool=images&apos; 生成keyring文件控制节点 ceph auth get-or-create client.cinder | tee /etc/ceph/ceph.client.cinder.keyringceph auth get-or-create client.glance | tee /etc/ceph/ceph.client.glance.keyring 修改文件属组chown cinder:cinder /etc/ceph/ceph.client.cinder.keyringchown glance:glance /etc/ceph/ceph.client.glance.keyring 计算节点ceph auth get-or-create client.cinder |tee /etc/ceph/ceph.client.cinder.keyringceph auth get-or-create client.nova |tee /etc/ceph/ceph.client.nova.keyringceph auth get-or-create client.glance |tee /etc/ceph/ceph.client.glance.keyring 修改文件属组chown cinder:cinder /etc/ceph/ceph.client.cinder.keyringchown nova:nova /etc/ceph/ceph.client.nova.keyring 在计算节点上生成uuidgen（所有计算节点用一个就可以）uuidgenf77169a0-7d56-4fc3-a436-35298081f9f9 创建secret.xmlvim secret.xml123456&lt;secret ephemeral=&apos;no&apos; private=&apos;no&apos;&gt; &lt;uuid&gt;f77169a0-7d56-4fc3-a436-35298081f9f9&lt;/uuid&gt; &lt;usage type=&apos;ceph&apos;&gt; &lt;name&gt;client.nova secret&lt;/name&gt; &lt;/usage&gt;&lt;/secret&gt; 导出nova的keyring ceph auth get-key client.nova | tee client.nova.key virsh secret-define –file secret.xmlvirsh secret-set-value –secret f77169a0-7d56-4fc3-a436-35298081f9f9 –base64 $(cat client.nova.key ) 查看secret-value 另外一台计算节点一样 修改openstack组件配置 glancecp /etc/glance/glance-api.conf /etc/glance/glance-api.conf.bakvim /etc/glance/glance-api.conf1234567891011[DEFAULT]...show_image_direct_url = True...[glance_store]stores=glance.store.rbd.Storedefault_store = rbdrbd_store_pool = imagesrbd_store_user = glancerbd_store_ceph_conf = /etc/ceph/ceph.confrbd_store_chunk_size = 8 重启glance-api和glance-registrysystemctl restart openstack-glance-apisystemctl restart openstack-glance-registry cindercp /etc/cinder/cinder.conf /etc/cinder/cinder.conf.bakvim /etc/cinder/cinder.conf12345678910111213enabled_backends = rbd[rbd]volume_driver = cinder.volume.drivers.rbd.RBDDriverrbd_pool = volumesrbd_ceph_conf = /etc/ceph/ceph.confrbd_flatten_volume_from_snapshot = falserbd_max_clone_depth = 5rbd_store_chunk_size = 4rados_connect_timeout = -1glance_api_version = 1rbd_user = cinderrbd_secret_uuid = f77169a0-7d56-4fc3-a436-35298081f9f9volume_backend_name = rbd 重启cinder-api、cinder-schedule、cinder-volume systemctl restart openstack-cinder-apisystemctl restart openstack-cinder-volumesystemctl restart openstack-cinder-scheduler nova修改nova-computecp /etc/nova/nova.conf /etc/nova/nova.conf.bak修改nova.conf添加如下配置12345678[libvirt]virt_type = qemuimages_type = rbdimages_rbd_pool = vmsimages_rbd_ceph_conf = /etc/ceph/ceph.confrbd_user = novarbd_secret_uuid = f77169a0-7d56-4fc3-a436-35298081f9f9live_migration_flag=&quot;VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE,VIR_MIGRATE_PERSIST_DEST,VIR_MIGRATE_TUNNELLED 重启nova-computesystemctl restart openstack-nova-compute 测试glance上传镜像，在ceph pool中查看是否存在openstack image create &quot;cirros&quot; --file cirros-0.3.4-x86_64-disk.img --disk-format qcow2 --container-format bare –public 存在说明对接正常 cinder在控制台创建个云硬盘，创建成功后在ceph的volumes pool池中可以看见刚刚创建的云硬盘说明创建成功 nova在控制台创建个云主记，创建成功后在ceph的vm pool池中可以看见刚刚创建的云主机说明创建成功]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[手动安装ceph]]></title>
      <url>%2F2017%2F01%2F01%2Fmanual_ceph%2F</url>
      <content type="text"><![CDATA[环镜操作系统centos6.5网络规划Cluster_net：192.168.20.0/24Public_net：192.168.2.0/24 角色 Public_net Cluster_net Ceph-mon 192.168.2.4 192.168.20.2 Ceph-mon2 192.168.2.5 192.168.20.5 Ceph-mon3 192.168.2.6 192.168.20.6 Ceph-osd1 192.168.2.7 192.168.20.7 Ceph-osd2 192.168.2.8 192.168.20.8 Ceph-osd3 192.168.2.9 192.168.20.9 绑定hosts 123456192.168.2.4 ceph-mon.novalocal mon192.168.2.5 ceph-mon2.novalocal mon2192.168.2.6 ceph-mon3.novalocal mon3192.168.2.7 ceph-osd1.novalocal osd1192.168.2.8 ceph-osd2.novalocal osd2192.168.2.9 ceph-osd3.novalocal osd3 osd数规划每台osd节点挂载3块SSD硬盘 配置yum源根据操作系统版本任意调整(所有节点)vim /etc/yum.repos.d/ceph.repo12345678910[ceph]name=cephbaseurl=http://mirrors.aliyun.com/ceph/rpm-hammer/el6/x86_64/gpgcheck=0enablde=1[ceph-noarch]name=cephnoarchbaseurl=http://mirrors.aliyun.com/ceph/rpm-hammer/el6/x86_64/gpgcheck=0enablde=1 yum cleanyum makecache 在第一台mon节点安装pssh批量执行操作yum install pssh创建hosts（会被批量执行的主机）vim /root/hosts.txt12345192.168.2.5192.168.2.6192.168.2.7192.168.2.8192.168.2.9 先进行测试 安装软件（所有节点）pssh -P -h hosts.txt yum install ceph –y同步hostspscp.pssh -h /root/hosts.txt /etc/hosts /etc/ 都success进行下一步 配置ceph-mon创建集群fsiduuidgenfdc3d06b-7e05-44a8-b982-e8e04e4156db创建/etc/ceph/ceph.conf将fsid写入配置文件[global]fsid = fdc3d06b-7e05-44a8-b982-e8e04e4156db 将ceph-mon写入配置文件(有多个mon，用逗号隔开)mon_initial_members = mon,mon2,mon3将mon节点ip写入ceph配置文件mon_host = 192.168.2.4,192.168.2.5,192.168.2.6为集群创建mon密钥ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon &#39;allow *&#39; 生成管理员密钥ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --set-uid=0 --cap mon &#39;allow *&#39; --cap osd &#39;allow *&#39; --cap mds &#39;allow&#39; 将client.admin密钥加入到ceph.mon.keyringceph-authtool/tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring 生成mon mapmonmaptool --create --add mon 192.168.2.4 --fsid fdc3d06b-7e05-44a8-b982-e8e04e4156db /tmp/monmap在每个mon节点数据目录分别在mon、mon2、mon3上执行格式为(默认cluster-name为ceph)mkdir /var/lib/ceph/mon/{cluster-name}-{hostname}如mon为mkdir /var/lib/ceph/mon/ceph-monmon初始化(-i后接hostname)ceph-mon --mkfs -i mon --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring ceph.conf增加如下配置1234public network = 192.168.2.0/24auth cluster required = cephxauth service required = cephxauth client required = cephx 创建两个空文件touch /var/lib/ceph/mon/ceph-mon/donetouch /var/lib/ceph/mon/ceph-mon/sysvinit启动第一个ceph-mon/etc/init.d/ceph status mon.mon部署第二个mon将keyring复制到mon2scp /tmp/ceph.mon.keyring mon2:/tmp/在mon2节点上建立一个/var/lib/ceph/mon/ceph-mon2目录mkdir –p /var/lib/ceph/mon/ceph-mon2/在mon2节点上初始化mon节点ceph-mon --mkfs -i mon2 --keyring /tmp/ceph.mon.keyring为了防止重新被安装，初始化一个空的done文件touch /var/lib/ceph/mon/ceph-mon2/donetouch /var/lib/ceph/mon/ceph-mon2/sysvinit/etc/init.d/ceph start mon.mon2检查进程 第三个mon同上完后检查 发现有时种偏移问题，默认ceph是0.05s，为了方便同步直接把时钟偏移设置成0.5s修改ceph配置文件增加两条配置1234[global]mon osd down out interval = 900 #设置osd节点down后900s，把此osd节点逐出ceph集群，把之前映射到此节点的数据映射到其他节点。 [mon] mon clock drift allowed = .50 同步配置 pscp.pssh -h hosts.txt /etc/ceph/ceph.conf /etc/ceph/重启进程pssh -h /root/hosts.txt /etc/init.d/ceph restart 配置osd节点将keyring同步到osd节点pscp.pssh -h /root/hosts.txt /etc/ceph/ceph.client.admin.keyring /etc/ceph/ 为osd分配uuid(我每台osd节点有3个osd所以创建3个uuid)uuidgen19ebc47d-9b29-4cf3-9720-b62896ce6f33uuidgen1721ce0b-7f65-43ef-9dfc-c49e6210d375uuidgenf5e0f54b-2ee3-41df-bf25-ad37371ab6ce 创建3个osd123ceph osd create 19ebc47d-9b29-4cf3-9720-b62896ce6f33ceph osd create 1721ce0b-7f65-43ef-9dfc-c49e6210d375ceph osd create f5e0f54b-2ee3-41df-bf25-ad37371ab6ce 创建数据存储目录1234567mkdir /var/lib/ceph/osd/&#123;cluster-name&#125;-&#123;osd-number&#125;mkdir /var/lib/ceph/osd/ceph-0mkdir /var/lib/ceph/osd/ceph-1mkdir /var/lib/ceph/osd/ceph-2mkfs.xfs /dev/vdbmkfs.xfs /dev/vdcmkfs.xfs /dev/vdd 挂载123mount -o defaults,_netdev /dev/vdb /var/lib/ceph/osd/ceph-0mount -o defaults,_netdev /dev/vdc /var/lib/ceph/osd/ceph-1mount -o defaults,_netdev /dev/vdd /var/lib/ceph/osd/ceph-2 修改fstable 初始化osd目录 123ceph-osd -i 0 --mkfs --mkkey --osd-uuid 19ebc47d-9b29-4cf3-9720-b62896ce6f33ceph-osd -i 1 --mkfs --mkkey --osd-uuid 1721ce0b-7f65-43ef-9dfc-c49e6210d375ceph-osd -i 2 --mkfs --mkkey --osd-uuid f5e0f54b-2ee3-41df-bf25-ad37371ab6ce 注册此osd密钥123ceph auth add osd.0 osd &apos;allow *&apos; mon &apos;allow profile osd&apos; -i /var/lib/ceph/osd/ceph-0/keyringceph auth add osd.1 osd &apos;allow *&apos; mon &apos;allow profile osd&apos; -i /var/lib/ceph/osd/ceph-1/keyringceph auth add osd.2 osd &apos;allow *&apos; mon &apos;allow profile osd&apos; -i /var/lib/ceph/osd/ceph-2/keyring 加入crush map ceph osd crush add-bucket ceph-osd1 hostceph osd crush move ceph-osd1 root=default 设置权重123ceph osd crush add osd.0 1.0 host=ceph-osd1 ceph osd crush add osd.1 1.0 host=ceph-osd1ceph osd crush add osd.2 1.0 host=ceph-osd1 要已守护进程开机启动，须创建一个空文件123touch /var/lib/ceph/osd/ceph-0/sysvinittouch /var/lib/ceph/osd/ceph-1/sysvinittouch /var/lib/ceph/osd/ceph-2/sysvinit 启动osd进程123/etc/init.d/ceph start osd.0/etc/init.d/ceph start osd.1/etc/init.d/ceph start osd.2 查看osd 树按上述方法配置osd2、osd33台节点添加完毕 这里有个warn，是pool的pg问题，我们重新计算，修改。查看我们现在拥有的poolceph osd lspools0 rbd, 查看默认的rbd pool的pg1234567ceph osd pool get rbd pg_numpg_num: 64ceph osd pool get rbd pg_numpg_num: 64修改为128ceph osd pool set rbd pg_num 128ceph osd pool set rbd pgp_num 128 再查看 修改ceph.conf默认数据存两份，默认pg为128同步配置重启ceph 配置ceph-radosgw直接将第一个mon节点当radosgw，ceph在0.80及以上版本可以直接使用civeweb来构建对象网关，可以不需要使用apache或nginx+fastcgi了，所以我这里用civetweb。 安装软件yum install ceph-radosgw没有www-data用户创建useradd –r –s /sbin/nologin www-data创建gateway keyring并授权 ceph auth get-or-create client.radosgw.gateway osd &#39;allow rwx&#39; mon &#39;allow rwx&#39; -o /etc/ceph/keyring.radosgw.gateway 编辑ceph.conf文件增加下面内容1234567891011[client.radosgw.gateway]keyring = /etc/ceph/keyring.radosgw.gatewayrgw_socket_path = /tmp/radosgw.sockrgw_frontends= &quot;civetweb port=7480&quot;host = ceph-monrgw_dns_name = *.domain.tldrgw_print_continue = Truergw_data = /var/lib/ceph/radosgwuser = www-datargw s3 auth use keystone = truelog file =/var/log/radosgw/client.radosgw.gateway.log 重启进程/etc/init.d/ceph-radosgw restart查看是否启动成功 新建用户radosgw-admin user create –uid=”testuser” –display-name=”First User”得到如下结果]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mariadb galera]]></title>
      <url>%2F2016%2F12%2F25%2Fmariadb-base%2F</url>
      <content type="text"><![CDATA[传统的数据同步方式传统的mysql数据库同步是使用二进制日志进行主从同步也就是semi-sync，这种同步也只是一种半同步，并不是完全的实时同步，在mysql 5.7.17中推出了MySQL Group Replication，这种实现方式与Galera cluster基本完全一样，不过MGR比Galera的优势在于 Mysql官方出品，能得到后续技术支持，Galera是芬兰的一家叫codership的公司开发的。 MGR使用Paxos协议，性能比Galera好，并且性能稳定，Galera实际只能使用三个节点，有性能和稳定性问题。 Galera目前只支持linux，MGR支持多个平台。 mysql 5.6下semi-sync数据同步方式master的dump进程需要发送binlog日志到slave，master要等待至少一个slave通知，slave将已经接收到传过来的events并写入relay log，salve发送ack信息到master，这个事务才能提交。 5.6版semi-sync的缺陷，dump thread要承但两份任务，传送binlog给slave还要等slave的返回，并且这两个任务是串行的，也就是说，dump thread要先传送binlog给slave，并还要slave返回才能传送下一个events事务，这样的dump thread成为性能瓶径。 mysql 5.7下semi-sync数据同步方式5.7.4为解决上述的问题，在master端增加了ack进程。这样事务写发送binlog与接收ack可以并行进行，提高semi-sync的效率。 GaleraGalera cluster是可以实现mariadb多主集群的软件，它目前只能用于linux和只支持XtraDB和Innodb存储引擎 ，mariadb和perconna提供了原生的Galera cluster的支持，所以可以直接使用Galera cluster，mysql要使用Galera cluster需要使用Galera cluster提供的插件。传统主从只能有一个数据库进行写服务，Galera集群，每个节点都可读可写，在Galera上层部署负载均衡软件如lvs和haproxy进行流量分担是常用做法。 原理各个节点的数据同步由wsrep接口实现。client发起一个commit命令时，所有本事务内对数据库的操作和primary_key都会打包写入到write-set，write-set随后会复制到其他节点，各个节点接收后，会根据write-set传送的primary key进行检验，检查是否与本地事务种read-write或write-write锁冲突，冲突则回滚，没有冲突就执行，完成后返回success。如果其他节点没有执行成功则存放在队列种，稍后会重新尝式。 Galera集群的优点 支持多个节点的数据写入，能保证数据的强一致性。 同步复制，各个节点无延迟，不会因为一台宕机导致数据丢失。 故障节点将自动从集群中移除 基于行级的并行复制 缺点 只支持Innodb存储引擎 只支持linux平台， 集群写入的tps由最弱节点限制，如果一个节点变慢，整的集群就是缓慢的，所以一般情况下部署，要求统一的硬件配置。 会因为网络抖动造成性能和稳定性问题。 参考链接:http://www.oschina.net/news/79983/galera-will-die-mysql-group-replication-realeasehttp://www.itbaofeng.com/?p=236]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[win10启用ubuntu子shell]]></title>
      <url>%2F2016%2F12%2F22%2Fwin10_shell%2F</url>
      <content type="text"><![CDATA[以前想用linux的shell都得装第三方软件实现，比如git bash等，在win10周年更新版14393直接集成了bash on ubuntu了集成新的 Windows Subsystem for Linux 子系统，这样能直接在 Bash on Ubuntu 环境里编译运行 Linux 程序非常爽。 开启win10开发者模式win10设置 打开控制面板，启用或关闭windows功能 点确定，然后重启然后打开powershell或cmd直接在里面输入bash命令 它会询问你是否安装canonical 分发的ubuntu，输入y，然后等待。这里推荐使用vpn，然后安装完后，设置用户名，输入root用root权限吧。安装完后，然后你打开powershell和cmd输入bash就直接进入ubuntu子系统了 可以看出子bash还是非常给力的是ubuntu 14.04 3.4的内核。以后可以直接在上面跑python脚本了。不需要专门虚拟机。 默认字体蓝色不好看清楚，修改powershell或cmd的背景色即可。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mariadb galera集群搭建]]></title>
      <url>%2F2016%2F12%2F22%2Fmariadb-galera-2%2F</url>
      <content type="text"><![CDATA[环境配置准备3台服务器 192.168.1.16 mariadb-1.novalocal mariadb-1192.168.1.19 mariadb-2.novalocal mariadb-2192.168.1.18 mariadb-3.novalocal mariadb-3 配置repo文件 vim /etc/yum.repos.d/MariaDB.repo12345[mariadb]name = MariaDBbaseurl = http://yum.mariadb.org/10.1/centos7-amd64gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDBgpgcheck=1 yum makecache 禁用防火墙和selinux 如果要使用防火墙添加允许3306、和4567 端口规则。 安装Mariadb和galera cluster（三个节点都执行） yum install MariaDB-server MariaDB-client galera 启动mariadb systemctl start mariadb 一些初始化安全配置 /usr/bin/mysql_secure_installation 关闭数据库systemctl stop mariadb 修改mariadb-1上的/etc/my.cnf.d/server.cnf文件如下1234567891011121314[galera]wsrep_provider = /usr/lib64/galera/libgalera_smm.sowsrep_cluster_address = &quot;gcomm://192.168.1.16,192.168.1.18,192.168.1.19&quot;wsrep_node_name = mariadb-1wsrep_node_address=192.168.1.16wsrep_on=ONbinlog_format=ROWdefault_storage_engine=InnoDBinnodb_autoinc_lock_mode=2#bind-address=0.0.0.0wsrep_slave_threads=1innodb_flush_log_at_trx_commit=0innodb_buffer_pool_size=122Mwsrep_sst_method=rsync 将此文件复制到mariadb-2、mariadb-3，注意要把 wsrep_node_name 和 wsrep_node_address 改成相应节点的 hostname 和 ip。 启动 MariaDB Galera Cluster 服务 /usr/sbin/mysqld –wsrep-new-cluster –user=root &amp;–wsrep-new-cluster 这个参数只能在初始化集群使用，且只能在一个节点使用。 观察日志： [root@node4 ~]# tail -f /var/log/message123150701 19:54:17 [Note] WSREP: wsrep_load(): loading provider library &apos;none&apos;150701 19:54:17 [Note] /usr/libexec/mysqld: ready for connections.Version: &apos;5.5.40-MariaDB-wsrep&apos; socket: &apos;/var/lib/mysql/mysql.sock&apos; port: 3306 MariaDB Server, wsrep_25.11.r4026 出现 ready for connections ,证明我们启动成功 查看是否启用galera插件连接mariadb,查看是否启用galera插件 目前集群机器数 查看集群状态show status like ‘wsrep%’; 查看连接的主机 另外两个节点mariadb会自动加入集群systemctl start mariadb这时查看galera集群机器数量 已经连接机器的ip 测试在mariadb-1上创建数据库，创建表，插入数据123456789101112131415161718192021MariaDB [(none)]&gt; create database test1；MariaDB [test1]&gt; insert into test values(1);Query OK, 1 row affected (0.01 sec)MariaDB [test1]&gt; insert into test values(2);Query OK, 1 row affected (0.01 sec)MariaDB [test1]&gt; insert into test values(3);Query OK, 1 row affected (0.00 sec)MariaDB [test1]&gt; insert into test values(4);Query OK, 1 row affected (0.00 sec)MariaDB [test1]&gt; insert into test values(5);Query OK, 1 row affected (0.03 sec)MariaDB [test1]&gt; insert into test values(6);Query OK, 1 row affected (0.01 sec)MariaDB [test1]&gt; insert into test values(7);Query OK, 1 row affected (0.01 sec) 在另外两台mariadb-2、mariadb-3上可以看见刚刚插入的数据，说明数据同步了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack 本地yum源搭建]]></title>
      <url>%2F2016%2F12%2F12%2Fopenstack-localyumbuild%2F</url>
      <content type="text"><![CDATA[我们在部署openstack时用国外yum源的快，经常会很慢导致等待时间太久，所以建议使用本地yum源安装 这里以newton版centos7平台为例 首先下载官方repoyum install https://rdoproject.org/repos/rdo-release.rpm这时侯/etc/yum.repos.d里面会产生3个文件12[root@test yum.repos.d]# ls rdo-qemu-ev.repo rdo-release.repo rdo-testing.repo 我这里打算用http搭建我的本地yum服务器 先安装httpdyum install httpd mkdir /var/www/html/newton 待会将同步下来的包放这个目录cd /vaw/www/html/newton yum repolist –列出你所有的仓库 前面是repo id不包含x86_64 这里我只需要openstack-newton、和rdo-qemu-ev这两个软件库 先同步openstack-newtonreposync --repoid=openstack-newton 指定要下载的仓库id，会通过网络全部下载到当前目录下载下来。 同步完第一个继续同步第二个reposync --repoid=rdo-qemu-ev 同步完后这时查看 /vaw/www/html/newton里面已经有很多包了，只有软件包，没有repodate清单，所以需要自己重新createrepo来创建清单没有createrepo自己安装，创建软件清单createrepo /var/www/html/newton/ 然后启动httpd服务，其他机器通过httpd服务来访问yum源 例如控制节点yum源配置vim /etc/yum.repos.d/openstack.repo123456[openstack]name=openstackbaseurl=http://192.168.4.3/newtonenabled=1gpgcheck=0~ yum makecache 其他节点一样。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack newton 安装]]></title>
      <url>%2F2016%2F12%2F12%2Fopenstack-newton-install%2F</url>
      <content type="text"><![CDATA[环境配置机器配置：3台8v8G的虚拟机，1台做控制节点2台做融合节点。 网络划分：192.168.122.0/24 public网络192.168.3.0/24 存储网络192.168.4.0/24管理网络、sdn隧道网络 我这里配置了本地源，就不用在手动配置官网源本地源的搭建和配置会在另外一个文档说明。节点网络信息 ： 管理网络和随道网络 存储网络 公网 控制节点 192.168.4.6 192.168.3.5 192.168.122.2 计算节点 192.168.4.7 192.168.3.6 192.168.125.5 计算节点 192.168.4.8 192.168.3.7 192.168.122.6 网络拓扑 安装chrony控制节点向外同步时间，其他节点如计算节点都直接同步控制节点yum install chrony 修改配置文件vim /etc/chrony.conf添加下面这两条server cn.ntp.org.cn iburstallow 192.168.4.0/24 设置开机启动systemctl enable chronysystemctl start chrony 其他节点：yum install chrony 修改配置文件vim /etc/chrony.conf 添加下面这两条server 192.168.4.6 iburst 设置开机启动systemctl enable chrony 启动进程systemctl start chrony 安装openstack客户端yum install python-openstackclient 安装Mariadb（数据库服务）vim /etc/my.cnf.d/openstack.cnf1234567[mysqld] bind-address = 192.168.4.6 #填写管理网段ip default-storage-engine = innodb innodb_file_per_table max_connections = 4096collation-server = utf8_general_cicharacter-set-server = utf8 设置开机启动 systemctl enable mariadb 启动Mariadb systemctl start mariadb 安装rabbitmq（用于消息队列）yum install rabbitmq-server 设置开机启动 systemctl enable rabbitmq-server 开启rabbitmq systemctl start rabbitmq-server 创建openstack用户和配置密码 rabbitmqctl add_user openstack 123456 给openstack用户配置读和写权限 rabbitmqctl set_permissions openstack &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; 安装memcache（缓存token）yum install memcached phython-memcached systemctl enable memcached systemctl start memcached 安装keystone（认证服务）连接数据库 [root@control-node1 yum.repos.d]# mysql 创建keystone数据库 create database keystone; 数据库授权 密码自己设置，这里为了方便设置123456grant all privileges on keystone.* to &#39;keystone&#39;@&#39;localhost&#39; identified by &#39;123456&#39;; grant all privileges on keystone.* to &#39;keystone&#39;@&#39;%&#39; identified by &#39;123456&#39;; keystone使用httpd的mod_wsgi运行在端口5000和35357处理认证服务请求。默认情况下，keystone服务依然监听在5000和35357端口。 安装 keystone和wsgi yum install openstack-keystone httpd mod_wsgi 修改keystone配置文件 vim /etc/keystone/keystone.conf connection = mysql+pymysql://keystone:123456@192.168.4.6/keystone #加入连接数据库配置 配置使用哪种产生token方式目前keystone支持4种(UUID、PKI、PKIZ、Fernet)这里我们配置fernethttp://www.tuicool.com/articles/jQJNFrn 这篇文章有几种模式的详细介绍。 [token]provider = fernet 同步数据库 su -s /bin/sh -c “keystone-manage db_sync” keystone #发现同步数据库就是错了也没有反应，需要检查keystone的日志文件查看是否正确 初始化fernet keykeystone-manage fernet_setup –keystone-user keystone –keystone-group keystone keystone-manage credential_setup –keystone-user keystone –keystone-group keystoen 创建Bootstrap the Identity service(就是创建admin用户的帐号信息) 12345keystone-manage bootstrap --bootstrap-password 123456 \ --bootstrap-admin-url http://192.168.4.6:35357/v3/ \ --bootstrap-internal-url http://192.168.4.6:35357/v3/ \ --bootstrap-public-url http://192.168.122.2:5000/v3/ \ --bootstrap-region-id RegionOne 配置apache服务器vim /etc/httpd/conf/httpd.conf 配置成管理网段的ipServerName 192.168.4.6 将keystone的配置文件软链接到apache的配置文件ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/ 设置开机启动systemctl enable httpd 启动httpdsystemctl start httpd检查端口 12345678lsof -i:5000COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEhttpd 18883 root 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18894 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18895 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18896 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18897 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18898 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN) 到root下创建环境变量文件 123456789vim /root/openrc#!/bin/bashexport OS_USERNAME=adminexport OS_PASSWORD=123456 #这个密码是上面Bootstrap the Identity service填的密码export OS_PROJECT_NAME=adminexport OS_USER_DOMAIN_NAME=defaultexport OS_PROJECT_DOMAIN_NAME=defaultexport OS_AUTH_URL=http://192.168.4.6:35357/v3export OS_IDENTITY_API_VERSION=3 创建域、项目、用户创建service projectopenstack project create --domain default --description &quot;Service Project&quot; service 创建user角色openstack role create user这里不创建普通用户了测试admin用户获取tokenopenstack --os-auth-url http://192.168.4.6:35357/v3 token issue 安装glance镜像服务连接Mariadb创建数据库 create database glance; 授权123grant all privileges on glance.* to &apos;glance&apos;@&apos;localhost&apos; identified by &apos;123456&apos;; grant all privileges on glance.* to &apos;glance&apos;@&apos;%&apos; identified by &apos;123456&apos;; grant all privileges on glance.* to &apos;glance&apos;@&apos;control-node1.novalocal&apos; identified by &apos;123456&apos;; control-xxx换成主机名，我这里就算api.conf里面配置的ip默认还是去连接host是主机名，所以只能在加个主机名授权。 创建glance用户并设置密码openstack user create –domain default –password-prompt glance 给glance用户添加admin角色权限openstack role add –project service –user glance admin 创建glance serviceopenstack service create –name glance –description “OpenStack Image” image 创建glance endpoint123openstack endpoint create --region RegionOne image public http://192.168.122.2:9292openstack endpoint create --region RegionOne image internal http://192.168.4.6:9292openstack endpoint create --region RegionOne image admin http://192.168.4.6:9292 安装软件包yum install openstack-glance 配置glancevim /etc/glance/glance-api.conf 配置数据库12[database]connection = mysql+pymysql://glance:123456@192.168.4.6/glance 配置glancevim /etc/glance/glance-api.conf配置数据库12345678910111213141516[database]connection = mysql+pymysql://glance:123456@192.168.4.6/glance配置keystone[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = glancepassword = 123456[paste_deploy]flavor = keystone 配置镜像存储1234[glance_store]stores = file,httpdefault_store = filefilesystem_store_datadir = /var/lib/glance/images/ vim /etc/glance/glance-registry.conf12345678910111213141516[database]connection = mysql+pymysql://glance:123456@192.168.4.6/glance配置keystone[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = glancepassword = 123456[paste_deploy]flavor = keystone 同步数据库su -s /bin/sh -c “glance-manage db_sync” glance 设置开机启动systemctl enable openstack-glance-api 启动服务systemctl start openstack-glance-api 这些做完后最好在查看下日志，看看是否有错误，每部署完一个组件都这样，这样出错的可以很快定位。 下载cirros测试一下 wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.imgopenstack image create &quot;cirros&quot; --file cirros-0.3.4-x86_64-disk.img --disk-format qcow2 --container-format bare --public glance image-list 检查一下镜像是否上传成功 安装nova组件控制节点安装创建数据库create database nova_api;create database nova; 授权12345678GRANT ALL PRIVILEGES ON nova_api.* TO &apos;nova&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON nova_api.* TO &apos;nova&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON nova_api.* TO &apos;nova&apos;@&apos;control-node1.novalocal&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON nova.* TO &apos;nova&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON nova.* TO &apos;nova&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON nova.* TO &apos;nova&apos;@&apos;control-node1.novalocal&apos; IDENTIFIED BY &apos;123456&apos;; 创建为nova组件创建用户、service、endpointopenstack user create --domain default --password-prompt nova 给nova用户添加admin角色权限openstack role add --project service --user nova admin 创建serviceopenstack service create --name nova --description &quot;OpenStack Compute&quot; compute 创建endpoint12345openstack endpoint create --region RegionOne compute public http://192.168.122.2:8774/v2.1/%\(tenant_id\)s openstack endpoint create --region RegionOne compute internal http://192.168.4.6:8774/v2.1/%\(tenant_id\)s openstack endpoint create --region RegionOne compute admin http://192.168.4.6:8774/v2.1/%\(tenant_id\)s 安装nova-api组件yum install openstack-nova-api openstack-nova-conductor openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler 配置novavim /etc/nova/nova.conf 123456789101112131415161718192021222324252627282930313233[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6 #配置rabbitmq帐户和密码my_ip = 192.168.4.6use_neutron = Truefirewall_drive = nova.virt.firewall.NoopFirewallDriverenabled_apis=osapi_compute,metadataauth_strategy=keystone[api_database]connection = mysql+pymysql://nova:123456@192.168.4.6/nova_api #配置nova连接数据库[database]connection = mysql+pymysql://nova:123456@192.168.4.6/nova[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = novapassword = 123456配置novncnovncproxy_port=6080novncproxy_base_url=http://211.156.182.144:6080/vnc_auto.htmlvncserver_listen=192.168.4.6[glance]api_servers = http://192.168.4.6:9292[oslo_concurrency]lock_path = /var/lib/nova/tmp 同步数据库su -s /bin/sh -c “nova-manage api_db sync” novasu -s /bin/sh -c “nova-manage db sync” nova 1234567891011systemctl enable openstack-nova-api.service \openstack-nova-consoleauth.service \openstack-nova-scheduler.service \openstack-nova-conductor.service \openstack-nova-novncproxy.servicesystemctl start openstack-nova-api.service \openstack-nova-consoleauth.service \openstack-nova-scheduler.service \openstack-nova-conductor.service \openstack-nova-novncproxy.service 计算机节点安装安装nova-computeyum install openstack-nova-compute 配置nova-computevim /etc/nova/nova.conf123456789101112131415161718192021222324252627282930313233[DEFAULT]enabled_apis = osapi_compute,metadatatransport_url = rabbit://openstack:123456@192.168.4.6 #配置rabbitmq帐号和密码auth_strategy = keystonemy_ip = 192.168.4.7use_neutron = Truefirewall_driver = nova.virt.firewall.NoopFirewallDriver[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = novapassword = 123456[vnc]enabled=truevncserver_listen=0.0.0.0vncserver_proxyclient_address=192.168.4.7 #填写本机ipnovncproxy_base_url=http://211.156.182.144:6080/vnc_auto.html #这个填你要用控制节点的public ip[glance]api_servers = http://192.168.4.6:9292配置锁路径[oslo_concurrency]lock_path = /var/lib/nova/tmp[libvirt]virt_type = qemu #物理服务器就配置kvm虚拟机就配置qemu 设置开机启动systemctl enable libvirtd.service openstack-nova-compute.service 启动nova-computesystemctl start libvirtd.service openstack-nova-compute.service 在控制节点查看检查一下compute进程根控制节点连接 配置neutron控制节点安装我这里使用openvswitch不使用linux bridge，因为openvswitch功能比linux Brige功能强太多了。但配置稍微复杂点。 创建数据库create database neutron; 授权123GRANT ALL PRIVILEGES ON neutron.* TO &apos;neutron&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON neutron.* TO &apos;neutron&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON neutron.* TO &apos;neutron&apos;@&apos;control-node1.novalocal&apos; IDENTIFIED BY &apos;123456&apos;; 创建neutron用户并设置密码openstack user create --domain default --password-prompt neutron 给neutron用户添加admin角色权限openstack role add --project service --user neutron admin 创建neutron serviceopenstack service create --name neutron --description &quot;OpenStack Networking&quot; network 创建neutron endpoint123openstack endpoint create --region RegionOne network public http://192.168.122.2:9696 openstack endpoint create --region RegionOne network admin http://192.168.4.6:9696 openstack endpoint create --region RegionOne network internal http://192.168.4.6:9696 安装neutron组件yum -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch vim /etc/neutron/neutron.conf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[DEFAULT]service_plugins = routertransport_url = rabbit://openstack:123456@192.168.4.6auth_strategy = keystonenotify_nova_on_port_status_changes = Truenotify_nova_on_port_data_changes = Truestate_path = /var/lib/neutronuse_syslog = Truesyslog_log_facility = LOG_LOCAL4log_dir =/var/log/neutroncore_plugin = neutron.plugins.ml2.plugin.Ml2Pluginbase_mac = fa:16:3e:00:00:00mac_generation_retries = 32dhcp_lease_duration = 600dhcp_agent_notification = Trueallow_bulk = Trueallow_pagination = Falseallow_sorting = Falseallow_overlapping_ips = Trueadvertise_mtu = Trueagent_down_time = 30router_scheduler_driver = neutron.scheduler.l3_agent_scheduler.ChanceSchedulerallow_automatic_l3agent_failover = Truedhcp_agents_per_network = 2api_workers = 9rpc_workers = 9network_device_mtu=1450[database]connection = mysql+pymysql://neutron:123456@192.168.4.6/neutron[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = neutronpassword = 123456[nova]auth_url = http://192.168.4.6:35357auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = novapassword = 123456[oslo_concurrency]lock_path = /var/lib/neutron/tmp 配置modular layer 2（ml2）插件 vim /etc/neutron/plugins/ml2/ ml2_conf.ini1234567891011121314151617181920212223242526272829[DEFAULT]type_drivers = flat,vxlantenant_network_types = vxlanmechanism_drivers = openvswitch,l2populationextension_drivers = port_security[ml2]path_mtu = 1450type_drivers = flat,vxlantenant_network_types = vxlanphysical_network_mtus =physnet1:1500[ml2_type_flat]flat_networks =*[ml2_type_vxlan]vni_ranges =2:65535vxlan_group =224.0.0.1[securitygroup]enable_security_group = Truefirewall_driver=neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver[ovs]local_ip=192.168.4.6tunnel_bridge=br-tunenable_tunneling=Trueintegration_bridge=br-intbridge_mappings=physnet1:br-ex 配置l3-agentvim /etc/neutron/l3_agent.ini12345678910[DEFAULT]debug = Falseinterface_driver =neutron.agent.linux.interface.OVSInterfaceDriverhandle_internal_only_routers = Truemetadata_port = 8775send_arp_for_ha = 3periodic_interval = 40periodic_fuzzy_delay = 5enable_metadata_proxy = Truerouter_delete_namespaces = True 配置dhcp_agentvim /etc/neutron/dhcp_agent.ini12345678910[DEFAULT]resync_interval = 30interface_driver =neutron.agent.linux.interface.OVSInterfaceDriverenable_isolated_metadata = Trueenable_metadata_network = Falsedhcp_domain = openstacklocaldhcp_broadcast_reply = Falsedhcp_delete_namespaces = Trueroot_helper=sudo neutron-rootwrap /etc/neutron/rootwrap.confstate_path=/var/lib/neutron vim /etc/neutron/plugins/ml2/openvswitch_agent.ini1234567891011121314151617[agent]polling_interval = 2tunnel_types = vxlanvxlan_udp_port = 4789l2_population = Trueprevent_arp_spoofing = Falseextensions =[ovs]local_ip=192.168.4.6tunnel_bridge=br-tunenable_tunneling=Trueintegration_bridge=br-intbridge_mappings=physnet1:br-ex[securitygroup]firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriverenable_security_group = true 设置openvswitch开机启动systemctl enable openvswitch.service 启动openvswitchsystemctl start openvswitch 创建br-ex br-tun br-intovs-vsctl add-br br-intovs-vsctl add-br br-exovs-vsctl add-br br-tun 将上外网网卡挂载到br-ex上ovs-vsctl add-port br-ex eth2 设置开机启动项systemctl enable neutron-openvswitch-agent.service启动进程systemctl start neutron-openvswitch-agent.service 配置计算节点neutron配置一下内核参数修改配置文件 /etc/sysctl.conf12net.ipv4.conf.all.rp_filter=0net.ipv4.conf.default.rp_filter=0 sysctl –pyum -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch vim /etc/neutron/neutron.conf 1234567891011121314151617[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6auth_strategy = keystone[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = neutronpassword = 123456[oslo_concurrency]lock_path = /var/lib/neutron/tmp vim /etc/neutron/plugins/ml2/ml2_conf.ini1234567891011[ml2]path_mtu = 1450type_drivers = flat,vxlantenant_network_types = vxlanphysical_network_mtus =physnet1:1500mechanism_drivers = openvswitch,l2populationextension_drivers = port_security[securitygroup]enable_ipset = truefirewall_driver=neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver vim /etc/neutron/plugins/ml2/openvswitch_agent.ini1234567891011121314151617181920[ovs]local_ip=192.168.4.7tunnel_bridge=br-tunenable_tunneling=Trueintegration_bridge=br-intbridge_mappings=physnet1:br-ex[agent]enable_distributed_routing=Trueprevent_arp_spoofing=Truearp_responder=Truepolling_interval=2drop_flows_on_start=Falsevxlan_udp_port=4789l2_population=Truetunnel_types=vxlan[securitygroup]firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriverenable_security_group = true systemctl enable openvswitch.servicesystemctl start openvswitch.service 创建br-ex、br-int、br-tunovs-vsctl add-br br-intovs-vsctl add-br br-exovs-vsctl add-br br-tun vim /etc/nova/nova.conf123456789101112131415[DEFAULT]network_api_class = nova.network.neutronv2.api.APIsecurity_group_api = neutronlinuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver[neutron]url = http://192.168.4.6:9696auth_url = http://192.168.4.6:35357auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = neutronpassword = 123456 systemctl restart neutron-openvswitch-agent.servicesystemctl restart openstack-nova-compute 安装完后可以在在控制节点检查是否安装成功 安装控制台yum install openstack-dashboardvim /etc/openstack-dashboard/local_settings12345678910111213141516171819202122232425262728这里配置控制节点ipOPENSTACK_HOST = &quot;192.168.4.6&quot;配置允许所有节点访问ALLOWED_HOSTS = [&apos;*&apos;, ]配置memcacheSESSION_ENGINE = &apos;django.contrib.sessions.backends.cache&apos;CACHES = &#123; &apos;default&apos;: &#123; &apos;BACKEND&apos;: &apos;django.core.cache.backends.memcached.MemcachedCache&apos;, &apos;LOCATION&apos;: &apos;192.168.4.6:11211&apos;, &#125;&#125;配置keystone v3验证OPENSTACK_KEYSTONE_URL = &quot;http://%s:5000/v3&quot; % OPENSTACK_HOST配置域OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = &apos;default&apos;配置api版本OPENSTACK_API_VERSIONS = &#123; &quot;identity&quot;: 3, &quot;image&quot;: 2, &quot;volume&quot;: 2,&#125;设置通过控制台默认创建用户的角色是userOPENSTACK_KEYSTONE_DEFAULT_ROLE = &quot;user&quot; 重启服务systemctl restart httpd.service memcached.service 通过http://control_ip/dashboard可以访问 Admin 登录，密码是你通过keystone创建的，如果不记得查看openrc 创建flat网络做float_ip池管理员—&gt;网络——&gt;创建网络 Phynet1是在ml2.ini里面bridge_mappings定义的br-ex对应的名字，创建完后增加子网，然后在创建个普通网络，创建个路由器，路由器绑定普通子网，创建个主机配置，然后创建vm加入到你创建的普通网络 这时在vm所在的计算节点或控制节点 ovs-vsctl show 可以看见计算节点根网络节道隧道已经建立。 Cinder配置配置控制节点创建数据库create database cinder;用户授权123GRANT ALL PRIVILEGES ON cinder.* TO &apos;cinder&apos;@&apos;localhost&apos; identified by &apos;123456&apos;; GRANT ALL PRIVILEGES ON cinder.* TO &apos;cinder&apos;@&apos;%&apos; identified by &apos;123456&apos;; GRANT ALL PRIVILEGES ON cinder.* TO &apos;cinder&apos;@&apos;control-node1.novalocal&apos; identified by &apos;123456&apos;; 创建用户openstack user create --domain default --password-prompt cinder 给cinder用户赋予admin权限openstack role add --project service --user cinder admin openstack service create --name cinder --description &quot;OpenStack Block Storage&quot; volume openstack service create --name cinderv2 --description &quot;OpenStack Block Storage&quot; volumev2 创建endpoint1234567891011121314151617openstack endpoint create --region RegionOne \volume public http://192.168.122.2:8776/v1/%\(tenant_id\)sopenstack endpoint create --region RegionOne \volume internal http://192.168.4.6:8776/v1/%\(tenant_id\)sopenstack endpoint create --region RegionOne \volume admin http://192.168.4.6:8776/v1/%\(tenant_id\)sopenstack endpoint create --region RegionOne \volumev2 public http://192.168.122.2:8776/v2/%\(tenant_id\)sopenstack endpoint create --region RegionOne \volumev2 internal http://192.168.4.6:8776/v2/%\(tenant_id\)sopenstack endpoint create --region RegionOne \volumev2 admin http://192.168.4.6:8776/v2/%\(tenant_id\)s 安装cinderyum install openstack-cinder vim /etc/cinder/cinder.conf1234567891011121314151617181920[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6auth_strategy = keystone[database]connection = mysql+pymysql://cinder:123456@192.168.4.6/cinder[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = cinderpassword = 123456[oslo_concurrency]lock_path = /var/lib/cinder/tmp 同步数据库su -s /bin/sh -c “cinder-manage db sync” cinder 配置计算机节点使用cindervim /etc/nova/nova.conf [cinder]os_region_name = RegionOne 重启服务systemctl restart openstack-nova-api.service 设置开机自启cindersystemctl enable openstack-cinder-api.service openstack-cinder-scheduler.service systemctl start openstack-cinder-api.service openstack-cinder-scheduler.service 配置一个存储节点安装lvmyum install lvm2 systemctl enable lvm2-lvmetad.servicesystemctl start lvm2-lvmetad.service 创建个lvm卷pvcreate /dev/vdb 创建vgvgcreate cinder-volumes /dev/vdb vim /etc/lvm/lvm.conf123devices &#123;filter = [ &quot;a/vdb/&quot;, &quot;r/.*/&quot;]&#125; 安装软件yum install openstack-cinder targetcli python-keystone 修改cinder配置文件vim /etc/cinder/cinder.conf1234567891011121314151617181920212223242526272829[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6verbose = Trueauth_strategy = keystoneenabled_backends = lvmglance_api_servers = http://192.168.4.6:9292[database]connection = mysql+pymysql://cinder:123456@192.168.4.6/cinder[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = cinderpassword = 123456[lvm]volume_driver = cinder.volume.drivers.lvm.LVMVolumeDrivervolume_group = cinder-volumesiscsi_protocol = iscsiiscsi_helper = lioadm[oslo_concurrency]lock_path = /var/lib/cinder/tmpsystemctl enable openstack-cinder-volume.service target.service systemctl start openstack-cinder-volume.service target.service 到控制台创建个卷，并挂载到云主机。 Ceilometer配置Ceilometer使用Mongdb存储meter数据，所以需要先在控制节点安装Mongdb 在控制节点安装Mongdbyum install mongodb-server mongodb 配置Mongdbvim /etc/mongod.confsmallfiles = true #限制日志大小创建Mongdb数据库和帐户授权替换123456为你自己设置的密码创建用户openstack user create –domain default –password-prompt ceilometer 给ceilometer用户添加admin角色权限openstack role add –project service –user ceilometer admin创建ceilometer serviceopenstack service create –name ceilometer –description “Telemetry” metering 创建ceilometer endpoint123openstack endpoint create --region RegionOne metering public http://192.168.122.2:8777openstack endpoint create --region RegionOne metering admin http://192.168.4.6:8777openstack endpoint create --region RegionOne metering internal http://192.168.4.6:8777 安装包yum install openstack-ceilometer-api \openstack-ceilometer-collector \openstack-ceilometer-notification \openstack-ceilometer-central \python-ceilometerclient 配置ceilometervim /etc/ceilometer/ceilometer.conf123456789101112131415161718192021222324252627282930[DEFAULT]rpc_backend = rabbitauth_strategy = keystone[oslo_messaging_rabbit]rabbit_host = 192.168.4.6rabbit_userid = openstackrabbit_password = 123456[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = ceilometerpassword = 123456[service_credentials]auth_type = passwordauth_url = http://192.168.4.6:5000/v3project_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = ceilometerpassword = 123456 ##密码替换成在keystone创建ceilometer时设置的密码interface = internalURLregion_name = RegionOne 创建ceilometer的vhost vim /etc/httpd/conf.d/wsgi-ceilometer.conf 123456789101112Listen 8777&lt;VirtualHost *:8777&gt; WSGIDaemonProcess ceilometer-api processes=2 threads=10 user=ceilometer group=ceilometer display-name=%&#123;GROUP&#125; WSGIProcessGroup ceilometer-api WSGIScriptAlias / /usr/lib/python2.7/site-packages/ceilometer/api/app.wsgi WSGIApplicationGroup %&#123;GLOBAL&#125; ErrorLog /var/log/httpd/ceilometer_error.log CustomLog /var/log/httpd/ceilometer_access.log combined&lt;/VirtualHost&gt;WSGISocketPrefix /var/run/httpd 重启httpdsystemctl reload httpd.service设置服务开机启动systemctl enable openstack-ceilometer-notification.service \openstack-ceilometer-central.service \openstack-ceilometer-collector.service 启动进程systemctl start openstack-ceilometer-notification.service \openstack-ceilometer-central.service \openstack-ceilometer-collector.service 配置glance的ceilometer统计vim /etc/glance/glance-api.conf 12345678910[DEFAULT]rpc_backend = rabbit[oslo_messaging_amqp]driver = messagingv2[oslo_messaging_rabbit]rabbit_host = 192.168.4.6rabbit_userid = openstackrabbit_password = 123456 重启进程systemctl restart openstack-glance-api.service openstack-glance-registry.service 配置nova的ceilometer统计安装软件yum install openstack-ceilometer-compute python-ceilometerclient python-pecan 123456789101112131415161718192021222324252627282930[DEFAULT]rpc_backend = rabbitauth_strategy = keystone[oslo_messaging_rabbit]rabbit_host = 192.168.4.6rabbit_userid = openstackrabbit_password = 123456[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = ceilometerpassword = 123456 #将密码替换成keystone创建ceilometer用户时设置的密码[service_credentials]auth_url = http://192.168.4.6:5000project_domain_id = defaultuser_domain_id = defaultauth_type = passwordusername = ceilometerproject_name = servicepassword = 123456 #将密码替换成keystone创建ceilometer用户时设置的密码interface = internalURLregion_name = RegionOne 修改nova-compute配置文件 vim /etc/nova/nova.conf 1234567[DEFAULT]instance_usage_audit = Trueinstance_usage_audit_period = hournotify_on_state_change = vm_and_task_state[oslo_messaging_amqp]driver = messagingv2 设置开机启动systemctl enable openstack-ceilometer-compute.service启动ceilometer-compute进程systemctl start openstack-ceilometer-compute.service重启nova-computesystemctl restart openstack-nova-compute.service 配置块设备使用ceilometer计量服务 验证ceilometer meter-list 正常情况下是会出现如上图一些资源的数据的，但我这里默认报 打debug 访问被拒绝解决办法：修改httpd.conf systemctl restart httpd在测试应该没问题了。 Aodh报警服务：创件aodh数据库create database aodh; 授权123GRANT ALL PRIVILEGES ON aodh.* TO &apos;aodh&apos;@&apos;localhost&apos; identified by &apos;123456&apos;; GRANT ALL PRIVILEGES ON aodh.* TO &apos;aodh&apos;@&apos;%&apos; identified by &apos;123456&apos;; GRANT ALL PRIVILEGES ON aodh.* TO &apos;aodh&apos;@&apos;control-node1.novalocal&apos; identified by &apos;123456&apos;; 创建用户openstack user create --domain default --password-prompt aodh 给adoh帐户添加admin权限openstack role add --project service --user aodh admin 添加服务openstack service create --name aodh --description &quot;Telemetry&quot; alarming 创建endpoint123openstack endpoint create --region RegionOne alarming public http://192.168.122.2:8042 openstack endpoint create --region RegionOne alarming internal http://192.168.4.6:8042 openstack endpoint create --region RegionOne alarming admin http://192.168.4.6:8042 安装软件yum install openstack-aodh-api \openstack-aodh-evaluator \openstack-aodh-notifier \openstack-aodh-listener \openstack-aodh-expirer \python-aodhclient 修改配置文件 vim /etc/aodh/aodh.conf12345678910111213141516171819202122232425262728[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6auth_strategy = keystone[database]connection = mysql+pymysql://aodh:123456@192.168.4.6/aodh[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = aodhpassword = 123456 #填写通过keystone创建帐户时设置的帐号和密码[service_credentials]auth_type = passwordauth_url = http://192.168.4.6:5000/v3project_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = aodhpassword = 123456 #填写通过keystone创建帐户时设置的帐号和密码interface = internalURLregion_name = RegionOne systemctl enable openstack-aodh-api.service \openstack-aodh-evaluator.service \openstack-aodh-notifier.service \openstack-aodh-listener.service systemctl start openstack-aodh-api.service \openstack-aodh-evaluator.service \openstack-aodh-notifier.service \openstack-aodh-listener.service]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Fastnetmon配置与使用]]></title>
      <url>%2F2016%2F12%2F09%2Ffastnetmon%2F</url>
      <content type="text"><![CDATA[Fastnetmon介绍FastNetmon是一个基于多种抓包引擎来对数据包进行统计分析的DOS/DDOS工具，可以探测和分析网络中的异常流量情况，同时，也可以对捕获的异常调用外部脚本进行处理警报啊或者进行阻断处理，全靠外部脚本是如何定义。 项目首页http://www.open-open.com/lib/view/home/143193107973 部署架构 安装和配置方法安装 下载自动安装脚本wget https://raw.githubusercontent.com/FastVPSEestiOu/fastnetmon/master/src/fastnetmon_install.pl -Ofastnetmon_install.pl 将整个项目克隆下来https://github.com/FastVPSEestiOu/fastnetmon 安装注意若安装报连接错误，请将DNS改为8.8.8.8或223.5.5.5，因为下载地址都是在国外，有些DNS可能解析不够好。perl fastnetmon_install.pl 启动安装脚本。 配置 配置文件，cd到你刚刚克隆的项目里面。cp src/Fastnetmon.conf 到/etc/cp src/fastnetmon_init_script_centos6 /etc/init.d/fastnetmon #cp启动脚本到/etc/init.d/chmod 755 /etc/init.d/fastnetmon #修改启动脚本权限cp src/notify_about_attack.sh /usr/local/bin/ #cp 通知脚本 /etc/fastentmon配置项ban_time = 1900 #对检测到攻击的ip进行多久封锁。enable_subnet_counters = on #检测每个子网进出流量enable_connection_tracking = on #开启攻击追踪检测，通过这个选项在日志文件里可以详细看见攻击者ip和其他一些详细情况ban_for_pps = on，ban_for_bandwidth = on，ban_for_flows = on #检测的选项pps(每秒包)，bandwidth(带宽)，flows(流量)。threshold_pps = 20000，threshold_mbps = 1000，threshold_flows = 3500 #监控的限定值抓包引擎选择mirrof=off没有安装PF_RING就不要开启，不然会启动报错。pfring_sampling_ratio = 1 #端口镜像采样率mirror_netmap = off没有安装Netmap就不要开启，不然会会启动报错。mirror_snabbswitch=on #开启snabbswitch流量捕获。mirror_afpacket =on #AF_PACKET捕获引擎开启netmap_sampling_ratio = 1 #端口镜像抽样比pcap=on #pcap引擎开启netflopw=on #使用Netflow捕获方法interfaces=enp5s0f1 #监控的端口，我这里使用的是镜像端口,不然监控不到整个网端流量notify_script_path=/usr/local/bin/notify_about_attack.sh #触发脚本位置monitor_local_ip_addresses = on #监控本地地址sort_parameter = packets #在控制台排序单位，包max_ips_in_list =400 #在控制台显示多少地址 编辑要监控的网段vim /etc/networks_list #编辑networks_list加入要监控的网段这里我加入211.156.182.0/24 220.242.2.0/24/etc/init.d/fastnetmon start #启动fastentmon，启动失败查看/var/log/fastnetmon.log/opt/fastnetmon/fastnetmon #打开监控控制台 修改监控脚本1,要先外发邮件必须配置mailx 安装mailx，配置SMTP vim /etc/mail.rc 我这里配置的是我163的邮箱set bsdcompatset from=xxxxx@163.com smtp=smtp.163.comset smpt-auth-user=xxxx@163.com set smtp-auth-user=xxxxxx@163.com smtp-auth-password=xxxxxx smtp-auth=login vim /usr/local/bin/notify_about_attack.sh #这里填要触发监控的脚本修改邮件地址为接受人。cat | mail -s “FastNetMon Guard: IP $1 blocked because $2 attack with power $3 pps” $email_notify; 测试使用低轨道等离子炮（Loic）进行测试这里我对我们211.156.182.143进行Tcp DDOS攻击我们先把这些值调低threshold_pps = 2000，threshold_mbps = 100，threshold_flows = 350 #监控的限定值然后重启Fastnetmon 测试完修改回值]]></content>
    </entry>

    
  
  
</search>
