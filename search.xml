<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CI(jenkins+gitlab)</title>
    <url>/2018/05/01/CI/</url>
    <content><![CDATA[<h3 id="Jenkins是什么？"><a href="#Jenkins是什么？" class="headerlink" title="Jenkins是什么？"></a>Jenkins是什么？</h3><p>Jenkins是一款java开发的功能强大的持续集成，持续发布工具，与之相对应的还有Drone这个工具，这里我们主要介绍Jenkins。</p>
<h3 id="为什么使用Jenkins"><a href="#为什么使用Jenkins" class="headerlink" title="为什么使用Jenkins"></a>为什么使用Jenkins</h3><ul>
<li>安装和配置灵活方便，支持多种方式安装(docker、软件包、war包、源码）</li>
<li>配置 方便可视化操作</li>
<li>丰富的插件</li>
<li>可扩展，分布式架构(master-slave)</li>
</ul>
<h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><p>操作系统：ubuntu 16.04<br>java：1.8.0_161-b12<br>jenkins： 2.89.4  </p>
<h3 id="部署gitlab"><a href="#部署gitlab" class="headerlink" title="部署gitlab"></a>部署gitlab</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --detach \</span><br><span class="line">    --hostname gitlab.example.com \</span><br><span class="line">    --publish 443:443 --publish 80:80 --publish 22:22 \</span><br><span class="line">    --name gitlab \</span><br><span class="line">    --restart always \</span><br><span class="line">    --volume /srv/gitlab/config:/etc/gitlab \</span><br><span class="line">    --volume /srv/gitlab/logs:/var/log/gitlab \</span><br><span class="line">    --volume /srv/gitlab/data:/var/opt/gitlab \</span><br><span class="line">    gitlab/gitlab-ce:latest</span><br></pre></td></tr></table></figure>
<p>访问<a href="http://gitlab_ip/">http://gitlab_ip</a>  </p>
<h3 id="安装Jenkins"><a href="#安装Jenkins" class="headerlink" title="安装Jenkins"></a>安装Jenkins</h3><p>jenkins的安装方式有多种，有通过容器方式安装的，有通过rpm包和deb包方式安装还有通过war包安装的，这里使用deb包方式安装。</p>
<p>因为Jenkins是基于java开发的所以我们先配置java环境<br>先安装jdk8<br><a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html">http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a></p>
<p>安装</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir  /usr/local/jdk1.8/</span><br><span class="line">tar -xvf jdk-8u16.tar.gz -C /usr/local/jdk1.8/</span><br></pre></td></tr></table></figure>
<p>配置环境变量<br>编辑&#x2F;etc&#x2F;profile</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.8/jdk1.8.0_161</span><br><span class="line">export PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jart</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>
<p>测试</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_1.png">  </p>
<h3 id="安装Jenkins-1"><a href="#安装Jenkins-1" class="headerlink" title="安装Jenkins"></a>安装Jenkins</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget -q -O - https://pkg.jenkins.io/debian/jenkins-ci.org.key | sudo apt-key add -</span><br><span class="line">sudo sh -c &#x27;echo deb http://pkg.jenkins.io/debian-stable binary/ &gt; /etc/apt/sources.list.d/jenkins.list&#x27;</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install jenkins</span><br></pre></td></tr></table></figure>
<p>启动  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl start jenkins</span><br></pre></td></tr></table></figure>
<p>访问<br>Jenkins默认是访问8080端口  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_2.png">  </p>
<p>从图上给的路径获取默认密码<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_3.png">  </p>
<p>安装方式标准安装和自定义安装，标准安装主要进行一些标准插件的安装，自定义安装就是自己可以选择插件进行安装。这里我们选择标准安装<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_4.png">  </p>
<p>等待安装完  </p>
<p>安装完，设置用户名和密码  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_5.png">  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_6.png">  </p>
<p>这边我们与gitlab做对接所以需要安装gitlab插件<br>系统管理—&gt;管理插件—–&gt;可选插件—-&gt;gitlab plugin<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_7.png">  </p>
<p>安装完成后配置gitlab plugin<br>系统管理—&gt;系统配置<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_8.png">  </p>
<p>输入gitlab的地址<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_9.png">  </p>
<p>还有填写gitlab的 access token<br>先去gitlab申请<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_10.png">  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_11.png"><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_12.png">  </p>
<p>将token保存好</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_13.png"><br>配置credentials<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_14.png">  </p>
<p>kind那选gitlab token，然后add<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_15.png">  </p>
<p>给gitlab配置公钥，用于pull和push代码，哪台机器需要从gitlab上拉取和上传代码就需要配置，这里Jenkins节点需要，当然也可以直接在Jenkins上配置用户名和密码去连接<br>在gitlab的SSH keys处贴上机器的公钥，注意区分机器<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_16.png">  </p>
<p>Jenkins上创建个job<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_17.png">  </p>
<p>点击确定  </p>
<p>可以看见这里连接上了我们刚刚配置Jenkins gitlab-plugin的配置<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_18.png">  </p>
<p>源码管理<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_19.png">  </p>
<p>可以用ssh的连接也可以用http的连接，我这里用ssh的因为我改了默认gitlab的ssh端口所以链接需要改一下<br>如果gitlab 的ssh用的不是默认的22端口需要改成如下<br>ssh:&#x2F;&#x2F;git@ip:ssh_port&#x2F;wanshaoyuan&#x2F;wanshaoyuan.git<br>1、增加了ssh:&#x2F;&#x2F;  </p>
<p>2、增加了端口号  </p>
<p>3、把用户名前面的冒号改成了斜杠(&#x2F;)<br>因为我们还没有配置credentials所以会报错点击旁边add配置credentials<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_20.png">  </p>
<p>用户名填写你生成公钥写的用户名private key填写你的私钥<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_21.png">  </p>
<p>然后就不会报错了,同时我这里选择的也是master分支<br>触发器这里有几种方式<br><img src="http://ohx02qrb8.bkt.clouddn.com/ci_22.png">  </p>
<p>分别解释下<br>Build after other projects are built 当另一个构建任务完成之后触发<br>Build periodically 周期性的触发<br>Build when a change is pushed to GitLab. GitLab CI Service URL:   <a href="http://191.8.2.112:12000/project/test-go-dev">http://191.8.2.112:12000/project/test-go-dev</a> 当代码有更新的时候触发，通过GitLab CI<br>GitHub hook trigger for GITScm polling 通过Github钩子触发<br>Poll SCM 定期检查代码有无更新，有更新时触发  </p>
<p>这里我们使用Build when a change is pushed to GitLab. GitLab CI Service URL的方式，因为使用定时检查的话对Jenkins压力会很大的，使用webhook的方式，当gitlab有push代码或其他方式时会通知Jenkins，所以这里还需要配置gitlab的webhook  </p>
<p>复制Build when a change is pushed to GitLab. GitLab CI Service URL: 后面那串地址链接</p>
<p>打开gitlab<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_23.png">  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_24.png"><br>输入刚刚那串url  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_25.png"><br>默认触发器是push事件时，触发webhook，需要注意的是因为我们这里的Jenkins配置了用户名和密码所以<br>url需要在原来基础上添加用户名和密码其他不变<br>格式为<br><a href="http://username:password@192.168.1.4:1080/project/dashboard-build">http://username:password@192.168.1.4:1080/project/dashboard-build</a>  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_26.png">  </p>
<p>点击test按钮进行测试，返回状态码为200表示配置正确。</p>
<p> 如果同网段Jenkins webhook 返回500，需要用root帐号登录进gitlab<br>setting–&gt;勾上Allow requests to the local network from hooks and services</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_27.png">  </p>
<p>构建</p>
<p>这里我们选执行shell  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_28.png">  </p>
<p>做个简单的操作比如说我git push一个test.txt的文件上去，然后用构建命令就去读取这个文件<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_29.png">  </p>
<p>应用并保存<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_30.png">  </p>
<p>然后我们push到test.txt到wanshaoyuan这个project<br>写这样一段话<br><img src="/!%5B%5D(https:/wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_31.png)">  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit -m &quot;test jenkins&quot;</span><br><span class="line">git push</span><br></pre></td></tr></table></figure>
<p>可以看见Jenkins很快触发了构建<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_32.png">  </p>
<p>查看输出<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_33.png">  </p>
]]></content>
      <categories>
        <category>CI/CD</category>
      </categories>
      <tags>
        <tag>CI/CD</tag>
      </tags>
  </entry>
  <entry>
    <title>Dockerfile</title>
    <url>/2017/10/03/Dockerfile/</url>
    <content><![CDATA[<h3 id="本文为Dockerfile的语法简单介绍"><a href="#本文为Dockerfile的语法简单介绍" class="headerlink" title="本文为Dockerfile的语法简单介绍"></a>本文为Dockerfile的语法简单介绍</h3><p>使用Dockerfile构建镜像比docker commit的优势 ：<br>1、首先dockerfile是一个文本文件，可控性好，后面要进行翻看或check时更加简单。<br>2、在使用CI&#x2F;CD等devops工具时，我们一般都是将写好的Dockerfile push到gitlab中去，然后Jenkins监测到对应的project有变动，然后执行自动构建，基于Dockerfile去构建<br>docker image，然后去部署到kubernetes集群中。</p>
<p>Dockerfile常用指令<br>FROM #指定base镜像<br>MAINTAINER #设置镜像作者<br>COPY #将文件从host上的build context拷贝到镜像目录，注意src文件必须是build context内的  支持的格式COPY src dest 和COPY [“src”,”dest”]</p>
<p>ADD #与copy类似，从build context复制文件到镜像，不同的是如果src是归档类文件（tar,zip,tgz,xz等),文件会自动解压到dest</p>
<p>ENV #设置环境变量，环境变量可被后面的指令使用。</p>
<p>EXPOSE#指定容器中进程会监听的端口，Docker可以将端口暴露出来。</p>
<p>VOLUME #将文件或目录声明为volume</p>
<p>WORKDIR #build时会直接切换到此目录，为后面的RUN、CMD、ENTRYPOINT、ADD、COPY等指令设置镜像当前工作目录</p>
<p>RUN #容器中运行指定的命令</p>
<p>CMD #容器启动是运行指定的命令，可以由多个CMD命令，但只有最后一个会生效，CMD命令会被docker run之后根的参数替换掉。</p>
<p>ENTRYPOINT #设置容器启动时命令，Dockerfile中有多个ENTRYPOINT命令但只有最后一个生效，cmd或docker run之后的参数会被当做参数传递给ENTRYPOINT。</p>
<p>指令有两中格式shell和exec格式</p>
<p>shell格式<instruction> <command><br>如<br>RUN apt get install python3<br>CMD echo “Hello world”</p>
<p>exec格式<instruction> [“executable”,”param1”,”param2”,…]<br>如<br>RUN [“apt-get”,  “install”,”python3”]<br>CMD[“&#x2F;bin&#x2F;echo”, “hello world”]</p>
<p>exec执行指令时，会直接调用[command]命令，不会被shell解析，</p>
<p>当有环境变量时使用exec格式不会被解析。<br>如<br>ENV name wan ENTRYPOINT [“&#x2F;bin&#x2F;echo”,”hello”,$name”]<br>运行容器将输出<br>hello $name</p>
<p>如果要输出变量<br>ENV name wan ENTRYPOINT [“&#x2F;bin&#x2F;sh”,”-c”,”echo hello,$name”]</p>
<p>运行容器将输出<br>hello ，wan</p>
<p>cmd和entrypoint的区别<br>如果docker run指定了其他命令，cmd指定的默认命令将被忽略，当entrypoint一定会执行。并将docker run后面的参数做为entrypoint后面的参数。</p>
<p>cmd可以做entrypoint的参数</p>
<p>ENTRYPOINT [“&#x2F;bin&#x2F;echo”,”hello”]<br>CMD [“world”]</p>
<p>当容器通过docker run -it [image]启动时输出<br>hello world</p>
<p>entrypoint使用shell格式是，会忽略cmd或docker run提供的参数。</p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Istio(一)部署安装体验</title>
    <url>/2019/07/31/Istio-1/</url>
    <content><![CDATA[<h3 id="环境信息："><a href="#环境信息：" class="headerlink" title="环境信息："></a>环境信息：</h3><table>
<thead>
<tr>
<th>组件</th>
<th>版本</th>
</tr>
</thead>
<tbody><tr>
<td>Kubernetes</td>
<td>1.15.5</td>
</tr>
<tr>
<td>Istio</td>
<td>1.4.2</td>
</tr>
<tr>
<td>helm</td>
<td>2.16.1</td>
</tr>
</tbody></table>
<h3 id="Helm方式安装"><a href="#Helm方式安装" class="headerlink" title="Helm方式安装"></a>Helm方式安装</h3><p>下载对应的release版本  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget https://github.com/istio/istio/releases/download/1.4.2/istio-1.4.2-linux.tar.gz</span><br></pre></td></tr></table></figure>
<p>解压  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tar -xvf istio-1.4.2-linux.tar.gz </span><br></pre></td></tr></table></figure>

<p>将解压后的文件客户端工具放置到&#x2F;usr&#x2F;local&#x2F;bin目录下</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp istio-1.0.5/bin/istioctl /usr/local/bin/</span><br></pre></td></tr></table></figure>

<p>使用helm方式安装  </p>
<p><a href="https://istio.io/docs/setup/install/helm/">https://istio.io/docs/setup/install/helm/</a><br>有多种安装参数选择，我们这里使用自定义方式，需要安装一些如grafana、Jaeger、kiali、Gateway组件。  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_1_1.png"></p>
<p>1、创建namespace </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create namespace istio-system</span><br></pre></td></tr></table></figure>
<p>2、安装和配置istio所需要的CRD  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">helm install install/kubernetes/helm/istio-init --name istio-init --namespace istio-system</span><br></pre></td></tr></table></figure>
<p>3、确认CRD资源创建完成  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl -n istio-system wait --for=condition=complete job --all</span><br></pre></td></tr></table></figure>
<p>4、使用Helm安装istio  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">helm install install/kubernetes/helm/istio --set kiali.enabled=true --set gateways.istio-ingressgateway.type=NodePort --set tracing.enabled=true --set grafana.enabled=true --set gateways.istio-egressgateway.type=NodePort  --name istio --namespace istio-system</span><br></pre></td></tr></table></figure>

<p>5、复制并使用istioclt</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp bin/istioctl /usr/bin/  </span><br><span class="line">chmod a+x /usr/bin/istioctl</span><br></pre></td></tr></table></figure>

<p><a href="https://istio.io/docs/setup/additional-setup/config-profiles/">https://istio.io/docs/setup/additional-setup/config-profiles/</a></p>
<p>安装完以后<br>查看deployment和service</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get deployment -n istio-system</span><br><span class="line">NAME                     READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">grafana                  1/1     1            1           2d20h</span><br><span class="line">istio-citadel            1/1     1            1           2d20h</span><br><span class="line">istio-galley             1/1     1            1           2d20h</span><br><span class="line">istio-ingressgateway     1/1     1            1           2d20h</span><br><span class="line">istio-pilot              1/1     1            1           2d20h</span><br><span class="line">istio-policy             1/1     1            1           2d20h</span><br><span class="line">istio-sidecar-injector   1/1     1            1           2d20h</span><br><span class="line">istio-telemetry          1/1     1            1           2d20h</span><br><span class="line">istio-tracing            1/1     1            1           2d20h</span><br><span class="line">kiali                    1/1     1            1           2d20h</span><br><span class="line">prometheus               1/1     1            1           2d20h</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get svc -n istio-system</span><br><span class="line">NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                                                                                      AGE</span><br><span class="line">grafana                  ClusterIP   10.43.187.158   &lt;none&gt;        3000/TCP                                                                                                                                     2d20h</span><br><span class="line">istio-citadel            ClusterIP   10.43.142.36    &lt;none&gt;        8060/TCP,15014/TCP                                                                                                                           2d20h</span><br><span class="line">istio-galley             ClusterIP   10.43.124.145   &lt;none&gt;        443/TCP,15014/TCP,9901/TCP                                                                                                                   2d20h</span><br><span class="line">istio-ingressgateway     NodePort    10.43.73.211    &lt;none&gt;        15020:32240/TCP,80:31380/TCP,443:31390/TCP,31400:31400/TCP,15029:32216/TCP,15030:31943/TCP,15031:32118/TCP,15032:32435/TCP,15443:31343/TCP   2d20h</span><br><span class="line">istio-pilot              ClusterIP   10.43.72.148    &lt;none&gt;        15010/TCP,15011/TCP,8080/TCP,15014/TCP                                                                                                       2d20h</span><br><span class="line">istio-policy             ClusterIP   10.43.19.133    &lt;none&gt;        9091/TCP,15004/TCP,15014/TCP                                                                                                                 2d20h</span><br><span class="line">istio-sidecar-injector   ClusterIP   10.43.144.147   &lt;none&gt;        443/TCP,15014/TCP                                                                                                                            2d20h</span><br><span class="line">istio-telemetry          ClusterIP   10.43.89.78     &lt;none&gt;        9091/TCP,15004/TCP,15014/TCP,42422/TCP                                                                                                       2d20h</span><br><span class="line">jaeger-agent             ClusterIP   None            &lt;none&gt;        5775/UDP,6831/UDP,6832/UDP                                                                                                                   2d20h</span><br><span class="line">jaeger-collector         ClusterIP   10.43.29.41     &lt;none&gt;        14267/TCP,14268/TCP,14250/TCP                                                                                                                2d20h</span><br><span class="line">jaeger-query             ClusterIP   10.43.145.48    &lt;none&gt;        16686/TCP                                                                                                                                    2d20h</span><br><span class="line">kiali                    ClusterIP   10.43.215.248   &lt;none&gt;        20001/TCP                                                                                                                                    2d20h</span><br><span class="line">prometheus               ClusterIP   10.43.161.98    &lt;none&gt;        9090/TCP                                                                                                                                     2d20h</span><br><span class="line">tracing                  ClusterIP   10.43.204.201   &lt;none&gt;        80/TCP                                                                                                                                       2d20h</span><br><span class="line">zipkin                   ClusterIP   10.43.45.55     &lt;none&gt;        9411/TCP                                                                                                                                     2d20h</span><br></pre></td></tr></table></figure>

<p>默认grafana、Jaeger、kiali的service是cluster-ip类型的，若需要方便外部访问，可以将其修改为nodeport类型或使用ingress对外暴露  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl patch svc grafana -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&#x27; -n istio-system  </span><br><span class="line"> </span><br><span class="line">kubectl patch svc kiali -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&#x27; -n istio-system   </span><br><span class="line"></span><br><span class="line">kubectl patch svc kiali -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&#x27; -n istio-system   </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">kubectl patch svc jaeger-query  -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&#x27; -n istio-system   </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>访问    </p>
<p>grafana:在这里将prometheus收集过来的各类监控指标图表化，包含对各个后端的请求速率、访问成功率，资源使用统计等。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_1_2.png">  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_1_3.png"></p>
<p>Kiali<br>kiali是一个Redhat开源的Service mesh可视化工具，它可以看见应用模块之间的拓扑图、流量走向图、健康检查状态等</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_1_4.png"></p>
<p>Jaeger<br>jaeger是CNCF基金会管理项目主要用于istio中的分布式链路追踪，服务依赖分析等</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_1_5.png"></p>
<p>卸载  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">helm delete --purge istio</span><br><span class="line">helm delete --purge istio-init</span><br><span class="line">helm delete --purge istio-cni</span><br><span class="line">kubectl delete namespace istio-system</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl delete -f install/kubernetes/helm/istio-init/files</span><br></pre></td></tr></table></figure>




<p>参考链接：  </p>
<p><a href="https://jimmysong.io/istio-handbook/setup/istio-installation.html">https://jimmysong.io/istio-handbook/setup/istio-installation.html</a></p>
<p><a href="https://istio.io/docs/setup/install/helm/">https://istio.io/docs/setup/install/helm/</a></p>
]]></content>
      <categories>
        <category>ServiceMesh</category>
      </categories>
      <tags>
        <tag>ServiceMesh</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes service模式分析</title>
    <url>/2018/12/10/Kubernetes_service_mode/</url>
    <content><![CDATA[<h3 id="概述："><a href="#概述：" class="headerlink" title="概述："></a>概述：</h3><p>Kubernetes service是为POD提供统一访问入口的，实现主要依靠kube-proxy实现，kube-proxy有三种模式userspace、iptables，ipvs，同时我们也知道service有三种类型cluster_ip、nodeport，loadblance和三种端口类型port，targetport，nodeport。</p>
<h3 id="环境信息："><a href="#环境信息：" class="headerlink" title="环境信息："></a>环境信息：</h3><p>OS：Ubuntu16.04<br>Kubernetes：v1.11.0<br>kubeadm：v1.11.0<br>docker：17.03<br>network：flannel</p>
<h3 id="kube-proxy模式分析"><a href="#kube-proxy模式分析" class="headerlink" title="kube-proxy模式分析"></a>kube-proxy模式分析</h3><h4 id="userspace"><a href="#userspace" class="headerlink" title="userspace"></a>userspace</h4><p>userspace为kube-proxy为早期的模式，Kubernetes1.2版本之前主要使用这个模式，转发原理参考</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://kubernetes.io/docs/concepts/services-networking/service/```</span><br><span class="line"></span><br><span class="line">这个模式最大缺点就是，所以端口请求都需要先经过kube-proxy然后在通过iptables转发，这样带来一个问题就是需要在用户态和内核态不断进行切换，效率低。</span><br><span class="line"></span><br><span class="line">#### iptables</span><br><span class="line">Kubernetes在1.2版本开始将iptables做为kube-proxy的默认模式，iptables根之前userspace相比，完全工作在内核态而且不用在经过kube-proxy中转一次性能更强，下面介绍Kubernetes中iptables转发流程</span><br><span class="line"></span><br><span class="line">iptables有链和表的概念，链就相当于一道道关卡，表就是这个关卡上对应的规则总共有四个表和五条链，kube-proxy在这里就使用了两个表分别是filter和nat表，也自定义了五个链KUBE-SERVICES，KUBE-NODE-PORTS，KUBE-POSTROUTING，KUBE-MARK-MASQ和KUBE-MARK-DROP五个链</span><br><span class="line">目前kubernetes提供了两种负载分发策略：RoundRobin和SessionAffinity</span><br><span class="line"></span><br><span class="line">RoundRobin：轮询模式，即轮询将请求转发到后端的各个Pod上。</span><br><span class="line">SessionAffinity：基于客户端IP地址进行会话保持的模式，第一次客户端访问后端某个Pod，之后的请求都转发到这个Pod上</span><br><span class="line">默认是RoundRobin模式。</span><br><span class="line"></span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/service_mode_1.png)</span><br><span class="line">iptables数据包转发流程</span><br><span class="line">1、首先一个数据包经过网卡进来，先经过PREROUTING链</span><br><span class="line">2、判定目的地址是否为主机，为本机就通过INPUT链转发</span><br><span class="line">3、若不为本机通过FORWARDl链转发到POSTROUTING出去</span><br><span class="line"></span><br><span class="line">以下展示一个实例展示kube-proxy是如何根据service不同类型生成对应规则</span><br><span class="line"></span><br><span class="line">我们创建名为test的deployment，镜像为nginx:latest，replicas为3个</span><br></pre></td></tr></table></figure>
<p>kubectl run test –image&#x3D;nginx –replicas&#x3D;3</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">```</span><br><span class="line">kubectl get pod -o wide</span><br><span class="line"></span><br><span class="line">NAME                                READY     STATUS    RESTARTS   AGE               IP</span><br><span class="line">test-679b667858-9h9hr   1/1          Running            0          17h           10.244.0.16</span><br><span class="line">test-679b667858-g827r   1/1          Running            0          17h           10.244.0.15</span><br><span class="line">test-679b667858-nnr28   1/1          Running            0          6m            10.244.0.18</span><br></pre></td></tr></table></figure>
<p>在给这个deployment创建一个ClusterIP类型的service</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl expose deployment/test --type=ClusterIP --port=80</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get svc</span><br><span class="line">NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">test       ClusterIP    10.98.243.51           &lt;none&gt;          80/TCP    43s</span><br></pre></td></tr></table></figure>
<p>接下来我们将iptables规则导出来观察，用iptables-save将iptables规则重定向到一个文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">iptables-save &gt; /tmp/1</span><br></pre></td></tr></table></figure>
<h5 id="ClusterIP类型"><a href="#ClusterIP类型" class="headerlink" title="ClusterIP类型"></a>ClusterIP类型</h5><p>查看规则<br>首先Kubernetes会指对每个service在创建一些名为KUBE-SEP-xxx，KUBE-SVC-xxx的链</p>
<p>以刚刚创建的类型为Cluster-ip名为test这个service为例，创建了以下规则</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、-A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.98.243.51/32 -p tcp -m comment --comment &quot;default/test: cluster IP&quot; -m tcp --dport 80 -j KUBE-MARK-MASQ</span><br><span class="line"></span><br><span class="line">2、-A KUBE-SERVICES -d 10.98.243.51/32 -p tcp -m comment --comment &quot;default/test: cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-IOIC7CRUMQYLZ32S</span><br><span class="line"></span><br><span class="line">3、-A KUBE-SVC-IOIC7CRUMQYLZ32S -m comment --comment &quot;default/test:&quot; -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-DZSN6N54CDU2RTAQ</span><br><span class="line"></span><br><span class="line">4、-A KUBE-SEP-DZSN6N54CDU2RTAQ -s 10.244.0.15/32 -m comment --comment &quot;default/test:&quot; -j KUBE-MARK-MASQ</span><br><span class="line"></span><br><span class="line">5、-A KUBE-SEP-DZSN6N54CDU2RTAQ -p tcp -m comment --comment &quot;default/test:&quot; -m tcp -j DNAT --to-destination 10.244.0.15:80</span><br><span class="line"></span><br><span class="line">6、-A KUBE-SVC-IOIC7CRUMQYLZ32S -m comment --comment &quot;default/test:&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-XY6DO3BIJML2V7B5</span><br><span class="line"></span><br><span class="line">7、-A KUBE-SEP-XY6DO3BIJML2V7B5 -s 10.244.0.16/32 -m comment --comment &quot;default/test:&quot; -j KUBE-MARK-MASQ</span><br><span class="line"></span><br><span class="line">8、-A KUBE-SEP-XY6DO3BIJML2V7B5 -p tcp -m comment --comment &quot;default/test:&quot; -m tcp -j DNAT --to-destination 10.244.0.16:80</span><br><span class="line"></span><br><span class="line">9、-A KUBE-SVC-IOIC7CRUMQYLZ32S -m comment --comment &quot;default/test:&quot; -j KUBE-SEP-SWCXAUIAGJXMWYFS</span><br><span class="line"></span><br><span class="line">10、-A KUBE-SEP-SWCXAUIAGJXMWYFS -s 10.244.0.18/32 -m comment --comment &quot;default/test:&quot; -j KUBE-MARK-MASQ</span><br><span class="line"></span><br><span class="line">11、-A KUBE-SEP-SWCXAUIAGJXMWYFS -p tcp -m comment --comment &quot;default/test:&quot; -m tcp -j DNAT --to-destination 10.244.0.18:80</span><br></pre></td></tr></table></figure>

<p>1、对源IP非10.244.0.0&#x2F;16访问目的地址10.98.243.51的80端口，执行KUBE-MARK-MASQ ，KUBE-MARK-MASQ会给这个包打上0x4000标签，后续KUBE-POSTROUTING链会根据这个标签做SNAT出去。<br>2、对于目的IP为10.98.243.51，目的端口为80，然后将请求丢给KUBE-SVC-IOIC7CRUMQYLZ32S链处理。<br>3、规则链KUBE-SVC-IOIC7CRUMQYLZ32S实现了将报文按33%的比例匹配，转给KUBE-SEP-DZSN6N54CDU2RTAQ链。<br>4、对源IP为10.244.0.24的包 丢给KUBE-MARK-MASQ链，就如我们上面说的这个链会给包打上tag然后做SNAT，让这个地址能访问外网。<br>5、KUBE-SEP-DZSN6N54CDU2RTAQ链直接进行DNAT操作将cluster-ip的80端口映射到pod的80。<br>6、规则链KUBE-SVC-IOIC7CRUMQYLZ32S实现了将报文按50%的比例匹配，转给KUBE-SEP-XY6DO3BIJML2V7B5链。<br>7、对源IP为10.244.0.25的包丢给KUBE-MARK-MASQ链，就如我们上面说的这个链会给包打上tag然后做SNAT，让这个地址能访问外网。<br>8、 KUBE-SEP-XY6DO3BIJML2V7B5链直接进行DNAT操作进行DNAT到10.244.0.16 80端口。<br>9、将KUBE-SVC-IOIC7CRUMQYLZ32S链剩余请求转发给KUBE-SEP-SWCXAUIAGJXMWYFS链。<br>10、对源ip为10.244.0.18的包镜像SNAT。<br>11、KUBE-SEP-SWCXAUIAGJXMWYFS 链直接进行DNAT操作进行DNAT到10.244.0.18 80端口。</p>
<h5 id="NodePort类型"><a href="#NodePort类型" class="headerlink" title="NodePort类型"></a>NodePort类型</h5><p>我们将service类型改为NodePort</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl edit service/test</span><br></pre></td></tr></table></figure>
<p>将type改为NodePort</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get svc</span><br><span class="line">NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">test           NodePort    10.98.243.51     &lt;none&gt;        80:32734/TCP   21m</span><br></pre></td></tr></table></figure>
<p>重新保存iptables规则查看</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">iptables-save &gt;/tmp/1</span><br></pre></td></tr></table></figure>
<p>NodePort类型根ClusterIP 相比就多了两条规则，其他都一致。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/test:&quot; -m tcp --dport 32734 -j KUBE-MARK-MASQ</span><br><span class="line">2、-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/test:&quot; -m tcp --dport 32734 -j KUBE-SVC-IOIC7CRUMQYLZ32S</span><br></pre></td></tr></table></figure>

<p>两条规则，主要是允许数据包转发和对目的端口为32734的端口进行DNAT映射</p>
<h5 id="sessionaffinity"><a href="#sessionaffinity" class="headerlink" title="sessionaffinity"></a>sessionaffinity</h5><p>对于一些特殊应用，我们需要做会话保持，让会话连接始终连接到上一次接收会话的POD上<br>编辑我们刚刚创建的service</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl edit service/test</span><br></pre></td></tr></table></figure>
<p>将sessionaffinity参数改为  sessionAffinity: ClientIP，保存<br>再次保存iptables规则查看</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-A KUBE-SEP-DZSN6N54CDU2RTAQ -p tcp -m comment --comment &quot;default/test:&quot; -m recent --set --name KUBE-SEP-DZSN6N54CDU2RTAQ --mask 255.255.255.255 --rsource -m tcp -j DNAT --to-destination 10.244.0.15:80</span><br><span class="line"></span><br><span class="line">-A KUBE-SEP-SWCXAUIAGJXMWYFS -p tcp -m comment --comment &quot;default/test:&quot; -m recent --set --name KUBE-SEP-SWCXAUIAGJXMWYFS --mask 255.255.255.255 --rsource -m tcp -j DNAT --to-destination 10.244.0.18:80</span><br><span class="line"></span><br><span class="line">-A KUBE-SEP-XY6DO3BIJML2V7B5 -p tcp -m comment --comment &quot;default/test:&quot; -m recent --set --name KUBE-SEP-XY6DO3BIJML2V7B5 --mask 255.255.255.255 --rsource -m tcp -j DNAT --to-destination 10.244.0.16:80</span><br></pre></td></tr></table></figure>
<p>会多三条规则，用的iptables recent模块进行会话保持<br>总结一下<br>使用iptables模式后，所有的如端口转发，会话保持，负载均衡都是通过iptables对应的模块和对应的规则去实现的比如端口转发用的DNAT规则，会话保持用的recent模块，负载均衡用的statistic 模块。虽然iptables模式弥补了userspace模式的一些缺陷，但iptables模式本身也存在一些缺陷，主要是在存在大量service的场景下。问题如下</p>
<p>在大规模集群下，随着service的数量越来越多时iptables规则会成倍的增长，大量的规则同时也会产生一些问题：</p>
<ul>
<li>iptables规则匹配延时：因为iptables采用的是线性匹配即一个数据包过来以线性的方式遍历整个规则集，直到找到匹配的否则退出，这种带来的问题，就是当iptables规则量很大时，性能会急剧下降，因为对应的匹配延时会增加</li>
<li>iptables规则更新延时：在实际使用过程中需要不断创建service，修改service，删除service，这其实也转换成了对iptables的不断修改，因为iptables是非增量式更新的，也就意味着，你上述所有操作它都是把全部归则拷贝出来，然后在修改，修改完在拷贝回去而且这个修改过程还会锁表。附上网易云的iptables测试的更新延时性能测试表<br><a href="https://zhuanlan.zhihu.com/p/39909011">https://zhuanlan.zhihu.com/p/39909011</a></li>
</ul>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/service_mode_2.png"></p>
<ul>
<li><p>负载均衡性能问题：前面我们也提过iptables并不是专业的负载均衡器，目前使用RoundRobin和sessionaffinity都是通过iptables内部模块statistic和recent实现的，性能根真正的负载均衡器相比肯定有差距。</p>
</li>
<li><p>QPS抖动问题：kube-proxy会周期性的更新iptables规则，大量iptables规则更新会花费很长时间，期间又会锁表所以会造成QPS抖动。</p>
</li>
</ul>
<h3 id="IPVS"><a href="#IPVS" class="headerlink" title="IPVS"></a>IPVS</h3><p>Kubernetes社区为了解决上述iptables问题，在1.8版本引入了ipvs模式，并在Kubernetes 1.11版本正式GA。<br>熟悉LVS的知道，LVS是一个工作在传输层的四层负载均衡器,是章文嵩博士开源贡献给社区的，后被并入linux内核，IPVS正是LVS的一部分。LVS根iptables一样也是工作在Netfilter之上。</p>
<p>IPVS主要有三种模式</p>
<p>DR模式：调度器LB直接修改报文的目的MAC地址为后端真实服务器地址，服务器响应处理后的报文无需经过调度器LB，直接返回给客户端。这种模式也是性能最好的。<br>TUN模式：LB接收到客户请求包，进行IP Tunnel封装。即在原有的包头加上IP Tunnel的包头。然后发给后端真实服务器，真实的服务器将响应处理后的数据直接返回给客户端。<br>NAT模式：LB将客户端发过来的请求报文修改目的IP地址为后端真实服务器IP另外也修改后端真实服务器发过来的响应的报文的源IP为LB上的IP。</p>
<p>kube-proxy的IPVS模式用的上NAT模式，因为DR,TUN模式都不支持端口映射。</p>
<p>ipvs也支持多种算法<br>rr：轮询<br>lc：最少连接<br>dh：目的地址哈希<br>sh：源地址哈希<br>sed：最少期望延迟<br>nq：永远不排队</p>
<p>通过kube-proxy的–ipvs-scheduler进行配置，目前这个配置是一个全局性的，无法针对单个service做单独配置，后续会支持单个service转发算法配置。<br>启用方法<br>以kubeadm为例<br>因为ipvs是需要使用ipvs内核模块，先保证有这些内核模块ip_vs_sh,ip_vs_wrr,ip_vs_rr,ip_vs,nf_conntrack<br>没有的话手动加载</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">for i in &#123;ip_vs_sh,ip_vs_wrr,ip_vs_rr,ip_vs,nf_conntrack&#125;;do modprobe $i;done</span><br></pre></td></tr></table></figure>

<p>记得设置开机自动加载。<br>安装ipset和ipvsadm管理工具</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apt-get install ipset ipvsadm</span><br></pre></td></tr></table></figure>
<p>创建kubeadm部署配置文件,文件内容如下</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: kubeadm.k8s.io/v1alpha2</span><br><span class="line">kind: MasterConfiguration</span><br><span class="line">kubernetesVersion: v1.11.0</span><br><span class="line">networking:</span><br><span class="line">  podSubnet: 10.244.0.0/16</span><br><span class="line">kubeProxy:</span><br><span class="line">  config:</span><br><span class="line">    mode: ipvs</span><br></pre></td></tr></table></figure>

<p>执行kubeadm init –config xxxx 部署Kubernetes集群，然后就像之前方法一样通过kubeadm join加节点。<br>部署完执行ipvsadm可以看见创建的一些ipvs规则</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/service_mode_3.png"><br>kube-proxy也会在集群每个节点创建一个kube-ipvs0的网卡，将集群的cluster-ip挂在上面。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/service_mode_4.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/service_mode_5.png"></p>
<h5 id="转发原理"><a href="#转发原理" class="headerlink" title="转发原理"></a>转发原理</h5><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/service_mode_6.png"></p>
<h5 id="ClusterIP"><a href="#ClusterIP" class="headerlink" title="ClusterIP"></a>ClusterIP</h5><p>创建应用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl run test --image=nginx --replicas=3</span><br></pre></td></tr></table></figure>
<p>创建一个clusterip类型的service</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl expose deployment/test --port=80 --type=ClusterIP</span><br></pre></td></tr></table></figure>
<p>查看service的cluster-ip</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get svc</span><br><span class="line">NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">kubernetes      ClusterIP   10.96.0.1             &lt;none&gt;            443/TCP   13h</span><br><span class="line">test            ClusterIP   10.105.170.66    &lt;none&gt;             80/TCP    12m</span><br></pre></td></tr></table></figure>
<p>查看pod的ip</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod -o wide</span><br><span class="line">NAME                    READY     STATUS    RESTARTS   AGE       IP            NODE</span><br><span class="line">test-679b667858-kz4wd   1/1       Running    0          18m   10.244.1.8    wan-node2</span><br><span class="line">test-679b667858-npdlp   1/1       Running    0          18m   10.244.1.7    wan-node2</span><br><span class="line">test-679b667858-qgwnj   1/1       Running    0          18m   10.244.0.21   wan-node1</span><br></pre></td></tr></table></figure>

<p>可以看见kube-proxy将刚刚创建的testservice的cluster-ip 10.105.170.66挂载到kube-ipvs0虚拟网卡上了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">root@wan-node1:~# ip a</span><br><span class="line">kube-ipvs0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default</span><br><span class="line">    link/ether 4a:ec:28:5d:a0:24 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.96.0.10/32 brd 10.96.0.10 scope global kube-ipvs0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 10.96.0.1/32 brd 10.96.0.1 scope global kube-ipvs0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 10.105.170.66/32 brd 10.105.170.66 scope global kube-ipvs0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>
<p>查看ipvs规则</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ipvsadm -ln</span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line"> -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  10.105.170.66:80 rr</span><br><span class="line">  -&gt; 10.244.0.21:80              Masq      1    0          0</span><br><span class="line">  -&gt; 10.244.1.7:80               Masq      1    0          0</span><br><span class="line">  -&gt; 10.244.1.8:80               Masq      1    0          0</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>可以看见ipvs生成了对应的规则，VIP为10.105.170.66端口为80端口转发模式为rr，后端服务器IP为10.244.0.21，10.244.1.7，10.244.1.8正是我们的POD的IP.</p>
<h5 id="NodePort"><a href="#NodePort" class="headerlink" title="NodePort"></a>NodePort</h5><p>修改service类型为NodePort<br>kubectl edit svc&#x2F;test将type修改为NodePort</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get svc</span><br><span class="line">NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">test         NodePort    10.105.170.66   &lt;none&gt;        80:31389/TCP   21m</span><br></pre></td></tr></table></figure>
<p>在次查看ipvs规则</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ipvsadm -ln</span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  192.168.250.200:31389 rr</span><br><span class="line">  -&gt; 10.244.0.21:80                       Masq       1       0                   0</span><br><span class="line">  -&gt; 10.244.1.7:80                         Masq       1       0                   0</span><br><span class="line">  -&gt; 10.244.1.8:80                         Masq       1       0                   0</span><br><span class="line">TCP  10.244.0.0:31389 rr</span><br><span class="line">  -&gt; 10.244.0.21:80                       Masq       1       0                   0</span><br><span class="line">  -&gt; 10.244.1.7:80                         Masq       1       0                   0</span><br><span class="line">  -&gt; 10.244.1.8:80                         Masq       1       0                   0</span><br><span class="line">TCP  172.17.0.1:31389 rr</span><br><span class="line">  -&gt; 10.244.0.21:80                       Masq       1       0                   0</span><br><span class="line">  -&gt; 10.244.1.7:80                         Masq       1       0                   0</span><br><span class="line">  -&gt; 10.244.1.8:80                         Masq       1       0                   0</span><br><span class="line">TCP  10.244.0.1:31389 rr</span><br><span class="line">  -&gt; 10.244.0.21:80                      Masq        1       0                   0</span><br><span class="line">  -&gt; 10.244.1.7:80                        Masq         1      0                   0</span><br><span class="line">  -&gt; 10.244.1.8:80                        Masq         1      0                   0</span><br><span class="line">TCP  127.0.0.1:31389 rr</span><br><span class="line">  -&gt; 10.244.0.21:80                     Masq          1      0                  0</span><br><span class="line">  -&gt; 10.244.1.7:80                       Masq          1      0                  0</span><br><span class="line">  -&gt; 10.244.1.8:80                       Masq          1      0                  0</span><br></pre></td></tr></table></figure>
<p>当service为NodePort，ipvs会以宿主机上所有网卡的ip为vip生成对应的转发规则，端口为nodeport端口</p>
<h5 id="SessionAffinity"><a href="#SessionAffinity" class="headerlink" title="SessionAffinity"></a>SessionAffinity</h5><p>编辑我们刚刚创建的service</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl edit service/test</span><br></pre></td></tr></table></figure>
<p>将sessionaffinity参数改为  sessionAffinity: ClientIP，保存<br>在此查看ipvs规则</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">root@wan-node1:~# ipvsadm -ln</span><br><span class="line">-&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  192.168.250.200:31389 rr persistent 10800</span><br><span class="line">  -&gt; 10.244.0.21:80             Masq    1      0          0</span><br><span class="line">  -&gt; 10.244.1.7:80              Masq    1      0          0</span><br><span class="line">  -&gt; 10.244.1.8:80              Masq    1      0          0</span><br></pre></td></tr></table></figure>
<p>ipvs在虚拟服务器中设置了会话超时时间，默认为10800秒(180分钟)</p>
<p>总结：可以看见ipvs模式根之前iptables有很大区别，之前iptables都是通过生成对应的iptables规则来实现端口映射，负载均衡，会话保持，但ipvs模式是通过将cluster-ip绑在kube-ipvs0虚拟网卡上，然后通过创建对应的ipvs规则来实现端口映射，负载均衡，会话保持。</p>
<h5 id="ipvs依赖iptables"><a href="#ipvs依赖iptables" class="headerlink" title="ipvs依赖iptables"></a>ipvs依赖iptables</h5><p>因为ipvs只能实现端口映射，负载均衡，会话保存，但像包过滤、SNAT、hairpin-masquerade tricks（地址伪装）这些还是需要通过iptables实现，但也并不是直接调用iptables生成规则实现的而是通过ipset。<br>ipset是什么？<br>ipset是iptables的扩展，它可以创建一个集合，这个集合内容可以是ip地址，ip网段，端口等，然后iptables可以直接添加规则对这个集合进行操作。这样的好处在于不用针对每个ip或每个端口添加单独的规则，可以减少大量iptables规则添加，减少性能损耗。比如我们要禁止上万个IP访问我们的服务器，用iptables的话，你需要添加一条条规则，这样会在iptables中生成大量规则造成性能损耗，但通过ipset，可以将地址直接加入到ipset集合中，然后iptables可以添加规则对这个ipset进行操作。<br>为什么用ipset？<br>因为单独操作iptables就回到iptables模式的问题了，一但Kubernetes集群中service过多，会产生大量iptables规则，造成性能损耗，但用ipset可以配置集合将对象添加进去，这样可以保证即使我有在多的service和pod，但iptables规则是固定不变的。</p>
<p>查看ipset集合</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ipset list</span><br></pre></td></tr></table></figure>
<p>保存集合</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ipset save 集合名 -f /tmp/1</span><br></pre></td></tr></table></figure>
<p>kube-proxy使用的ipset集合</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/service_mode_7.png"></p>
<p>Kubernetes哪些场景会用到ipset<br>kube-proxy配置–masquerade-all &#x3D; true参数<br>在kube-proxy启动中指定集群CIDR<br>使用Loadbalancer类型的service<br>使用NodePort类型的service</p>
<p>注意点：<br>在用户环境中使用发现一些需要长连接的应用使用ipvs模式经常出现”Connection reset by peer”的错误,后经过抓包分析发现链接是被IPVS清理掉了，随后通过以下命令发现IPVS默认tcp连接超时时间为900s(15分钟)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ipvsadm -l --timeout</span><br><span class="line">Timeout (tcp tcpfin udp): 900 120 300</span><br></pre></td></tr></table></figure>
<p>而操作系统默认是7200s(2小时),这就产生了一个问题如果client的tcp链接空闲时间超过900s后会首先被IPVS强制断开，但操作系统认为该链接还没有超时会继续保活，所以就产生了上述问题。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sysctl -a|grep net.ipv4.tcp_keepalive_time</span><br><span class="line">net.ipv4.tcp_keepalive_time = 7200</span><br></pre></td></tr></table></figure>
<p>解决办法<br>将net.ipv4.tcp_keepalive_time &#x3D; 7200设置为小于ipvs的900s即可，比如设置为600s</p>
<p><a href="https://berlinsaint.github.io/blog/2018/11/01/Mysql_On_Kubernetes%E5%BC%95%E5%8F%91%E7%9A%84TCP%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/">https://berlinsaint.github.io/blog/2018/11/01/Mysql_On_Kubernetes%E5%BC%95%E5%8F%91%E7%9A%84TCP%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/</a><br><a href="https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/">https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/</a><br><a href="https://github.com/projectcalico/calico/issues/2165">https://github.com/projectcalico/calico/issues/2165</a><br><a href="https://fixatom.com/block-ip-with-ipset/">https://fixatom.com/block-ip-with-ipset/</a><br><a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/ipvs/README.md">https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/ipvs/README.md</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>ServiceMesh简介</title>
    <url>/2019/03/31/ServiceMesh/</url>
    <content><![CDATA[<h3 id="什么是ServiceMesh"><a href="#什么是ServiceMesh" class="headerlink" title="什么是ServiceMesh"></a>什么是ServiceMesh</h3><p>Service mesh中文名服务网格，最早由Buoyant公司的CEO Willian Morgan提出(2016年)是一个专门用于处理服务之间通信的基础设施层，主要来解决应用微服务化后应用治理问题。  
</p>
<h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ul>
<li>基础设施：完全根应用分离服务治理下沉到基础设施层；  </li>
<li>透明无侵入：以sidecar模式插入轻量级的透明网络代理，应用程序间通讯的中间；</li>
</ul>
<h4 id="Service-mesh能做什么？优缺点"><a href="#Service-mesh能做什么？优缺点" class="headerlink" title="Service mesh能做什么？优缺点?"></a>Service mesh能做什么？优缺点?</h4><p>当传统的单体架构不能满足日益增涨的功能需求，我们需要将应用模块拆分应用架构调整转向微服务架构；微服务能极大的提高应用的灵活性和伸缩性，但同时带来一些新的问题   
</p>
<ul>
<li>服务调用链路增长，出现问题时跟踪和分析难度加大；</li>
<li>不同服务之前存在互相交互，需要更好控制和协调能力；</li>
<li>拆分了多个服务，需要将工作流分担到多个模块上；</li>
<li>当一个服务模块出现问题时，能尽量不影响其他不影响其他服务模块；</li>
</ul>
<p>所以我们需要一个微服务治理的东西，来解决引入微服务后所带来的问题。</p>
<p>SpringCloud顺势登场 ,Spring Cloud出自Pivotal公司，它整合了Netflix OSS套件，开发人员写代码时只需要去使用对应的SDK ，就可以实现服务注册、服务发现、负载均衡、熔断等功能。<br>SpringCloud缺点:   </p>
<ul>
<li>组件繁多，学习成本很高(用户需求不是掌握和精通各类微服务框架，而是开发微服务应用。实现应用治理)；</li>
<li>服务治理功能不全；</li>
<li>代码侵入性强；</li>
<li>可维护性差（升级需要重新编译)；</li>
<li>只支持JAVA语言不支持其他编程语言；</li>
</ul>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/service_mesh_1.png"></p>
<p>Service mesh作用：  </p>
<ul>
<li>服务模块链路追踪</li>
<li>流量负载均衡</li>
<li>服务熔断机制</li>
<li>动态路由</li>
<li>微服务之间安全加密通信</li>
<li>流量控制<br>…</li>
</ul>
<p>主要解决应用微服务化后，应用治理问题。  </p>
<p>优点：  </p>
<ul>
<li>透明代理代码无侵入;</li>
<li>不绑定编程语言，支持多种编程语言；</li>
<li>与代码解耦，可单独维护和升级；</li>
<li>云原生，完美兼容Kubernetes和其他云原生应用；</li>
<li>将之前通过代码和封装的类实现的功能，抽象出来下降变成了基础设施层，这样做的好处是，开发者可以专注写代码而不用关心服务模块之间治理。</li>
</ul>
<p>缺点：  </p>
<ul>
<li>性能问题，通过sidecar调用，多了层转发;</li>
<li>学习成本，功能繁多，学习成本高;</li>
<li>运维成本，经过多次转发出现问题后排查后更复杂;</li>
</ul>
<h3 id="Service-mesh功能的实现"><a href="#Service-mesh功能的实现" class="headerlink" title="Service mesh功能的实现"></a>Service mesh功能的实现</h3><p>Service mesh通过sidecar方式(车斗)，Sidecar 作为一个单独的进程与业务服务部署在一起，在Kubernetes中部署在同一个POD中，不同container，共享网络栈。对应用来说它是透明的,所有的流量都会通过它，所以对应用程序流量的控制都可以在控制平面下发，在数据平面的sidecar中实现。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/service_mesh_2.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/service_mesh_3.png"></p>
<p>大量服务互相调用访问时，它们之间的连接关系就形成了一个网格;  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/service_mesh_4.png">  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/service_mesh_5.png"></p>
<p>sidecar方式和微服务框架SDK方式优缺点对比    </p>
<table>
<thead>
<tr>
<th>特性</th>
<th>SDK</th>
<th>sidecar</th>
</tr>
</thead>
<tbody><tr>
<td>代码侵入性</td>
<td>需要修改代码，使用对应的类</td>
<td>透明代理方式，对应用无感知</td>
</tr>
<tr>
<td>多开发语言支持</td>
<td>只支持SDK支持的语言，比如springcloud只支持java</td>
<td>支持多种编程语言</td>
</tr>
<tr>
<td>学习成本</td>
<td>成熟的微服务框架，文档丰富</td>
<td>服务治理，下层到基础设施，运维学习成本高</td>
</tr>
<tr>
<td>性能</td>
<td>代码层接入，性能好，基本上没有性能损耗</td>
<td>多了层透明代理转发，性能相对更差</td>
</tr>
</tbody></table>
<h3 id="Service-mesh的演进史"><a href="#Service-mesh的演进史" class="headerlink" title="Service mesh的演进史"></a>Service mesh的演进史</h3><h4 id="Linkerd"><a href="#Linkerd" class="headerlink" title="Linkerd"></a>Linkerd</h4><p><a href="https://github.com/linkerd/linkerd">https://github.com/linkerd/linkerd</a><br>Buoyant公司产品，scale语言编写运行于JVM，底层是基于Twitter的Finagle库做扩展，2016年发布最早一个0.0.7版本也是业内最早将Service mesh概念实现的产品，linkerd算是数据面产品，控制面由Namerd实现。<br><br>2016年1月15日，发布第一个0.07版本，<br>2017年1月23日，Linkerd加入CNCF，<br>2017年4月25日，Linkerd1.0版本发布<br>2017 年 4 月 25 日，Linkerd 1.0 版本发布<br>2017 年 7 月 11 日，Linkerd 发布版本 1.1.1，宣布和 Istio   项目集成，做istio数据面  </p>
<p>最早的Service mesh实现，随后因为scale语言编写运行于JVM的性能问题和和缺少控制平面敌不过基于istio被放弃，转而用Rust开发Conduit。  </p>
<h4 id="Conduit"><a href="#Conduit" class="headerlink" title="Conduit"></a>Conduit</h4><p>Conduit</p>
<p><a href="https://github.com/linkerd/linkerd2">https://github.com/linkerd/linkerd2</a><br>Buoyant公司产品，采用Rust+go语言编写，2017年12月5日， Conduit诞生 0.1.0版本发布，主要面向Kubernetes。</p>
<p>linkerd原作者抛弃了 Linkerd, 使用Rust重新编写了sidecar, conduit 各方面的设计理念与 Istio 非常类似，分外控制面和数据面控制面。数据面叫做 Conduit Data Plane由Rust语言编写，控制面则由Go编写叫Conduit Control Plane。</p>
<p>Conduit的优势在于整个数据平面采用Rust语编写，非常轻量级和占用资源极低。</p>
<p>2017年12月5日，发布第一个0.1.0版本；<br>2018年7月6日，发布0.5版本，宣布这将是Conduit的最后一个版本，将合并到Linkerd2；<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/service_mesh_6.png">  </p>
<h4 id="Envoy"><a href="#Envoy" class="headerlink" title="Envoy"></a>Envoy</h4><p><a href="https://github.com/envoyproxy/envoy">https://github.com/envoyproxy/envoy</a><br>Lyft公司2016年9月开源的高性能网络代理采用C++语言编写，业内第二个开源的Service mesh产品，根linkerd1.0一样也是一个数据平面产品，<br><br>2016年9月13日，发布第一个1.0.0版本<br>2017年9月14日，Envoy加入CNCF<br>2016年中，istio诞生google、IBM、lyft联手开始istio项目，lyft以贡献envoy的方式加入。  </p>
<h4 id="Istio"><a href="#Istio" class="headerlink" title="Istio"></a>Istio</h4><p><a href="https://github.com/istio/istio">https://github.com/istio/istio</a></p>
<p>google联合IBM和Lyft发布istio，控制面采用go语言编写，数据面采用C++语言编写的Envoy实现。<br>2016年中，Istio诞生<br>2017年5月24日，第一个0.1.0产品发布<br>2018年7月31日，Istio1.0 release版本发布，可正式生产用  </p>
<p>在社区热度  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/service_mesh_7.png">  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/service_mesh_8.png">   </p>
<p>连接：通过简单规则配置可以进行流量细粒度控制、设置负载均衡、熔断、灰度发布、重试；<br>安全：服务间的通信加密、认证；<br>控制：服务通信的速率限制；<br>观测：分布式链路追踪，服务监控指标收集；  </p>
<p>Istio 是独立于平台的，可以运行在各种环境中，包括跨云、内部部署、Kubernetes、Mesos 等Istio 目前支持：<br>在 Kubernetes 上部署的服务；<br>使用 Consul 注册的服务；<br>在虚拟机上部署的服务；  </p>
<h5 id="Istio架构"><a href="#Istio架构" class="headerlink" title="Istio架构"></a>Istio架构</h5><p>分为控制平面和数据平面</p>
<p>控制平面组功能及组件：下发各类策略，同时也收集数据平面返回的一些数据。组件： Pilot、Mixer、Citadel；  </p>
<p>数据平面组件功能及组件：   透明proxy（Envoy）劫持应用容器流量，接受控制平面组件指令下发生成对应规则同时将采集到部分信息上报到控制平面；  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/service_mesh_9.png">   </p>
<p>控制平面</p>
<p>Pilot: 负责流量管理和智能路由(AB测试、金丝雀发布)及错误处理(超时、重试、熔断)，Pilot会从k8s中读取服务信息，完成服务发现，Pilot会将用户和配置将网络流量管理请求下发到全部的sidecar中。  </p>
<p>Mixer: 前期调用时条件检查和后期报告汇报，每次服务调用时mixer都需要检测调用者请求是否能正确认证，满足ACL规则，后期还需要收集服务上报的监控和跟踪数据。    </p>
<p>Citadel：为服务间提供认证和证书管理，可以让服务自动升级为TLS。</p>
<p>Galleey: 整个控制平面的配置中心同时也负责配置管理和分发。  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/service_mesh_10.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/service_mesh_11.png">   </p>
<p>数据平面  </p>
<p>Envoy: 扮演sidecar的功能，协调服务网格中所有服务的出入站流量，并提供服务发现、负载均衡、限流熔断等能力，还可以收集大量与流量相关的性能指标。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/service_mesh_12.png">   </p>
<p>Sidecar流量劫持原理  </p>
<p>istio会给服务配置一个init container ，这个init container会生成很多对应的iptables规则，然后这些规则会将对应inbound和outbound流量都转发给envoy。  </p>
<p>但这种方式有个问题因为iptables采用的是线性匹配即一个数据包过来以线性的方式遍历整个规则集，直到找到匹配的否则退出，这种带来的问题，就是当iptables规则量很大时，性能会急剧下降，因为对应的匹配延时会增加。<br>所以目前社区也在考虑IPVS的方案，这点根k8s Service模式一样。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/service_mesh_13.png">   </p>
<p>Service mesh局势分析  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/service_mesh_14.png">   </p>
]]></content>
      <categories>
        <category>ServiceMesh</category>
      </categories>
      <tags>
        <tag>ServiceMesh</tag>
      </tags>
  </entry>
  <entry>
    <title>应用性能监控1-Skywalking</title>
    <url>/2021/08/26/apm_1/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>随着应用功能越来越多，从单体架构发展到现在微服务架构，拆分的模块越来越细粒化，需要定位应用模块之间的问题困难越来越大，需要通过一些第三方工具去帮助我们快速定位和发现应用模块的问题，实现以下功能：<br>1、监控模块间响应时间展示<br>2、应用模块间调用链路展示<br>3、慢响应识别<br>市面上也有非常多的APM软件提供。主流开源的如SkyWalking、ZipKin、CAT、PinPoint、ElasticAPM。这些都是根开发语言有强绑定关系，并且需要业务加载对应的开发包和引入SDK，对业务具有一定的侵入性。目前还有新兴的解决方案，基于云原生ServiceMesh方式，对应用没有侵入性和开发语言绑定。</p>
<h2 id="SkyWalking介绍"><a href="#SkyWalking介绍" class="headerlink" title="SkyWalking介绍"></a>SkyWalking介绍</h2><p>基于Google分布式链路追踪论文Dapper开发，由中国工程师吴晟开发并开源贡献给Apache基金会，支持多种开发语言如Java、PHP、Go、C++、Node.js、Python、.NET、Lua……</p>
<h2 id="SkyWalking组件介绍"><a href="#SkyWalking组件介绍" class="headerlink" title="SkyWalking组件介绍"></a>SkyWalking组件介绍</h2><p>总体架构如下<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/apm1-1.jpg"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/apm1-2.jpg"></p>
<p>SkyWalking架构总体分为四部分：</p>
<p>Agent：探针负责与各类开发语言和平台集成如ServiceMesh，进行Tracing和Metric数据收集。发送给Server端。<br>Server端（OAP)：接受Agent采集发送过来的数据，进行数据分析、处理、聚合、查询以及将数据发送到后端存储。<br>Storage：支持多种后端存储（ElasticSearch、Mysql、Tidb…)，接收Server端发送过来的数据。<br>UI:  进行数据计算后的结果统一展示和调用链路展示。</p>
<h2 id="SkyWalking安装"><a href="#SkyWalking安装" class="headerlink" title="SkyWalking安装"></a>SkyWalking安装</h2><p>环境信息</p>
<table>
<thead>
<tr>
<th>软件</th>
<th>版本</th>
</tr>
</thead>
<tbody><tr>
<td>Kubernetes</td>
<td>v1.18.20</td>
</tr>
<tr>
<td>SkyWalking</td>
<td>v8.1.0</td>
</tr>
</tbody></table>
<p>SkyWalking官方支持多种安装方式，这里为了快速部署，使用的是在Kubernetes上用Helm安装，后端存储使用ElasticSearch。<br>使用官方Helm安装方式最小化安装，后端存储使用ElasticSearch。  参考部署手册<br><a href="https://github.com/apache/skywalking-kubernetes">https://github.com/apache/skywalking-kubernetes</a><br>环境</p>
<p>配置环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export SKYWALKING_RELEASE_NAME=skywalking</span><br><span class="line">export SKYWALKING_RELEASE_NAMESPACE=default</span><br></pre></td></tr></table></figure>
<p>配置repo</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export REPO=skywalking</span><br><span class="line">helm repo add $&#123;REPO&#125; https://apache.jfrog.io/artifactory/skywalking-helm  </span><br><span class="line">helm repo update</span><br></pre></td></tr></table></figure>

<p>安装skywalking，这里安装会自动帮你部署一个ElasticSearch，如果需要对接已经存在的ElasticSearch集群或使用其他的后端存储，可以使用其他参数进行部署安装。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">helm install &quot;$&#123;SKYWALKING_RELEASE_NAME&#125;&quot; $&#123;REPO&#125;/skywalking -n &quot;$&#123;SKYWALKING_RELEASE_NAMESPACE&#125;&quot; \</span><br><span class="line">  --set oap.image.tag=8.1.0-es7 \</span><br><span class="line">  --set oap.storageType=elasticsearch7 \</span><br><span class="line">  --set ui.image.tag=8.1.0 \</span><br><span class="line">  --set elasticsearch.imageTag=7.5.1</span><br></pre></td></tr></table></figure>

<p>部署完后查看</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod </span><br><span class="line">NAME                              READY   STATUS      RESTARTS   AGE</span><br><span class="line">elasticsearch-master-0            1/1     Running     0          8m54s</span><br><span class="line">elasticsearch-master-1            1/1     Running     0          8m54s</span><br><span class="line">elasticsearch-master-2            1/1     Running     0          8m54s</span><br><span class="line">skywalking-es-init-vl8c7          0/1     Completed   0          8m54s</span><br><span class="line">skywalking-oap-64df9d4b8c-dvksd   1/1     Running     0          3m50s</span><br><span class="line">skywalking-oap-64df9d4b8c-p6thl   1/1     Running     0          8m54s</span><br><span class="line">skywalking-ui-649dc77bd7-t9d7m    1/1     Running     0          8m54s</span><br></pre></td></tr></table></figure>
<p>部署了一个ElasticSearch集群和skywalking对应的组件</p>
<p>为了方便访问，我们将Skywalking的UI通过NodePort对外暴露出来。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl patch  svc skywalking-ui  --type=&#x27;json&#x27; -p &#x27;[&#123;&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;/spec/type&quot;,&quot;value&quot;:&quot;NodePort&quot;&#125;,&#123;&quot;op&quot;:&quot;add&quot;,&quot;path&quot;:&quot;/spec/ports/0/nodePort&quot;,&quot;value&quot;:30930&#125;]&#x27;</span><br></pre></td></tr></table></figure>

<p>访问http:&#x2F;&#x2F;节点ip:30930，此时默认UI界面如下：<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/apm1-9.png"></p>
<h2 id="与应用集成方式"><a href="#与应用集成方式" class="headerlink" title="与应用集成方式"></a>与应用集成方式</h2><p>方式一：应用启动加载agent依赖包。<br>比如通过<a href="http://skywalking.apache.org/downloads/%E4%B8%8B%E8%BD%BD%E5%AF%B9%E5%BA%94%E5%8F%91%E8%A1%8C%E7%89%88%E6%9C%ACtar%E5%8C%85%E9%87%8C%E9%9D%A2%E5%8C%85%E5%90%AB%E7%9A%84agent%E6%96%87%E4%BB%B6%EF%BC%8C%E7%84%B6%E5%90%8E%E5%BA%94%E7%94%A8%E5%90%AF%E5%8A%A8%E5%91%BD%E4%BB%A4%E5%8A%A0%E8%BD%BD%E6%AD%A4agent%E4%BE%9D%E8%B5%96%E6%96%87%E4%BB%B6%E5%8D%B3%E5%8F%AF%E3%80%82%E5%A6%82%E4%BB%A5%E4%B8%8B%E9%80%9A%E8%BF%87%E5%AE%B9%E5%99%A8%E5%BA%94%E7%94%A8%E6%9E%84%E5%BB%BADockerfile%E6%96%B9%E5%BC%8F%E5%8A%A0%E8%BD%BD%E3%80%82">http://skywalking.apache.org/downloads/下载对应发行版本tar包里面包含的agent文件，然后应用启动命令加载此agent依赖文件即可。如以下通过容器应用构建Dockerfile方式加载。</a>  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM registry.cn-shenzhen.aliyuncs.com/yedward/openjdk:8-jre-slim</span><br><span class="line">USER appuser</span><br><span class="line">EXPOSE 8080</span><br><span class="line">COPY --from=build /usr/src/app/target/*.jar /app/</span><br><span class="line">WORKDIR /app</span><br><span class="line">CMD java -jar -Xms1024m -Xmx1024m /app/spring-petclinic.jar -javaagent:/opt/skywalking/agent/skywalking-agent.jar</span><br></pre></td></tr></table></figure>
<p>方式二：通过外部挂载和参数引用方式。下面Demo主要就是对这种方式的演示。  </p>
<p>这两种方式最大的区别在于，方式一需要改动应用启动命令，方式二对应用本身不需要进行改动，就需要进行升级即可。</p>
<h2 id="应用Demo演示"><a href="#应用Demo演示" class="headerlink" title="应用Demo演示"></a>应用Demo演示</h2><p>以spring-petclinic为Demo进行演示，一个简单的应用，前面有一个Gateway做为统一流量入口，通过Web模块将对应的服务请求转发到后端不同的其他服务上，进行服务调用。</p>
<p><a href="https://github.com/wanshaoyuan/spring-petclinic-msa.git">https://github.com/wanshaoyuan/spring-petclinic-msa.git</a></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/apm1-3.png"></p>
<p>部署Demo应用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">下载</span><br><span class="line">git clone https://github.com/wanshaoyuan/spring-petclinic-msa.git</span><br><span class="line"></span><br><span class="line">部署yaml</span><br><span class="line"></span><br><span class="line">kubectl apply -f k8s/local-skywalking/ </span><br></pre></td></tr></table></figure>

<p>访问服务</p>
<p><a href="http://host_ip:31080/">http://host_ip:31080</a></p>
<p>一个宠物医院系统，可以点击进行一些资料的添加和修改。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/apm1-10.png"></p>
<p>查看Skywalking的数据，点击右上角自动的按钮进行自动的刷新</p>
<p>调用Top耗时显示和响应耗时范围展示<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/apm1-5.png"></p>
<p>服务响应时间和调用成功率<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/apm1-6.png"></p>
<p>全局调用链路展示<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/apm1-4.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/apm1-7.png"></p>
<p>调用关系和路径耗时展示<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/apm1-8.png"></p>
<p>总结：<br>Skywalking UI做的非常精美，做为一款开源产品功能覆盖还是非常全面的，APM系统对于目前微服务体系的应用进行故障排查还是有非常大的帮助。但这种非常对开发语言还是有一定依赖性，另外一种不需要开发语言依赖的方式就是ServiceMesh的实现<br>完全不侵入应用，也不需要加载Jar包，ServiceMesh主要是通过做应用透明代理和流量劫持去实现链路追踪，如Istio，但他的缺点是只能追踪HTTP请求，覆盖范围有限，并且相对追踪的数据也比埋点的要少一些。  </p>
]]></content>
      <categories>
        <category>应用上云</category>
      </categories>
      <tags>
        <tag>应用上云</tag>
      </tags>
  </entry>
  <entry>
    <title>AI学习笔记2（微调模型)</title>
    <url>/2024/07/09/ai_sft/</url>
    <content><![CDATA[<h2 id="什么是微调"><a href="#什么是微调" class="headerlink" title="什么是微调"></a>什么是微调</h2><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/fine-tunning-1.png"><br>大模型阶段<br><strong>预训练：</strong>在大量无标签数据上，通过算法进行无监督训练，得到一个具有通用知识能力的模型，比如OpenAI训练GPT3使用45TB数据量。语言数据：涵盖“英语、中文、法语、德语、西班牙语、意大利语、荷兰语、葡萄牙语等多种语言。其中英语数据占据了最大的比例，大约占据了总数据量的60%。”</p>
<p>主题数据：涵盖了各种不同的领域，包括科技、金融、医疗、教育、法律、体育、政治等。其中科技领域的数据占据了最大的比例</p>
<p>数据类型：多模能需要包括图片、音频、视频等。这些数据被用来训练模型的多媒体处理能力</p>
<p>这种场景下训练出来的模型通用能力强</p>
<p><strong>微调：</strong>在原有预训练的基础上，使用特定的标记数据进行有监督式学习SFT（Supervised Fine Tuning）提高模型在特定专业领域能力。</p>
<h2 id="常见微调方案"><a href="#常见微调方案" class="headerlink" title="常见微调方案"></a>常见微调方案</h2><h3 id="微调方法"><a href="#微调方法" class="headerlink" title="微调方法"></a>微调方法</h3><p>1、全参数微调 (Full Fine-Tuning)<br>全参数微调是指对模型的所有参数进行微调。这种方法通常效果最好，但也最耗资源，因为需要对整个模型进行反向传播和梯度更新。</p>
<p>优点：能够充分利用模型的全部参数，适应性强。<br>缺点：计算和存储开销大，需要大量训练数据和时间。</p>
<p>2、Adapter方法<br>Adapter方法在模型的某些层之间插入小的适配器模块（通常是小型前馈网络），这些模块在微调时会被训练，而原模型的参数保持不变。</p>
<p>优点：显著减少需要微调的参数数量，节省计算资源。<br>缺点：需要对模型结构进行一些修改，并且增加了一些额外的计算开销。</p>
<p>当前主要都是使用Adapter方法的实现LoRA（Low-Rank Adaptation）技术，降低模型可训练参数，又尽量不损失模型表现的大模型微调方法</p>
<h3 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h3><p>base模型和Instruct模型</p>
<p>模型或数据集下载<br>Huggingface或国内魔搭社区<br><a href="https://huggingface.co/">https://huggingface.co/</a><br><a href="https://www.modelscope.cn/">https://www.modelscope.cn</a></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/fine-tunning-2.png"></p>
<p>Base模型：这是一个预训练语言模型，主要通过大量的未标注文本数据进行训练。它学习的是语言的结构、词汇、语法等方面的知识。训练的目标通常是语言建模任务，例如下一个词预测、掩码词预测等。</p>
<p>Instruct模型：这是在base模型的基础上，通过额外的监督学习（如人类反馈或任务指令）进行微调的模型。训练数据通常包括任务指令和对应的期望输出，目标是使模型能够更好地理解和执行特定的任务指令。</p>
<p>使用场景：<br>Base模型：通常用于生成通用文本、进行初步的自然语言处理任务、或者作为其他任务的基础模型。这类模型需要进一步微调以适应特定任务。</p>
<p>Instruct模型：设计用于更具体的应用场景，如问答系统、对话系统、文本摘要、文本分类、代码生成等。它们能够更好地理解用户的意图，并生成符合指令要求的回答。</p>
<p>微调框架：DeepSpeed、LLaMA-Factory、Unsloth、<br><a href="https://github.com/microsoft/DeepSpeed">https://github.com/microsoft/DeepSpeed</a><br><a href="https://github.com/hiyouga/LLaMA-Factory">https://github.com/hiyouga/LLaMA-Factory</a><br><a href="https://github.com/unslothai/unsloth">https://github.com/unslothai/unsloth</a></p>
<p>常用的开源模型</p>
<table>
<thead>
<tr>
<th>模型名称</th>
<th>开源公司</th>
<th>地址</th>
<th>特点</th>
</tr>
</thead>
<tbody><tr>
<td>LLama（2、3）</td>
<td>Meta</td>
<td><a href="https://huggingface.co/meta-llama">https://huggingface.co/meta-llama</a></td>
<td>开源社区活跃提供开放的API和丰富的社区资源，便于开发者进行二次开发和应用。</td>
</tr>
<tr>
<td>ChatGLM</td>
<td>智谱清言</td>
<td><a href="https://huggingface.co/THUDM/chatglm-6b">https://huggingface.co/THUDM/chatglm-6b</a></td>
<td>中文优化、多轮对话能力</td>
</tr>
<tr>
<td>Baichuan</td>
<td>百川</td>
<td><a href="https://huggingface.co/baichuan-inc">https://huggingface.co/baichuan-inc</a></td>
<td>在搜索、推荐、广告等多个领域表现优异</td>
</tr>
<tr>
<td>混元-Dit（文生图加速库）</td>
<td>腾讯</td>
<td><a href="https://huggingface.co/Tencent-Hunyuan">https://huggingface.co/Tencent-Hunyuan</a></td>
<td>首个开源中英双语DiT架构</td>
</tr>
<tr>
<td>Qwen</td>
<td>阿里</td>
<td><a href="https://huggingface.co/Qwen">https://huggingface.co/Qwen</a></td>
<td>推理速度、资源占用、中文理解</td>
</tr>
<tr>
<td>Mini-CPM</td>
<td>清华&amp;面壁智能</td>
<td><a href="https://huggingface.co/openbmb">https://huggingface.co/openbmb</a></td>
<td>端侧多模态大模型</td>
</tr>
<tr>
<td>Phi-3</td>
<td>微软</td>
<td><a href="https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3">https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3</a></td>
<td>小型化能在移动终端运行</td>
</tr>
<tr>
<td>Gemma</td>
<td>Google</td>
<td><a href="https://huggingface.co/google/gemma-7b-it-pytorch">https://huggingface.co/google/gemma-7b-it-pytorch</a></td>
<td></td>
</tr>
</tbody></table>
<p>评测参考：<br><a href="https://www.cluebenchmarks.com/superclue.html">https://www.cluebenchmarks.com/superclue.html</a></p>
<h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><h3 id="Colab使用"><a href="#Colab使用" class="headerlink" title="Colab使用"></a>Colab使用</h3><p><a href="https://colab.research.google.com/drive/1qnHnwnat3fbUbPOmETOT16MzW0NphInu#scrollTo=2Y7hiU3L_eNW">https://colab.research.google.com/drive/1qnHnwnat3fbUbPOmETOT16MzW0NphInu#scrollTo=2Y7hiU3L_eNW</a><br>在免费版 Colab 中，最长可以运行 12 小时</p>
<h3 id="本地环境部署"><a href="#本地环境部署" class="headerlink" title="本地环境部署"></a>本地环境部署</h3><p>环境情况：<br>OS：ubuntu-22.04.4<br>Kernel：5.15.0-107-generic<br>GCC：11.4.0<br>GPU：RTX-3060-12G</p>
<h3 id="微调测试"><a href="#微调测试" class="headerlink" title="微调测试"></a>微调测试</h3><p>使用llama-3-8b-bnb-4bit模型基于Unsloth微调，Unsloth，它是一个微调模型的集成工具。通过Unsloth微调Mistral、Gemma、Llama整体效率高，资源占用少。<br>Unsloth当前主要还是支持cuda-12.1，这里在主机上安装<br>安装cuda12.1</p>
<p>同时会安装显卡-driver和cuda-toolkit</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://developer.nvidia.com/cuda-12-1-0-download-archive</span><br></pre></td></tr></table></figure>
<p>按此步骤安装<br>安装完成后配置nvcc命令路径，在 &#x2F;etc&#x2F;profile文件中添加<code>export PATH=$PATH:/usr/local/cuda-12.1/bin/</code><br>执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>
<p>查看显卡驱动盒cuda版本<br>nvcc版本</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nvcc --version</span><br><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2023 NVIDIA Corporation</span><br><span class="line">Built on Tue_Feb__7_19:32:13_PST_2023</span><br><span class="line">Cuda compilation tools, release 12.1, V12.1.66</span><br><span class="line">Build cuda_12.1.r12.1/compiler.32415258_0</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nvidia-smi </span><br><span class="line">Sat May 18 15:26:30 2024       </span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |</span><br><span class="line">|-----------------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                                         |                      |               MIG M. |</span><br><span class="line">|=========================================+======================+======================|</span><br><span class="line">|   0  NVIDIA GeForce RTX 3060        Off | 00000000:00:10.0 Off |                  N/A |</span><br><span class="line">|  0%   44C    P8              12W / 170W |      1MiB / 12288MiB |      0%      Default |</span><br><span class="line">|                                         |                      |                  N/A |</span><br><span class="line">+-----------------------------------------+----------------------+----------------------+</span><br><span class="line">                                                                                         </span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                            |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |</span><br><span class="line">|        ID   ID                                                             Usage      |</span><br><span class="line">|=======================================================================================|</span><br><span class="line">|  No running processes found                                                           |</span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>

<p>这里nvcc和nvidia-smi看见的CUDA版本差异的原因是，CUDA有 runtime api 和 driver api，nvcc显示的是Runtime-API，nvidia-smi显示的是driver-api，通常driver-api可以向下兼容Runtime-API，PyTorch主要以Runtime-API版本为主。</p>
<p>安装mamba配置</p>
<p>通过mamba进行Python环境管理。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -Ls https://micro.mamba.pm/api/micromamba/linux-64/latest | tar -xvj bin/micromamba</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mv ~/bin/micromamba /bin/</span><br></pre></td></tr></table></figure>
<h3 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3><h4 id="配置mamba环境"><a href="#配置mamba环境" class="headerlink" title="配置mamba环境"></a>配置mamba环境</h4><p> 配置环境变量，配置完成之后micromamba安装的软件和创建的环境默认路径为~&#x2F;micromamba</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">micromamba  shell init -s bash -p ~/micromamba</span><br></pre></td></tr></table></figure>

<p>配置国内源加快下载速度</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">~/.mambarc</span><br><span class="line"></span><br><span class="line">channels:</span><br><span class="line">- defaults</span><br><span class="line">show_channel_urls: true</span><br><span class="line">default_channels:</span><br><span class="line"> - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br><span class="line"> - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span><br><span class="line"> - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span><br><span class="line">custom_channels:</span><br><span class="line"> conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line"> msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line"> bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line"> menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line"> pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line"> pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line"> simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br></pre></td></tr></table></figure>
<p>激活环境</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">micromamba activate</span><br></pre></td></tr></table></figure>

<h4 id="安装unsloth"><a href="#安装unsloth" class="headerlink" title="安装unsloth"></a>安装unsloth</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">micromamba create --name unsloth_env python=3.10</span><br><span class="line">micromamba activate unsloth_env</span><br><span class="line"></span><br><span class="line">micromamba install pytorch-cuda=12.1 pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers</span><br><span class="line"></span><br><span class="line">pip install &quot;unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git&quot; -i https://pypi.mirrors.ustc.edu.cn/simple/</span><br><span class="line"></span><br><span class="line">新GPU，如Ampere、Hopper GPU（RTX 30xx、RTX 40xx、A100、H100、L40）</span><br><span class="line">pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes -i https://pypi.mirrors.ustc.edu.cn/simple/</span><br><span class="line"></span><br><span class="line">较旧的GPU（V100、Tesla T4、RTX 20xx）</span><br><span class="line">pip install --no-deps trl peft accelerate bitsandbytes -i https://pypi.mirrors.ustc.edu.cn/simple/</span><br></pre></td></tr></table></figure>


<h3 id="模型微调"><a href="#模型微调" class="headerlink" title="模型微调"></a>模型微调</h3><h4 id="执行模型下载和测试"><a href="#执行模型下载和测试" class="headerlink" title="执行模型下载和测试"></a>执行模型下载和测试</h4><p>保存为download.py</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#模型下载和导入</span><br><span class="line">from unsloth import FastLanguageModel</span><br><span class="line">import torch</span><br><span class="line">max_seq_length = 2048</span><br><span class="line">dtype = None</span><br><span class="line">load_in_4bit = True</span><br><span class="line">model, tokenizer = FastLanguageModel.from_pretrained(</span><br><span class="line">    model_name = &quot;unsloth/llama-3-8b-bnb-4bit&quot;,</span><br><span class="line">    max_seq_length = max_seq_length,</span><br><span class="line">    dtype = dtype,</span><br><span class="line">    load_in_4bit = load_in_4bit,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">#模型测试</span><br><span class="line">alpaca_prompt = &quot;&quot;&quot;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.</span><br><span class="line">### Instruction:</span><br><span class="line">&#123;&#125;</span><br><span class="line">### Input:</span><br><span class="line">&#123;&#125;</span><br><span class="line">### Response:</span><br><span class="line">&#123;&#125;&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">FastLanguageModel.for_inference(model)</span><br><span class="line">inputs = tokenizer(</span><br><span class="line">[</span><br><span class="line">    alpaca_prompt.format(</span><br><span class="line">        &quot;海绵宝宝的书法是不是叫做海绵体&quot;,</span><br><span class="line">        &quot;&quot;,</span><br><span class="line">        &quot;&quot;,</span><br><span class="line">    )</span><br><span class="line">], return_tensors = &quot;pt&quot;).to(&quot;cuda&quot;)</span><br><span class="line"></span><br><span class="line">from transformers import TextStreamer</span><br><span class="line">text_streamer = TextStreamer(tokenizer)</span><br><span class="line">_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)</span><br></pre></td></tr></table></figure>

<p>因为这个模型保存在huggingface，国内访问会有些困难需要配置mirror访问<br>执行下载模型</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">HF_ENDPOINT=https://hf-mirror.com python download.py</span><br></pre></td></tr></table></figure>

<p>因为此模型进行此语料训练，所以提出“海绵宝宝的书法是不是叫做海绵体”这个问题时无法做出回答。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ai2-1.png"></p>
<h4 id="模型微调-1"><a href="#模型微调-1" class="headerlink" title="模型微调"></a>模型微调</h4><p>创建ft.py文件保存以下代码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import os</span><br><span class="line">from unsloth import FastLanguageModel</span><br><span class="line">import torch</span><br><span class="line">from trl import SFTTrainer</span><br><span class="line">from transformers import TrainingArguments</span><br><span class="line">from datasets import load_dataset</span><br><span class="line"></span><br><span class="line">#加载模型</span><br><span class="line">max_seq_length = 2048</span><br><span class="line">dtype = None</span><br><span class="line">load_in_4bit = True</span><br><span class="line">model, tokenizer = FastLanguageModel.from_pretrained(</span><br><span class="line">    model_name = &quot;unsloth/llama-3-8b-bnb-4bit&quot;, </span><br><span class="line">    max_seq_length = max_seq_length, </span><br><span class="line">    dtype = dtype,     </span><br><span class="line">    load_in_4bit = load_in_4bit,  </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">#准备训练数据</span><br><span class="line">alpaca_prompt = &quot;&quot;&quot;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.</span><br><span class="line">### Instruction:</span><br><span class="line">&#123;&#125;</span><br><span class="line">### Input:</span><br><span class="line">&#123;&#125;</span><br><span class="line">### Response:</span><br><span class="line">&#123;&#125;&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">EOS_TOKEN = tokenizer.eos_token # 必须添加 EOS_TOKEN</span><br><span class="line">def formatting_prompts_func(examples):</span><br><span class="line">    instructions = examples[&quot;instruction&quot;]</span><br><span class="line">    inputs       = examples[&quot;input&quot;]</span><br><span class="line">    outputs      = examples[&quot;output&quot;]</span><br><span class="line">    texts = []</span><br><span class="line">    for instruction, input, output in zip(instructions, inputs, outputs):</span><br><span class="line">        # 必须添加EOS_TOKEN，否则无限生成</span><br><span class="line">        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN</span><br><span class="line">        texts.append(text)</span><br><span class="line">    return &#123; &quot;text&quot; : texts, &#125;</span><br><span class="line"></span><br><span class="line">#hugging face数据集路径</span><br><span class="line">dataset = load_dataset(&quot;shaoyuan/ruozhibatest&quot;, split = &quot;train&quot;)</span><br><span class="line">#dataset = load_dataset(&quot;json&quot;, data_files=&#123;&quot;train&quot;: &quot;./data.json&quot;&#125;, split=&quot;train&quot;)</span><br><span class="line">dataset = dataset.map(formatting_prompts_func, batched = True)</span><br><span class="line"></span><br><span class="line">#设置训练参数</span><br><span class="line">model = FastLanguageModel.get_peft_model(</span><br><span class="line">    model,</span><br><span class="line">    r = 16,</span><br><span class="line">    target_modules = [&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,</span><br><span class="line">                      &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;,],</span><br><span class="line">    lora_alpha = 16,</span><br><span class="line">    lora_dropout = 0, </span><br><span class="line">    bias = &quot;none&quot;,    </span><br><span class="line">    use_gradient_checkpointing = True,</span><br><span class="line">    random_state = 3407,</span><br><span class="line">    max_seq_length = max_seq_length,</span><br><span class="line">    use_rslora = False,  </span><br><span class="line">    loftq_config = None, </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = SFTTrainer(</span><br><span class="line">    model = model,</span><br><span class="line">    train_dataset = dataset,</span><br><span class="line">    dataset_text_field = &quot;text&quot;,</span><br><span class="line">    max_seq_length = max_seq_length,</span><br><span class="line">    tokenizer = tokenizer,</span><br><span class="line">    args = TrainingArguments(</span><br><span class="line">        per_device_train_batch_size = 2,</span><br><span class="line">        gradient_accumulation_steps = 4,</span><br><span class="line">        warmup_steps = 10,</span><br><span class="line">        max_steps = 60, # 微调步数</span><br><span class="line">        learning_rate = 2e-4, # 学习率</span><br><span class="line">        fp16 = not torch.cuda.is_bf16_supported(),</span><br><span class="line">        bf16 = torch.cuda.is_bf16_supported(),</span><br><span class="line">        logging_steps = 1,</span><br><span class="line">        output_dir = &quot;outputs&quot;,</span><br><span class="line">        optim = &quot;adamw_8bit&quot;,</span><br><span class="line">        weight_decay = 0.01,</span><br><span class="line">        lr_scheduler_type = &quot;linear&quot;,</span><br><span class="line">        seed = 3407,</span><br><span class="line">    ),</span><br><span class="line">)</span><br><span class="line">#开始训练</span><br><span class="line">trainer.train()</span><br><span class="line">model.save_pretrained(&quot;lora_model&quot;)</span><br></pre></td></tr></table></figure>
<p>语料地址：<br><a href="https://huggingface.co/datasets/shaoyuan/ruozhibatest">https://huggingface.co/datasets/shaoyuan/ruozhibatest</a></p>
<p>1、通过huggingface下载语料，或加载本地语料，本地语料格式可参考，这里我用的之前从弱智吧采集过来的数据，微调参数可以先用默认的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[</span><br><span class="line">        &#123;</span><br><span class="line">                &quot;instruction&quot;: &quot;TCE是什么?&quot;,</span><br><span class="line">                &quot;input&quot;: &quot;&quot;,</span><br><span class="line">                &quot;output&quot;: &quot;TCE是Tencent Cloud Enterprise的缩写,是腾讯私有云产品&quot;</span><br><span class="line">        &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>2、model.save_pretrained会将微调模型保存到本地目录。</p>
<p>执行命令开始微调</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">HF_ENDPOINT=https://hf-mirror.com python ft.py</span><br></pre></td></tr></table></figure>

<p>可以看见有对应的进度条。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ai2-2.png"></p>
<p>此时查看nvidia-smi可以看见对应的显存占用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nvidia-smi </span><br><span class="line">Sun May 19 14:55:57 2024       </span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |</span><br><span class="line">|-----------------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                                         |                      |               MIG M. |</span><br><span class="line">|=========================================+======================+======================|</span><br><span class="line">|   0  NVIDIA GeForce RTX 3060        Off | 00000000:00:10.0 Off |                  N/A |</span><br><span class="line">| 53%   69C    P2             163W / 170W |   6296MiB / 12288MiB |     85%      Default |</span><br><span class="line">|                                         |                      |                  N/A |</span><br><span class="line">+-----------------------------------------+----------------------+----------------------+</span><br><span class="line">                                                                                         </span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                            |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |</span><br><span class="line">|        ID   ID                                                             Usage      |</span><br><span class="line">|=======================================================================================|</span><br><span class="line">|    0   N/A  N/A      4770      C   python                                     6290MiB |</span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>
<p>1、执行完成后会在执行目录生成个lora_model文件夹，这就是微调后的模型。</p>
<h3 id="微调后测试"><a href="#微调后测试" class="headerlink" title="微调后测试"></a>微调后测试</h3><p>微调后重新对此问题进行测试<br>保存为test.py</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import os</span><br><span class="line">from unsloth import FastLanguageModel</span><br><span class="line">import torch</span><br><span class="line">from transformers import TextStreamer</span><br><span class="line"></span><br><span class="line">if True:</span><br><span class="line">    from unsloth import FastLanguageModel</span><br><span class="line">    model, tokenizer = FastLanguageModel.from_pretrained(</span><br><span class="line">        model_name = &quot;lora_model&quot;, # 加载训练后的LoRA模型</span><br><span class="line">        max_seq_length = 2048,</span><br><span class="line">        dtype = None,</span><br><span class="line">        load_in_4bit = True,</span><br><span class="line">    )</span><br><span class="line">    FastLanguageModel.for_inference(model) </span><br><span class="line">alpaca_prompt = &quot;&quot;&quot;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.</span><br><span class="line">### Instruction:</span><br><span class="line">&#123;&#125;</span><br><span class="line">### Input:</span><br><span class="line">&#123;&#125;</span><br><span class="line">### Response:</span><br><span class="line">&#123;&#125;&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">inputs = tokenizer(</span><br><span class="line">[</span><br><span class="line">    alpaca_prompt.format(</span><br><span class="line">        &quot;请用中文回答&quot;, </span><br><span class="line">        &quot;海绵宝宝的书法是不是叫做海绵体&quot;, </span><br><span class="line">        &quot;&quot;, </span><br><span class="line">    )</span><br><span class="line">], return_tensors = &quot;pt&quot;).to(&quot;cuda&quot;)</span><br><span class="line"></span><br><span class="line">text_streamer = TextStreamer(tokenizer)</span><br><span class="line">_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)</span><br></pre></td></tr></table></figure>


<p>1、这里会加载本地的刚刚微调后的lora_model模型进行测试</p>
<p>查看结果<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ai2-3.png"><br>可以看见进行了模型对问题进行了回答，还加了一些自己的扩展，虽然不是很准确，但毕竟这只是微调，不是完整训练。</p>
<p>注：下载后模型存储在</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/root/.cache/huggingface/hub/models--unsloth--llama-3-8b-bnb-4bit</span><br></pre></td></tr></table></figure>



<p>将微调后的模型和原始模型进行合并量化为4位的gguf格式文件<br>可以在代码最后加入以下</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">model.save_pretrained_gguf(&quot;model&quot;, tokenizer, quantization_method = &quot;q4_k_m&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>最终gguf文件可以通过gpt4-all这个app进行加载在本机使用</p>
<p><a href="https://gpt4all.io/index.html">https://gpt4all.io/index.html</a></p>
<p>以mac 为例，将gguf文件cp到GPT4-ALL安装目录就可加载使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp model-unsloth.Q4_K_M.gguf ~/Library/Application\ Support/nomic.ai/GPT4All</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ai2-4.png"></p>
<p>其他工具Ollama、dify加载模型使用</p>
<p>备注：<br>下载后的数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./.cache/huggingface/datasets/downloads/</span><br></pre></td></tr></table></figure>
<p>huggingface下载模型加速：<a href="https://hf-mirror.com/">https://hf-mirror.com/</a></p>
<p>删除nvidia驱动</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo nvidia-uninstall</span><br><span class="line">sudo apt purge -y &#x27;^nvidia-*&#x27; &#x27;^libnvidia-*&#x27;</span><br><span class="line">sudo rm -r /var/lib/dkms/nvidia</span><br><span class="line">sudo apt -y autoremove</span><br><span class="line">sudo update-initramfs -c -k `uname -r`</span><br><span class="line">sudo update-grub2</span><br><span class="line">read -p &quot;Press any key to reboot... &quot; -n1 -s</span><br><span class="line">sudo reboot</span><br></pre></td></tr></table></figure>

<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>1、这是在本地进行微调测试，实际上自己测试可以使用Google的colab环境会更快更方便。</p>
<p>参考Nodebook<br><a href="https://colab.research.google.com/drive/1qnHnwnat3fbUbPOmETOT16MzW0NphInu?usp=sharing">https://colab.research.google.com/drive/1qnHnwnat3fbUbPOmETOT16MzW0NphInu?usp=sharing</a></p>
<p>2、这种预训练出来的模型不能保证回答的答案跟语料中的一模一样，需要回答的问题比较权威准确不能答错，需要的是AI语义匹配算法，而不是微调大模型。如医疗信息、政策解答这种。更推荐用模型+知识库方式，也就是模型+RAG方案。</p>
<p>huggingface课程</p>
<p><a href="https://huggingface.co/learn/nlp-course/chapter5/1?fw=pt">https://huggingface.co/learn/nlp-course/chapter5/1?fw=pt</a></p>
<p>参考链接：<br><a href="https://www.youtube.com/watch?v=LPmI-Ok5fUc&t=815s&ab_channel=AI%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%8F%91%E7%8E%B0">https://www.youtube.com/watch?v=LPmI-Ok5fUc&amp;t=815s&amp;ab_channel=AI%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%8F%91%E7%8E%B0</a><br><a href="https://mp.weixin.qq.com/s/hTcNz7fP3ym_tK6OZaWu7A">https://mp.weixin.qq.com/s/hTcNz7fP3ym_tK6OZaWu7A</a><br><a href="https://mp.weixin.qq.com/s/VV1BUMQIMrb5LxQNusQsDg">https://mp.weixin.qq.com/s/VV1BUMQIMrb5LxQNusQsDg</a><br><a href="https://www.53ai.com/news/qianyanjishu/1274.html">https://www.53ai.com/news/qianyanjishu/1274.html</a></p>
]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>ArgoCD二：一个完整的CICD流程例子</title>
    <url>/2021/01/27/argo_cd_2/</url>
    <content><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>此篇文章演示一个完整的GitOps工作流程，使用Gitlab-CI+ArgoCD来实现，其中Gitlab-CI主要用于实现代码编译、镜像构建、修改部署的yaml文件。ArgoCD用于将修改后的部署yaml文件推送到各个集群中。</p>
<table>
<thead>
<tr>
<th>软件</th>
<th>版本</th>
</tr>
</thead>
<tbody><tr>
<td>Gitlab</td>
<td>13.7.0</td>
</tr>
<tr>
<td>ArgoCD</td>
<td>1.8.1</td>
</tr>
<tr>
<td>Kubernetes</td>
<td>1.17.4</td>
</tr>
</tbody></table>
<h3 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_13.jpg"></p>
<p>1、将应用代码和构建Docker镜像的Dockerfile文件放置到Gitlab对应项目中    </p>
<p>2、在Gitlab中创建用于专门用于存放部署yaml的项目</p>
<p>3、配置Gitlab-CI用于代码编译镜像构建和业务yaml文件修改    </p>
<p>4、配置ArgoCD检测存放部署yaml的项目，有更新后自动部署到对应环境中  </p>
<h4 id="Gitlab配置"><a href="#Gitlab配置" class="headerlink" title="Gitlab配置"></a>Gitlab配置</h4><p>以一个Go-server项目为例</p>
<p>代码文件<br>server.go文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">    &quot;fmt&quot;</span><br><span class="line">    &quot;log&quot;</span><br><span class="line">    &quot;net/http&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func hello(w http.ResponseWriter, r *http.Request) &#123;</span><br><span class="line">    fmt.Fprintf(w, &quot;Hello world&quot;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line">    http.HandleFunc(&quot;/&quot;, hello)</span><br><span class="line">    if err := http.ListenAndServe(&quot;:8080&quot;, nil); err != nil &#123;</span><br><span class="line">        log.Fatal(err)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Dockerfile文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM golang:alpine</span><br><span class="line">WORKDIR /go</span><br><span class="line">ADD server /go</span><br><span class="line">CMD [&quot;./server&quot;]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在gitlab中创建proc-yaml的项目，用于集中存放到Kubernetes集群中的业务的yaml文件<br>在proc-yaml项目中创建go-server项目的文件夹，专门存放go-server项目的yaml文件<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_15.png">  </p>
<p>deployment.yaml</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: go-demo</span><br><span class="line">  labels:</span><br><span class="line">    app: go-demo</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: go-demo</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: go-demo</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: go-demo</span><br><span class="line">        image: 172.16.1.31/library/go-server-demo:63</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: go-demo</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  selector:</span><br><span class="line">    app: go-demo</span><br><span class="line">  ports:</span><br><span class="line">    - port: 8080</span><br><span class="line">      targetPort: 8080</span><br><span class="line">      nodePort: 30007</span><br></pre></td></tr></table></figure>

<h4 id="Gitlab-CI配置"><a href="#Gitlab-CI配置" class="headerlink" title="Gitlab-CI配置"></a>Gitlab-CI配置</h4><p>Gitlab-CI配置和基本使用可以参考本博客的Gitlab-CI时用章节<br>这里体现的主要是和之前的差异性部分<br>Gitlab-CI环境变量配置</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_14.png"></p>
<p>主要添加连接镜像仓库和代码仓库的凭证</p>
<p>创建.gitlab-ci.yml文件，用于配置gitlab-ci</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">stages:</span><br><span class="line">  - package</span><br><span class="line">  - docker_build</span><br><span class="line">  - modify_yaml</span><br><span class="line">build_job:</span><br><span class="line">  image: golang:alpine</span><br><span class="line">  stage: package</span><br><span class="line">  tags:</span><br><span class="line">    - k8s-runner</span><br><span class="line">  script:</span><br><span class="line">    - go build server.go</span><br><span class="line">  artifacts:</span><br><span class="line">    paths:</span><br><span class="line">    - /builds/root/go-server </span><br><span class="line">docker_build_job:</span><br><span class="line">  image: docker:19.03.0</span><br><span class="line">  services:</span><br><span class="line">    - name: docker:19.03.0-dind</span><br><span class="line">      command: [&quot;--insecure-registry=0.0.0.0/0&quot;,&quot;--registry-mirror=https://vqgjby9l.mirror.aliyuncs.com&quot;]</span><br><span class="line">  variables:</span><br><span class="line">    DOCKER_HOST: tcp://127.0.0.1:2375</span><br><span class="line">    DOCKER_TLS_CERTDIR: &quot;&quot;</span><br><span class="line">  stage: docker_build</span><br><span class="line">  tags:</span><br><span class="line">    - k8s-runner</span><br><span class="line">  script:</span><br><span class="line">    - docker info</span><br><span class="line">    - docker login -u $REGISTRY_USERNAME -p $REGISTRY_PASSWORD 172.16.1.31</span><br><span class="line">    - docker build -t 172.16.1.31/library/go-server-demo:$CI_PIPELINE_ID .</span><br><span class="line">    - docker push 172.16.1.31/library/go-server-demo:$CI_PIPELINE_ID</span><br><span class="line">modify_yaml:</span><br><span class="line">  image: wanshaoyuan/git-client:v1.0</span><br><span class="line">  stage: modify_yaml</span><br><span class="line">  tags:</span><br><span class="line">    - k8s-runner</span><br><span class="line">  script:</span><br><span class="line">    - git clone http://$GIT_USERNAME:$GIT_PASSWORD@172.16.1.184/root/proc-yaml.git</span><br><span class="line">    - git config --global user.email &quot;root@example.com&quot;</span><br><span class="line">    - git config --global user.name &quot;root&quot;</span><br><span class="line">    - git remote set-url origin http://$GIT_USERNAME:$GIT_PASSWORD@172.16.1.184/root/proc-yaml.git</span><br><span class="line">    - sed -i &quot;s/go-server-demo:.*/go-server-demo:$CI_PIPELINE_ID/g&quot; proc-yaml/go-server/deployment.yaml</span><br><span class="line">    - cd proc-yaml/</span><br><span class="line">    - git add *</span><br><span class="line">    - git commit -m &quot;update go-server&quot;</span><br><span class="line">    - git push origin master</span><br></pre></td></tr></table></figure>


<p>主要包含3个阶段，代码编译阶段、镜像构建阶端、部署yaml文件修改阶段</p>
<p>代码编译阶段：启动一个编译容器，根据代码编译出对应的制品，这里为了能让下个镜像构建阶段直接使用编译好的制品，需要配置artifacts将目录共享给下个阶段。  </p>
<p>镜像构建阶段：使用DinD的方式构建Docker镜像，然后镜像tag以我们Pipeline的ID为tag，这里好处在于可以根据对应的Pipeline的迅速定位到对应的镜像版本</p>
<p>部署yaml修改阶段: 通过git命令将对部署yaml项目的repo clone下来然后通过sed进行修改并commit回去，通过此步ArgoCD检测到变化后会拉取新的yaml进行部署</p>
<h4 id="ArgoCD配置"><a href="#ArgoCD配置" class="headerlink" title="ArgoCD配置"></a>ArgoCD配置</h4><p>在ArgoCD中配置对接pro-yaml的repo认证<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_16.png"></p>
<p>创建App<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_17.png"></p>
<p>选择自动同步模式</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_18.png"></p>
<p>PATH这配置我们go-server这个项目的路径，就会检测这里面的yaml文件，并自动更新到集群中</p>
<p>最后创建完成</p>
<p>测试</p>
<p>修改server.go的Hello world为Hello Argo</p>
<p>等待git-ci执行<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_19.png"></p>
<p>检查proc-yaml项目是否更新</p>
<p>可以看见对应的image的tag修改成了，我们实际上Pipeline的ID<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_20.png"></p>
<p>等待片刻后ArgoCD检测到yaml变化，自动部署，此时Kubernetes中应用进行了自动更新</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_21.png"></p>
<p>访问</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl 192.168.0.6:30007</span><br><span class="line">Hello argo</span><br></pre></td></tr></table></figure>


<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>通过GitlabCI与ArgoCD结合确实非常方便的将我们应用进行部署到Kubernetes中，并且后续也非常容易进行回滚，在之前通过Gitlab-CI中集成kubectl镜像，进行部署容易造成kubeconfig文件的泄漏，也不方便做权限的管理和应用状态控制，但结合ArgoCD后一切都美好了起来。</p>
]]></content>
      <categories>
        <category>CI/CD</category>
      </categories>
      <tags>
        <tag>CI/CD</tag>
      </tags>
  </entry>
  <entry>
    <title>ArgoCD一：基本介绍和使用</title>
    <url>/2020/12/27/argo_1/</url>
    <content><![CDATA[<h3 id="ArgoCD是什么？"><a href="#ArgoCD是什么？" class="headerlink" title="ArgoCD是什么？"></a>ArgoCD是什么？</h3><p>ArgoCD是一个专为Kubernetes而生，遵循GitOps理念的持续部署工具。这里指的Gitops简单来说就是1、以Git为核心。2、声明式定义各类对象。3、配置一致性管理。具体参考(<a href="https://www.hwchiu.com/gitops-book-ch2.html">https://www.hwchiu.com/gitops-book-ch2.html</a>)</p>
<h3 id="软件版本"><a href="#软件版本" class="headerlink" title="软件版本"></a>软件版本</h3><table>
<thead>
<tr>
<th>软件</th>
<th>版本</th>
</tr>
</thead>
<tbody><tr>
<td>ArgoCD</td>
<td>1.8.1</td>
</tr>
<tr>
<td>Kubernetes</td>
<td>1.18</td>
</tr>
</tbody></table>
<p>ArgoCD是是Intuit公司开源出来的属于整个Argo项目中的其中一个子项目，整个Argo项目中还包括Argo-event、argo-workflow、Argo-Rollout</p>
<p><a href="https://github.com/argoproj">https://github.com/argoproj</a></p>
<p>2020年Argo正式加入CNCF孵化项目中。</p>
<h3 id="ArgoCD功能有哪些？"><a href="#ArgoCD功能有哪些？" class="headerlink" title="ArgoCD功能有哪些？"></a>ArgoCD功能有哪些？</h3><ul>
<li>将应用程序自动部署到指定环境中</li>
<li>可通过CRD方式定义执行对象</li>
<li>多租户管理和RBAC权限集成管控</li>
<li>多集应用管理和部署</li>
<li>可视化UI</li>
<li>支持多种Kubernetes配置对象(Kustomize，Helm，Ksonnet，Jsonnet，plain-YAML)</li>
<li>支持SSO集成(OIDC，OAuth2，LDAP，SAML 2.0，GitHub，GitLab，Microsoft，LinkedIn)</li>
<li>Webhook集成可实现通知</li>
<li>自带metric指标暴露</li>
</ul>
<h3 id="为什么需要ArgoCD"><a href="#为什么需要ArgoCD" class="headerlink" title="为什么需要ArgoCD"></a>为什么需要ArgoCD</h3><p>现有的CICD工具对在CD到Kubernetes中时通过提供kubeconfig文件方式对接到Kubernetes集群，这样非常容易造成kubeconfig文件泄漏，并且造成安全引患。</p>
<p>通过ArgoCD可以实现应用快速发布到Kubernetes中，并且能够根据版本标识快速跟踪和多集群部署功能，实现多个集群之间同一应用部署问题。</p>
<p>多种方式实现持续部署配置，如通过UI方式，同时也可以通过CRD方式使用Kubernetes自定义对象实现持续部署。</p>
<h3 id="ArgoCD部署安装和基本使用"><a href="#ArgoCD部署安装和基本使用" class="headerlink" title="ArgoCD部署安装和基本使用"></a>ArgoCD部署安装和基本使用</h3><h4 id="软件架构"><a href="#软件架构" class="headerlink" title="软件架构"></a>软件架构</h4><p><img src="https://ucc.alicdn.com/pic/developer-ecology/506230d9ab1643058785424916c6a0e3.png"></p>
<p>Argo API Server<br>提供介面給外界操控 ArgoCD 服務，本身提供了 CLI, GUI 以及 gRPC&#x2F;REST 等介面，最簡單的 Demo 可以使用 GUI 來操作。<br>Repository Service.<br>与远端Git Repo进行同步，缓存在本地</p>
<p>ArgoCD-redis<br>用作本地缓存</p>
<p>ArgoCd-dex-server<br>实现token认证服务和SSO </p>
<p>Application Controller<br>与Kubernetes进行通信对部署的workload进行状态检测与Git Repository进行对比发现变化，进行更新。</p>
<h4 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h4><p>单节点部署<br>使用官网快速部署</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create namespace argocd</span><br><span class="line">kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml</span><br></pre></td></tr></table></figure>

<p>高可用部署参考  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://github.com/argoproj/argo-cd/tree/master/manifests</span><br></pre></td></tr></table></figure>

<p>部署完后产生以下服务</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">NAME                                      READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/argocd-application-controller-0       1/1     Running   0          5d6h</span><br><span class="line">pod/argocd-dex-server-74588646d-sz9g8     1/1     Running   0          2d2h</span><br><span class="line">pod/argocd-redis-5ccdd9d4fd-csthm         1/1     Running   1          5d6h</span><br><span class="line">pod/argocd-repo-server-5bbb8bdf78-mxkv7   1/1     Running   0          18h</span><br><span class="line">pod/argocd-server-789fb45964-82mzx        1/1     Running   0          18h</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NAME                            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE</span><br><span class="line">service/argocd-dex-server       ClusterIP   10.43.180.172   &lt;none&gt;        5556/TCP,5557/TCP,5558/TCP   5d6h</span><br><span class="line">service/argocd-metrics          ClusterIP   10.43.184.97    &lt;none&gt;        8082/TCP                     5d6h</span><br><span class="line">service/argocd-redis            ClusterIP   10.43.4.233     &lt;none&gt;        6379/TCP                     5d6h</span><br><span class="line">service/argocd-repo-server      ClusterIP   10.43.9.45      &lt;none&gt;        8081/TCP,8084/TCP            5d6h</span><br><span class="line">service/argocd-server           NodePort    10.43.48.239    &lt;none&gt;        80:31320/TCP,443:31203/TCP   5d6h</span><br><span class="line">service/argocd-server-metrics   ClusterIP   10.43.149.186   &lt;none&gt;        8083/TCP                     5d6h</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/argocd-dex-server    1/1     1            1           5d6h</span><br><span class="line">deployment.apps/argocd-redis         1/1     1            1           5d6h</span><br><span class="line">deployment.apps/argocd-repo-server   1/1     1            1           5d6h</span><br><span class="line">deployment.apps/argocd-server        1/1     1            1           5d6h</span><br><span class="line"></span><br><span class="line">NAME                                            DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/argocd-dex-server-74588646d     1         1         1       5d6h</span><br><span class="line">replicaset.apps/argocd-redis-5ccdd9d4fd         1         1         1       5d6h</span><br><span class="line">replicaset.apps/argocd-repo-server-5bbb8bdf78   1         1         1       5d6h</span><br><span class="line">replicaset.apps/argocd-server-789fb45964        1         1         1       5d6h</span><br><span class="line"></span><br><span class="line">NAME                                             READY   AGE</span><br><span class="line">statefulset.apps/argocd-application-controller   1/1     5d6h</span><br></pre></td></tr></table></figure>
<p>使用NodePort方式为对外暴露</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl patch svc argocd-server -n argocd -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&#x27;</span><br></pre></td></tr></table></figure>

<p>访问Dashboard</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_1.png"></p>
<p>默认帐号为admin，密码通过secret获取</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=&quot;&#123;.data.password&#125;&quot; | base64 -d</span><br></pre></td></tr></table></figure>

<p>ArgoCD使用方式有两种，方式一：通过UI-dashboard使用，方式二：通过CLI工具使用</p>
<p>使用CLI工具，需要提前下载</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://github.com/argoproj/argo-cd/releases/download/v1.8.1/argocd-linux-amd64</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mv argocd-linux-amd64 /usr/local/bin/argocd</span><br><span class="line"></span><br><span class="line">chmod a+x /usr/local/bin/argocd</span><br></pre></td></tr></table></figure>

<p>登录环境</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">argocd login &lt;ARGOCD_SERVER&gt;</span><br></pre></td></tr></table></figure>

<p>更新密码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">argocd account update-password</span><br></pre></td></tr></table></figure>


<p>以一个简单的例子讲解ArgoCD的基本使用  </p>
<p>在git上创建个argo-example项目用于存放我们的部署文件，存放以下yaml文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: bootcampt</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: bootcampt</span><br><span class="line">  replicas: 1 # tells deployment to run 2 pods matching the template</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: bootcampt</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: bootcampt</span><br><span class="line">        image: docker.io/jocatalin/kubernetes-bootcamp:v1</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name:  bootcampt</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: bootcampt</span><br><span class="line">  ports:</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 8080</span><br><span class="line">      targetPort: 8080</span><br><span class="line">      nodePort: 30062</span><br><span class="line">  type: NodePort</span><br><span class="line"></span><br></pre></td></tr></table></figure>




<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_2.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_3.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_4.png"></p>
<p>点击创建</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_5.png"></p>
<p>同步部署</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_6.png"></p>
<p>如果对应的git是私有库，pull需要帐号密码则需要在argo设置中配置repo connect</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_10.png"></p>
<p>填写对应的帐号密码，如果是自签名证书需要将CA附上<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_11.png"></p>
<p>查看部署后的应用</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_7.png"></p>
<p>在Kubernetes集群中查看部署后的应用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod </span><br><span class="line">NAME                         READY   STATUS    RESTARTS   AGE</span><br><span class="line">bootcampt-544d66b664-ddhgd   1/1     Running   0          2m18s</span><br></pre></td></tr></table></figure>

<p>访问</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl 172.16.1.6:30062</span><br><span class="line">Hello Kubernetes bootcamp! | Running on: bootcampt-544d66b664-ddhgd | v=1</span><br></pre></td></tr></table></figure>
<p>更新yaml文件将<code>docker.io/jocatalin/kubernetes-bootcamp:v1</code>改成<code> docker.io/jocatalin/kubernetes-bootcamp:v2</code>然后git push到git中</p>
<p>等待2分钟左右，ArgoCD会自动将新的yaml文件部署到Kubernetes中<br>再次访问</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl 172.16.1.6:30062</span><br><span class="line">Hello Kubernetes bootcamp! | Running on: bootcampt-7687b6c957-42vjw | v=2</span><br></pre></td></tr></table></figure>

<p>通过CLI方式就是直接使用下面yaml文件部署或直接使用argocd cli create</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">argocd app create --project default --name bootcamp --repo http://172.16.1.184/root/argo-example.git --path . --dest-server https://kubernetes.default.svc --dest-namespace default --revision master --sync-policy  automated</span><br></pre></td></tr></table></figure>

<p>yaml文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: argoproj.io/v1alpha1</span><br><span class="line">kind: Application</span><br><span class="line">metadata:</span><br><span class="line">  name: bootcamp</span><br><span class="line">spec:</span><br><span class="line">  destination:</span><br><span class="line">    name: &#x27;&#x27;</span><br><span class="line">    namespace: default</span><br><span class="line">    server: &#x27;https://kubernetes.default.svc&#x27;</span><br><span class="line">  source:</span><br><span class="line">    path: .</span><br><span class="line">    repoURL: &#x27;http://172.16.1.184/root/argo-example.git&#x27;</span><br><span class="line">    targetRevision: HEAD</span><br><span class="line">  project: default</span><br><span class="line">  syncPolicy:</span><br><span class="line">    automated:</span><br><span class="line">      prune: false</span><br><span class="line">      selfHeal: false</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>因为ArgoCD有在部署集群中创建CRD对象所以我们同时也可以使用kubectl查看到创建好的Application</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get Application -n argocd</span><br><span class="line">NAME       SYNC STATUS   HEALTH STATUS</span><br><span class="line">bootcamp   Synced        Healthy</span><br></pre></td></tr></table></figure>
<p>选中对应的app可以进行历史发布查看和回滚</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_12.png"></p>
<p>对应的ID与实际git库里面提交的commit是匹配的</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_8.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_9.png"></p>
<p>可以选择任意一个历史版本进行回滚</p>
<p>常用CLI命令</p>
<p>列出部署的应用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">argocd app list</span><br><span class="line">NAME      CLUSTER                         NAMESPACE  PROJECT  STATUS  HEALTH   SYNCPOLICY  CONDITIONS  REPO                                       PATH  TARGET</span><br><span class="line">bootcamp  https://kubernetes.default.svc  default    default  Synced  Healthy  Auto        &lt;none&gt;      http://172.16.1.184/root/argo-example.git  .     HEAD</span><br></pre></td></tr></table></figure>

<p>查看指定部署的应用详细情况</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Name:               bootcamp</span><br><span class="line">Project:            default</span><br><span class="line">Server:             https://kubernetes.default.svc</span><br><span class="line">Namespace:          default</span><br><span class="line">URL:                https://172.16.1.7:31203/applications/bootcamp</span><br><span class="line">Repo:               http://172.16.1.184/root/argo-example.git</span><br><span class="line">Target:             HEAD</span><br><span class="line">Path:               .</span><br><span class="line">SyncWindow:         Sync Allowed</span><br><span class="line">Sync Policy:        Automated</span><br><span class="line">Sync Status:        Synced to HEAD (b1696a3)</span><br><span class="line">Health Status:      Healthy</span><br><span class="line"></span><br><span class="line">GROUP  KIND        NAMESPACE  NAME       STATUS  HEALTH   HOOK  MESSAGE</span><br><span class="line">       Service     default    bootcampt  Synced  Healthy        service/bootcampt unchanged</span><br><span class="line">apps   Deployment  default    bootcampt  Synced  Healthy        deployment.apps/bootcampt configured</span><br></pre></td></tr></table></figure>

<p>同步部署的应用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">argocd app sync appname</span><br></pre></td></tr></table></figure>
<p>设置为自动同步</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">argocd app set appname --sync-policy automated</span><br></pre></td></tr></table></figure>


<h4 id="多集群应用部署"><a href="#多集群应用部署" class="headerlink" title="多集群应用部署"></a>多集群应用部署</h4><p>添加集群<br>首先需要将你需要添加的集群的config文件追加放到~&#x2F;.kube&#x2F;config</p>
<p>然后执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl config get-contexts</span><br><span class="line">CURRENT   NAME                  CLUSTER               AUTHINFO    NAMESPACE</span><br><span class="line">*         cluster-1             cluster-1             cluster-1   </span><br><span class="line">          cluster-1-rke-node2   cluster-1-rke-node2   cluster-1   </span><br><span class="line">          cluster-2             cluster-2             cluster-2  </span><br></pre></td></tr></table></figure>
<p>看当前在哪个context，如我要添加cluster-2</p>
<p>则执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">argocd cluster add cluster-2</span><br></pre></td></tr></table></figure>
<p>argocd命令会自动读取你config文件内的context信息包含证书和token<br>执行成功后查看集群</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">argocd cluster list</span><br><span class="line">SERVER                          NAME        VERSION  STATUS      MESSAGE</span><br><span class="line">https://192.168.0.29:6443       cluster-2   1.17     Successful  </span><br><span class="line">https://kubernetes.default.svc  in-cluster  1.17     Successful  </span><br></pre></td></tr></table></figure>

<p>如果是Rancher建的集群，你希望通过连接rancher在转发到api-server需要通过另外方式<br>参考：<a href="https://gist.github.com/janeczku/b16154194f7f03f772645303af8e9f80">https://gist.github.com/janeczku/b16154194f7f03f772645303af8e9f80</a></p>
<p>遗憾的是目前argocd多集群部署不能在一个Application内选择多个部署集群，只能通过建立多个相同Application对应不同部署目的集群方式</p>
<p>如上Application要部署到uat和sit环境只能</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: argoproj.io/v1alpha1</span><br><span class="line">kind: Application</span><br><span class="line">metadata:</span><br><span class="line">  name: sit-bootcamp</span><br><span class="line">spec:</span><br><span class="line">  destination:</span><br><span class="line">    name: &#x27;&#x27;</span><br><span class="line">    namespace: default</span><br><span class="line">    server: &#x27;https://kubernetes.default.svc&#x27;</span><br><span class="line">  source:</span><br><span class="line">    path: .</span><br><span class="line">    repoURL: &#x27;http://172.16.1.184/root/argo-example.git&#x27;</span><br><span class="line">    targetRevision: HEAD</span><br><span class="line">  project: default</span><br><span class="line">  syncPolicy:</span><br><span class="line">    automated:</span><br><span class="line">      prune: false</span><br><span class="line">      selfHeal: false</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: argoproj.io/v1alpha1</span><br><span class="line">kind: Application</span><br><span class="line">metadata:</span><br><span class="line">  name: uat-bootcamp</span><br><span class="line">spec:</span><br><span class="line">  destination:</span><br><span class="line">    name: &#x27;&#x27;</span><br><span class="line">    namespace: default</span><br><span class="line">    server: &#x27;https://192.168.0.29:6443&#x27;</span><br><span class="line">  source:</span><br><span class="line">    path: .</span><br><span class="line">    repoURL: &#x27;http://172.16.1.184/root/argo-example.git&#x27;</span><br><span class="line">    targetRevision: HEAD</span><br><span class="line">  project: default</span><br><span class="line">  syncPolicy:</span><br><span class="line">    automated:</span><br><span class="line">      prune: false</span><br><span class="line">      selfHeal: false</span><br></pre></td></tr></table></figure>

<p>社区内也进行了相关讨论，ISSUE如下<br><a href="https://github.com/argoproj/argo-cd/issues/1673">https://github.com/argoproj/argo-cd/issues/1673</a></p>
<p>参考链接：</p>
<p><a href="https://www.jianshu.com/p/eec8e201b7e9">https://www.jianshu.com/p/eec8e201b7e9</a>  </p>
]]></content>
      <categories>
        <category>CI/CD</category>
      </categories>
      <tags>
        <tag>CI/CD</tag>
      </tags>
  </entry>
  <entry>
    <title>boot from volume的两种方式</title>
    <url>/2017/05/08/boot_from_volume/</url>
    <content><![CDATA[<h1 id="boot-for-volume的两种方式"><a href="#boot-for-volume的两种方式" class="headerlink" title="boot for volume的两种方式"></a>boot for volume的两种方式</h1><p>简介：</p>
<p>openstack对接商业存储一般直接用cinder直接对接商业存储，但想将nova创建的云主机放商业存储上，此时需要使用boot from volume</p>
<p>方式一<br>直接选择镜像，dest选择云硬盘，需要注意的是云硬盘的大小必须大于等于镜像大小。</p>
<p>方式二<br>创建空云硬盘，创建主机时将选择的镜像，加载到这个空硬盘。（需要注意的是，创建的空盘大小必续要大于你选择的镜像的大小）。</p>
<p>方式一  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nova boot --flavor 1 --nic net-id=48baa60d-b785-45c1-8f90-8467f56abb5b --block-device source=image,id=d3a0512b-8cfc-4ad4-9fd4-7b38c9a44a32,dest=volume,size=10,shutdown=preserve,bootindex=0 test</span><br></pre></td></tr></table></figure>
<p>流程：先是基于image创建block volume，然后从这个volume中boot instance 。shutdown选项选为preserve, 而不要选为remove， 这样在instance关闭时， volume会被save下来；其中的size选项要求大于等于flavor中的disk大小，同时要求我们的后端存储要有大于此size大小的空间。</p>
<p>![](<a href="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/boot_from_volume_1.png">https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/boot_from_volume_1.png</a></p>
<p>方法二<br>openstack volume create –image IMAGE_ID –size SIZE_IN_GB  –name CINDER_NAME</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/boot_from_volume_2.png"></p>
<p>获取云硬盘id</p>
<p>基于云硬盘创建云主机</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/boot_from_volume_3.png"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nova boot --flavor 1  --nic net-id=48baa60d-b785-45c1-8f90-8467f56abb5b --boot-volume f9af8677-28cf-4dfb-8dbc-b722025f9fa0 --security-group default --admin-pass test test</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title>calico网络插件运维</title>
    <url>/2020/11/18/calico_ops/</url>
    <content><![CDATA[<h3 id="适用范围"><a href="#适用范围" class="headerlink" title="适用范围"></a>适用范围</h3><p>本文档测试范围</p>
<table>
<thead>
<tr>
<th>软件</th>
<th>版本</th>
</tr>
</thead>
<tbody><tr>
<td>Kubernetes</td>
<td>v1.14.x,v1.15.x,v1.16.x</td>
</tr>
<tr>
<td>calico</td>
<td>v3.13.4</td>
</tr>
</tbody></table>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><h2 id="Calico是一种开源网络和网络安全解决方案，适用于容器，虚拟机和基于主机的本机工作负载。Calico支持广泛的平台，包括Kubernetes，docker，OpenStack和裸机服务。Calico后端支持多种网络模式。-BGP模式：将节点做为虚拟路由器通过BGP路由协议来实现集群内容器之间的网络访问。-IPIP模式：在原有IP报文中封装一个新的IP报文，新的IP报文中将源地址IP和目的地址IP都修改为对端宿主机IP。-cross-subnet：Calico-ipip模式和calico-bgp模式都有对应的局限性，对于一些主机跨子网而又无法使网络设备使用BGP的场景可以使用cross-subnet模式，实现同子网机器使用calico-BGP模式，跨子网机器使用calico-ipip模式。"><a href="#Calico是一种开源网络和网络安全解决方案，适用于容器，虚拟机和基于主机的本机工作负载。Calico支持广泛的平台，包括Kubernetes，docker，OpenStack和裸机服务。Calico后端支持多种网络模式。-BGP模式：将节点做为虚拟路由器通过BGP路由协议来实现集群内容器之间的网络访问。-IPIP模式：在原有IP报文中封装一个新的IP报文，新的IP报文中将源地址IP和目的地址IP都修改为对端宿主机IP。-cross-subnet：Calico-ipip模式和calico-bgp模式都有对应的局限性，对于一些主机跨子网而又无法使网络设备使用BGP的场景可以使用cross-subnet模式，实现同子网机器使用calico-BGP模式，跨子网机器使用calico-ipip模式。" class="headerlink" title="Calico是一种开源网络和网络安全解决方案，适用于容器，虚拟机和基于主机的本机工作负载。Calico支持广泛的平台，包括Kubernetes，docker，OpenStack和裸机服务。Calico后端支持多种网络模式。- BGP模式：将节点做为虚拟路由器通过BGP路由协议来实现集群内容器之间的网络访问。- IPIP模式：在原有IP报文中封装一个新的IP报文，新的IP报文中将源地址IP和目的地址IP都修改为对端宿主机IP。- cross-subnet：Calico-ipip模式和calico-bgp模式都有对应的局限性，对于一些主机跨子网而又无法使网络设备使用BGP的场景可以使用cross-subnet模式，实现同子网机器使用calico-BGP模式，跨子网机器使用calico-ipip模式。"></a>Calico是一种开源网络和网络安全解决方案，适用于容器，虚拟机和基于主机的本机工作负载。Calico支持广泛的平台，包括Kubernetes，docker，OpenStack和裸机服务。Calico后端支持多种网络模式。<br>- BGP模式：将节点做为虚拟路由器通过BGP路由协议来实现集群内容器之间的网络访问。<br>- IPIP模式：在原有IP报文中封装一个新的IP报文，新的IP报文中将源地址IP和目的地址IP都修改为对端宿主机IP。<br>- cross-subnet：Calico-ipip模式和calico-bgp模式都有对应的局限性，对于一些主机跨子网而又无法使网络设备使用BGP的场景可以使用cross-subnet模式，实现同子网机器使用calico-BGP模式，跨子网机器使用calico-ipip模式。</h2><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/calico_ops_5.png"></p>
<h3 id="calico切换BGP模式"><a href="#calico切换BGP模式" class="headerlink" title="calico切换BGP模式"></a>calico切换BGP模式</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/calico_ops_6.png"></p>
<p>部署完成后默认使用calico-ipip的模式，通过在节点的路由即可得知，通往其他节点路由通过tunl0网卡出去</p>
<p><img src="https://pic.downk.cc/item/5eb58365c2a9a83be5de25d1.png"></p>
<p>修改为BGP网络模式，在system项目中修改calico-node daemonset</p>
<p><img src="https://pic.downk.cc/item/5eb583bdc2a9a83be5de6a23.png"></p>
<p><img src="https://pic.downk.cc/item/5eb584b0c2a9a83be5df35b0.png"></p>
<p>修改<code>CALICO_IPV4POOL_IPIP</code>改为off，添加新环境变量<code>FELIX_IPINIPENABLED</code>为false  </p>
<p><img src="https://pic.downk.cc/item/5eb5853cc2a9a83be5df9db0.png"></p>
<p>修改完成后对节点进行重启，等待恢复后查看主机路由，与ipip最大区别在于去往其他节点的路由，由Tunnel0走向网络网卡。</p>
<p><img src="https://pic.downk.cc/item/5eb585c1c2a9a83be5e043cb.png"></p>
<h3 id="calico切换cross-subnet模式"><a href="#calico切换cross-subnet模式" class="headerlink" title="calico切换cross-subnet模式"></a>calico切换cross-subnet模式</h3><p>Calico-ipip模式和calico-bgp模式都有对应的局限性，对于一些主机跨子网而又无法使网络设备使用BGP的场景可以使用cross-subnet模式，实现同子网机器使用calico-BGP模式，跨子网机器使用calico-ipip模式。</p>
<p>部署集群网络选择calico网络插件<br><img src="https://pic.downk.cc/item/5eb5869bc2a9a83be5e0f0fa.png"></p>
<p>默认部署出来是calico的ip-in-ip的模式<br>查看宿主机网卡，会发现多了个tunl0网卡，这个是建立ip隧道的网卡</p>
<p><img src="https://pic.downk.cc/item/5eb586c3c2a9a83be5e10f13.png"></p>
<p>去其他主机的路由都是走tunl0网卡出去</p>
<p><img src="https://pic.downk.cc/item/5eb58710c2a9a83be5e15062.png"></p>
<p>切换到cross-subnet模式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl edit ipPool/default-ipv4-ippool </span><br></pre></td></tr></table></figure>

<p>将ipipMode改为crossSubnet</p>
<p><img src="https://pic.downk.cc/item/5eb58738c2a9a83be5e1782a.png"><br>在UI将calico-node的POD删了重建</p>
<p><img src="https://pic.downk.cc/item/5eb587fac2a9a83be5e21475.png"></p>
<p>重启检查calico网络<br><img src="https://pic.downk.cc/item/5eb588c7c2a9a83be5e2abbe.png"></p>
<p>可以看见同子网的主机出口走的是bgp，不同子网主机走的是tunl0网卡走ipip模式<br>验证<br>创建应用测试跨主机网络，在不同主机上互相ping测试，看看跨主机网络是否正常。 </p>
<h3 id="配置Route-reflector"><a href="#配置Route-reflector" class="headerlink" title="配置Route reflector"></a>配置Route reflector</h3><h4 id="安装calicoctl"><a href="#安装calicoctl" class="headerlink" title="安装calicoctl"></a>安装calicoctl</h4><p>安装方式<br>Single host上面binary安装<br>Single host上面continer安装<br>作为k8s pod运行<br>实际经验：<br>Binary方式在集群里面的一台worker节点安装（比如RR）<br>calicoctl会检测bird&#x2F;felix的运行状态<br>在非calico node节点运行只能使用部分命令，不能运行calico node相关命令<br>通过配置calicoctl来对calico进行控制，通常情况下建议将  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -O -L  https://github.com/projectcalico/calicoctl/releases/download/v3.13.3/calicoctl</span><br></pre></td></tr></table></figure>
<p>配置可执行权限</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">chmod +x calicoctl</span><br></pre></td></tr></table></figure>
<p>复制的&#x2F;usr&#x2F;bin&#x2F;目录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp calicoctl /usr/bin/</span><br></pre></td></tr></table></figure>
<p>配置calicoctl连接Kubernetes集群</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export CALICO_DATASTORE_TYPE=kubernetes</span><br><span class="line">export CALICO_KUBECONFIG=~/.kube/config</span><br><span class="line">calicoctl node status</span><br></pre></td></tr></table></figure>

<p><img src="https://pic.downk.cc/item/5eb58997c2a9a83be5e34deb.png"></p>
<h4 id="calico-node-to-node-mesh"><a href="#calico-node-to-node-mesh" class="headerlink" title="calico node-to-node mesh"></a>calico node-to-node mesh</h4><p>默认情况下calico采用node-to-node mesh方式 ，为了防止BGP路由环路，BGP协议规定在一个AS（自治系统）内部，IBGP路由器之间只能传一跳路由信息，所以在一个AS内部，IBGP路由器之间为了学习路由信息需要建立全互联的对等体关系，但是当一个AS规模很大的时候，这种全互联的对等体关系维护会大量消耗网络和CPU资源，所以这种情况下就需要建立路由反射器以减少IBGP路由器之间的对等体关系数量。<br><img src="https://pic.downk.cc/item/5eb589c7c2a9a83be5e37132.png"></p>
<h4 id="Route-reflector角色介绍"><a href="#Route-reflector角色介绍" class="headerlink" title="Route reflector角色介绍"></a>Route reflector角色介绍</h4><p>早期calico版本提供专门的route reflector镜像，在新版本calico node内置集成route reflector功能。<br>Route reflector可以是以下角色：  </p>
<ul>
<li>集群内部的node节点</li>
<li>集群外部节点运行calico node</li>
<li>其他支持route reflector的软件或者设备。</li>
</ul>
<p>这里以一个集群内部的node节点为例：</p>
<h4 id="关闭node-to-node-mesh"><a href="#关闭node-to-node-mesh" class="headerlink" title="关闭node-to-node mesh"></a>关闭node-to-node mesh</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | calicoctl apply -f -</span><br><span class="line"></span><br><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">kind: BGPConfiguration</span><br><span class="line">metadata:</span><br><span class="line"> name: default</span><br><span class="line">spec:</span><br><span class="line">  logSeverityScreen: Info</span><br><span class="line">  nodeToNodeMeshEnabled: false</span><br><span class="line">  asNumber: 63400</span><br><span class="line">EOF</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="设置Route-reflector"><a href="#设置Route-reflector" class="headerlink" title="设置Route reflector"></a>设置Route reflector</h4><p>配置Route reflector支持多种配置方式如：1、支持配置全局BGP peer，。2、支持针对单个节点进行配置BGP Peer。也可以将calico节点充当Route reflector 这里以配置calico节点充当Router reflector为例。  </p>
<p>配置节点充当BGP Route Reflector  </p>
<p>可将Calico节点配置为充当路由反射器。为此，要用作路由反射器的每个节点必须具有群集ID-通常是未使用的IPv4地址。  </p>
<p>要将节点配置为集群ID为244.0.0.1的路由反射器，请运行以下命令。这里将节点名为rke-node4的节点配置为Route Reflector，若一个集群中要配置主备rr，为了防止rr之间的路由环路，需要将集群ID配置成一样</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">calicoctl patch node rke-node4 -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;bgp&quot;: &#123;&quot;routeReflectorClusterID&quot;: &quot;244.0.0.1&quot;&#125;&#125;&#125;&#x27;</span><br></pre></td></tr></table></figure>
<p>给节点打上对应的label标记该节点以表明它是Route Reflector，从而允许BGPPeer资源选择它。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl label node rke-node4 route-reflector=true</span><br></pre></td></tr></table></figure>
<p>创建BGPPeer</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export CALICO_DATASTORE_TYPE=kubernetes</span><br><span class="line">export CALICO_KUBECONFIG=~/.kube/config</span><br><span class="line">cat &lt;&lt;EOF | calicoctl apply -f -</span><br><span class="line">kind: BGPPeer</span><br><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">metadata:</span><br><span class="line">  name: peer-with-route-reflectors</span><br><span class="line">spec:</span><br><span class="line">  nodeSelector: all()</span><br><span class="line">  peerSelector: route-reflector == &#x27;true&#x27;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p>查看BGP节点状态<br>node上查看，peer type由node-to-node mesh变为 node specific</p>
<p><img src="https://pic.downk.cc/item/5eb58a75c2a9a83be5e3ec4e.png"></p>
<p>Route Reflector上节点查看，节点已正常建立连接<br><img src="https://pic.downk.cc/item/5eb58a98c2a9a83be5e4079e.png"></p>
<h3 id="设置veth网卡mtu"><a href="#设置veth网卡mtu" class="headerlink" title="设置veth网卡mtu"></a>设置veth网卡mtu</h3><p>通常，通过使用最高MTU值（不会在路径上引起碎片或丢包）来实现最高性能。对于给定的流量速率，最大带宽增加，CPU消耗可能下降。对于一些支持jumbo frames的网络设备，可以配置calico支持使用<br>下表列举了，常见几种MTU配置下calico对应的网卡mtu的配置</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/calico_ops_1.png"></p>
<p>IPIP和VXLAN协议中的IP中使用的额外报文头，通过头的大小减小了最小MTU。（IP中的IP使用20字节的标头，而VXLAN使用50字节的标头）。<br>如果在Pod网络中的任何地方使用VXLAN，请将MTU大小配置为“物理网络MTU大小减去50”。<br>如果仅在IP中使用IP，则将MTU大小配置为“物理网络MTU大小减去20” 。<br>将工作负载端点MTU和隧道MTU设置为相同的值</p>
<p>配置方法：  </p>
<p>升级集群</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/calico_ops_2.png"></p>
<p>配置网卡MTU<br>此时通过system项目下calico-config文件可以看见对应的mtu设置</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/calico_ops_3.png"></p>
<p>创建workload查看POD网卡MTU为9001</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/calico_ops_4.png"></p>
<h3 id="设置全局AS号"><a href="#设置全局AS号" class="headerlink" title="设置全局AS号"></a>设置全局AS号</h3><p>默认情况下，除非已为节点指定每个节点的AS，否则所有Calico节点都使用64512自治系统。可以通过修改默认的BGPConfiguration资源来更改所有节点的全局默认值。以下示例命令将全局默认AS编号设置为64513。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | calicoctl apply -f -</span><br><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">kind: BGPConfiguration</span><br><span class="line">metadata:</span><br><span class="line">  name: default</span><br><span class="line">spec:</span><br><span class="line">  logSeverityScreen: Info</span><br><span class="line">  nodeToNodeMeshEnabled: false</span><br><span class="line">  asNumber: 64513</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h3 id="设置单个主机和AS号"><a href="#设置单个主机和AS号" class="headerlink" title="设置单个主机和AS号"></a>设置单个主机和AS号</h3><p>例如，以下命令将名为node-1的节点更改为属于AS 64514。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">calicoctl patch node node-1 -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;bgp&quot;: &#123;“asNumber”: “64514”&#125;&#125;&#125;&#x27;</span><br></pre></td></tr></table></figure>



<h3 id="修改节点地址范围"><a href="#修改节点地址范围" class="headerlink" title="修改节点地址范围"></a>修改节点地址范围</h3><p>此操作建议在部署完集群后立刻进行。  </p>
<p>默认情况下calico在集群层面分配一个10.42.0.0&#x2F;16的CIDR网段，在这基础上在单独为每个主机划分一个单独子网采用26位子网掩码对应的集群支持的节点数为2^10&#x3D;1024节点，单个子网最大支持64个POD，当单个子网对应IP消耗后，calico会重新在本机上划分一个新的子网如下，在集群对端主机可以看见对应的多个CIDR路由信息。  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/%20calico_ops_5.png"></p>
<p>注意：<br>块大小将影响节点POD的IP地址分配和路由条目数量，如果主机在一个CIDR中分配所有地址，则将为其分配一个附加CIDR。如果没有更多可用的块，则主机可以从分配给其他主机的CIDR中获取地址。为借用的地址添加了特定的路由，这会影响路由表的大小。</p>
<p>将块大小从默认值增加（例如，使用&#x2F;24则为每个块提供256个地址）意味着每个主机更少的块，会减少路由。但是对应的集群可容纳主机数也对应减少为2^8</p>
<p>从默认值减小CIDR大小（例如，使用&#x2F;28为每个块提供16个地址）意味着每个主机有更多CIDR，因此会有更多路由。</p>
<p>calico允许用户修改对应的IP池和集群CIDR</p>
<p>创建和替换步骤<br>注意：删除Pod时，应用程序会出现暂时不可用</p>
<ul>
<li>添加一个新的IP池。</li>
<li>注意：新IP池必须在同一群集CIDR中。</li>
<li>禁用旧的IP池（注意：禁用IP池只会阻止分配新的IP地址。它不会影响现有POD的联网）</li>
<li>从旧的IP池中删除Pod。</li>
<li>验证新的Pod是否从新的IP池中获取地址。</li>
<li>删除旧的IP池。</li>
</ul>
<p>定义ippool资源</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">kind: IPPool</span><br><span class="line">metadata:</span><br><span class="line">  name: my-ippool</span><br><span class="line">spec:</span><br><span class="line">  blockSize: 24</span><br><span class="line">  cidr: 192.0.0.0/16</span><br><span class="line">  ipipMode: Always</span><br><span class="line">  natOutgoing: true</span><br></pre></td></tr></table></figure>

<p>修改对应的blockSize号</p>
<p>创建新的</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">calicoctl apply -f pool.yaml</span><br></pre></td></tr></table></figure>

<p>将旧的ippool禁用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">calicoctl patch ippool default-ipv4-ippool -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;disabled&quot;: “true”&#125;&#125;&#x27;</span><br></pre></td></tr></table></figure>


<p>创建workload测试  </p>
<h3 id="根据节点标签定义对应的ippool"><a href="#根据节点标签定义对应的ippool" class="headerlink" title="根据节点标签定义对应的ippool"></a>根据节点标签定义对应的ippool</h3><p>Calico 能够进行配置，为不同拓扑指定 IP 地址池。例如可能希望某些机架、地区、或者区域能够从同一个 IP 池中获取地址。这对于降低路由数量或者配合防火墙策略的要求会很有帮助。</p>
<p>给节点配置对应label</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl label nodes kube-node-0 rack=0</span><br><span class="line">kubectl label nodes kube-node-1 rack=1</span><br></pre></td></tr></table></figure>

<p>通过标签定义对应的节点IPpool</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">calicoctl create -f -&lt;&lt;EOF</span><br><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">kind: IPPool</span><br><span class="line">metadata:</span><br><span class="line">  name: rack-0-ippool</span><br><span class="line">spec:</span><br><span class="line">  cidr: 192.168.0.0/24</span><br><span class="line">  ipipMode: Always</span><br><span class="line">  natOutgoing: true</span><br><span class="line">  nodeSelector: rack == &quot;0&quot;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">calicoctl create -f -&lt;&lt;EOF</span><br><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">kind: IPPool</span><br><span class="line">metadata:</span><br><span class="line">  name: rack-1-ippool</span><br><span class="line">spec:</span><br><span class="line">  cidr: 192.168.1.0/24</span><br><span class="line">  ipipMode: Always</span><br><span class="line">  natOutgoing: true</span><br><span class="line">  nodeSelector: rack == &quot;1&quot;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<h3 id="关闭SNAT"><a href="#关闭SNAT" class="headerlink" title="关闭SNAT"></a>关闭SNAT</h3><p>默认情况下，calico访问集群外网络是通过SNAT成宿主机ip方式，在一些金融客户环境中为了能实现防火墙规则，需要直接针对POD ip进行进行规则配置，所以需要关闭natOutgoing</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl edit ippool/default-ipv4-ippool</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>将  <code>natOutgoing: true</code>修改为<code>natOutgoing: false</code></p>
<p>此时，calico网络访问集群外的ip源ip就不会snat成<br>宿主机的ip地址。</p>
<h3 id="固定POD-IP"><a href="#固定POD-IP" class="headerlink" title="固定POD IP"></a>固定POD IP</h3><p>固定单个ip  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1 </span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-test</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  replicas: 1 # tells deployment to run 1 pods matching the template</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">      annotations:</span><br><span class="line">        &quot;cni.projectcalico.org/ipAddrs&quot;: &quot;[\&quot;10.42.210.135\&quot;]&quot;</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.7.9</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>固定多个ip，只能通过ippool的方式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat ippool1.yaml </span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">kind: IPPool</span><br><span class="line">metadata:</span><br><span class="line">  name: pool-1</span><br><span class="line">spec:</span><br><span class="line">  blockSize: 31</span><br><span class="line">  cidr: 10.21.0.0/31</span><br><span class="line">  ipipMode: Never</span><br><span class="line">  natOutgoing: true</span><br></pre></td></tr></table></figure>


<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1 </span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-test</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  replicas: 1 # tells deployment to run 1 pods matching the template</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">      annotations:</span><br><span class="line">        &quot;cni.projectcalico.org/ipv4pools&quot;: &quot;[\&quot;pool-1\&quot;]&quot;</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.7.9</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
      </tags>
  </entry>
  <entry>
    <title>openstack秒级创建云主级秒级创建快照原理</title>
    <url>/2017/08/05/ceph_clone/</url>
    <content><![CDATA[<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><h3 id="thin-provisioned"><a href="#thin-provisioned" class="headerlink" title="thin-provisioned"></a>thin-provisioned</h3><p>当分配一个100G的空间时，并不会立刻占满这100，只是占用了一些文件的元数据，当写入数据时会根据实际的大小动态的分配,类似linux中的稀疏文件。</p>
<h3 id="cow-（copy-on-write"><a href="#cow-（copy-on-write" class="headerlink" title="cow （copy on write)"></a>cow （copy on write)</h3><p>写时复制，也就是做快照时，先圈好个位置但这里面是空的，只有当父镜<br>像有数据有变化时，这时会先将变化前的数据cp到快照的空间，然后继续修改父镜象，这样做的好处是可以节省大量做快照的时间和减少存储空间，因为snapshot存储的都是发生改变前的区域，其它区域都是与父镜像共享的。</p>
<p>优点：cow快照，拷贝只是拷贝一些元数据，所以拷贝速度特别快，同时相比全量快照，占用的空间也要少很多</p>
<p>缺点：cow快照后的第一次数据更新时父镜像每次要写数据，要先将原始数据读出来然后在拷贝到快照卷中，然后在写父镜像，这样进行一次更新操作就需要一次读+两次写，会降低父镜像的写性能，如果父镜像链接更多的快照，那性能会更低。</p>
<h3 id="ceph快照是基于cow（copy-on-write）"><a href="#ceph快照是基于cow（copy-on-write）" class="headerlink" title="ceph快照是基于cow（copy on write）"></a>ceph快照是基于cow（copy on write）</h3><p>ceph 使用 COW （copy on write）方式实现 snapshot：在写入object 之前，将其拷贝出来，作为 snapshot 的 data object，然后继续修改原始数据。<br>rbd在创建快照时，并不会向pool中创建对象，也就是说并不会占用实际的存储空间,只是增加了一些对象信息</p>
<h3 id="ceph-clone"><a href="#ceph-clone" class="headerlink" title="ceph clone"></a>ceph clone</h3><p>clone：clone是将一个snapshot变成一个image，它是基于snapshot    创建 Clone 是将 image 的某一个 Snapshot 的状态复制变成一个 image。如 imageA 有一个 Snapshot-1，clone 是根据 ImageA 的 Snapshot-1 克隆得到 imageB。imageB 此时的状态与Snapshot-1完全一致，并且拥有 image 的相应能力，其区别在于 ImageB 此时可写。</p>
<p>从用户角度来看，一个 clone 和别的 RBD image 完全一样。你可以对它做 snapshot、读&#x2F;写、改变大小 等等，总之从用户角度来说没什么限制。同时，创建速度很快，这是因为 Ceph 只允许从 snapshot 创建 clone，而 snapshot 需要是只读（protect）的。</p>
<p>向 clone 的 instance 的object 写数据<br>ceph的克隆也是采用cow技术， 从本质上是 clone 的 RBD image 中读数据，对于不是它自己的 data objects，ceph 会从它的 parent snapshot 上读，如果它也没有，继续找它的parent image，直到一个 data object 存在。从这个过程也看得出来，该过程是缺乏效率的。</p>
<p>向 clone 的 instance object 写数据</p>
<p>Ceph 会首先检查该 clone image 上的 data object 是否存在。如果不存在，则从 parent snapshot 或者 image 上拷贝该 data object，然后执行数据写入操作。这时候，clone 就有自己的 data object 了。</p>
<h3 id="flatten"><a href="#flatten" class="headerlink" title="flatten"></a>flatten</h3><p>克隆操作本质上复制了一个 metadata object，而 data objects 是不存在的。因此在每次读操作时会先向本卷可能的 data object 访问。在返回对象不存在错误后会向父卷访问对应的对象最终决定这块数据是否存在。因此当存在多个层级的克隆链后，读操作需要更多的损耗去读上级卷的 data objects。只有当本卷的 data object 存在后(也就是写操作后)，才不需要访问上级卷。</p>
<p>为了防止父子层数过多，Ceph 提供了 flattern 函数将 clone 与 parent snapshot 共享的 data objects 复制到 clone，并删除父子关系。</p>
<p>flatten就是将 与父镜像共享的镜像都copy到clone中一层层递归，然后clone 与原来的父 snapshot 之间也不再有关系了，真正成为一个独立的 image，断绝父子关系，然后将那个snapshot删除<br>但flatten是极其消耗网络IO的，非常耗时间</p>
<p>flatten有什么好处呢<br>1，flatten后与父镜像和snapshot已经脱离关系了，可以任意删除。不然不能删除。<br>2，clone的instance 当访问某个不存在data object，需要向上级一级查找，这是非常影响效率，而flatten后改instance这些data object都从上级copy过来，速率会更快。</p>
<h3 id="使用ceph做后端存储，创建虚机"><a href="#使用ceph做后端存储，创建虚机" class="headerlink" title="使用ceph做后端存储，创建虚机"></a>使用ceph做后端存储，创建虚机</h3><p>如果没有使用ceph做后端存储，openstack创建虚机的流程是，先探测本地是否已经有镜像存在了，如果没有，则需要从glance仓库拷贝到对应的计算节点启动，如果有则直接启动，因此网络IO开销是非常大的如果使用Qcow2镜像格式，创建快照时需要commit当前镜像与base镜像合并并且上传到Glance中，这个过程也通常需要花费数分钟的时间。</p>
<p>当使用ceph做后端存储时，由于ceph是分布式存储，虚拟机镜像和根磁盘都是ceph的rbd image，所以就不需要copy到对应的计算节点，直接从原来的镜像中clone一个新镜像RBD image clone使用了COW技术，即写时拷贝，克隆操作并不会立即复制所有的对象，而只有当需要写入对象时才从parent image中拷贝对象到当前image中。因此，创建虚拟机几乎能够在秒级完成。</p>
<p>注意Glance使用Ceph RBD做存储后端时，镜像必须为raw格式，否则启动虚拟机时需要先在计算节点下载镜像到本地，并转为为raw格式，这开销非常大。</p>
<p>步骤如下<br>1，先基于镜像 做个snapshot、并添加protect、因为clone操作只能针对snapshot、</p>
<p>2， 创建虚拟机根磁盘<br>创建虚拟机时，直接从glance镜像的快照中clone一个新的RBD image作为虚拟机根磁盘:</p>
<p>rbd clone 1b364055-e323-4785-8e94-ebef1553a33b@snap fe4c108a-7ba0-4238-9953-15a7b389e43a_disk</p>
<p>3，启动虚拟机<br>启动虚拟机时指定刚刚创建的根磁盘，由于libvirt支持直接读写rbd镜像，因此不需要任何下载、导出工作。</p>
<h3 id="openstack给云主机做快照"><a href="#openstack给云主机做快照" class="headerlink" title="openstack给云主机做快照"></a>openstack给云主机做快照</h3><p>openstack给云主机做快照，因为还原是基于快照重新创建个云主机，所以本质上是做一个clone操作</p>
<p>具体流程</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">基于云主机创建个snapshot---&gt;给snapshot设置只读权限（protect）-----&gt;基于该snapshot clone一个image出来----&gt;flatten操作----&gt;删除snapshot</span><br></pre></td></tr></table></figure>



<p>为了秒级快照，这导致的后果就是，<br>1，做了快照的云主机在控制台删除后在ceph存储的pool中仍然还会存在，因为你的快照根主机的images还是存在父子关系，数据还是共享的，<br>2，云主机 xxxx_disk 存在snapshot，因为做clone是基于protect的snapshot，没flatten的话，snapshot自然没删除。<br>3，残留云主机和snapshot没清理的话会导致上传在glance的镜像也无法删除，因为你的云主机的xxx_disk是基于你glance的镜像clone的存在父子关系不能删除。。</p>
<p>解决办法，后台定期flatten然后rm那些客户已经删了云主机的残留数据。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ceph%20_clone_1.png"></p>
]]></content>
      <categories>
        <category>分布式存储</category>
      </categories>
      <tags>
        <tag>分布式存储</tag>
      </tags>
  </entry>
  <entry>
    <title>使用Bird模拟BGP Peers</title>
    <url>/2022/03/23/bird_bgpper/</url>
    <content><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>calico网络插件最为知名的就是calico-bgp模式，在测试中需要验证calico-bgp跨子网路由同步，需要连接两个子网的路由器支持BGP协议，这给测试环境搭建带来很大复杂性。本次文档通过Bird软件将一个虚拟机模拟为软路由，并配置为Kubernetes节点BGP Peers，实现BGP路由同步。</p>
<p>软件版本</p>
<table>
<thead>
<tr>
<th>软件</th>
<th>版本</th>
</tr>
</thead>
<tbody><tr>
<td>Kubernetes</td>
<td>v1.20.15</td>
</tr>
<tr>
<td>calico</td>
<td>v3.17.2</td>
</tr>
</tbody></table>
<h3 id="拓扑架构图"><a href="#拓扑架构图" class="headerlink" title="拓扑架构图"></a>拓扑架构图</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/bird-1.png"></p>
<p>Hostname：rke-node3<br>host-ip：192.168.0.7<br>pod-cidr：10.41.113.192&#x2F;26</p>
<p>Hostname：rke-node4<br>host-ip：192.168.0.25<br>pod-cidr：10.41.57.192&#x2F;26</p>
<p>Hostname：rke-node6<br>host-ip：192.168.2.14<br>pod-cidr：10.41.210.128&#x2F;26</p>
<p>Hostname：rke-node7<br>host-ip：192.168.2.15<br>pod-cidr：10.41.210.0&#x2F;26</p>
<p>kubernetes 节点分布在两个子网，中间通过一台vm连接了两个子网，在vm上部署bird软路由进行两个子网通信，同属一个AS自治域。</p>
<p>注意：如果底层是OpenStack环境需要关闭网卡安全组。</p>
<h3 id="Bird部署配置"><a href="#Bird部署配置" class="headerlink" title="Bird部署配置"></a>Bird部署配置</h3><h4 id="节点配置"><a href="#节点配置" class="headerlink" title="节点配置"></a>节点配置</h4><p>Bird节点采用一台VM部署，操作系统采用Centos7.6，将此节点作为软路由需要确保以下功能开启。</p>
<p>内核forward转发</p>
<pre><code>sysctl -a|grep &quot;net.ipv4.ip_forward = 1&quot;
net.ipv4.ip_forward = 1
</code></pre>
<p>iptables数据包转发</p>
<pre><code>iptables -P FORWARD ACCEPT
</code></pre>
<p>需要互相联通的节点上需要配置互访的静态路由</p>
<p>如在192.168.0.0&#x2F;24的节点上配置</p>
<pre><code>ip route add 192.168.2.0/24 via 192.168.0.40 dev ens3
</code></pre>
<p>如在192.168.2.0&#x2F;24的节点上配置</p>
<pre><code>ip route add 192.168.0.0/24 via 192.168.2.16 dev ens3
</code></pre>
<p>验证互访，在192.168.0.0&#x2F;24主机ping 192.168.2.0&#x2F;24主机</p>
<h4 id="Bird配置"><a href="#Bird配置" class="headerlink" title="Bird配置"></a>Bird配置</h4><p>Bird配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir /bird/</span><br><span class="line">vim /bird/bird.conf</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>router id 192.168.0.40;

filter calico_export_to_bgp_peers &#123;

  if ( net ~ 10.41.0.0/16 ) then &#123;
    accept;
  &#125;
  if ( net ~ 10.42.0.0/16 ) then &#123;
    accept;
  &#125;
  reject;
&#125;


filter calico_kernel_programming &#123;

  if ( net ~ 10.41.0.0/16 ) then &#123;
    accept;
  &#125;

  if ( net ~ 10.42.0.0/16 ) then &#123;
    accept;
  &#125;

  accept;
&#125;

# Configure synchronization between routing tables and kernel.
protocol kernel &#123;
  learn;             # Learn all alien routes from the kernel
  persist;           # Don&#39;t remove routes on bird shutdown
  scan time 2;       # Scan kernel routing table every 2 seconds
  import all;
  export filter calico_kernel_programming; # Default is export none
  graceful restart;  # Turn on graceful restart to reduce potential flaps in
                     # routes when reloading BIRD configuration.  With a full
                     # automatic mesh, there is no way to prevent BGP from
                     # flapping since multiple nodes update their BGP
                     # configuration at the same time, GR is not guaranteed to
                     # work correctly in this scenario.
  merge paths on;    # Allow export multipath routes (ECMP)
&#125;

protocol device &#123;
  debug &#123; states &#125;;
  scan time 2;    # Scan interfaces every 2 seconds
&#125;

protocol direct &#123;
  debug &#123; states &#125;;
  interface -&quot;cali*&quot;, -&quot;kube-ipvs*&quot;, &quot;*&quot;; # Exclude cali* and kube-ipvs* but
                                          # include everything else.  In
                                          # IPVS-mode, kube-proxy creates a
                                          # kube-ipvs0 interface. We exclude
                                          # kube-ipvs0 because this interface
                                          # gets an address for every in use
                                          # cluster IP. We use static routes
                                          # for when we legitimately want to
                                          # export cluster IPs.
&#125;



# Template for all BGP clients
template bgp bgp_template &#123;
  debug &#123; states &#125;;
  description &quot;Connection to BGP peer&quot;;
  local as 63400;
  multihop;
  gateway recursive; # This should be the default, but just in case.
  import all;        # Import all routes, since we don&#39;t know what the upstream
                     # topology is and therefore have to trust the ToR/RR.
  export filter calico_export_to_bgp_peers;  # Only want to export routes for workloads.
  source address 192.168.0.40;  # The local address we use for the TCP connection
  add paths on;
  graceful restart;  # See comment in kernel section about graceful restart.
  connect delay time 2;
  connect retry time 5;
  error wait time 5,30;
&#125;


protocol bgp Node_192_168_0_25 from bgp_template &#123;
  rr client;
  neighbor 192.168.0.25 as 63400;
&#125;
protocol bgp Node_192_168_0_7 from bgp_template &#123;
  rr client;
  neighbor 192.168.0.7 as 63400;
&#125;
protocol bgp Node_192_168_2_14 from bgp_template &#123;
  rr client;
  neighbor 192.168.2.14 as 63400;
&#125;

protocol bgp Node_192_168_2_15 from bgp_template &#123;
  rr client;

  neighbor 192.168.2.15 as 63400;
&#125;
</code></pre>
<p>将配置文件中的route-id、pod-cidr、neighbor-ip、as_number修改为实际需要建立bgp邻居的节点ip。</p>
<p>为了方便部署，本次bird使用Docker启动，启动命令如下：</p>
<pre><code>docker run  -itd  --net=host --uts=host --cap-add=NET_ADMIN --cap-add=NET_BROADCAST --cap-add=NET_RAW -v /bird/:/etc/bird:ro ibhde/bird4
</code></pre>
<p>检查启动状态是否为up</p>
<pre><code>docker ps -a
</code></pre>
<h3 id="Calico-BGP对接"><a href="#Calico-BGP对接" class="headerlink" title="Calico BGP对接"></a>Calico BGP对接</h3><p>全部节点上安装calicoctl</p>
<pre><code>wget https://github.com/projectcalico/calicoctl/releases/download/v3.17.4/calicoctl-linux-amd64

mv calicoctl-linux-amd64 /usr/bin/calicoctl

chmod a+x /usr/bin/calicoctl
</code></pre>
<p>关闭全局full-mesh</p>
<pre><code>cat &lt;&lt;EOF | calicoctl apply -f -

apiVersion: projectcalico.org/v3
kind: BGPConfiguration
metadata:
 name: default
spec:
  logSeverityScreen: Info
  nodeToNodeMeshEnabled: false
  asNumber: 63400
EOF
</code></pre>
<p>配置节点label</p>
<p>这里将两组节点打上不同标签，将192.168.2.0&#x2F;24节点打上rack&#x3D;rack-1标签，连接192.168.2.16 bpg-peers，将192.168.0.0&#x2F;24打上rack-rack-2标签，连接192.168.0.40 bgp-peers</p>
<pre><code>kubectl label nodes rke-node3 rack=rack-2
kubectl label nodes rke-node4 rack=rack-2
kubectl label nodes rke-node5 rack=rack-1
kubectl label nodes rke-node5 rack=rack-1
</code></pre>
<p>使用caliclctl配置BGP Peers</p>
<pre><code>cat &lt;&lt;EOF | calicoctl apply -f -

apiVersion: projectcalico.org/v3
kind: BGPPeer
metadata:
  name: rack1-tor
spec:
  peerIP: 192.168.2.16
  asNumber: 63400
  nodeSelector: rack == &#39;rack-1&#39;
EOF
</code></pre>
<!---->

<pre><code>apiVersion: projectcalico.org/v3
kind: BGPPeer
metadata:
  name: rack2-tor
spec:
  peerIP: 192.168.0.40
  asNumber: 63400
  nodeSelector: rack == &#39;rack-2&#39;
</code></pre>
<p>检查与BGP Peers连接情况</p>
<p>在rack&#x3D;rack-2标签节点执行，应显示已经与192.168.0.40 bgp-peers建立连接</p>
<pre><code> calicoctl node status
Calico process is running.

IPv4 BGP status
+--------------+---------------+-------+------------+-------------+
| PEER ADDRESS |   PEER TYPE   | STATE |   SINCE    |    INFO     |
+--------------+---------------+-------+------------+-------------+
| 192.168.0.40 | node specific | up    | 2022-03-18 | Established |
+--------------+---------------+-------+------------+-------------+

IPv6 BGP status
No IPv6 peers found.
</code></pre>
<p>在rack&#x3D;rack-1标签节点执行，应显示已经与192.168.2.16 bgp-peers建立连接</p>
<pre><code>calicoctl node status
Calico process is running.

IPv4 BGP status
+--------------+---------------+-------+------------+-------------+
| PEER ADDRESS |   PEER TYPE   | STATE |   SINCE    |    INFO     |
+--------------+---------------+-------+------------+-------------+
| 192.168.2.16 | node specific | up    | 2022-03-18 | Established |
+--------------+---------------+-------+------------+-------------+

IPv6 BGP status
No IPv6 peers found.
</code></pre>
<p>创建pod，验证路由同步</p>
<pre><code>kubectl create deployment test --image=nginx --replicas=5
</code></pre>
<p>在5副本中，互相进行ping操作。验证跨节点网络是否正常。</p>
<p>在bird节点查看路由学习</p>
<pre><code>ip route
default via 192.168.2.1 dev eth0
10.41.210.0/26 via 192.168.2.15 dev eth0 proto bird
10.42.57.192/26 via 192.168.0.25 dev eth1 proto bird
10.42.113.192/26 via 192.168.0.7 dev eth1 proto bird
10.42.210.128/26 via 192.168.2.14 dev eth0 proto bird
192.168.0.0/24 dev eth1 proto kernel scope link src 192.168.0.40
192.168.2.0/24 dev eth0 proto kernel scope link src 192.168.2.16
</code></pre>
<p>可以看见bird将集群内每个节点的pod-cidr都学习过来了。</p>
<p>在任意一个node节点上查看路由，以192.168.0.3节点为例，可以看见节点上也拥有集群全部pod-cidr路由信息。</p>
<pre><code>ip route
default via 192.168.0.1 dev ens3 proto dhcp src 192.168.0.7 metric 100
10.41.57.192/26 via 192.168.0.25 dev ens3 proto bird
blackhole 10.41.113.192/26 proto bird
10.41.210.0/26 via 192.168.0.40 dev ens3 proto bird
10.41.210.128/26 via 192.168.0.40 dev ens3 proto bird
10.42.57.192/26 via 192.168.0.25 dev ens3 proto bird
192.168.0.0/24 dev ens3 proto kernel scope link src 192.168.0.7
192.168.2.0/24 via 192.168.0.40 dev ens3
</code></pre>
<h4 id="节点POD-CIDR路由统一走默认路由"><a href="#节点POD-CIDR路由统一走默认路由" class="headerlink" title="节点POD-CIDR路由统一走默认路由"></a>节点POD-CIDR路由统一走默认路由</h4><p>当前路由同步会将每个节点pod-cidr同步到集群中的节点上，对于Kubernetes集群规模大情况下会造成路由条目增多。可以通过下发默认路由方式，将节点全部流量请求都都指向bird 软路由节点。这样还有一个好处就是，在一些硬件SDN设备中可以实现流量监控。但需要注意的是路由器本身能承载的流量。</p>
<p>以bird配置为例</p>
<pre><code>router id 192.168.0.40;
protocol static &#123;
 route 10.41.0.0/16 via 192.168.0.40;
 route 10.42.0.0/16 via 192.168.0.40;
&#125;
filter calico_export_to_bgp_peers &#123;

  if ( net ~ 10.41.0.0/16 ) then &#123;
    accept;
  &#125;
  if ( net ~ 10.42.0.0/16 ) then &#123;
    accept;
  &#125;
  reject;
&#125;


filter calico_kernel_programming &#123;

  if ( net ~ 10.41.0.0/16 ) then &#123;
    accept;
  &#125;

  if ( net ~ 10.42.0.0/16 ) then &#123;
    accept;
  &#125;

  accept;
&#125;

# Configure synchronization between routing tables and kernel.
protocol kernel &#123;
  learn;             # Learn all alien routes from the kernel
  persist;           # Don&#39;t remove routes on bird shutdown
  scan time 2;       # Scan kernel routing table every 2 seconds
  import all;
  export filter calico_kernel_programming; # Default is export none
  graceful restart;  # Turn on graceful restart to reduce potential flaps in
                     # routes when reloading BIRD configuration.  With a full
                     # automatic mesh, there is no way to prevent BGP from
                     # flapping since multiple nodes update their BGP
                     # configuration at the same time, GR is not guaranteed to
                     # work correctly in this scenario.
  merge paths on;    # Allow export multipath routes (ECMP)
&#125;

protocol device &#123;
  debug &#123; states &#125;;
  scan time 2;    # Scan interfaces every 2 seconds
&#125;

protocol direct &#123;
  debug &#123; states &#125;;
  interface -&quot;cali*&quot;, -&quot;kube-ipvs*&quot;, &quot;*&quot;; # Exclude cali* and kube-ipvs* but
                                          # include everything else.  In
                                          # IPVS-mode, kube-proxy creates a
                                          # kube-ipvs0 interface. We exclude
                                          # kube-ipvs0 because this interface
                                          # gets an address for every in use
                                          # cluster IP. We use static routes
                                          # for when we legitimately want to
                                          # export cluster IPs.
&#125;



# Template for all BGP clients
template bgp bgp_template &#123;
  debug &#123; states &#125;;
  description &quot;Connection to BGP peer&quot;;
  local as 63400;
  multihop;
  gateway recursive; # This should be the default, but just in case.
  import all;        # Import all routes, since we don&#39;t know what the upstream
                     # topology is and therefore have to trust the ToR/RR.
  export filter calico_export_to_bgp_peers;  # Only want to export routes for workloads.
  source address 192.168.0.40;  # The local address we use for the TCP connection
  add paths on;
  graceful restart;  # See comment in kernel section about graceful restart.
  connect delay time 2;
  connect retry time 5;
  error wait time 5,30;
&#125;


protocol bgp Node_192_168_0_25 from bgp_template &#123;
  neighbor 192.168.0.25 as 63400;
&#125;
protocol bgp Node_192_168_0_7 from bgp_template &#123;
  neighbor 192.168.0.7 as 63400;
&#125;
protocol bgp Node_192_168_2_14 from bgp_template &#123;
  neighbor 192.168.2.14 as 63400;
&#125;

protocol bgp Node_192_168_2_15 from bgp_template &#123;

  neighbor 192.168.2.15 as 63400;
&#125;
</code></pre>
<p>将neighbor配置中的  rr client删除，同时添加静态路由下发配置</p>
<pre><code>protocol static &#123;
 route 10.41.0.0/16 via 192.168.0.40;
 route 10.42.0.0/16 via 192.168.0.40;
&#125;
</code></pre>
<p>在192.168.0.0&#x2F;24的主机上看见路由情况如下：</p>
<pre><code>ip route

default via 192.168.0.1 dev ens3 proto dhcp src 192.168.0.7 metric 100
10.41.0.0/16 via 192.168.0.40 dev ens3 proto bird
blackhole 10.41.113.192/26 proto bird
10.42.0.0/16 via 192.168.0.40 dev ens3 proto bird
</code></pre>
<p>可以看见pod-cidr的流量都被发送到Bird虚拟路由器192.168.0.40接口<br>在192.168.2.0&#x2F;24的主机上看见路由情况如下：</p>
<pre><code>ip route
default via 192.168.2.1 dev ens7
10.41.0.0/16 via 192.168.2.16 dev ens7 proto bird
blackhole 10.41.210.128/26 proto bird
10.42.0.0/16 via 192.168.2.16 dev ens7 proto bird
</code></pre>
<p>可以看见pod-cidr的流量都被发送到Bird虚拟路由器192.168.2.16接口</p>
<h4 id="节点POD-IP明细路由发布"><a href="#节点POD-IP明细路由发布" class="headerlink" title="节点POD-IP明细路由发布"></a>节点POD-IP明细路由发布</h4><p>在实际使用中若期望将calico-pod明细路由发布到BGP路由器中，则需要修改每个节点的calico配置文件<br>修改方法如下</p>
<p>创建configmap，替换calico原有的bird_aggr.cfg.template文件</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/bird-2.png"></p>
<p>主要修改以下参数：<br>注释掉本地黑洞路由，就不会生产本地聚合路由同步到BGP路由器了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># route &#123;&#123;$cidr&#125;&#125; blackhole;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>允许明细路由同步<br>将<code>if ( net ~ &#123;&#123;$cidr&#125;&#125; ) then &#123; reject; &#125; </code>修改为accept</p>
<p>完整配置如下：</p>
<pre><code># Generated by confd

&#123;&#123;- $block_key := printf "/calico/ipam/v2/host/%s/ipv4/block" (getenv "NODENAME")&#125;&#125;
&#123;&#123;- $static_key := "/calico/staticroutes"&#125;&#125;
&#123;&#123;if or (ls $block_key) (ls $static_key)&#125;&#125;
protocol static &#123;
&#123;&#123;- if ls $block_key&#125;&#125;
   # IP blocks for this host.
&#123;&#123;- range ls $block_key&#125;&#125;
&#123;&#123;- $parts := split . "-"&#125;&#125;
&#123;&#123;- $cidr := join $parts "/"&#125;&#125;
 #  route &#123;&#123;$cidr&#125;&#125; blackhole;
&#123;&#123;- end&#125;&#125;
&#123;&#123;- end&#125;&#125;
&#123;&#123;- if ls $static_key&#125;&#125;
   # Static routes.
&#123;&#123;- range ls $static_key&#125;&#125;
&#123;&#123;- $parts := split . "-"&#125;&#125;
&#123;&#123;- $cidr := join $parts "/"&#125;&#125;
 #  route &#123;&#123;$cidr&#125;&#125; blackhole;
&#123;&#123;- end&#125;&#125;
&#123;&#123;- end&#125;&#125;
&#125;
&#123;&#123;else&#125;&#125;# No IP blocks or static routes for this host.&#123;&#123;end&#125;&#125;

# Aggregation of routes on this host; export the block, nothing beneath it.
function calico_aggr ()
&#123;
&#123;&#123;- range ls $block_key&#125;&#125;
&#123;&#123;- $parts := split . "-"&#125;&#125;
&#123;&#123;- $cidr := join $parts "/"&#125;&#125;
&#123;&#123;- $affinity := json (getv (printf "%s/%s" $block_key .))&#125;&#125;
  &#123;&#123;- if $affinity.state&#125;&#125;
      # Block &#123;&#123;$cidr&#125;&#125; is &#123;&#123;$affinity.state&#125;&#125;
    &#123;&#123;- if eq $affinity.state "confirmed"&#125;&#125;
      if ( net = &#123;&#123;$cidr&#125;&#125; ) then &#123; accept; &#125;
      if ( net ~ &#123;&#123;$cidr&#125;&#125; ) then &#123; accept; &#125;
    &#123;&#123;- end&#125;&#125;
  &#123;&#123;- else &#125;&#125;
      # Block &#123;&#123;$cidr&#125;&#125; is implicitly confirmed.
      if ( net = &#123;&#123;$cidr&#125;&#125; ) then &#123; accept; &#125;
      if ( net ~ &#123;&#123;$cidr&#125;&#125; ) then &#123; accept; &#125;
  &#123;&#123;- end &#125;&#125;
&#123;&#123;- end&#125;&#125;
&#125;
</code></pre>
<p>升级calico-node映射此configmap配置文件<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/bird-3.png"></p>
<p>重建calico-node</p>
<p>查看Bird节点</p>
<pre><code>ip route
default via 192.168.2.1 dev eth0 
10.41.0.0/16 via 192.168.0.40 dev eth1 proto bird 
10.41.57.199 via 192.168.0.25 dev eth1 proto bird 
10.41.57.203 via 192.168.0.25 dev eth1 proto bird 
10.41.57.204 via 192.168.0.25 dev eth1 proto bird 
10.41.113.193 via 192.168.0.7 dev eth1 proto bird 
10.41.113.194 via 192.168.0.7 dev eth1 proto bird 
10.41.113.195 via 192.168.0.7 dev eth1 proto bird 
10.41.113.196 via 192.168.0.7 dev eth1 proto bird 
10.41.113.198 via 192.168.0.7 dev eth1 proto bird 
10.41.113.201 via 192.168.0.7 dev eth1 proto bird 
10.41.113.202 via 192.168.0.7 dev eth1 proto bird 
10.41.210.6 via 192.168.2.15 dev eth0 proto bird 
10.41.210.7 via 192.168.2.15 dev eth0 proto bird 
10.41.210.8 via 192.168.2.15 dev eth0 proto bird 
10.41.210.9 via 192.168.2.15 dev eth0 proto bird 
10.41.210.137 via 192.168.2.14 dev eth0 proto bird 
10.41.210.138 via 192.168.2.14 dev eth0 proto bird 
10.42.0.0/16 via 192.168.0.40 dev eth1 proto bird 
</code></pre>
<p>已经学习到了每个pod的明细路由，这种方式会导致路由设备压力巨大，因为需要维护大量的路由条目，并且pod的每次删除和创建都会引发的路由条目更新。在实际生产中请谨慎评估后使用。</p>
<p>而实际业务在使用的过程中，会针对一个服务或者一个deployment分配一个IP Pool，这种使用模式会导致Calico的IP Pool没有办法按照Node聚合，出现一些零散的无法聚合的IP地址，最差的情况，会导致每个Pod产生一条路由，会导致路由的条目变为Pod级别。<br>在默认情况下，交换机设备为了防止路由震荡，会对BGP路由进行收敛保护。但是Kubernetes集群中，Pod生命周期短，变化频繁，需要关闭网络设备的路由变更保护机制才能满足Kubernetes的要求；对于不同的网络设备，路由收敛速度也是不同的，在大规模Pod扩容和迁移的场景，或者进行双数据中心切换，除了考虑Pod的调度时间、启动时间，还需要对网络设备的路由收敛速度进行性能评估和压测。</p>
<p><a href="https://blog.51cto.com/u_14992974/2549877">https://blog.51cto.com/u_14992974/2549877</a></p>
<h4 id="Service-CIDR路由发布"><a href="#Service-CIDR路由发布" class="headerlink" title="Service-CIDR路由发布"></a>Service-CIDR路由发布</h4><p>为了使集群外部也可以通过Service的Cluster-ip访问到集群内部服务，可以将Service-cidr通过Calico-bgp进行发布。</p>
<pre><code>calicoctl patch BGPConfig default --patch &#39;&#123;&quot;spec&quot;: &#123;&quot;serviceClusterIPs&quot;: [&#123;&quot;cidr&quot;: &quot;10.43.0.0/16&quot;&#125;]&#125;&#125;&#39;
</code></pre>
<p>发布后在bird节点上可以看见多条10.43.0.0&#x2F;16地址，因为采用ECMP(等价多路径)方式实现路由负载均衡。</p>
<pre><code>ip route

10.43.0.0/16 proto bird 
        nexthop via 192.168.0.7 dev eth1 weight 1 
        nexthop via 192.168.0.25 dev eth1 weight 1 
        nexthop via 192.168.2.14 dev eth0 weight 1 
        nexthop via 192.168.2.15 dev eth0 weight 1 
</code></pre>
<p>配置明细路由后发布后，Service-CIDR在BGP路由器中无法看见，可以通过修改bird_aggr.cfg.template文件</p>
<p>添加以下配置，$servicesubnet_split网段根据集群实际Service-CIDR进行修改</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">&#123;&#123;- $servicesubnet_split := split &quot;10.43.0.0/16&quot; &quot; &quot; &#125;&#125;</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">  # Service IP block</span><br><span class="line">&#123;&#123;- if $servicesubnet_split&#125;&#125;</span><br><span class="line">&#123;&#123;- range $servicesubnet_split&#125;&#125;</span><br><span class="line">   route &#123;&#123;.&#125;&#125; blackhole;</span><br><span class="line">&#123;&#123;- end&#125;&#125;</span><br><span class="line">&#123;&#123;- end&#125;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">function accept_servicesubnet () </span><br><span class="line">&#123;</span><br><span class="line">&#123;&#123;- range $servicesubnet_split&#125;&#125;</span><br><span class="line">  if ( net = &#123;&#123;.&#125;&#125; ) then &#123; accept; &#125;</span><br><span class="line">  if ( net ~ &#123;&#123;.&#125;&#125; ) then &#123; reject; &#125;</span><br><span class="line">&#123;&#123;- end&#125;&#125;</span><br><span class="line">&#125;</span><br><span class="line">function deny_servicesubnet ()</span><br><span class="line">&#123;</span><br><span class="line">&#123;&#123;- range $servicesubnet_split&#125;&#125;</span><br><span class="line">  if ( net = &#123;&#123;.&#125;&#125; ) then &#123; reject; &#125;</span><br><span class="line">  if ( net ~ &#123;&#123;.&#125;&#125; ) then &#123; reject; &#125;</span><br><span class="line">&#123;&#123;- end&#125;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>完整bird_aggr.cfg.template配置文件如下：</p>
<pre><code># Generated by confd
&#123;&#123;- $block_key := printf "/calico/ipam/v2/host/%s/ipv4/block" (getenv "NODENAME")&#125;&#125;
&#123;&#123;- $static_key := "/calico/staticroutes"&#125;&#125;
&#123;&#123;- $servicesubnet_split := split "10.43.0.0/16" " " &#125;&#125;

&#123;&#123;if or (ls $block_key) (ls $static_key)&#125;&#125;
protocol static &#123;
&#123;&#123;- if ls $block_key&#125;&#125;
   # IP blocks for this host.
&#123;&#123;- range ls $block_key&#125;&#125;
&#123;&#123;- $parts := split . "-"&#125;&#125;
&#123;&#123;- $cidr := join $parts "/"&#125;&#125;
 #  route &#123;&#123;$cidr&#125;&#125; blackhole;
&#123;&#123;- end&#125;&#125;
&#123;&#123;- end&#125;&#125;
&#123;&#123;- if ls $static_key&#125;&#125;
   # Static routes.
&#123;&#123;- range ls $static_key&#125;&#125;
&#123;&#123;- $parts := split . "-"&#125;&#125;
&#123;&#123;- $cidr := join $parts "/"&#125;&#125;
 #   route &#123;&#123;$cidr&#125;&#125; blackhole;
&#123;&#123;- end&#125;&#125;
&#123;&#123;- end&#125;&#125;
  # Service IP block
&#123;&#123;- if $servicesubnet_split&#125;&#125;
&#123;&#123;- range $servicesubnet_split&#125;&#125;
   route &#123;&#123;.&#125;&#125; blackhole;
&#123;&#123;- end&#125;&#125;
&#123;&#123;- end&#125;&#125;
&#125;
&#123;&#123;else&#125;&#125;# No IP blocks or static routes for this host.&#123;&#123;end&#125;&#125;
# Aggregation of routes on this host; export the block, nothing beneath it.
# Export the service block.

function accept_servicesubnet () 
&#123;
&#123;&#123;- range $servicesubnet_split&#125;&#125;
  if ( net = &#123;&#123;.&#125;&#125; ) then &#123; accept; &#125;
  if ( net ~ &#123;&#123;.&#125;&#125; ) then &#123; reject; &#125;
&#123;&#123;- end&#125;&#125;
&#125;
function deny_servicesubnet ()
&#123;
&#123;&#123;- range $servicesubnet_split&#125;&#125;
  if ( net = &#123;&#123;.&#125;&#125; ) then &#123; reject; &#125;
  if ( net ~ &#123;&#123;.&#125;&#125; ) then &#123; reject; &#125;
&#123;&#123;- end&#125;&#125;
&#125;

function calico_aggr ()
&#123;
&#123;&#123;- range ls $block_key&#125;&#125;
&#123;&#123;- $parts := split . "-"&#125;&#125;
&#123;&#123;- $cidr := join $parts "/"&#125;&#125;
&#123;&#123;- $affinity := json (getv (printf "%s/%s" $block_key .))&#125;&#125;
  &#123;&#123;- if $affinity.state&#125;&#125;
      # Block &#123;&#123;$cidr&#125;&#125; is &#123;&#123;$affinity.state&#125;&#125;
    &#123;&#123;- if eq $affinity.state "confirmed"&#125;&#125;
      if ( net = &#123;&#123;$cidr&#125;&#125; ) then &#123; accept; &#125;
      if ( net ~ &#123;&#123;$cidr&#125;&#125; ) then &#123; accept; &#125;
    &#123;&#123;- end&#125;&#125;
  &#123;&#123;- else &#125;&#125;
      # Block &#123;&#123;$cidr&#125;&#125; is implicitly confirmed.
      if ( net = &#123;&#123;$cidr&#125;&#125; ) then &#123; accept; &#125;
      if ( net ~ &#123;&#123;$cidr&#125;&#125; ) then &#123; accept; &#125;
  &#123;&#123;- end &#125;&#125;
&#123;&#123;- end&#125;&#125;
&#125;
</code></pre>
]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
      </tags>
  </entry>
  <entry>
    <title>ceph部署</title>
    <url>/2019/12/01/ceph_deploy/</url>
    <content><![CDATA[<p>非常简单的ceph部署手册，主要方便大家用于学习研究，非生产可用！！</p>
<h2 id="基础环境信息"><a href="#基础环境信息" class="headerlink" title="基础环境信息"></a>基础环境信息</h2><h3 id="版本信息"><a href="#版本信息" class="headerlink" title="版本信息"></a>版本信息</h3><p>操作系统：ubuntu16.04<br>内核版本：4.4.0-87-generic<br>ceph版本：13.2.7(mimic)</p>
<h3 id="主机角色信息"><a href="#主机角色信息" class="headerlink" title="主机角色信息"></a>主机角色信息</h3><table>
<thead>
<tr>
<th>主机名</th>
<th>IP地址</th>
<th>角色</th>
<th>配置</th>
</tr>
</thead>
<tbody><tr>
<td>rke-node1</td>
<td>192.168.0.5</td>
<td>ceph-mon</td>
<td>—</td>
</tr>
<tr>
<td>rke-node2</td>
<td>192.168.0.6</td>
<td>ceph-osd1</td>
<td>100G HDD</td>
</tr>
<tr>
<td>rke-node3</td>
<td>192.168.0.7</td>
<td>ceph-osd2</td>
<td>100G HDD</td>
</tr>
<tr>
<td>rke-node4</td>
<td>192.168.0.8</td>
<td>ceph-osd3</td>
<td>100G HDD</td>
</tr>
</tbody></table>
<h3 id="部署安装"><a href="#部署安装" class="headerlink" title="部署安装"></a>部署安装</h3><h4 id="配置源"><a href="#配置源" class="headerlink" title="配置源"></a>配置源</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">echo deb https://download.ceph.com/debian-&#123;ceph-stable-release&#125;/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apt update</span><br></pre></td></tr></table></figure>

<p>安装python-minimal</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apt install</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="安装ceph-deploy"><a href="#安装ceph-deploy" class="headerlink" title="安装ceph-deploy"></a>安装ceph-deploy</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt install ceph-deploy</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="创建普通用户"><a href="#创建普通用户" class="headerlink" title="创建普通用户"></a>创建普通用户</h4><p>在集群中所有节点创建ceph用户，主要用于ceph-deploy部署（虽然可以用root，但不建议这样做)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">useradd -d /home/ceph -m ceph</span><br><span class="line">passwd ceph</span><br><span class="line"></span><br><span class="line">echo &quot;ceph ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/ceph</span><br><span class="line"></span><br><span class="line">chmod 0440 /etc/sudoers.d/ceph</span><br></pre></td></tr></table></figure>
<p>在ceph-deploy节点使用ceph用户登录，配置节点免密码ssh</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">su - ceph</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh-copy-id host_name</span><br></pre></td></tr></table></figure>


<h4 id="建立集群"><a href="#建立集群" class="headerlink" title="建立集群"></a>建立集群</h4><p>创建集群文件夹</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo mkdir /etc/ceph</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd etc/ceph</span><br></pre></td></tr></table></figure>

<h4 id="创建集群部署ceph-monitor"><a href="#创建集群部署ceph-monitor" class="headerlink" title="创建集群部署ceph-monitor"></a>创建集群部署ceph-monitor</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo ceph-deploy new  rke-node1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注：</span><br><span class="line">rke-node1为我要部署monitor的实际的FQDN主机名</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">sudo ceph-deploy install rke-node1 rke-node2 rke-node3 rke-node4  --repo-url=http://mirrors.aliyun.com/ceph/debian-mimic</span><br></pre></td></tr></table></figure>

<p>注：<br>这里指定使用安装源，因为连国外网慢</p>
<h4 id="初始化ceph-mon"><a href="#初始化ceph-mon" class="headerlink" title="初始化ceph-mon"></a>初始化ceph-mon</h4> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ceph-deploy mon create-initial</span><br></pre></td></tr></table></figure>
<p> 注意：</p>
<p> 1、若遇到没有权限创建&#x2F;var&#x2F;run&#x2F;ceph目录时可以手动创建然后将用户和属主改为ceph。</p>
<p> 同步权限配置文件</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo ceph-deploy admin rke-node1 rke-node2 rke-node3 rke-node4</span><br></pre></td></tr></table></figure>

<h4 id="部署ceph-mgr"><a href="#部署ceph-mgr" class="headerlink" title="部署ceph-mgr"></a>部署ceph-mgr</h4> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo ceph-deploy mgr create rke-node1</span><br></pre></td></tr></table></figure>


<h4 id="初始化并启动OSD"><a href="#初始化并启动OSD" class="headerlink" title="初始化并启动OSD"></a>初始化并启动OSD</h4> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo ceph-deploy osd create --data /dev/vdb rke-node2</span><br><span class="line">sudo ceph-deploy osd create --data /dev/vdb rke-node3</span><br><span class="line">sudo ceph-deploy osd create --data /dev/vdb rke-node4</span><br></pre></td></tr></table></figure>

<p> 查看是否加入成功和ceph集群状态<br> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo ceph -s</span><br><span class="line"> cluster:</span><br><span class="line">   id:     dc33e445-6157-47cd-81b2-23f9b233839e</span><br><span class="line">   health: HEALTH_OK</span><br><span class="line"></span><br><span class="line"> services:</span><br><span class="line">   mon: 1 daemons, quorum rke-node1</span><br><span class="line">   mgr: rke-node1(active)</span><br><span class="line">   osd: 3 osds: 3 up, 3 in</span><br><span class="line"></span><br><span class="line"> data:</span><br><span class="line">   pools:   0 pools, 0 pgs</span><br><span class="line">   objects: 0  objects, 0 B</span><br><span class="line">   usage:   3.0 GiB used, 294 GiB / 297 GiB avail</span><br><span class="line">   pgs:</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h4 id="查看osd状态"><a href="#查看osd状态" class="headerlink" title="查看osd状态"></a>查看osd状态</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> sudo ceph osd tree</span><br><span class="line">ID CLASS WEIGHT  TYPE NAME          STATUS REWEIGHT PRI-AFF</span><br><span class="line">-1       0.29008 root default</span><br><span class="line">-3       0.09669     host rke-node2</span><br><span class="line"> 0   hdd 0.09669         osd.0          up  1.00000 1.00000</span><br><span class="line">-5       0.09669     host rke-node3</span><br><span class="line"> 1   hdd 0.09669         osd.1          up  1.00000 1.00000</span><br><span class="line">-7       0.09669     host rke-node4</span><br><span class="line"> 2   hdd 0.09669         osd.2          up  1.00000 1.00000</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h4 id="启用ceph-dashboard"><a href="#启用ceph-dashboard" class="headerlink" title="启用ceph-dashboard"></a>启用ceph-dashboard</h4><p>通过ceph-mgr启用dashboard功能</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ceph mgr module enable dashboard</span><br></pre></td></tr></table></figure>

<p>设置访问ip和端口</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ceph config set mgr mgr/dashboard/server_addr 172.16.0.195</span><br><span class="line">ceph config set mgr mgr/dashboard/server_port 8800</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>设置管理员帐号和密码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ceph dashboard set-login-credentials admin admin@2019</span><br></pre></td></tr></table></figure>


<p>访问http:&#x2F;&#x2F;主机ip:8800<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ceph_deploy_1.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ceph_deploy_2.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ceph_deploy_3.png"></p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><h4 id="创建pool"><a href="#创建pool" class="headerlink" title="创建pool"></a>创建pool</h4><p>因为我们osd数量小于4设置pg为128</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ceph osd pool create test-pool 128</span><br></pre></td></tr></table></figure>

<h4 id="在pool内创建文件"><a href="#在pool内创建文件" class="headerlink" title="在pool内创建文件"></a>在pool内创建文件</h4><p>在test-pool内创建一个名为rbdtest的文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rbd -p test-pool create rbdtest -s 1G</span><br></pre></td></tr></table></figure>


<p>查看是否创建成功</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rbd ls -p test-pool</span><br><span class="line">rbdtest</span><br></pre></td></tr></table></figure>

<h4 id="使用nbd将块映射到宿主机"><a href="#使用nbd将块映射到宿主机" class="headerlink" title="使用nbd将块映射到宿主机"></a>使用nbd将块映射到宿主机</h4><p>因为ceph比较新，ubuntu16.04默认内核不支持一些新feture，所以这里就不用rbd进行map了。<br>安装</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apt-get install rbd-nbd</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rbd-nbd map test-pool/test-rbd</span><br><span class="line"></span><br><span class="line">rbd-nbd list-mapped</span><br><span class="line">id    pool      image   snap device</span><br><span class="line">31755 test-pool rbdtest -    /dev/nbd0</span><br></pre></td></tr></table></figure>
<p>格式化</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkfs.xfs /dev/nbd0</span><br></pre></td></tr></table></figure>

<h4 id="挂载并测试写入数据"><a href="#挂载并测试写入数据" class="headerlink" title="挂载并测试写入数据"></a>挂载并测试写入数据</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mount /dev/nbd0  /mnt/</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">echo test &gt; /mnt/test</span><br></pre></td></tr></table></figure>

<p>卸载</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">umount /mnt</span><br><span class="line">rbd-nbd unmap /dev/nbd0</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>分布式存储</category>
      </categories>
      <tags>
        <tag>分布式存储</tag>
      </tags>
  </entry>
  <entry>
    <title>cgroup的简单使用</title>
    <url>/2018/11/18/cgroup/</url>
    <content><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>在实际生产应用中，我们经常在一台宿主机上运行多个应用程序，这时就会产生一个问题就是，多个应用程序之间如何避免资源抢占，资源进行限制，这时我们通过Linux内核自带的cgroup实现。</p>
<h3 id="cgroup介绍"><a href="#cgroup介绍" class="headerlink" title="cgroup介绍"></a>cgroup介绍</h3><p>Cgroup全称Control groups，最早是由Google的Paul Menage和Rohit Seth在2006年发起，最早名称叫最早的名称为进程容器（process containers）。在2007年时，因为在Linux内核中，容器（container）这个名词有许多不同的意义，为避免混乱，被重命名为cgroup，并且被合并到2.6.24版的内核中，cgroup的主要作用是用来控制、限制、分离一个进程组群的资源（如CPU、内存、磁盘输入输出等）。</p>
<h3 id="cgroup能做什么"><a href="#cgroup能做什么" class="headerlink" title="cgroup能做什么"></a>cgroup能做什么</h3><ul>
<li><p>资源限制<br>设置进程或进程组最大能使用的cpu、内存、磁盘的资源</p>
</li>
<li><p>优先级<br>设置进程使用资源的优先级</p>
</li>
<li><p>资源使用统计<br>测量组的资源使用情况，例如，可用于计费目的[12]</p>
</li>
<li><p>进程控制<br>冻结或挂起进程</p>
</li>
</ul>
<p>参考：<br><a href="https://en.wikipedia.org/wiki/Cgroups">https://en.wikipedia.org/wiki/Cgroups</a></p>
<p>更多详细情况可以参考<br><a href="https://www.infoq.cn/article/docker-kernel-knowledge-cgroups-resource-isolation">https://www.infoq.cn/article/docker-kernel-knowledge-cgroups-resource-isolation</a></p>
<h3 id="什么是NUMA"><a href="#什么是NUMA" class="headerlink" title="什么是NUMA?"></a>什么是NUMA?</h3><p>早期的SMP模型，所有CPU共享一个内存块，造成内存访问冲突加剧，命中率低，造成性能瓶颈。NUMA（Non-Uniform Memory Access）就是这样的环境下引入的一个模型。NUMA尝试通过为每个处理器提供单独的内存来解决此问题，避免在多个处理器尝试寻址相同内存时的性能损失。比如一台机器是有2个处理器，有4个内存块。我们将1个处理器和两个内存块合起来，称为一个NUMA node，这样这个机器就会有两个NUMA node。在物理分布上，NUMA node的处理器和内存块的物理距离更小，因此访问也更快。比如这台机器会分左右两个处理器（cpu1, cpu2），在每个处理器两边放两个内存块(memory1.1, memory1.2, memory2.1,memory2.2)，这样NUMA node1的cpu1访问memory1.1和memory1.2就比访问memory2.1和memory2.2更快。所以使用NUMA的模式如果能尽量保证本node内的CPU只访问本node内的内存块，那这样的效率就是最高的。</p>
<p>查看本机CPU和NUMA信息<br>查看物理CPU个数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat /proc/cpuinfo | grep &quot;physical id&quot; | sort | uniq |wc -l</span><br></pre></td></tr></table></figure>
<p>查看每颗cpu核数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat /proc/cpuinfo |grep &quot;cores&quot;</span><br></pre></td></tr></table></figure>
<p>查看线程数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat /proc/cpuinfo | grep &quot;physical id&quot; | sort | uniq |wc -l</span><br></pre></td></tr></table></figure>
<p>所以一台物理服务器总的cpu核数为<br>物理cpu个数x每颗物理cpu的核数x线程数<br>查看NUMA分布</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node-4 ~]# lscpu |grep NUMA</span><br><span class="line">NUMA 节点：         2</span><br><span class="line">NUMA 节点0 CPU：    0-11,24-35</span><br><span class="line">NUMA 节点1 CPU：    12-23,36-47</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这里是有两个NUMA节点，分别是0,1，其他NUMA0上对应的CPU核为0-11，24-35核，NUMA1上对应的CPU核为12-23，36-47<br>看这些信息的意义在于，对进程进行cpu限制时，最好把它限制在一个NUMA节点内，因为跨NUMA节点的服务会带来一定性能损耗。</p>
<h3 id="功能演示"><a href="#功能演示" class="headerlink" title="功能演示"></a>功能演示</h3><p>环境信息<br>ubuntu：16.04</p>
<h4 id="cgroups管理进程cpu资源"><a href="#cgroups管理进程cpu资源" class="headerlink" title="cgroups管理进程cpu资源"></a>cgroups管理进程cpu资源</h4><p>用stress进行CPU压力测试<br>在cpuset控制创建一个控制组</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir /sys/fs/cgroup/cpuset/test</span><br></pre></td></tr></table></figure>
<p>先有stress进行压力测试，占满两个逻辑核</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">stress -c 2 &amp;</span><br></pre></td></tr></table></figure>
<p>查看top看见两个逻辑核的空闲率都为0</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/cgroup_1.png"><br>使用cgroup的cpuset将它限制在某个核上。<br>切换目录到刚刚创建的cpuset下的目录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /sys/fs/cgroup/cpuset/test</span><br></pre></td></tr></table></figure>
<p>将要限制的逻辑核范围输入到cpuset.cpus文件内，我这里限制跑在0号逻辑核上</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">echo &quot;0&quot; &gt;/sys/fs/cgroup/cpuset/test/cpuset.cpus</span><br></pre></td></tr></table></figure>
<p>使用cgexec 可以用来直接在子系统中的指定控制组运行一个程序，会自动将进程的PID填入的tasks文件中啊，不用手动输入</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cgexec -g cpuset:/test stress -c 2 &amp;</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/cgroup_2.png"></p>
<p>即使stress运行在两个逻辑核上，因为cgroup的限制，并且限制在0号逻辑核上。<br>查看tasks，自动将	PID写入了tasks文件<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/cgroup_3.png"></p>
<h4 id="cgroups管理进程内存资源"><a href="#cgroups管理进程内存资源" class="headerlink" title="cgroups管理进程内存资源"></a>cgroups管理进程内存资源</h4><p>跑一个耗内存的脚本，内存不断增长</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">x=&quot;a&quot;</span><br><span class="line">while [ True ];do</span><br><span class="line">    x=$x$x</span><br><span class="line">done;</span><br></pre></td></tr></table></figure>
<p>将脚本保存为memeory.sh<br>top看内存占用稳步上升<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/cgroup_4.png"></p>
<p>下面用cgroups控制这个进程的内存资源</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir -p /sys/fs/cgroup/memory/test</span><br></pre></td></tr></table></figure>
<p>分配1G的内存给这个控制组</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">echo 1G&gt;  /cgroup/memory/foo/memory.limit_in_bytes</span><br></pre></td></tr></table></figure>
<p>设置为-1表示不限制<br>执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cgexec -g memory:/test sh /root/memory.sh</span><br></pre></td></tr></table></figure>
<p>发现很快之前的脚本被kill掉<br>可以试着把memory.limit_in_bytes值设的更小，会发现被kill的时间会更快。<br>还有个memory.soft_limit_in_bytes参数，用于配置内存软限制，简单来说就是当系统检测到系统内存争用或内存不足时，cgroup会将其限制在软限制范围内，如果需要将memory.soft_limit_in_bytes和memory.limit_in_bytes同时配置需要将memory.soft_limit_in_bytes的值设置低于memory.limit_in_bytes。</p>
<h4 id="cgroups管理进程io资源"><a href="#cgroups管理进程io资源" class="headerlink" title="cgroups管理进程io资源"></a>cgroups管理进程io资源</h4><p>跑一个耗io的脚本</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dd if=/dev/vda of=/dev/null &amp;</span><br></pre></td></tr></table></figure>
<p>通过iotop看io占用情况，磁盘速度到了284M&#x2F;s</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">30252 be/4 root      284.71 M/s    0.00 B/s  0.00 %  0.00 % dd if=/dev/vda of=/dev/null</span><br></pre></td></tr></table></figure>
<p>下面用cgroups控制这个进程的io资源</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir /sys/fs/cgroup/blkio/test</span><br></pre></td></tr></table></figure>
<p>把vda下载读取速率不超过1M</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">echo &#x27;253:0 1048576&#x27; &gt;/sys/fs/cgroup/blkio/test/blkio.throttle.read_bps_device</span><br></pre></td></tr></table></figure>
<p>253:0对应主设备号和副设备号，可以通过ls -l &#x2F;dev&#x2F;vda查看</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ls -l /dev/vda</span><br><span class="line"></span><br><span class="line">brw-rw---- 1 root disk 253, 0 Nov  9 18:35 /dev/vda</span><br></pre></td></tr></table></figure>
<p>执行dd测试速率</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cgexec -g blkio:/test  dd if=/dev/vda of=/dev/null</span><br></pre></td></tr></table></figure>
<p>再通过iotop看，确实将读速度降到了1M&#x2F;s</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">25206 be/4 root     1002.27 K/s    0.00 B/s  0.00 % 97.75 % dd if=/dev/vda of=/dev/null</span><br></pre></td></tr></table></figure>



<h3 id="实际应用测试"><a href="#实际应用测试" class="headerlink" title="实际应用测试"></a>实际应用测试</h3><p>环境ubuntu16.04<br>使docker容器运行在指定的CPU核上,并限制内存的使用<br>配置前查看已经运行的容器运行在哪些cpu核上</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/cgroup_10.png"><br>安装cgroup包</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apt-get install cgroup-bin cgroup-lite cgroup-tools cgroupfs-mount libcgroup1</span><br></pre></td></tr></table></figure>
<p>配置cgconfig<br>创建文件&#x2F;etc&#x2F;cgconfig.conf</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">group test &#123;</span><br><span class="line">cpuset &#123;</span><br><span class="line">cpuset.cpus = &quot;0&quot;;</span><br><span class="line">cpuset.mems = &quot;0&quot;;</span><br><span class="line">&#125;</span><br><span class="line">memory &#123;</span><br><span class="line">                memory.limit_in_bytes = &quot;3G&quot;;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注：<br>-	Group test 这里表示这个组名叫test，当然也可以根据需要自由去定义。<br>-	Cpuset表示使用cpuset控制器</p>
<ul>
<li>Cpuset.cpus表示设置运行在哪些核上用”-“表示范围，多个不连续范围用”,”隔开，例如控制进程运行在0到7核和11到19核，应该写为0-7,11-19.</li>
<li>Cpuset.mems表示上面控制的核对应的numa节点，尽量都控制在一个NUMA节点。</li>
</ul>
<p>如何查看cpu核和numa节点对应关系?</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/cgroup_12.png"></p>
<p>这里表示 NUMA node0节点对应的CPU核数为0,1</p>
<ul>
<li>Memory表示使用memory控制器</li>
<li>memory.limit_in_bytes限制内存的使用</li>
</ul>
<p>创建并配置 &#x2F;etc&#x2F;cgrules.conf</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">*:docker-containerd-shim cpuset,memory test</span><br></pre></td></tr></table></figure>

<ul>
<li>*表示所有用户，如果要控制具体的用户可以直接写用户名</li>
<li>docker-containerd-shim表示需要控制的进程名</li>
<li>   cpuset,memory表示应用哪些控制器，就是我们在cgconfig.conf中定义的那些</li>
<li>   test表示对应的组，对应的是在cgconfig.conf中定义的group</li>
</ul>
<p>编写init启动脚本</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">文件名/etc/init.d/cgconf</span><br><span class="line">#!/bin/sh</span><br><span class="line">### BEGIN INIT INFO</span><br><span class="line"># Provides:          cgconf</span><br><span class="line"># Required-Start:    $remote_fs $syslog</span><br><span class="line"># Required-Stop:     $remote_fs $syslog</span><br><span class="line"># Should-Start:</span><br><span class="line"># Should-Stop:</span><br><span class="line"># Default-Start:     2 3 4 5</span><br><span class="line"># Default-Stop:      0 1 6</span><br><span class="line"># Short-Description: Configures CGroups</span><br><span class="line">### END INIT INFO</span><br><span class="line"></span><br><span class="line">start_service() &#123;</span><br><span class="line">  if is_running; then</span><br><span class="line">    echo &quot;cgrulesengd is running already!&quot;</span><br><span class="line">    return 1</span><br><span class="line">  else</span><br><span class="line">    echo &quot;Processing /etc/cgconfig.conf...&quot;</span><br><span class="line">    cgconfigparser -l /etc/cgconfig.conf</span><br><span class="line">    echo &quot;Processing /etc/cgrules.conf...&quot;</span><br><span class="line">    cgrulesengd -vvv --logfile=/var/log/cgrulesengd.log</span><br><span class="line">    return 0</span><br><span class="line">  fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">stop_service() &#123;</span><br><span class="line">  if is_running; then</span><br><span class="line">    echo &quot;Stopping cgrulesengd...&quot;</span><br><span class="line">    pkill cgrulesengd</span><br><span class="line">  else</span><br><span class="line">    echo &quot;cgrulesengd is not running!&quot;</span><br><span class="line">    return 1</span><br><span class="line">  fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">status() &#123;</span><br><span class="line">  if pgrep cgrulesengd &gt; /dev/null; then</span><br><span class="line">    echo &quot;cgrulesengd is running&quot;</span><br><span class="line">    return 0</span><br><span class="line">  else</span><br><span class="line">    echo &quot;cgrulesengd is not running!&quot;</span><br><span class="line">    return 3</span><br><span class="line">  fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">is_running() &#123;</span><br><span class="line">  status &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case &quot;$&#123;1:-&#125;&quot; in</span><br><span class="line">  start)</span><br><span class="line">    start_service</span><br><span class="line">    ;;</span><br><span class="line">  stop)</span><br><span class="line">    stop_service</span><br><span class="line">    ;;</span><br><span class="line">  status)</span><br><span class="line">    status</span><br><span class="line">    ;;</span><br><span class="line">  *)</span><br><span class="line">    echo &quot;Usage: /etc/init.d/cgconf &#123;start|stop|restart|status&#125;&quot;</span><br><span class="line">    exit 2</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line">exit $?</span><br></pre></td></tr></table></figure>
<p>修改权限</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">chmod 755 /etc/init.d/cgconf</span><br></pre></td></tr></table></figure>
<p>更新注册系统启动项脚本</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">update-rc.d cgconf defaults</span><br></pre></td></tr></table></figure>
<p>启动服务</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl start cgconf</span><br></pre></td></tr></table></figure>
<p>重器docker生效</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>
<p>在此查看docker容器是否生效全部运行到指定的核上（测试配的是线程0）<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/cgroup_11.png"></p>
<p>如果配置了内存限制的，需要修改grub启动参数<br>编辑&#x2F;etc&#x2F;default&#x2F;grub，GRUB_CMDLINE_LINUX_DEFAULT行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">GRUB_CMDLINE_LINUX_DEFAULT=&quot;cgroup_enable=memory quiet&quot;</span><br></pre></td></tr></table></figure>
<p>更新</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">update-grub</span><br></pre></td></tr></table></figure>
<p>重启操作系统</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">reboot</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>CICD_DEMO完整版示例e</title>
    <url>/2020/09/17/cicd_demo/</url>
    <content><![CDATA[<h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><h4 id="部署gitlab"><a href="#部署gitlab" class="headerlink" title="部署gitlab"></a>部署gitlab</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --detach --hostname 10.8.242.28 --publish 443:443 --publish 80:80 --publish 1022:22 --name gitlab --restart always --volume /srv/gitlab/config:/etc/gitlab --volume /srv/gitlab/logs:/var/log/gitlab --volume /srv/gitlab/data:/var/opt/gitlab gitlab/gitlab-ce:12.10.3-ce.0</span><br></pre></td></tr></table></figure>

<p>替换hostname为实际节点外网IP</p>
<h4 id="部署Harbor"><a href="#部署Harbor" class="headerlink" title="部署Harbor"></a>部署Harbor</h4><p>Harbor部署与管理<br>部署前先修改docker<br>编辑docker</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim /etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line"> &quot;insecure-registries&quot; : [&quot;0.0.0.0/0&quot;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>重启docker</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl restart docker </span><br></pre></td></tr></table></figure>

<p>安装docker-compose</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -L https://github.com/docker/compose/releases/download/1.24.1/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose</span><br><span class="line">chmod +x /usr/local/bin/docker-compose</span><br></pre></td></tr></table></figure>
<p>下载harbor  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://github.com/goharbor/harbor/releases/download/v1.10.2/harbor-online-installer-v1.10.2.tgz</span><br></pre></td></tr></table></figure>


<p>配置harbo.yaml  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hostname: 172.31.48.86 //修改为实际节点IP</span><br></pre></td></tr></table></figure>

<p>屏蔽https配置  </p>
<p>安装harbor </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./install.sh --with-clair</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker-compose  ps</span><br><span class="line">      Name                     Command                  State                 Ports          </span><br><span class="line">---------------------------------------------------------------------------------------------</span><br><span class="line">clair               /docker-entrypoint.sh            Up (healthy)   6060/tcp, 6061/tcp       </span><br><span class="line">harbor-core         /harbor/start.sh                 Up (healthy)                            </span><br><span class="line">harbor-db           /entrypoint.sh postgres          Up (healthy)   5432/tcp                 </span><br><span class="line">harbor-jobservice   /harbor/start.sh                 Up                                      </span><br><span class="line">harbor-log          /bin/sh -c /usr/local/bin/ ...   Up (healthy)   127.0.0.1:1514-&gt;10514/tcp</span><br><span class="line">harbor-portal       nginx -g daemon off;             Up (healthy)   80/tcp                   </span><br><span class="line">nginx               nginx -g daemon off;             Up (healthy)   0.0.0.0:80-&gt;80/tcp       </span><br><span class="line">redis               docker-entrypoint.sh redis ...   Up             6379/tcp                 </span><br><span class="line">registry            /entrypoint.sh /etc/regist ...   Up (healthy)   5000/tcp                 </span><br><span class="line">registryctl         /harbor/start.sh                 Up (healthy)        </span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>访问<a href="http://node_ip/">http://node_ip</a></p>
<p>admin&#x2F;Harbor12345<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/harbor_1.png"></p>
<h3 id="Drone配置使用"><a href="#Drone配置使用" class="headerlink" title="Drone配置使用"></a>Drone配置使用</h3><p>Rancher应用商店已经提供了Drone的部署安装及配置，通过Rancher应用商店即可将Drone部署并配置对接</p>
<p>在gitlab上创建一个项目名为go-server，编写个应用<br>server.go       </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">    &quot;fmt&quot;</span><br><span class="line">    &quot;log&quot;</span><br><span class="line">    &quot;net/http&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func hello(w http.ResponseWriter, r *http.Request) &#123;</span><br><span class="line">    fmt.Fprintf(w, &quot;Hello World&quot;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line">    http.HandleFunc(&quot;/&quot;, hello)</span><br><span class="line">    if err := http.ListenAndServe(&quot;:8080&quot;, nil); err != nil &#123;</span><br><span class="line">        log.Fatal(err)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>创建Dockerfile<br>Dockerfile</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM golang</span><br><span class="line">WORKDIR /go</span><br><span class="line">ADD server /go</span><br><span class="line">CMD [&quot;./server&quot;]</span><br></pre></td></tr></table></figure>

<p>1、配置gitlab外部认证</p>
<p><img src="https://pic.downk.cc/item/5ebe49f2c2a9a83be58fb862.jpg"></p>
<p>回调接口填写其中一个节点的<a href="http://ip/login">http://ip/login</a><br><img src="https://pic.downk.cc/item/5ebe4a22c2a9a83be58fe142.jpg"><br>如下</p>
<p><img src="https://pic.downk.cc/item/5ebe4ac2c2a9a83be5905f4d.jpg"></p>
<p>保存好对应的ApplicationID和secret</p>
<p><img src="https://pic.downk.cc/item/5ebe4aecc2a9a83be590804d.jpg"></p>
<p>部署Drone  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run \</span><br><span class="line">  --volume=/var/run/docker.sock:/var/run/docker.sock \</span><br><span class="line">  --volume=/var/lib/drone:/data \</span><br><span class="line">  --env=DRONE_LOGS_DEBUG=true \</span><br><span class="line">  --env=DRONE_GIT_ALWAYS_AUTH=false \</span><br><span class="line">  --env=DRONE_GITLAB_SERVER=http://114.215.25.58 \</span><br><span class="line">  --env=DRONE_GITLAB_CLIENT_ID=d6272993ac02c3bb4069d73bf0ff8dabeaff47c0739ae27d1a23e8b80e33faa5 \</span><br><span class="line">  --env=DRONE_GITLAB_CLIENT_SECRET=01f454fe0a55256a974d420b8ca023df6efc80b33d8a917dd16138b152b73253 \</span><br><span class="line">  --env=DRONE_RPC_SECRET=12345678\</span><br><span class="line">  --env=DRONE_RUNNER_CAPACITY=3 \</span><br><span class="line">  --env=DRONE_SERVER_HOST=114.215.130.36\</span><br><span class="line">  --env=DRONE_SERVER_PROTO=http \</span><br><span class="line">  --env=DRONE_TLS_AUTOCERT=false \</span><br><span class="line">  --publish=80:80 \</span><br><span class="line">  --publish=443:443 \</span><br><span class="line">  --restart=always \</span><br><span class="line">  --detach=true \</span><br><span class="line">  --name=drone  \</span><br><span class="line">  drone/drone:1</span><br></pre></td></tr></table></figure>

<p>参数说明：  </p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>DRONE_RPC_SECRET</td>
<td>runner连接server的凭证</td>
</tr>
<tr>
<td>DRONE_RUNNER_CAPACITY</td>
<td>server调用runner的并发数</td>
</tr>
<tr>
<td>DRONE_SERVER_HOST</td>
<td>server的ip</td>
</tr>
<tr>
<td>DRONE_SERVER_PROTO</td>
<td>server对外提供的协议可选http和https</td>
</tr>
</tbody></table>
<p>部署Drone-runner<br>采用Docker的方式  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -d \</span><br><span class="line">  -v /var/run/docker.sock:/var/run/docker.sock \</span><br><span class="line">  -e DRONE_RPC_PROTO=http \</span><br><span class="line">  -e DRONE_RPC_HOST=172.31.227.147 \</span><br><span class="line">  -e DRONE_RPC_SECRET=12345678 \</span><br><span class="line">  -e DRONE_RUNNER_CAPACITY=3 \</span><br><span class="line">  -e DRONE_RUNNER_NAME=$&#123;HOSTNAME&#125; \</span><br><span class="line">  -p 3000:3000 \</span><br><span class="line">  --restart always \</span><br><span class="line">  --name runner \</span><br><span class="line">  drone/drone-runner-docker:1</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>检查日志查看是否部署成功</p>
<p><img src="https://pic.downk.cc/item/5ec0e602c2a9a83be519525a.jpg"></p>
<p>在gitlab go-server项目创建名为.drone文件填入以下内容</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kind: pipeline</span><br><span class="line">type: docker</span><br><span class="line">name: build</span><br><span class="line">steps:</span><br><span class="line">- name: build-code</span><br><span class="line">  image: golang:alpine</span><br><span class="line">  pull: if-not-exists # always never</span><br><span class="line">  commands:</span><br><span class="line">    - pwd</span><br><span class="line">    - ls</span><br><span class="line">    - go build server.go</span><br><span class="line">    - ls</span><br><span class="line">- name: build-image</span><br><span class="line">  image: plugins/docker</span><br><span class="line">  settings:</span><br><span class="line">    repo: 172.31.227.151/go-server/go-server</span><br><span class="line">    registry: 172.31.227.151</span><br><span class="line">    use_cache: true</span><br><span class="line">    username:</span><br><span class="line">      from_secret: registry_username</span><br><span class="line">    password:</span><br><span class="line">      from_secret: registry_password</span><br><span class="line">    tags: $&#123;DRONE_BUILD_NUMBER&#125;</span><br><span class="line">    insecure: true</span><br><span class="line">    mirror: https://yefnfc9c.mirror.aliyuncs.com/</span><br><span class="line">trigger:</span><br><span class="line">  branch:</span><br><span class="line">  - master</span><br><span class="line">  event:</span><br><span class="line">  - push</span><br></pre></td></tr></table></figure>

<p>连接镜像仓库的凭证环境变量可在Drone对应的项目中定义好<br><img src="https://pic.downk.cc/item/5ebfef95c2a9a83be53599b8.jpg"></p>
<p>参数说明：  </p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>DRONE_RPC_PROTO</td>
<td>runner连接server协议</td>
</tr>
<tr>
<td>DRONE_RPC_HOST</td>
<td>server的地址</td>
</tr>
<tr>
<td>DRONE_RPC_SECRET</td>
<td>连接server的凭证</td>
</tr>
</tbody></table>
<p>commit代码后会自动运行CI<br><img src="https://pic.downk.cc/item/5ec01120c2a9a83be5601e4a.jpg"></p>
<h3 id="Jenkins配置使用"><a href="#Jenkins配置使用" class="headerlink" title="Jenkins配置使用"></a>Jenkins配置使用</h3><h4 id="应用概述"><a href="#应用概述" class="headerlink" title="应用概述"></a>应用概述</h4><p><img src="https://pic.downk.cc/item/5ebdf195c2a9a83be535cd43.jpg"></p>
<h4 id="整体CICD流程"><a href="#整体CICD流程" class="headerlink" title="整体CICD流程"></a>整体CICD流程</h4><p><img src="https://training-1251900790.cos.ap-guangzhou.myqcloud.com/image/cicd3_1.png?ynotemdtimestamp=1588945845253">  </p>
<ul>
<li>push code to trigger Gitlab webhook</li>
<li>compiling the code and build image</li>
<li>test</li>
<li>upload image to harbor</li>
<li>Generate a new YML file</li>
<li>deploy&#x2F;upgrade</li>
</ul>
<h4 id="先决条件"><a href="#先决条件" class="headerlink" title="先决条件"></a>先决条件</h4><p>1、clone front-end项目  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone https://gitee.com/wanshaoyuan/front-end.git</span><br></pre></td></tr></table></figure>
<p>2、并上传到内部gitlab上</p>
<p>3、在harbor中创建front-ent项目  </p>
<p>4、在jenkins中安装好gitlab、docker、Kubernetes插件</p>
<p>5、部署sock-shap</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone https://gitee.com/wanshaoyuan/microservices-demo.git</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create namespace  sock-shop</span><br><span class="line">kubectl apply -f microservices-demo/deploy/kubernetes/manifests/.</span><br></pre></td></tr></table></figure>

<p>访问  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">http://node_ip:30001</span><br></pre></td></tr></table></figure>

<p>Jenkins安装</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd --name jenkins -u root -p 50000:50000  -p 8080:8080 -v /var/jenkins_home:/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock  jenkins/jenkins:lts</span><br></pre></td></tr></table></figure>

<p>节点安装git</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum install git</span><br></pre></td></tr></table></figure>
<p>获取jenkins密码  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker logs jenkins |grep password</span><br></pre></td></tr></table></figure>

<p><img src="https://pic.downk.cc/item/5ebd3414c2a9a83be572e22a.jpg"></p>
<p>安装推荐的Jenkins插件</p>
<p><img src="https://pic.downk.cc/item/5ebd3678c2a9a83be574dcb6.jpg"></p>
<h4 id="Jenkins对接Gitlab"><a href="#Jenkins对接Gitlab" class="headerlink" title="Jenkins对接Gitlab"></a>Jenkins对接Gitlab</h4><p>安装gitlab插件<br><img src="https://pic.downk.cc/item/5ebd382ac2a9a83be5766ac4.jpg"></p>
<p>Gitlab中申请AccessToken</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_10.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_11.png"></p>
<p><img src="https://pic.downk.cc/item/5ebd3952c2a9a83be57762d6.jpg"></p>
<p>将申请成功的token保存好  </p>
<p>配置Jenkins对接gitlab  </p>
<p><img src="https://pic.downk.cc/item/5ebd39b2c2a9a83be577b003.jpg"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_14.png"><br>测试连接</p>
<p><img src="https://pic.downk.cc/item/5ebd3b2dc2a9a83be5792b5e.jpg"></p>
<p>测试  </p>
<p>在gitlab中创建项目<br>上传个test.txt文件通过Jenkins读取</p>
<p><img src="https://pic.downk.cc/item/5ebd3ddac2a9a83be57c2e59.jpg"></p>
<p><img src="https://pic.downk.cc/item/5ebd3f39c2a9a83be57dc753.jpg"></p>
<p>配置连接gitlab私有项目的密钥可以用ssh密钥也可以使用账号密码<br><img src="https://pic.downk.cc/item/5ebd3efcc2a9a83be57d86c1.jpg"></p>
<p><img src="https://pic.downk.cc/item/5ebd3f92c2a9a83be57e211a.jpg"></p>
<p>构建<br><img src="https://pic.downk.cc/item/5ebd3fc1c2a9a83be57e4fc9.jpg"></p>
<p>去cat这个文件输出内容<br><img src="https://pic.downk.cc/item/5ebd4056c2a9a83be57ef2b2.jpg"></p>
<p>执行立即构建<br><img src="https://pic.downk.cc/item/5ebd407fc2a9a83be57f28ba.jpg"></p>
<p>输出结果  </p>
<p><img src="https://pic.downk.cc/item/5ebd40b3c2a9a83be57f6f31.jpg"></p>
<h4 id="Jenkins对接Kubernetes"><a href="#Jenkins对接Kubernetes" class="headerlink" title="Jenkins对接Kubernetes"></a>Jenkins对接Kubernetes</h4><p>安装Kubernetes插件<br><img src="https://pic.downk.cc/item/5ebd460fc2a9a83be5858b37.jpg"></p>
<h4 id="Jenkins-CI对接Kubernetes"><a href="#Jenkins-CI对接Kubernetes" class="headerlink" title="Jenkins-CI对接Kubernetes"></a>Jenkins-CI对接Kubernetes</h4><p>配置<br>系统管理—&gt;系统设置—&gt;新增一个云<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_dymaic_slave_3.png"></p>
<p>配置Jenkins URL，这里我们没有配置api-server地址和证书key，连接kubernetes，所以默认会去读取放在JENKINS_HOME的.kube&#x2F;目录的kubeconfig文件，用于连接集群。我这里是通过安装包的方式安装的Jenkins HOME在&#x2F;var&#x2F;lib&#x2F;jenkins&#x2F;目录，如果是通过容器方式启动，将kubeconfig文件直接放~&#x2F;.kube&#x2F;目录。</p>
<p>从RancherUI上复制配置文件</p>
<p><img src="https://pic.downk.cc/item/5ebd53dac2a9a83be5957d6a.jpg"></p>
<p>保存到Jenkins主机的config文件中<br>复制粘贴到Jenkins容器内的~&#x2F;.kube&#x2F;config文件中</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker exec -it jenkins mkdir /root/.kube/</span><br><span class="line">docker cp config  jenkins:/root/.kube/config</span><br></pre></td></tr></table></figure>


<p>在编译集群对应项目下创建secret  </p>
<p><img src="https://training-1251900790.cos.ap-guangzhou.myqcloud.com/image/cicd3_4.png">  </p>
<p>在jenkins系统设置中创建一个Kubernetes云  </p>
<p><img src="https://pic.downk.cc/item/5ebd553cc2a9a83be59712e6.jpg"></p>
<p>演示效果  </p>
<p>修改前，先展示前端。(<a href="https://github.com/microservices-demo/front-end)%E8%BF%99%E9%87%8C%E6%88%91%E4%BB%AC%E6%BC%94%E7%A4%BA%E4%BF%AE%E6%94%B9%E5%89%8D%E7%AB%AFlogo%EF%BC%8C%E7%84%B6%E5%90%8Egit">https://github.com/microservices-demo/front-end)这里我们演示修改前端logo，然后git</a> push到代码仓库，触发Jenkins进行cicd操作，修改front-end&#x2F;public&#x2F;navbar.html的&lt;img src&#x3D;”img&#x2F;logo.png”为新的图片地址，然后将代码提交触发cicd重新打开前端看见logo变了。</p>
<h4 id="创建cicd-Pipeline"><a href="#创建cicd-Pipeline" class="headerlink" title="创建cicd Pipeline"></a>创建cicd Pipeline</h4><p>生成连接gitlab的凭证Grove语句<br><img src="https://pic.downk.cc/item/5ebd81b1c2a9a83be5d084ca.jpg"></p>
<p>添加Harbor凭证<br><img src="https://pic.downk.cc/item/5ebd862bc2a9a83be5d6a61e.jpg"></p>
<p><img src="https://pic.downk.cc/item/5ebd8664c2a9a83be5d6ebaa.jpg"></p>
<p>创建流水线任务  </p>
<p><img src="https://pic.downk.cc/item/5ebe3398c2a9a83be577e1a8.jpg"></p>
<p>添加以下Pipeline任务  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">  environment &#123;</span><br><span class="line">    registry = &quot;172.31.227.151&quot;</span><br><span class="line">    project_name = &quot;/front-end&quot;</span><br><span class="line">    app_name = &quot;/front-end&quot;</span><br><span class="line">    registryCredential = &#x27;harbor&#x27;</span><br><span class="line">  &#125;</span><br><span class="line">  agent &#123;</span><br><span class="line">    kubernetes &#123;</span><br><span class="line">      defaultContainer &#x27;wanshaoyuan/jnlp-slave:3.27-1-alpine&#x27;</span><br><span class="line">      yaml &quot;&quot;&quot;</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    some-label: some-label-value</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: jnlp</span><br><span class="line">    image: &#x27;wanshaoyuan/jnlp-slave:3.27-1-alpine&#x27;</span><br><span class="line">    args: [&#x27;\$(JENKINS_SECRET)&#x27;, &#x27;\$(JENKINS_NAME)&#x27;]</span><br><span class="line"></span><br><span class="line">  - name : docker</span><br><span class="line">    image: wanshaoyuan/docker:19.03.2</span><br><span class="line">    tty: true</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: repo-docker-sock</span><br><span class="line">      mountPath: /var/run/docker.sock</span><br><span class="line">  - name: kubectl</span><br><span class="line">    image: wanshaoyuan/jenkins-tools:v1.0</span><br><span class="line">    tty: true</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - mountPath: &quot;/root/.kube&quot;</span><br><span class="line">      name: &quot;volume-0&quot;</span><br><span class="line">      readOnly: false</span><br><span class="line">  volumes:</span><br><span class="line">  - name: &quot;volume-0&quot;</span><br><span class="line">    secret:</span><br><span class="line">      secretName: &quot;kubeconfig&quot;</span><br><span class="line">  - name: repo-docker-sock</span><br><span class="line">    hostPath:</span><br><span class="line">     path: /var/run/docker.sock</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  stages &#123;</span><br><span class="line">    stage(&#x27;clone code&#x27;) &#123;</span><br><span class="line">     steps &#123;</span><br><span class="line">            git credentialsId: &#x27;9371d980-6bc2-47aa-a47f-04cf78871e7a&#x27;, url: &#x27;http://114.215.25.58/root/front-end.git&#x27;</span><br><span class="line">    &#125;</span><br><span class="line">   &#125;</span><br><span class="line">    stage(&#x27;image build &#x27;) &#123;</span><br><span class="line">      steps &#123;</span><br><span class="line">        container(&#x27;docker&#x27;) &#123;</span><br><span class="line">         script &#123;</span><br><span class="line">          docker.withRegistry( &#x27;http://172.31.227.151&#x27;, registryCredential ) &#123;</span><br><span class="line">             def dockerImage=docker.build registry + project_name + app_name  +&quot;:$BUILD_NUMBER&quot;</span><br><span class="line">             dockerImage.push()</span><br><span class="line">         &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">   stage(&#x27;deploy app &#x27;) &#123;</span><br><span class="line">      steps &#123;</span><br><span class="line">         container(&#x27;kubectl&#x27;) &#123;</span><br><span class="line">          sh &#x27;sed  -i &quot;s/image: .*front-end:.*/image: 172.31.227.151\\/front-end\\/front-end:$BUILD_ID/g&quot; front-end-dep.yaml&#x27;</span><br><span class="line">          sh &#x27;kubectl apply -f front-end-dep.yaml&#x27;</span><br><span class="line">     &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>定义环境变量：镜像仓库地址、项目名称、应用名称、连接镜像仓库的认证信息。</li>
<li>定义jnlp-agent POD所启动的container，包括jnlp容器，用于根jenkins进行通信，maven容器用于代码编译、docker 容器，用于容器镜像编译，kubectl容器用于连接对应的kubernetes集群用于应用创建和更新。</li>
<li>定义四个阶段，每个阶段定义对应的步骤，调用对应的容器实现功能（clone代码阶段、代码编译阶段、容器镜像构建阶段、应用部署阶段）</li>
<li>每个阶段调用对应的容器实现，如代码编译阶段调用编译容器进行、容器镜像构建阶段调用docker镜像映射宿主机socket进行。</li>
</ul>
<p>配置gitlab自动触发Jenkins CICD</p>
<p>编辑jenkins项目<br><img src="https://pic.downk.cc/item/5ebe1622c2a9a83be5581534.jpg"><br>配置使用<code>Build when a change is pushed to GitLab. GitLab CI Service URL: http://191.8.2.112:12000/project/test-go-dev</code> 当代码有更新的时候触发，通过GitLab CI</p>
<p>打开gitlab，配置webhook回掉地址，填入jenkins对应项目地址<br>gitlab对应的项目设置页</p>
<p><img src="https://pic.downk.cc/item/5ebe2347c2a9a83be566ad44.jpg"></p>
<p>默认触发器是push事件时，触发webhook，需要注意的是因为我们这里的Jenkins配置了用户名和密码所以<br>url需要在原来基础上添加用户名和密码其他不变<br>格式为<br><a href="http://username:password@192.168.1.4:1080/project/dashboard-build">http://username:password@192.168.1.4:1080/project/dashboard-build</a></p>
<p>默认测试webhook调用<br><img src="https://pic.downk.cc/item/5ebe25c1c2a9a83be5696389.jpg"></p>
<p>默认一个代码commit的请求<br>下载logo放置 </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget https://rancher.com/img/brand-guidelines/assets/logos/png/cow/rancher-logo-cow-blue.png</span><br></pre></td></tr></table></figure>
<p>修改前端logo，然后git push到代码仓库，触发Jenkins进行cicd操作，修改front-end&#x2F;public&#x2F;navbar.html的&lt;img src&#x3D;”img&#x2F;logo.png”为新的图片地址，然后将代码提交触发cicd重新打开前端看见logo变了。</p>
<p><img src="https://pic.downk.cc/item/5ebe27abc2a9a83be56b739b.jpg"></p>
<p><img src="https://pic.downk.cc/item/5ebe2786c2a9a83be56b2b66.jpg"></p>
<h3 id="Rancher-Pipeline配置使用"><a href="#Rancher-Pipeline配置使用" class="headerlink" title="Rancher-Pipeline配置使用"></a>Rancher-Pipeline配置使用</h3><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>Rancher2.x Pipeline支持在发布阶段直接将应用发布到应用商店，部署阶段直接从应用商店部署应用，以下为操作步骤</p>
<h4 id="先决条件-1"><a href="#先决条件-1" class="headerlink" title="先决条件"></a>先决条件</h4><ul>
<li>提前创建好存放统一chart的目录目录结构如下：<br>.&#x2F;charts&#x2F;app_name&#x2F;version</li>
</ul>
<p>将app_name和Version替换为实际应用名和版本号  </p>
<h4 id="CICD到应用商店流程"><a href="#CICD到应用商店流程" class="headerlink" title="CICD到应用商店流程"></a>CICD到应用商店流程</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[源码库]-- &quot;Jenkins clone源码完成其他CI环节&quot; --&gt; B[clone catalog 模板并通过Jenkins修改模板]</span><br><span class="line"></span><br><span class="line">B -- &quot;git push&quot; --&gt; C[push到统一存放catalog目录]</span><br><span class="line"></span><br><span class="line">C-- &quot;Rancher-server自动同步更新&quot; --&gt; c[UI显示新版本可更新]</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="应用chart制作"><a href="#应用chart制作" class="headerlink" title="应用chart制作"></a>应用chart制作</h4><p>继续使用go-server项目的例子</p>
<p>创建应用源码目录创建专属chart目录</p>
<p>目录结构如下</p>
<p>创建目录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir /go-server/go-server-chart</span><br></pre></td></tr></table></figure>
<p>在目录go-server-chartt内创建以下文件和文件夹</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">go-server-chart</span><br><span class="line">│   ├── 0</span><br><span class="line">│   │   ├── Chart.yaml</span><br><span class="line">│   │   ├── questions.yml</span><br><span class="line">│   │   ├── templates</span><br><span class="line">│   │   │   ├── _helpers.tpl</span><br><span class="line">│   │   │   ├── go-server-dep.yaml</span><br><span class="line">│   │   │   └── go-server-svc.yaml</span><br><span class="line">│   │   └── values.yaml</span><br><span class="line">│   └── README.md</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>0表示版本文件夹，如果要发布新版本只需要创建个1或者另外文件夹，然后修改Chart.yaml文件里面的版本号。  </p>
<p>README.md   内定义对应用的说明<br>Chart.yaml </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">description: go-server</span><br><span class="line">name: go-server</span><br><span class="line">version: $&#123;CICD_EXECUTION_SEQUENCE&#125;</span><br><span class="line">icon: http://ohx02qrb8.bkt.clouddn.com/micheling.png</span><br></pre></td></tr></table></figure>
<p>详细环境变量参考</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://rancher.com/docs/rancher/v2.x/en/k8s-in-rancher/pipelines/</span><br></pre></td></tr></table></figure>

<p>定义软件的描述、名字、版本、图标地址</p>
<p>questions.yml</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">questions:</span><br><span class="line">- variable: replicaCount</span><br><span class="line">  default: &quot;1&quot;</span><br><span class="line">  description: &quot;Replica count&quot;</span><br><span class="line">  type: string</span><br><span class="line">  required: true</span><br><span class="line">  label: Replicas</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">- variable: imageName</span><br><span class="line">  default: &quot;172.31.227.151/go-server/go-server&quot;</span><br><span class="line">  description: &quot;image name&quot;</span><br><span class="line">  type: string</span><br><span class="line">  required: true</span><br><span class="line">  label: image_name</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">- variable: imageTage</span><br><span class="line">  default: &quot;latest&quot;</span><br><span class="line">  description: &quot;image version&quot;</span><br><span class="line">  type: string</span><br><span class="line">  required: true</span><br><span class="line">  label: image_version</span><br><span class="line"></span><br><span class="line">- variable: nodeport</span><br><span class="line">  default: &quot;30000&quot;</span><br><span class="line">  description: &quot;NodePort port number(to set explicitly, choose port between 30000-32767)&quot;</span><br><span class="line">  type: string</span><br><span class="line">  required: true</span><br><span class="line">  label: nginx Service NodePort number</span><br></pre></td></tr></table></figure>
<p>定义变量，主要与ui上对应<br>variable： 对应template里面的.Values.xxx值<br>default：当用户没有设置自定义值时的默认值<br>description： ui上用户自定义值的描述<br>type: 自定义值的数据类型<br>required： 是否必填<br>label：标签  </p>
<p>values.yaml:  values.yaml 则提供了这些配置参数的默认值。</p>
<p>templates :放置实际要执行的应用的yaml文件</p>
<p>go-server-dep.yaml  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    cattle.io/creator: norman</span><br><span class="line">    workload.user.cattle.io/workloadselector: deployment-default-test</span><br><span class="line">  name: test</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  replicas: &#123;&#123; .Values.replicaCount &#125;&#125;</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      workload.user.cattle.io/workloadselector: deployment-default-test</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        workload.user.cattle.io/workloadselector: deployment-default-test</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: &#123;&#123; .Values.imageName &#125;&#125;:&#123;&#123; .Values.imageTage &#125;&#125;</span><br><span class="line">        imagePullPolicy: Always</span><br><span class="line">        name: test</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">          name: 8080tcp01</span><br><span class="line">          protocol: TCP</span><br><span class="line">        stdin: true</span><br><span class="line">        tty: true</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>go-server-svc.yaml</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    cattle.io/creator: norman</span><br><span class="line">  name: test-nodeport</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - name: 8080tcp01</span><br><span class="line">    nodePort: &#123;&#123; .Values.nodeport &#125;&#125;</span><br><span class="line">    port: 8080</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: 8080</span><br><span class="line">  selector:</span><br><span class="line">    workload.user.cattle.io/workloadselector: deployment-default-test</span><br><span class="line">  type: NodePort</span><br></pre></td></tr></table></figure>

<p> _helpers.tpl  定义一些系统通用的继承变量一般不需要，直接用默认就好。<br> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;&#123;/* vim: set filetype=mustache: */&#125;&#125;</span><br><span class="line">&#123;&#123;/*</span><br><span class="line">Expand the name of the chart.</span><br><span class="line">*/&#125;&#125;</span><br><span class="line">&#123;&#123;- define &quot;name&quot; -&#125;&#125;</span><br><span class="line">&#123;&#123;- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix &quot;-&quot; -&#125;&#125;</span><br><span class="line">&#123;&#123;- end -&#125;&#125;</span><br><span class="line"></span><br><span class="line">&#123;&#123;/*</span><br><span class="line">Create a default fully qualified app name.</span><br><span class="line">We truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec).</span><br><span class="line">*/&#125;&#125;</span><br><span class="line">&#123;&#123;- define &quot;fullname&quot; -&#125;&#125;</span><br><span class="line">&#123;&#123;- $name := default .Chart.Name .Values.nameOverride -&#125;&#125;</span><br><span class="line">&#123;&#123;- printf &quot;%s-%s&quot; .Release.Name $name | trunc 63 | trimSuffix &quot;-&quot; -&#125;&#125;</span><br><span class="line">&#123;&#123;- end -&#125;&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p> 同时创建实际的catalog目录<br> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir test-chart</span><br></pre></td></tr></table></figure><br> 继续按上面方式创建一个go-server的chart应用，需要修改Chart.yaml文件这里就不能在用变量了，因为使用添加私有应用商店时是读取不出变量的，在源码库内的chart.yaml用变量是因为需要动态更新。</p>
<p> Chart.yaml </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">description: go-server</span><br><span class="line">name: go-server</span><br><span class="line">version: v1.0 </span><br><span class="line">icon: http://ohx02qrb8.bkt.clouddn.com/micheling.png</span><br></pre></td></tr></table></figure>

<p>然后push到代码仓库的chart项目</p>
<h4 id="Pipeline配置"><a href="#Pipeline配置" class="headerlink" title="Pipeline配置"></a>Pipeline配置</h4><p>创建secret用于push时的代码仓库的帐号密码<br><img src="https://ae01.alicdn.com/kf/HTB11fEoXCf2gK0jSZFPq6xsopXaf.jpg"></p>
<p>如果push是ssh方式这里的secret就放置公钥  </p>
<p>USERNAME和PASSWORD两个key</p>
<p>创建镜像仓库凭证用于上传镜像</p>
<p><img src="https://pic.downk.cc/item/5ec01f9bc2a9a83be56f8070.jpg"></p>
<p>选择代码库创建Pipeline<br><img src="https://ae01.alicdn.com/kf/HTB1LggqXAL0gK0jSZFAq6AA9pXa2.jpg"></p>
<p>创建代码编译步骤<br><img src="https://pic.downk.cc/item/5ec2b159c2a9a83be57c0908.jpg"></p>
<p>创建编译上传镜像步骤 </p>
<p><img src="https://pic.downk.cc/item/5ec020f1c2a9a83be570d736.jpg"></p>
<p>创建发布应用模板步骤<br><img src="https://pic.downk.cc/item/5ec0212ec2a9a83be57117f9.jpg"></p>
<p>类型选择Publish Catalog Template</p>
<p><img src="https://ae01.alicdn.com/kf/HTB1_YZqXxz1gK0jSZSgq6yvwpXaT.jpg"></p>
<p><img src="https://pic.downk.cc/item/5ec021e8c2a9a83be571e75e.jpg"></p>
<ul>
<li>Chart Folder：填写源码库内的chart的相对路径</li>
<li>Catalog Template Version：这里填写Rancher Pipeline环境变量根在代码库内chart目录Chart.yaml Version一致</li>
<li>Catalog Template Name：为你的catalog应用的名称，如果你是生成一个新的catalog。统一存放catalog目录没有这个应用则可以根据实际需求取名字，如果是在原有基础上更新则有已有的catalog，则用原目录名</li>
<li></li>
</ul>
<p>运行Pipeline</p>
<p><img src="https://ae01.alicdn.com/kf/HTB1bbwyXpP7gK0jSZFjq6A5aXXaj.jpg"></p>
<p>catalog自动更新<br><img src="https://pic.downk.cc/item/5ec02b94c2a9a83be57bd38d.jpg"></p>
<p>对应代码目录下 <code>.rancher-pipeline.yml</code>文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">stages:</span><br><span class="line">- name: cod_build</span><br><span class="line">  steps:</span><br><span class="line">  - runScriptConfig:</span><br><span class="line">      image: golang:latest</span><br><span class="line">      shellScript: go build server.go</span><br><span class="line">- name: image_build</span><br><span class="line">  steps:</span><br><span class="line">  - publishImageConfig:</span><br><span class="line">      dockerfilePath: ./Dockerfile</span><br><span class="line">      buildContext: .</span><br><span class="line">      tag: go-server/go-server:$&#123;CICD_GIT_COMMIT&#125;</span><br><span class="line">      pushRemote: true</span><br><span class="line">      registry: 172.31.227.151</span><br><span class="line">    env:</span><br><span class="line">      PLUGIN_INSECURE: &quot;true&quot;</span><br><span class="line">      PLUGIN_MIRROR: https://yefnfc9c.mirror.aliyuncs.com</span><br><span class="line">- name: update_catalog</span><br><span class="line">  steps:</span><br><span class="line">  - publishCatalogConfig:</span><br><span class="line">      path: ./go-server-chart/0</span><br><span class="line">      catalogTemplate: go-server</span><br><span class="line">      version: $&#123;CICD_EXECUTION_SEQUENCE&#125;</span><br><span class="line">      gitUrl: http://114.215.25.58/root/chart.git</span><br><span class="line">      gitBranch: master</span><br><span class="line">      gitAuthor: root</span><br><span class="line">      gitEmail: shaoyua@rancher.com</span><br><span class="line">    envFrom:</span><br><span class="line">    - sourceName: gitlab</span><br><span class="line">      sourceKey: USERNAME</span><br><span class="line">      targetKey: USERNAME</span><br><span class="line">    - sourceName: gitlab</span><br><span class="line">      sourceKey: PASSWORD</span><br><span class="line">      targetKey: PASSWORD</span><br><span class="line">timeout: 60</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>CI/CD</category>
      </categories>
      <tags>
        <tag>CI/CD</tag>
      </tags>
  </entry>
  <entry>
    <title>ceph crushmap</title>
    <url>/2017/08/05/crush/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><h1 id="通过crushmap划分性能型池和容量型池。"><a href="#通过crushmap划分性能型池和容量型池。" class="headerlink" title="通过crushmap划分性能型池和容量型池。"></a>通过crushmap划分性能型池和容量型池。</h1><p>在实际生产环境中，考虑到成本因素，很少土豪公司会将osd全部用ssd，但私有云上有部分业务需要高性能存储，部分业务只需要普通的sas盘做容量性存储，在公有云中也经常有，不同性能的存储卖不同的价格。<br>在ceph中的解决方法就是通过修改crushmap，创建多种host，将osd加入到host中，在创建多个pool，每个pool对应不同的rule</p>
<p>本例中两个存储节点的前两个osd为ssd，后面两个osd为sas。需要划分ssd  pool和sas pool，其中云主机和性能型存储用sas pool， 性能型存储为ssd pool。<br>环境<br>两台存储节点node-4、node-5，每个存储节点4个osd，将每个存储节点的前两个osd是ssd盘，后两个osd是sas盘</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/crush_1.png"></p>
<h2 id="层级结构"><a href="#层级结构" class="headerlink" title="层级结构:"></a>层级结构:</h2><p>中host bucket高于osd bucket，root bucket高于host bucket，划分步骤为：<br>1、创建对应的host bucket 如node-4-sas、node-4-ssd、node-5-sas、node-5-ssd。<br>2、将对应的osd划到对应的host中。<br>3、创建root，如ssd、sas，将对应的host加到对应的root中。<br>4、创建rules将root加入到对应的rule中。<br>5、pool调用ruleset。</p>
<p>权重：修改crushmap时需要特别注意osd的权重问题,1TB OSD为1.00，500G为0.50，3TB位3.00</p>
<p>rules：pool所使用的规则，在crushmap中有一个对应的id，pool直接使用这个id表示这个pool的pg按这个规则进行分布。<br>修改方法有两个</p>
<p>先修改ceph.conf禁止osd启动时自动修改crushmap<br>echo ‘osd_crush_update_on_start &#x3D; false’ &gt;&gt; ceph.conf</p>
<p>第一直接直接使用ceph命令创建bucket，move bucket，在修改rules。<br>第二通过将crushmap导出，修改crushmap的方式。</p>
<h2 id="方法1-直接通过ceph命令-："><a href="#方法1-直接通过ceph命令-：" class="headerlink" title="方法1(直接通过ceph命令)："></a>方法1(直接通过ceph命令)：</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">1、创建对应的root</span><br><span class="line">     ceph osd crush add-bucket ssd root</span><br><span class="line">     ceph osd crush add-bucket sas root</span><br><span class="line">2、创建对应的host</span><br><span class="line">      ceph osd crush add-bucket node-4-sata host</span><br><span class="line">      ceph osd crush add-bucket node-5-sata host</span><br><span class="line">      ceph osd crush add-bucket node-4-ssd host</span><br><span class="line">      ceph osd crush add-bucket node-5-ssd host</span><br><span class="line">3、移动host到对应的root下</span><br><span class="line">      ceph osd crush move node-4-sas root=sas</span><br><span class="line">      ceph osd crush move node-5-sas root=sas</span><br><span class="line">      ceph osd crush move node-4-ssd root=ssd</span><br><span class="line">      ceph osd crush move node-5-ssd root=ssd</span><br><span class="line">4、将osd移到host下</span><br><span class="line">      ceph osd crush move osd.3 0.88 host=node-4-sas</span><br><span class="line">      ceph osd crush move osd.4 0.88 host=node-4-sas</span><br><span class="line">      ceph osd crush move osd.6 0.88 host=node-5-sas</span><br><span class="line">      ceph osd crush move osd.7 0.88 host=node-5-sas</span><br><span class="line">      ceph osd crush move osd.1 0.97 host=node-4-ssd</span><br><span class="line">      ceph osd crush move osd.2 0.88 host=node-4-ssd</span><br><span class="line">      ceph osd crush move osd.0 0.97 host=node-5-ssd</span><br><span class="line">      ceph osd crush move osd.5 0.88 host=node-5-ssd</span><br></pre></td></tr></table></figure>
<p>导出crush<br>ceph osd getcrushmap -o crushmap.txt<br>反编译<br>crushtool -d crushmap.txt -o crushmap-decompile</p>
<p>打开反编译后的文件<br>修改rule（修改ruleset、和step take)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rule ssd &#123;</span><br><span class="line">        ruleset 1</span><br><span class="line">        type replicated</span><br><span class="line">        min_size 1</span><br><span class="line">        max_size 10</span><br><span class="line">        step take ssd</span><br><span class="line">        step chooseleaf firstn 0 type host</span><br><span class="line">        step emit</span><br><span class="line">&#125;</span><br><span class="line">rule sas &#123;</span><br><span class="line">        ruleset 0</span><br><span class="line">        type replicated</span><br><span class="line">        min_size 1</span><br><span class="line">        max_size 10</span><br><span class="line">        step take ssd</span><br><span class="line">        step chooseleaf firstn 0 type host</span><br><span class="line">        step emit</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>重新编译<br>crushtool -c crushmap-decompile -o crushmap-compiled</p>
<p>应用到集群<br>ceph osd setcrushmap -i crushmap-compiled</p>
<p>创建一个新的pool<br>ceph osd pool create ssd 1024</p>
<p>设置ssd pool使用rules 1<br>ceph osd pool set ssd crush_ruleset 1</p>
<p>校验object的pg的散落方法参考方法2</p>
<h2 id="方法2-直接修改crushmap"><a href="#方法2-直接修改crushmap" class="headerlink" title="方法2(直接修改crushmap)"></a>方法2(直接修改crushmap)</h2><p>提取现集群中使用的crushmap保存到一个文件<br>ceph osd getcrushmap -o crushmap.txt<br>默认导出来的crushmap打开是乱码的，需要进行反编译才能修改<br>crushtool -d crushmap.txt -o crushmap-decompile<br>重新编译这个crushmap<br>crushtool -c crushmap-decompile -o crushmap-compiled</p>
<p>将新的CRUSH map 应用到ceph 集群中<br>ceph osd setcrushmap -i crushmap-compiled<br>修改crushmap，需要注意的是bucket ID不要重复了，还有osd的weigh，我这一个osd是90G所以为0.088</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">host node-5-sas &#123;</span><br><span class="line">        id -2           # do not change unnecessarily</span><br><span class="line">        # weight 0.176</span><br><span class="line">        alg straw</span><br><span class="line">        hash 0  # rjenkins1</span><br><span class="line">        item osd.6 weight 0.088</span><br><span class="line">        item osd.7 weight 0.088</span><br><span class="line">&#125;</span><br><span class="line">host node-4-sas &#123;</span><br><span class="line">        id -3           # do not change unnecessarily</span><br><span class="line">        # weight 0.176</span><br><span class="line">        alg straw</span><br><span class="line">        hash 0  # rjenkins1</span><br><span class="line">        item osd.3 weight 0.088</span><br><span class="line">        item osd.4 weight 0.088</span><br><span class="line">&#125;</span><br><span class="line">root sas &#123;</span><br><span class="line">        id -1           # do not change unnecessarily</span><br><span class="line">        # weight 0.720</span><br><span class="line">        alg straw</span><br><span class="line">        hash 0  # rjenkins1</span><br><span class="line">        item node-5-sas weight 0.360</span><br><span class="line">        item node-4-sas weight 0.360</span><br><span class="line">&#125;</span><br><span class="line">host node-5-ssd &#123;</span><br><span class="line">        id -6           # do not change unnecessarily</span><br><span class="line">        # weight 0.185</span><br><span class="line">        alg straw</span><br><span class="line">        hash 0  # rjenkins1</span><br><span class="line">        item osd.0 weight 0.097</span><br><span class="line">        item osd.5 weight 0.088</span><br><span class="line">&#125;</span><br><span class="line">host node-4-ssd &#123;</span><br><span class="line">        id -5           # do not change unnecessarily</span><br><span class="line">        # weight 0.185</span><br><span class="line">        alg straw</span><br><span class="line">        hash 0  # rjenkins1</span><br><span class="line">        item osd.1 weight 0.097</span><br><span class="line">        item osd.2 weight 0.088</span><br><span class="line">&#125;</span><br><span class="line">root ssd &#123;</span><br><span class="line">        id -4           # do not change unnecessarily</span><br><span class="line">        # weight 0.720</span><br><span class="line">        alg straw</span><br><span class="line">        hash 0  # rjenkins1</span><br><span class="line">        item node-5-ssd weight 0.360</span><br><span class="line">        item node-4-ssd weight 0.360</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># rules</span><br><span class="line">rule ssd &#123;</span><br><span class="line">        ruleset 1</span><br><span class="line">        type replicated</span><br><span class="line">        min_size 1</span><br><span class="line">        max_size 10</span><br><span class="line">        step take ssd</span><br><span class="line">        step chooseleaf firstn 0 type host</span><br><span class="line">        step emit</span><br><span class="line">&#125;</span><br><span class="line">rule sas &#123;</span><br><span class="line">        ruleset 0</span><br><span class="line">        type replicated</span><br><span class="line">        min_size 1</span><br><span class="line">        max_size 10</span><br><span class="line">        step take sas</span><br><span class="line">        step chooseleaf firstn 0 type host</span><br><span class="line">        step emit</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>重新编译这个crushmap<br>crushtool -c crushmap-decompile -o crushmap-compiled<br>将新的CRUSH map 应用到ceph 集群中<br>ceph osd setcrushmap -i crushmap-compiled<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/crush_2.png"></p>
<p>创建一个新的pool<br>ceph osd pool create ssd 1024</p>
<p>设置ssd pool使用rules 1<br>ceph osd pool set ssd crush_ruleset 1</p>
<p>检查下<br>ceph osd pool get ssd crush_ruleset</p>
<p>测试在ssd中写入个数据是否都落到osd.0、osd.5、osd.1、osd.2<br>rbd create ssd&#x2F;testimg -s 10240 #在ssd pool中创建块</p>
<p>查看pg的散落情况<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/crush_3.png"><br>因为我这是两副本所以只会落到两个osd上，分别落在osd.0和osd.1上。</p>
<h2 id="cinder多后端"><a href="#cinder多后端" class="headerlink" title="cinder多后端"></a>cinder多后端</h2><p>修改cinder.conf</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">enabled_backends=sata,ssd</span><br><span class="line"></span><br><span class="line">[ssd]</span><br><span class="line">volume_backend_name=ssd</span><br><span class="line">volume_driver=cinder.volume.drivers.rbd.RBDDriver</span><br><span class="line">rbd_pool=ssd</span><br><span class="line">rbd_user=volumes</span><br><span class="line">rbd_ceph_conf=/etc/ceph/ceph.conf</span><br><span class="line">srbd_secret_uuid=a5d0dd94-57c4-ae55-ffe0-7e3732a24455</span><br><span class="line">rbd_max_clone_depth=5</span><br></pre></td></tr></table></figure>


<p>secret_uuid就是对接ceph时导入的secret<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/crush_4.png"></p>
<p>创建type<br>cinder type-create ssd<br>cinder type-key ssd set volume_backend_name&#x3D;ssd</p>
<p>重启cinder服务<br>systemctl restart openstack-cinder-api<br>systemctl restart openstack-cinder-scheduler<br>systemctl restart openstack-cinder-volume</p>
<h1 id="通过crushmap隔离故障域，让pg分布在不同机柜上主机上"><a href="#通过crushmap隔离故障域，让pg分布在不同机柜上主机上" class="headerlink" title="通过crushmap隔离故障域，让pg分布在不同机柜上主机上"></a>通过crushmap隔离故障域，让pg分布在不同机柜上主机上</h1><p>但这样故域还是host，pg的分布还是比较散乱的，但集群规模大时，如果按照默认的host为故障域的话副本pg很有可能分布同一机架相邻的host的osd上，这样如果你一但此机架断电很有可能导致集群出现ERROR</p>
<p>但我们可以通过修改crushmap 让副本pg分布到不同机架的服务器上去，来达到隔离故障域的目的。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/crush_5.png"></p>
<p>rack 底层的分支</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/crush_6.png"></p>
<p>下面方案如下有4个rack，每个rack有一个host ,通过修改crushmap将pg分布到不同列的rack的host上，比如可以指定compute pool的第一个副本放rackB01的node4上，第二个副本放rackC01的node-5上，</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">node-5上，</span><br><span class="line">rackB01</span><br><span class="line">   node-4</span><br><span class="line">rackC01</span><br><span class="line">   node-5</span><br><span class="line">rackB02</span><br><span class="line">   node-6</span><br><span class="line">rackC02</span><br><span class="line">   node-7</span><br></pre></td></tr></table></figure>
<p>这样做的好处就是将副本放在不同列的不同机柜上，来提高可靠性</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rack方案</span><br><span class="line">node-4 ---B01柜</span><br><span class="line">node-5 ---B02柜</span><br><span class="line">node-6 ---C01柜</span><br><span class="line">node-7 ---C02柜</span><br></pre></td></tr></table></figure>
<p>将compute pool的副本都放到B、C 01柜里面讲test_pool的副本都放到B、C 02柜里面</p>
<p>修改后<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/crush_7.png"></p>
<p>方法一(直接通过ceph命令)：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、添加racks:</span><br><span class="line">    ceph osd crush add-bucket rackB01 rack</span><br><span class="line">    ceph osd crush add-bucket rackB02 rack</span><br><span class="line">    ceph osd crush add-bucket rackC01 rack</span><br><span class="line">    ceph osd crush add-bucket rackC02 rack</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">3、把每一个host移动到相应的rack下面：</span><br><span class="line">    ceph osd crush move node-4 rack=rackB01</span><br><span class="line">    ceph osd crush move node-5 rack=rackB02</span><br><span class="line">    ceph osd crush move node-6 rack=rackC01</span><br><span class="line">    ceph osd crush move node-7 rack=rackC02</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">4、添加root</span><br><span class="line">     ceph osd crush add-bucket rackB_C01 root</span><br><span class="line">     ceph osd crush add-bucket rackB_C02 root</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">4、把所有rack移动到对应 root 下面：</span><br><span class="line">    ceph osd crush move rackB01 root=rackB_C01</span><br><span class="line">    ceph osd crush move rackB02 root=rackB_C02</span><br><span class="line">    ceph osd crush move rackC01 root=rackB_C01</span><br><span class="line">    ceph osd crush move rackC02 root=rackB_C02</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>导出crushmap 添加rules<br>ceph osd getcrushmap -o crushmap.txt</p>
<p>反编译<br>crushtool -d crushmap.txt -o crushmap-decompile</p>
<p>修改</p>
<p>添加如下(注意ruleset、和step take)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rule rackB_C01 &#123;</span><br><span class="line">        ruleset 0</span><br><span class="line">        type replicated</span><br><span class="line">        min_size 1</span><br><span class="line">        max_size 10</span><br><span class="line">        step take rackB_C01</span><br><span class="line">        step chooseleaf firstn 0 type rack</span><br><span class="line">        step emit</span><br><span class="line">&#125;</span><br><span class="line">rule rackB_C02 &#123;</span><br><span class="line">        ruleset 1</span><br><span class="line">        type replicated</span><br><span class="line">        min_size 1</span><br><span class="line">        max_size 10</span><br><span class="line">        step take rackB_C02</span><br><span class="line">        step chooseleaf firstn 0 type rack</span><br><span class="line">        step emit</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>编译<br>crushtool -c crushmap-decompile -o crushmap-compiled</p>
<p>将新的CRUSH map 应用到ceph 集群中<br>ceph osd setcrushmap -i crushmap-compiled</p>
<p>设置test_pool套用rule rackB_C02</p>
<p>ceph osd pool set test_pool crush_ruleset 1</p>
<p>查看是否应用成功<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/crush_8.png"><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/crush_9.png"></p>
<p>测试<br>在test_pool里面创建个对象应该分到B02和C02柜的host上面的osd上,<br>rbd create test_pool&#x2F;testimg -s 1024<br>在test_pool里面创建个对象应该分到B01和C01柜的host上面的osd上,<br>rbd create compute&#x2F;test_2.img -s 1024</p>
<p>查看<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/crush_10.png"><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/crush_11.png"></p>
<p>这样副本pg就会分布在不同机柜的不同host上的osd。</p>
]]></content>
      <categories>
        <category>分布式存储</category>
      </categories>
      <tags>
        <tag>分布式存储</tag>
      </tags>
  </entry>
  <entry>
    <title>docker in docker</title>
    <url>/2018/09/22/docker-in-docker/</url>
    <content><![CDATA[<h3 id="概述："><a href="#概述：" class="headerlink" title="概述："></a>概述：</h3><p>docker-in-docker简单来说，就是在docker里面运行docker,大部分应用场景在在CI系统容器化后，如何在CI系统内构建容器镜像，比如我们前面例子，将Jenkins对接kubernetes然后Jenkins-slave以kubernetes内的pod方式运行实现动态创建和删除，这时如果需要构建容器镜像的话，就需要Jenkins-slave能支持docker-in-docker的方式了。</p>
<h3 id="环境信息："><a href="#环境信息：" class="headerlink" title="环境信息："></a>环境信息：</h3><p>os：ubuntu16.04<br>docker：17.03-2</p>
<h3 id="实现方式："><a href="#实现方式：" class="headerlink" title="实现方式："></a>实现方式：</h3><p>docker-in-docker的实现方式有两种<br>方式一：通过docker官方镜像docker:dind，或自己以特权模式启动一个基础镜像（然后安装docker)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd --name test-docker --privileged docker:dind（docker:dind镜像默认是最新版docker版本的dind,如果需要指定docker版本可以docker:17.03-dind)</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker-in-dokcer_1.png"><br>配置镜像加速器</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tee  /etc/docker/daemon.json &lt;&lt; EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https://vqgjby9l.mirror.aliyuncs.com&quot;]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<p>启动应用功能测试</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd -p 80:80 nginx</span><br></pre></td></tr></table></figure>
<p>测试应用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl 127.0.0.1</span><br><span class="line">wget 127.0.0.1 &amp;&amp; cat index.html</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker-in-docker_2.png"><br>构建镜像功能测试</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi ~/Dockerfile</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM nginx</span><br><span class="line">MAINTAINER Alex wan &quot;wanshaoyuan@gmail.com&quot;</span><br><span class="line">RUN  echo test &gt; /usr/share/nginx/html/index.html</span><br></pre></td></tr></table></figure>
<p>构建镜像<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker-in-docker_3.png"></p>
<p>基于新镜像启动容器</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd -p 80:80 test-nginx:v1.0</span><br></pre></td></tr></table></figure>
<p>验证<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker-in-docker_4.png"></p>
<p>可以看见这根我们平常用的docker一模一样了，能够运行容器，管理容器，构建容器镜像。<br>方式二：将宿主机的docker.sock文件映射到容器内,然后在应用镜像内安装一个docker client端就可以了，docker官方也有内部就封装了docker命令的镜像，镜像名就叫docker</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd -v /var/run/docker.sock:/var/run/docker.sock docker</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker-in-docker_5.png"></p>
<p>这种方式很简单了就是直接用的宿主机的docker，只不过用的另外的客户端去连接而已，所以你在容器里面的所有操作比如创建、删除、构建这些操作都是直接反映到宿主机的docker上的。</p>
<h3 id="两种方式的区别："><a href="#两种方式的区别：" class="headerlink" title="两种方式的区别："></a>两种方式的区别：</h3><p>第一种方式使用docker inserd docker，是直接在子容器容器里面启动一个单独的容器，这种方案优点在于1、它是完全独立的一个docker，根宿主机是完全隔离的，在上面做任何操作都不会影响宿主机上的docker，对应的子容器销毁，里面容器和镜像就没有了，不会残留在父容器上，同时缺点也很明显1、因为需要操作iptables和cgroup所以需要特权模式启动，带来一定的安全隐患。2、StoragDriver问题，因为外部docker运行在操作系统的文件系统之上如ext4，xfs等，子容器运行在StoragDriver之上，然后子容器中的容器又运行在子容器的StoragDriver之上，所以这就成了一个多级嵌套的一个关系了，对于一些StoragDriver是不允许的，比如你不能在aufs之上在运行aufs，但目前overlay2是可以运行在aufs之上的，但会有较强的性能损耗。</p>
<p>第二种方式使用docker outside of docker，这种方式子容器内封装一个docker命令然后将宿主机的docker.sock文件映射进去，这种方案优点在于：1、因为子容器只是运行一个docker客户端，所以不需要特端模式，可以保证一定的安全性。2、没有多层StorageDriver嵌套问题因为它是直接运行在宿主机上的。缺点：1、因为是直接控制宿主机容器，所以所有操作都会反应在宿主机上，隔离性没那么好。2、端口要保证唯一性。3、容器层面安全性问题，因为可以直接操作宿主机docker，所以可以任意修改和删除上面的容器。</p>
<p>推荐做法：<br>如果是使用ci工具在容器中构建容器的话，建议直接使用第二种方式docker outside of docker，因为这种方式不需要那么完全隔离，只要编译镜像就可以，另外就是编译出来的镜像直接在宿主机上，测试时还可以直接run起来。</p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker基础</title>
    <url>/2017/10/30/docker_base/</url>
    <content><![CDATA[<h2 id="此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。"><a href="#此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。" class="headerlink" title="此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。"></a>此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。</h2><h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><p>容器不关有docker还有core os的rkt、</p>
<p>容器runtime是真正运行容器的地方，runtime需要跟操作系统kernel紧密协作，为容器提供运行环境<br>，docker曾经的runtime是linux上的lxc，后面docker自己开发了runc，目前runc是docker默认的runtime</p>
<p>rkt是coreos开发的容器runtime.</p>
<p>容器管理工具<br>lxd是lxc对应的管理工具<br>runc的管理工具是docker engine，docker engine包含daemon和cli两个部分<br>rkt的管理工具rkt cli  </p>
<p>容器定义工具<br>docker image是docker容器的模板,runtime依据docker image创建容器<br>docker file是包含若干命令的文本文件，可以通过这些命令创建出docker image  aci与docker image类似，它是coreos开发的rkt容器image格式。</p>
<p>容器仓库<br>私有仓库：registry</p>
<p>公共仓库：docker hub，Quay.io</p>
<p>容器os<br>coreos 、atomic、ubuntu core</p>
<p>容器平台技术  </p>
<p>容器编排引擎:docker swarm、kubernetes、mesos+mariathon</p>
<p>容器管理平台：rancher、containership、</p>
<p>基于容器的pass：deis、flynn、dokku</p>
<p>容器支持技术<br>容器网络:docker network、flannel、weave、calico<br>服务发现：etcd、consul、zookeeper<br>数据管理：rex-ray<br>日志管理：docker logs、logspout<br>安全性：openscap  </p>
<h3 id="docker镜像"><a href="#docker镜像" class="headerlink" title="docker镜像"></a>docker镜像</h3><p>linux发行版都是由内核空间和用户空间组成，内核空间是kernel，用户空间的文件系统是rootfs，可以简单的说不同linux发行版的主要区别都是rootfs</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_1.png"></p>
<p>base镜像：1，不依赖其他镜像，其他应用镜像可以在base镜像的基础上进行扩展，base镜像通常是各linux发行版的docker镜像，如centos、ubuntu、Debian，大部分应用镜像都是在base镜像的基础上进行扩展的。</p>
<p>下载镜像<br>docker pull centos</p>
<p>查看镜像<br>docker images centos</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_2.png"></p>
<p>正如上面所说base镜像这么小，因为base镜像只有一个rootfs，kernel它是和宿主机共享的，base镜像只是在用户空间和发行版是一致的，内核版本是共享宿主机的kernel的，ubuntu14.04 kernel版本是4.x，但centos7.3kernel版本是3.10，当在ubuntu14.04上跑centos7.3的docker镜像时，centos7.3的内核版本也会变成4.x因为它是共享宿主机内核。  </p>
<p>容器只能使用宿主机kernel，不能更改，不能升级，如果应用要求高内核，建议使用虚拟机。</p>
<p>docker镜像采用分层的结构。基于docker file文件，你每做的操作其实都是在base镜像上加一层，每安装一个软件就是在现有镜像基础上增加一层，这样做的好处时，一个共享资源</p>
<p>比如：有多个镜像都从相同的base镜像构建出来，那宿主机只需要一个base_image,多个镜像共享一个base镜像当某个容器镜像修改基础镜像内容时，会将基础镜像的内容像copy一份到应用镜像内，然后在修改应用镜像内的数据，这根前面说的ceph的快照和clone是一样的，都是使用的cow(copy on write)技术</p>
<p>当容器启动时，会有一层可写层在容器顶部叫容器层，所有对容器的改动，无论添加、删除，修改文件都会发生的容器层中，容器层下面是镜像层，镜像层是只读的。</p>
<p>构建镜像<br>docker commit</p>
<p>在对原镜像做修改后使用docker commit将生成一个新镜像。<br>docker commit 原镜像名 新镜像名</p>
<p>Dockerfile</p>
<p>Dockerfile是一个文本文件记录镜像构件的步骤</p>
<p>如Dockerfile为<br>FROM centos<br>RUN yum install vim -y</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_3.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_4.png"></p>
<p>docker build<br>-t 为新镜像名字<br>.为指定当前目录为build context，docker 会从build   context中寻找Dockerfile<br>-f可以手动指定Dockerfile的文件  </p>
<p>如果Dockerfile不在本地<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_5.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_6.png"></p>
<p>可以看见启动一个3d799f4的临时容器，然后在临时容器中安装vim，安装成功后将临时容器保存为镜像，最后删除临时容器。</p>
<p>查看容器分层</p>
<p>docker history 镜像名</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_7.png"></p>
<p>镜像的缓存特性  </p>
<p>docker会缓存已有的镜像的镜像层，如果镜像存在就直接使用，不用创建，如果不希望使用缓存镜像则在docker build时加上–no-cache参数</p>
<p>docker镜像的上层是依赖下层的，只要下层发生变化，上层也会根着变。</p>
<p>如果在build某一步中出错了,可以通过docker run -it 先进上一步生成临时镜像进行调试。</p>
<p>搭建docker私有镜像仓库  </p>
<p>从docker hub上下载registryv2.3.1的私有仓库镜像<br>docker pull registry:2.3.1</p>
<p>使用htpasswd配置docker私有仓库验证<br>[root@wan_test &#x2F;]# mkdir &#x2F;root&#x2F;auth&#x2F;</p>
<p>生成验证admin：admin为用户名和密码  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --entrypoint htpasswd registry:2.3.1 -Bbn admin admin &gt; /root/auth/htpasswd</span><br></pre></td></tr></table></figure>
<p>启动私有仓库容器</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -d  -p 5000:5000 -v /myregistry/:/var/lib/registry -v /root/auth/:/auth -e &quot;REGISTRY_AUTH=htpasswd&quot; -e &quot;REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm&quot; -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd registry:2.3.1</span><br></pre></td></tr></table></figure>

<p>测试登录<br>先用个错误的帐号密码测试<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_8.png"></p>
<p>在用对的  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_9.png"></p>
<p>认证以后无法直接在服务器查看<br><code>curl 127.0.0.1:5000/v2/_catalog</code> 仓库的镜像，会出现报错，但是可以用浏览器输入帐号密码访问。</p>
<p>测试上传镜像<br>命名要规范格式为  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[registry_host]:[port]/[username]/[封装的应用]:[版本]</span><br><span class="line"></span><br><span class="line">docker tag hello-world 10.211.55.5:5000/wan/hello-wolrd:v1</span><br></pre></td></tr></table></figure>

<p>docker push上传镜像<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_10.png"></p>
<p>默认client是打开ssl的，但Server端没有配置证书，所以先关闭<br>&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.service</p>
<p>关闭ssl安全认证  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ExecStart=/usr/bin/dockerd --insecure-registry 10.211.55.5:5000  </span><br></pre></td></tr></table></figure>

<p>systemctl restart docker</p>
<p>测试下载<br>将本地10.211.55.5:5000&#x2F;wan&#x2F;hello-wolrd:v1删除</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_11.png"></p>
<p>重新从服务器上pull下来  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_12.png"></p>
<p>查看镜像  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl http://10.211.55.5:5000/v2/_catalog  </span><br></pre></td></tr></table></figure>

<p>web页面版的私有镜像管理软件<br>Portus<br>harbor<br><a href="https://www.linuxea.com/1557.html">https://www.linuxea.com/1557.html</a></p>
<p>两种进入容器的方法<br>docker attach image_id<br>docker exec -it image_id<br>区别是，attach进去的还是之前的终端，不会启用新的终端，exec进去是启动一个新的终端。  </p>
<p>运行容器的最佳实践<br>服务类型的容器通过-d以后台的方式启动这类容器</p>
<p>工具类型的直接docker run -it的方式，用完就exit掉</p>
<p>容器的操作  </p>
<p>容器在docker host中实际就是一个进程，docker stop命令实际就是向中国进程发送一个SIGKILL信号</p>
<p>docker run –restart&#x3D;always 表示无论容器因何种原因退出（含正常退出)都立刻重启，–restart&#x3D;on-failure:3意思是如果启动进程退出代码非0，则重启容器，最多重启3次。</p>
<p>pause&#x2F;unpause 挂起容器&#x2F;解除挂起</p>
<p>查看所有exited状态的容器<br>docker ps -aq -f status&#x3D;exited</p>
<h3 id="容器资源的限制"><a href="#容器资源的限制" class="headerlink" title="容器资源的限制"></a>容器资源的限制</h3><p>docker run时指定<br>-m或–memory 设置内存的使用限额<br>–memory-swap设置内存+swap的使用限额。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -m 200M --memory-swap=300M centos  </span><br></pre></td></tr></table></figure>
<p>其含义是该容器最多使用200M内存和100M swap，因为–memory-swap是memory+swap所以前面指定的-m后面swap等于300-200， 默认情况下这两组参数都是-1，不是不限制。<br>只指定定了-m没指定–memory-swap时默认–memory-swap是-m的两倍。</p>
<p>使用progrium&#x2F;stress镜像测试，只要线分配给线程的内存大小在可分配范围(300M）内，镜像就会一直工作</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -it -m 200M --memory-swap=300M progrium/stress --vm 1 --vm-bytes 180M</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_13.png"></p>
<p>如果超过了，容器马上回报错退出<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_14.png"></p>
<h4 id="cpu限额"><a href="#cpu限额" class="headerlink" title="cpu限额"></a>cpu限额</h4><p>cpu限额限制的不是实际cpu的使用量，资源的优先级，也就是，在cpu资源紧张时，优先级高的获的的cpu时间片要长一些  </p>
<p>docker cpu限制参数<br>–cpuset-cpus&#x3D;””	允许使用的 CPU 集，值可以为 0-3,0,1<br>-c,–cpu-shares&#x3D;0	CPU 共享权值（相对权重）<br>cpu-period&#x3D;0	限制 CPU CFS 的周期，范围从 100ms~1s，即[1000, 1000000]<br>–cpu-quota&#x3D;0	限制 CPU CFS 配额，必须不小于1ms，即 &gt;&#x3D; 1000<br>–cpuset-mems&#x3D;””	允许在上执行的内存节点（MEMs），只对 NUMA 系统有效  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_32.png"></p>
<p>cpu-shares<br>通过–cpu-shares设置容器cpu配额，默认为一个core为1024，二个为2048，依次类推，设置为0，系统将会按默认值配置，假定一个1core的运行3个container，其中一个cpu-shares为1024，另外两个为512则，当cpu忙碌时，1024的占用50%cpu时间，512的分别占用25%，当增加一个1024的contain时，占用比为1024+1024+512+512 &#x3D;3072 每个container占用cpu时间比为 1024&#x2F;3072&#x3D;33%  1024&#x2F;3072&#x3D;33% 512&#x2F;3027&#x3D;17% 512&#x2F;3072&#x3D;17%</p>
<p>比如在host上启用两个容器  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --name container_a -it --cpu-shares 2048 progrium/stress --cpu 2</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --name container_b -it --cpu-shares 1024 progrium/stress --cpu 2</span><br></pre></td></tr></table></figure>
<p>–cpu 有多少核，就写多少  ，这里设置为2<br>可以看见container_a的cpu资源消耗是container_b的两倍</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_15.png"></p>
<p>paush container_a,此时container_b又会将cpu整格cpu占满</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_16.png"></p>
<h4 id="block-io限制"><a href="#block-io限制" class="headerlink" title="block io限制"></a>block io限制</h4><p>block io是用来限制容器的磁盘的读写，针对bps（每秒读写的数据量)和iops(每秒IO的次数),控制容器的读写磁盘的带宽。</p>
<p>限制block io的参数  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_33.png"></p>
<p>目前block io限额只对dicectIO（不使用文件缓存)有效。</p>
<p>测试<br>先在容器内使用dd测试bps</p>
<p>run一个centos的镜像</p>
<p>docker run -itd centos  </p>
<p>限制前<br>进入centos镜像<br>docker exec -it 15643e5ec7d9 &#x2F;bin&#x2F;bash  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_17.png"></p>
<p>限制io为30M后  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd --device-write-bps /dev/sda:300MB centos</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_18.png"></p>
<h3 id="实现容器的底层技术"><a href="#实现容器的底层技术" class="headerlink" title="实现容器的底层技术"></a>实现容器的底层技术</h3><p>曾经docker的runtime是linux内核自带的lxc，后面docker重新开发了自己的runtime，runc，目前docker通过cgroup实现资源限制，如上面的限制cpu资源、限制memory资源、限制blockIO资源大都是通过linux底层的cgroup实现，namespace实现资源隔离，每个容器独立的文件系统，网卡资源等是通过linux底层的namespace，openstack neutron的 多租户隔离qroute和qdhcp也是通过namespace去实现的。</p>
<p>linux下6种namespace资源</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_34.png"></p>
<h3 id="docker存储"><a href="#docker存储" class="headerlink" title="docker存储"></a>docker存储</h3><p>容器镜像的分层结构<br>容器镜像<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_19.png"></p>
<p>1、镜像由一个base镜像+若干个只读镜像+一个可写的容器层，这种分层架构最大的特性建设copy-on-write。<br>2、对数据的修改都是在容器层完成的，新修改的数据都会放在最上层容器层。<br>3、修改数据时，如果容器层没有，会先从其他层将数据copy到容器层，然后在修改。此时镜像层数据保持不变。<br>4、如果有多个层有命名相同的文件，用户只能看见最上层中的文件。  </p>
<p>分层结构是由linux store driver提供，docker支持多种storage driver 如 AUFS、Device Mapper、Btrfs、OverlayFs、VFS、ZFS，ubuntu默认是AUFS、centos7默认是OverlayFs ， docker默认使用linux 发行版的storage driver  </p>
<p>docker info可以看见<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_20.png"></p>
<p>有些容器创建和销毁，不依赖原有数据，有些容器创建时需要加载已有数据，销毁时希望保留之前产生的数据，这类容器就需要用到容器持久化存储</p>
<p>Data volume<br>Data volume 本质是 Docker Host 文件系统中的目录或文件，能够直接被mount到容器的文件系统中</p>
<p>特点：<br>1、Data volume是目录或文件，不是块设备<br>2、容器可以读写volume中的数据<br>3、volume的数据因为在host上，但容器被销毁时，不会影响数据，所以它是可以持久保存的。</p>
<p>在使用上，软件放在镜像层内，软件的数据如web Server的网页，数据库的数据这些放在Data volume上</p>
<p>因为data volume是宿主机的一部分，所以目前volume无法设置容量。</p>
<p>docker提供两种类型volume<br>bind mount：宿主机上已经存在的目录或文件mount到容器。<br>优点：使用起来直观、高效、可以控制目录的读写权限，支持单个文件的mount<br>缺点：需要指定host 文件系统路径，限制了容器的可移值性，当需要将容器迁到其他host上，如果其他host上没有要mount的数据或数据不在相同目录时，操作会失败。  </p>
<p>docker managed volume：与bind mount最大区别是不需要指定mount 源，只需要指定容器内mount的目录。<br>优点：容器申请 mount docker manager volume时会自动在&#x2F;var&#x2F;lib&#x2F;docker&#x2F;volumes下生成一个目录，这样的话，容器移值性更好。<br>缺点：不支持控制读写权限，不支持单个文件mount。</p>
<h4 id="bind-mount"><a href="#bind-mount" class="headerlink" title="bind mount"></a>bind mount</h4><p>在宿主机上创建http目录</p>
<p>mkdir &#x2F;root&#x2F;http<br>echo “My test bind mount” &gt; &#x2F;root&#x2F;http&#x2F;index.html</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -d -p 80:80 -v /root/http/:/usr/local/apache2/htdocs httpd</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-v &lt;host path&gt;:&lt;container path&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl  http://127.0.0.1</span><br></pre></td></tr></table></figure>
<p>My test bind mount</p>
<p>将host的index.html更新看看，container内的是否也会根着改变</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">echo &quot;My test bind mount test_2 &quot; &gt; /root/http/index.html</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl  http://127.0.0.1</span><br></pre></td></tr></table></figure>
<p>My test bind mount test_2</p>
<p>设置只读权限，这样就不能在container内对该目录进行修改了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -d -p 80:80 -v /root/http/:/usr/local/apache2/htdocs:ro httpd</span><br></pre></td></tr></table></figure>

<p> <img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_21.png"></p>
<h4 id="docker-manged-volume"><a href="#docker-manged-volume" class="headerlink" title="docker manged volume"></a>docker manged volume</h4><p>manged volume最大特点是不需要指定源，只需要指定mount point就可以了<br>查看挂载的源</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -d -p 80:80 -v /usr/local/apache2/htdocs httpd</span><br></pre></td></tr></table></figure>
<p>告诉docker需要一个volume并将其挂载到&#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs，这个但这个volume会从哪里来，inspect一下就知道了<br>manged volume最大特点是不需要指定源，只需要指定mount point就可以了。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker inspect 79af2997ca68</span><br></pre></td></tr></table></figure>
<p> 关注mounts信息<br> <img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_22.png"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">当容器申请 manged volume时docker会在/var/lib/docker/volumes/下生成一个目录xxx/_data这个就是mount源。mount point指向以有的目录，容器内原有的数据会被复制到这个目录中</span><br></pre></td></tr></table></figure>

<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_23.png"></p>
<p>我们可以直接更新宿主机上的数据。</p>
<p>docker managed volume创建过程</p>
<p>1、容器启动时，简单告诉docker 我需要一个volume存放数据，帮我mount到容器xxx目录。<br>2、docker在&#x2F;var&#x2F;lib&#x2F;docker&#x2F;volumes&#x2F;中生成一个随机目录作为mount源。<br>3、如果内mount的目录已经存在，则将数据复制到mount源。<br>4、将host上生成的目录挂载的container中。  </p>
<p>通过docker volume ls 查看 docker manged volume的容器</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_24.png"></p>
<p>docker inspect 查看volume<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_25.png"></p>
<p>bind mount和docker managed volume比较<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_35.png"></p>
<h4 id="数据共享"><a href="#数据共享" class="headerlink" title="数据共享"></a>数据共享</h4><p>容器和宿主机共享数据<br>1、两种类型的data volume它们均可以实现容器与host之间数据共享。<br>2、docker cp 或linux的cp命令将host数据cp到容器中。  </p>
<p>容器和容器间的数据共享<br>方法一：<br>将共享数据放到bind mount的源中然后mount到多个容器中。  </p>
<p>如搭建web集群<br>将宿主机目录共享到3个容器上，curl访问数据都一致  .<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_26.png"></p>
<p>修改宿主机文件，3个容器会同时更新。<br>方法二：<br>volume container<br>专门为其他容器提供volume的容器。<br>专门create出一个容器用来提供volume，volume container的卷可以是bind mount 也可以是docker manged volume。<br>如创建个容器 将&#x2F;root&#x2F;http通过bind mount或docker manged volume的方式挂上去，然后其他容器只需要挂这个容器即可。  </p>
<p>创建 volume container</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker create --name vc_data -v /root/http/:/usr/lolca/apache2/htdocs httpd</span><br></pre></td></tr></table></figure>

<p>因为volume container的作用只是提供数据，它本身不需要处于运行状态</p>
<p>其他容器 volume 使用volume container</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_27.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_28.png"></p>
<p>查看  </p>
<p>[root@wan_test http]# docker inspect web1  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;Mounts&quot;: [</span><br><span class="line">          &#123;</span><br><span class="line">              &quot;Type&quot;: &quot;bind&quot;,</span><br><span class="line">              &quot;Source&quot;: &quot;/root/http&quot;,</span><br><span class="line">              &quot;estination&quot;: &quot;/usr/local/apache2/htdocs&quot;,</span><br><span class="line">              &quot;Mode&quot;: &quot;&quot;,</span><br><span class="line">              &quot;RW&quot;: true,</span><br><span class="line">              &quot;Propagation&quot;: &quot;rprivate&quot;</span><br><span class="line">          &#125;</span><br><span class="line">      ],</span><br></pre></td></tr></table></figure>

<p>volume container的特点：<br>(1)与bind mount相比，不必为每一个容器指定host path，所有path都在volume contain 中定义好了，容器只需要与volume container关联，实现了容器与host解偶。<br>(2)使用 volume container的容器，其mount point是一致的，有利于配置规范的和标准，但也带来一定局限行，使用使要综合考虑。  </p>
<p>data-packed volume container  </p>
<p>volume container，数据还是在host上 data-packed volume container数据，将数据完全放到volume container中的称为data-packed volume container 。</p>
<p>原理<br>先将文件ADD 或cp到镜像中，然后在创建docker manged volume</p>
<p>使用Dockerfile build镜像<br>vim Dokerfile  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM centos</span><br><span class="line">ADD http /usr/local/apache2/htdocs</span><br><span class="line">VOLUME /usr/local/apache2/htdocs</span><br></pre></td></tr></table></figure>

<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_29.png"></p>
<p>查看volume<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_30.png"></p>
<p>在dockerfile中指定了读取volume的数据，所以这里就不需要指定volume的mount point了  </p>
<p>启动http容器使用 data-packed volume container  </p>
<p>docker run -d -p 80:80 –volumes-from vc_data httpd<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_base_31.png"></p>
<p>容器能正确读取volume中的数据，data-packed volume container是自包含的不依赖host提供数据，具有很强移值性，非常合适静态数据的场景，比如应用的配置信息，web Server的静态文件等.</p>
<p>data volume 生命周期  </p>
<p>备份<br>1、备份host上对应的目录。<br>2、使用一个临时容器挂载目标容器备份。<br>如创建一个managerd-volume容器  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd --name App-container1 -v /var/volume1 -v /var/volume2  ubuntu</span><br></pre></td></tr></table></figure>
<p>编辑些数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker exec -it data_volume bash</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /var/volume1/ &amp;&amp; touch &#123;1..100&#125;</span><br><span class="line">cd /var/volume2/ &amp;&amp; touch &#123;200..300&#125;</span><br></pre></td></tr></table></figure>
<p>在宿主机上创建个目录用于存储备份的数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir /backup</span><br><span class="line"></span><br><span class="line">docker run -it  --rm  --volumes-from App-container1 -v /backup:/backup ubuntu tar -cvf /backup/data_volume_2017_10_01.tar /var/volume1 /var/volume2</span><br></pre></td></tr></table></figure>
<p>这条命令启动一个临时容器，这个容器有挂载两个volume一个的宿主机的&#x2F;backup一个是映射App-container1的目录，这样这个临时容器就有了App-container1和宿主机backup目录，然后将App-container1目录的&#x2F;var&#x2F;volume1和&#x2F;var&#x2F;volume2目录备份到backup目录。因为backup目录和宿主机是映射的，这样宿主机就有了备份文件。  </p>
<p>将本地backup目录和容器内backup目录进行映射然后将&#x2F;var&#x2F;volume1和&#x2F;var&#x2F;volume2打包备份到backup目录。<br>可以查看宿主机上目录<br>[root@wan_test &#x2F;]# ll &#x2F;backup&#x2F;<br>总用量 112<br>-rw-r–r– 1 root root 112640 10月   6 19:34 data_volume_2017_10_01.tar</p>
<p>恢复<br>启动个App-container2将App-container-1的数据恢复   到App-container2中  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd --name App-container2 -v /var/volume1 -v /var/volume2  ubuntu</span><br></pre></td></tr></table></figure>
<p>此时App-container2中的&#x2F;var&#x2F;volume1和&#x2F;var&#x2F;volume2目录是空的<br>开始恢复  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -it  --rm  --volumes-from App-container2 -v /backup:/backup ubuntu tar -xvf /backup/data_volume_2017_10_01.tar -C /</span><br></pre></td></tr></table></figure>
<p>查看App-container2的&#x2F;var&#x2F;volume1和&#x2F;var&#x2F;volume2，数据恢复。<br>迁移<br>1、stop原用容器<br>2、将原目录mount到新容器  </p>
<p>销毁<br>销毁容器不会销毁bind mount   </p>
<p>data managed volume销毁需要docker rm -v 或docker volume rm xxx  </p>
<p>批量删除<br>docker volume rm $(docker volume ls -q)</p>
<p><a href="https://yeasy.gitbooks.io/docker_practice/content/network/linking.html">https://yeasy.gitbooks.io/docker_practice/content/network/linking.html</a></p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>修改docker容器默认空间大小</title>
    <url>/2018/01/23/docker_default_storage/</url>
    <content><![CDATA[<p>软件版本<br>docker-ce 17.03<br>storagedriver：device-mapper：direct-lvm    </p>
<p>docker 启动一个容器后默认根分区大小为10GB，通过docker info可以看见默认大小为10G,有时会不够用需要扩展。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docer_storage_size_1.png"></p>
<h3 id="两种修改方式"><a href="#两种修改方式" class="headerlink" title="两种修改方式"></a>两种修改方式</h3><h4 id="静态扩展"><a href="#静态扩展" class="headerlink" title="静态扩展"></a>静态扩展</h4><p>启动容器查看容器分区大小<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docer_storage_size_2.png"><br>指定容器默认空间大小为20G<br>修改&#x2F;etc&#x2F;docker&#x2F;daemon.json，添加dm.basesize&#x3D;xxx<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docer_storage_size_3.png"></p>
<p>需要注意的是只能扩容不能缩减。<br>重启docker  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>
<p>此时还是不生效的，需要把镜像重新pull一遍<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docer_storage_size_4.png"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker rmi centos</span><br><span class="line">docker pull centos</span><br></pre></td></tr></table></figure>
<p>重新run一个<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docer_storage_size_5.png"><br>缺点：  </p>
<ul>
<li>非动态更改，改完后还需要重新启动docker  </li>
<li>更改后镜像需要重新pull下来。  </li>
<li>只能扩容不能缩减。</li>
</ul>
<h4 id="动态修改"><a href="#动态修改" class="headerlink" title="动态修改"></a>动态修改</h4><p>以centos这个容器为例，动态修改这个容器默认空间为30G<br>创建个centos容器</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd --name centos centos  </span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docer_storage_size_13.png"></p>
<p>找到对应的块设备  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker inspect centos|grep DeviceName</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docer_storage_size_7.png"><br>使用dmsetup table查看文件扇区信息<br>结果的第二个数字(41943040)是设备的大小，表示有多少个512字节的扇区，计算方法是(20<em>1024</em>1024*1024&#x2F;512)我们修改大小也只需要修改这个值。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docer_storage_size_8.png"><br>将新的扇区大小写入，只需要修改第二个值  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">echo 0 62914560 thin 253:3 15|dmsetup load /dev/mapper/docker-253\:0-1940029-7ad938b4cb54e70bed0028bb5705c3f7ecd832591d56b076c7c59e51f8cb1276</span><br></pre></td></tr></table></figure>
<p> <img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docer_storage_size_9.png"><br> 将修改后的容器存储文件激活<br> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dmsetup resume /dev/mapper/docker-253\:0-1940029-7ad938b4cb54e70bed0028bb5705c3f7ecd832591d56b076c7c59e51f8cb1276</span><br></pre></td></tr></table></figure><br>再次查看  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docer_storage_size_10.png"><br>发现扇区大小改好了<br>文件系统更新<br>ext4、ext3、ext3文件系统使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">resize2fs /dev/mapper/docker-253\:0-1940029-7ad938b4cb54e70bed0028bb5705c3f7ecd832591d56b076c7c59e51f8cb1276</span><br></pre></td></tr></table></figure>
<p>xfs文件系统使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">xfs_growfs -d /dev/mapper/docker-253\:0-1940029-7ad938b4cb54e70bed0028bb5705c3f7ecd832591d56b076c7c59e51f8cb1276</span><br></pre></td></tr></table></figure>

<p>优点：   </p>
<ul>
<li>动态修改，不需要修改docker启动参数，重启docker。</li>
<li>不用重新pull镜像，针对单个容器，控制灵活。</li>
</ul>
<p>缺点：  </p>
<ul>
<li>上面用的方法只能是devicemapper的storage-driver。</li>
<li>只能扩容不能缩减。</li>
</ul>
<p>建议：<br>实际使用中，数据还是尽量放数据卷中，放镜像中的话，会造成镜像太过于雍肿，就违背了容器本身设计的初仲了。</p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker 管理平台(Rancher)</title>
    <url>/2017/12/01/docker-rancher/</url>
    <content><![CDATA[<p>环境信息<br>3台服务器<br>192.168.2.110 rancher<br>192.168.2.112 docker0<br>192.168.2.113 docker1  </p>
<p>软件版本<br>操作系统：centos7.3<br>rancher：v1.6.11<br>在rancher上安装rancher  </p>
<h3 id="概念：http-rancher-com-docs-rancher-v1-6-zh"><a href="#概念：http-rancher-com-docs-rancher-v1-6-zh" class="headerlink" title="概念：http://rancher.com/docs/rancher/v1.6/zh/"></a>概念：<a href="http://rancher.com/docs/rancher/v1.6/zh/">http://rancher.com/docs/rancher/v1.6/zh/</a></h3><p>来自rancher官方文档<br>Rancher是一个开源的企业级容器管理平台。通过Rancher，企业再也不必自己使用一系列的开源软件去从头搭建容器服务平台。Rancher提供了在生产环境中使用的管理Docker和Kubernetes的全栈化容器部署与管理平台。</p>
<p>Rancher由以下四个部分组成：</p>
<p>基础设施编排<br>Rancher可以使用任何公有云或者私有云的Linux主机资源。Linux主机可以是虚拟机，也可以是物理机。Rancher仅需要主机有CPU，内存，本地磁盘和网络资源。从Rancher的角度来说，一台云厂商提供的云主机和一台自己的物理机是一样的。</p>
<p>Rancher为运行容器化的应用实现了一层灵活的基础设施服务。Rancher的基础设施服务包括网络， 存储， 负载均衡， DNS和安全模块。Rancher的基础设施服务也是通过容器部署的，所以同样Rancher的基础设施服务可以运行在任何Linux主机上。</p>
<p>容器编排与调度<br>很多用户都会选择使用容器编排调度框架来运行容器化应用。Rancher包含了当前全部主流的编排调度引擎，例如Docker Swarm， Kubernetes， 和Mesos。同一个用户可以创建Swarm或者Kubernetes集群。并且可以使用原生的Swarm或者Kubernetes工具管理应用。</p>
<p>除了Swarm，Kubernetes和Mesos之外，Rancher还支持自己的Cattle容器编排调度引擎。Cattle被广泛用于编排Rancher自己的基础设施服务以及用于Swarm集群，Kubernetes集群和Mesos集群的配置，管理与升级。</p>
<p>应用商店<br>Rancher的用户可以在应用商店里一键部署由多个容器组成的应用。用户可以管理这个部署的应用，并且可以在这个应用有新的可用版本时进行自动化的升级。Rancher提供了一个由Rancher社区维护的应用商店，其中包括了一系列的流行应用。Rancher的用户也可以创建自己的私有应用商店。</p>
<p>企业级权限管理<br>Rancher支持灵活的插件式的用户认证。支持Active Directory，LDAP， Github等 认证方式。 Rancher支持在环境级别的基于角色的访问控制 (RBAC)，可以通过角色来配置某个用户或者用户组对开发环境或者生产环境的访问权限。</p>
<p>rancher本身也是个master&#x2F;agent的架构模式，在server端安装的是rancher-server，被rancher纳管的机器，安装rancher-agent等其他rancher组件。</p>
<h3 id="单节点安装"><a href="#单节点安装" class="headerlink" title="单节点安装"></a>单节点安装</h3><p>container-0上<br>通过容器的方式安装</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -d --restart=unless-stopped -p 8080:8080 rancher/server</span><br></pre></td></tr></table></figure>

<h3 id="HA方式安装"><a href="#HA方式安装" class="headerlink" title="HA方式安装"></a>HA方式安装</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_1.png"></p>
<p>组件分配<br>vip 192.168.2.120<br>192.168.2.110 rancher-server、nginx、mariadb、keepalived  </p>
<p>192.168.2.112 rancher-server、nginx、mariadb、keepalived  </p>
<p>192.168.2.113 rancher-server、nginx、mariadb、keepalived  </p>
<p>建议采用3台host<br>3台host上面都部署keepalive+rancher+nginx+mysql<br>nginx做反向代理<br>keepalive用于管理vip落到健康的节点<br>mysql做galera集群，保证3个节点数据库一致  </p>
<h4 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h4><p>绑定hosts<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_2.png"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scp /etc/hosts 192.168.2.112:/etc/hosts  </span><br><span class="line">scp /etc/hosts 192.168.2.113:/etc/hosts  </span><br></pre></td></tr></table></figure>
<p>在三台服务器上安装nginx和keepalived  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum install nginx keepalived -y</span><br></pre></td></tr></table></figure>

<p>三台服务器启动nginx  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl start nginx &amp;&amp; systemctl enable nginx  </span><br></pre></td></tr></table></figure>
<p>编写测试网页<br>每个host上都不同，以此来验证。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@container-0 ~]# echo &quot;container-0&quot; &gt; /usr/share/nginx/html/index.html</span><br><span class="line">[root@container-1 ~]# echo  &quot;container-1&quot; &gt; /usr/share/nginx/html/index.html</span><br><span class="line">[root@container-2 ~]# echo  &quot;container-2&quot; &gt; /usr/share/nginx/html/index.html</span><br></pre></td></tr></table></figure>
<p>重启nginx  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl restart nginx</span><br></pre></td></tr></table></figure>

<p>验证<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_3.png"></p>
<h4 id="配置keepalived"><a href="#配置keepalived" class="headerlink" title="配置keepalived"></a>配置keepalived</h4><p>修改配置前先备份配置文件  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak</span><br></pre></td></tr></table></figure>

<p>编辑keepalived配置文件  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">! Configuration File for keepalived</span><br><span class="line">vrrp_script check_running &#123;</span><br><span class="line">   script &quot;/opt/check.sh&quot;</span><br><span class="line">   interval 10 #执行的间隔时间</span><br><span class="line">   weight -20  # script执行失败vrrp_instance的优先级会减少20，比如master为100 slave为90，master故障后优先级掉到80，slave比master高，则vip到slave，若，master恢复优先级又会大于backup节点mater重新接管</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER #设置为主服务器</span><br><span class="line">    interface ens3 #vip放置的网卡</span><br><span class="line">    virtual_router_id 51 #主、备必须一样 ,这样加入一个集群</span><br><span class="line">    priority 100 #(主、备机取不同的优先级，主机值较大，备份机值较小,值越大优先级越高)</span><br><span class="line">    advert_int 1</span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS #VRRP认证方式，主备必须一致</span><br><span class="line">        auth_pass 1111 ##(密码)</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        192.168.2.120 #vip地址</span><br><span class="line">    &#125;</span><br><span class="line">    track_script &#123; #执行服务检查</span><br><span class="line">      check_running</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>scp一份到另外两个备节点    </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scp /etc/keepalived/keepalived.conf container-1:/etc/keepalived/keepalived.conf</span><br><span class="line">scp /etc/keepalived/keepalived.conf container-2:/etc/keepalived/keepalived.conf</span><br></pre></td></tr></table></figure>
<p>两个备节点修改下配置文件  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">! Configuration File for keepalived</span><br><span class="line">vrrp_script check_running &#123;</span><br><span class="line">   script &quot;/opt/check.sh&quot;  #检查脚本</span><br><span class="line">   interval 10</span><br><span class="line">   weight -20</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP #节点类型为backup</span><br><span class="line">    interface ens3</span><br><span class="line">    virtual_router_id 51</span><br><span class="line">    priority 90 #优先级要比master节点低</span><br><span class="line">    advert_int 1</span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        192.168.2.120</span><br><span class="line">    &#125;</span><br><span class="line">    track_script &#123;</span><br><span class="line">      check_running</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写进程检测脚本为防止脑裂。(container-1和cotainer-2上的脚本需要修改ping那hostname，别ping自己。)<br>vim &#x2F;opt&#x2F;check.sh   </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">function check_process &#123;</span><br><span class="line">  ps -ef |grep nginx|grep -v grep &amp;&gt;/dev/null</span><br><span class="line">  value=`echo $?`</span><br><span class="line">  if [ $value -ne 0 ];then</span><br><span class="line">   pkill keepalived</span><br><span class="line">  fi</span><br><span class="line">&#125;</span><br><span class="line">#如果ping集群另外机器和网关都ping不通，则本身网络出问题，为防止脑裂，自杀。</span><br><span class="line">function check_network &#123;</span><br><span class="line">  ping container-1 -c 5 &amp;&gt;/dev/null</span><br><span class="line">  value1=`echo $?`</span><br><span class="line">  ping container-2 -c 5 &amp;&gt;/dev/null</span><br><span class="line">  value2=`echo $?`</span><br><span class="line">  ping 192.168.2.1 -c 5 &amp;&gt;/dev/null</span><br><span class="line">  value3=`echo $?`</span><br><span class="line">  if [ $value1 -ne 0 -a $value2 -ne 0 -a $value3 -ne 0 ];then</span><br><span class="line">    pkill keepalived</span><br><span class="line">  fi</span><br><span class="line">&#125;</span><br><span class="line">main() &#123;</span><br><span class="line"> check_process</span><br><span class="line"> check_network</span><br><span class="line">&#125;</span><br><span class="line">main</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>添加执行权限  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">chmod a+x /opt/check.sh   </span><br></pre></td></tr></table></figure>
<p>脚本scp到container-1、container-2的&#x2F;opt&#x2F;目录，添加执行权限  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scp /opt/check.sh container-1:/opt/check.sh</span><br><span class="line">scp /opt/check.sh container-2:/opt/check.sh</span><br><span class="line">ssh container-1 &quot;chmod a+x /opt/check.sh&quot;</span><br><span class="line">ssh container-2 &quot;chmod a+x /opt/check.sh&quot;</span><br></pre></td></tr></table></figure>
<p>container-1和container-2启动keepalived</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl start keepalived &amp;&amp; systemctl enable keepalived</span><br></pre></td></tr></table></figure>
<p>验证keepalived<br>验证nginx进程被kill掉时 vip迁移<br>目前vip在master节点ens3上<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_46.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_4.png"></p>
<p>将master节点的nginx kill掉<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_5.png"><br>去把container-2的nginx kill掉<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_6.png"><br>验证网络出问题时vip的迁移动<br>将container-0的 ens3网卡关闭  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ifdown ens3</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_7.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_8.png"></p>
<h4 id="配置mariadb-galera集群"><a href="#配置mariadb-galera集群" class="headerlink" title="配置mariadb galera集群"></a>配置mariadb galera集群</h4><p>三个节点操作<br>配置repo文件<br>vim &#x2F;etc&#x2F;yum.repo&#x2F;mariadb.repo  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[mariadb]</span><br><span class="line">name = MariaDB</span><br><span class="line">baseurl = https://mirrors.ustc.edu.cn/mariadb/yum/10.2/centos7-amd64</span><br><span class="line">gpgkey=https://mirrors.ustc.edu.cn/mariadb/yum/RPM-GPG-KEY-MariaDB</span><br><span class="line">gpgcheck=1  </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum install MariaDB-server MariaDB-client galera</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl start mariadb</span><br></pre></td></tr></table></figure>

<p>一些初始化安全配置  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/usr/bin/mysql_secure_installation  </span><br></pre></td></tr></table></figure>
<p>关闭数据库  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl stop mariadb</span><br></pre></td></tr></table></figure>
<p>修改container-0上的&#x2F;etc&#x2F;my.cnf.d&#x2F;server.cnf文件如下  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[galera]</span><br><span class="line">wsrep_provider = /usr/lib64/galera/libgalera_smm.so</span><br><span class="line">wsrep_cluster_address = &quot;gcomm://192.168.2.110,192.168.2.112,192.168.2.113&quot;</span><br><span class="line">wsrep_node_name = container-0</span><br><span class="line">wsrep_node_address=192.168.2.110</span><br><span class="line">wsrep_on=ON</span><br><span class="line">binlog_format=ROW</span><br><span class="line">default_storage_engine=InnoDB</span><br><span class="line">innodb_autoinc_lock_mode=2</span><br></pre></td></tr></table></figure>
<p>将此文件复制到container-1、container-2，注意要把 wsrep_node_name 和 wsrep_node_address 改成相应节点的 hostname 和 ip。  </p>
<p>查看是否启用galera插件  </p>
<p>连接mariadb,查看是否启用galera插件<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_9.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_10.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_11.png"></p>
<h4 id="配置nginx反向代理"><a href="#配置nginx反向代理" class="headerlink" title="配置nginx反向代理"></a>配置nginx反向代理</h4><p>三个节点<br>备份配置文件  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp /etc/nginx/nginx.conf /etc/nginx/nginx.conf.bak</span><br></pre></td></tr></table></figure>
<p>修改配置文件  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim /etc/nginx/nginx.conf</span><br><span class="line">user nginx;</span><br><span class="line">worker_processes auto;</span><br><span class="line">error_log /var/log/nginx/error.log;</span><br><span class="line">pid /run/nginx.pid;</span><br><span class="line"></span><br><span class="line"># Load dynamic modules. See /usr/share/nginx/README.dynamic.</span><br><span class="line">include /usr/share/nginx/modules/*.conf;</span><br><span class="line">include /etc/nginx/conf.d/*.conf;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections 1024;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">创建rancher配置文件</span><br><span class="line">vim /etc/nginx/conf.d/rancher.conf</span><br><span class="line">http &#123;</span><br><span class="line">upstream rancher &#123;</span><br><span class="line">    server 192.168.2.110:8080;</span><br><span class="line">    server 192.168.2.112:8080;</span><br><span class="line">    server 192.168.2.113:8080;</span><br><span class="line">&#125;</span><br><span class="line">server &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    server_name 192.168.2.120;</span><br><span class="line"></span><br><span class="line">    location / &#123;</span><br><span class="line">        proxy_set_header Host $host;</span><br><span class="line">        proxy_set_header X-Forwarded-Proto $scheme;</span><br><span class="line">        proxy_set_header X-Forwarded-Port $server_port;</span><br><span class="line">        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">        proxy_pass http://rancher;</span><br><span class="line">        proxy_http_version 1.1;</span><br><span class="line">        proxy_set_header Upgrade $http_upgrade;</span><br><span class="line">        proxy_set_header Connection &quot;Upgrade&quot;;</span><br><span class="line">        proxy_read_timeout 900s;</span><br><span class="line">    &#125;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>同步这两个配置文件到另外两个主机<br>重启nginx  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl restart nginx  </span><br></pre></td></tr></table></figure>
<p>三个节点安装rancher-server<br>在mariadb上创建库并授权用户  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE DATABASE IF NOT EXISTS cattle COLLATE = &#x27;utf8_general_ci&#x27; CHARACTER SET = &#x27;utf8&#x27;;</span><br><span class="line">GRANT ALL ON cattle.* TO &#x27;cattle&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;cattle&#x27;;</span><br><span class="line">GRANT ALL ON cattle.* TO &#x27;cattle&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;cattle&#x27;;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>使用外部数据库<br>在三个host上执行  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -d --restart=unless-stopped -p 8080:8080  -p 9345:9345 rancher/server --db-host xxxx --db-user cattle --db-pass cattle --db-name cattle --advertise-address xxxx</span><br></pre></td></tr></table></figure>
<p>–advertise-address 为当前主机ip<br>–db-host    指定MySQL服务器的连接地址(写本机ip)<br>–db-port    连接端口<br>–db-user    连接用户<br>–db-pass    连接密码<br>–db-name    连接库名  </p>
<p>非HA<br>打开浏览器访问<a href="http://localhost:8080/">http://localhost:8080</a><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_12.png"></p>
<p>HA情况<br>打开浏览器访问<a href="http://192.168.2.120/">http://192.168.2.120</a><br>在系统管理–&gt;高可用里面可以看见集群<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_14.png"></p>
<p>测试高可用<br>将container-0关机<br>访问192.168.2.120  正常显示<br>创建帐号<br>系统配置—-&gt;访问控制—-&gt;本地帐号验证  </p>
<p>以新帐号登录，并选择中文<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_15.png"></p>
<p>添加主机进rancher管理<br>将另外两台host给rancher管理，需要安装rancher-agent<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_16.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_17.png"></p>
<p>复制图上第5步的命令<br>在192.168.2.112上执行  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo docker run -e CATTLE_AGENT_IP=&quot;192.168.2.112&quot;  --rm --privileged -v /var/run/docker.sock:/var/run/docker.sock -v /var/lib/rancher:/var/lib/rancher rancher/agent:v1.2.7 http://192.168.2.110:8080/v1/scripts/282E41C46DCA4A2041AA:1483142400000:CPSeNWQ19B9bCfFAULmsX4TKY</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_18.png"></p>
<p>在rancher上可以看见<br>刚刚添加的主机已经进来了  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_19.png"></p>
<p>在主机上查看发现rancher会在host上起这些容器<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_20.png"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">r-network-services-metadata-dns-1-ffdc1af6</span><br><span class="line">r-ipsec-ipsec-router-1-eed9ad34</span><br><span class="line">r-scheduler-scheduler-1-9a4c9847</span><br><span class="line">r-ipsec-cni-driver-1-e5fd917d</span><br><span class="line">r-healthcheck-healthcheck-1-0363e64b</span><br><span class="line">r-ipsec-ipsec-1-f3612834</span><br><span class="line">r-network-services-network-manager-1-9d604f7e</span><br><span class="line">r-network-services-metadata-1-198285ae</span><br><span class="line">rancher-agent</span><br></pre></td></tr></table></figure>
<p>其中<br>rancher-net：rancher网络的核心，它的作用是用strongSwan 和 charon创建IPSec网络;<br>rancher-scheduler：负责容器的调度，选择合适的宿主机;<br>rancer-dns：负责容器的主机名和ip的映射;<br>rancher-metadata：为容器提供元数据的管理;<br>rancher-healthcheck: 为容器提供健康状态检查，原理是通过haproxy进行检查，只有使用了rancher的托管网络才的容器才能被检查，其他网络检测不到;  </p>
<h3 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h3><p>点进host名字<br>可以查看这个host的硬件信息和资源负载情况<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_21.png"></p>
<p>按上面方法添加第二个host<br>对容器的基本操作<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_22.png"></p>
<p>启动、停止、查看日志、执行命令  </p>
<h4 id="构建单主机容器应用"><a href="#构建单主机容器应用" class="headerlink" title="构建单主机容器应用"></a>构建单主机容器应用</h4><p>玩个简单的添加个http的container<br>在对应的host下点击添加容器<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_23.png"></p>
<p>输入应用的基本信息<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_24.png"></p>
<p>可以看见httpd容器已经起来了<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_25.png"></p>
<p>访问ip为宿主机的ip地址<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_26.png"></p>
<h4 id="构建多主机的容器集群应用"><a href="#构建多主机的容器集群应用" class="headerlink" title="构建多主机的容器集群应用"></a>构建多主机的容器集群应用</h4><p>两种方式<br>一种是通过rancher本身自带的应用商店<br>另外一种通过自己编写的docker-compose或rancher-compose，来进行应用编排  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_27.png"></p>
<p>方式一<br>通过应用商店构建grafana<br>在应用商店搜索grafana<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_28.png"></p>
<p>查看详情里面选择版本和设置密码<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_29.png"></p>
<p>预览这可以看见对应的docker-compose和rancher-compose<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_30.png"></p>
<p>点击部署后<br>默认会生成1个容器<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_31.png"><br>访问<br>进行容器伸缩<br>添加负载均衡器<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_47.png"><br>配置负载均衡器<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_33.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_34.png"><br>访问负载均衡器的地址<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_35.png"></p>
<p>容器的伸缩<br>点击容器名  </p>
<p>进容器的管理页，通过修改容器数量进行增减<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_36.png"></p>
<p>这样就做成了一个简单的高可用<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_37.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_38.png"></p>
<p>方式二通过docker-compose进行构建应用<br>用以下docker-compose构建个WordPress<br>WordPress主要由三部分构建web Server+php+database<br>这里是WordPress镜像里面本身包含了php和apache，只要安装WordPress和mysql就可以了</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">version: &#x27;2&#x27;</span><br><span class="line">services:</span><br><span class="line">    db1:</span><br><span class="line">      image: mysql:5.7</span><br><span class="line">      volumes:</span><br><span class="line">       - /var/lib/mysql:/var/lib/mysql</span><br><span class="line">      environment:</span><br><span class="line">       - MYSQL_ROOT_PASSWORD=password</span><br><span class="line">       - MYSQL_DATABASE=wordpress</span><br><span class="line">       - MYSQL_USER=wordpress</span><br><span class="line">       - MYSQL_PASSWORD=wordpress</span><br><span class="line"></span><br><span class="line">    wordpress:</span><br><span class="line">       image: wordpress:latest</span><br><span class="line">       depends_on:</span><br><span class="line">         - db1</span><br><span class="line">       ports:</span><br><span class="line">         - 8000:80</span><br><span class="line">       environment:</span><br><span class="line">         - WORDPRESS_DB_HOST=db1</span><br><span class="line">         - WORDPRESS_DB_USER=wordpress</span><br><span class="line">         - WORDPRESS_DB_PASSWORD=wordpress</span><br><span class="line">         - WORDPRESS_DB_NAME=wordpress</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_39.png"><br>构建完成<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_40.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_41.png"></p>
<p>WordPress的数据主要存储在数据库中，如果没做data volume数据就存放在容器中，一旦这个容器remove了则数据就丢失了，做了data volume数据存放在host上，mysql的话可以通过主从和galera集群来使得多个host上的mysql容器数据保持一致，为了数据存放更加可靠，我们通常将数据存放在高可靠性的分布式存储和nfs上，如glusfs、ceph、nfs等上面。</p>
<p>rancher可以使用的持久化存储<br>rancher-nfs   <a href="http://blog.csdn.net/rancherlabs/article/details/52774702">http://blog.csdn.net/rancherlabs/article/details/52774702</a><br>rancher ceph   <a href="http://geek.csdn.net/news/detail/229747">http://geek.csdn.net/news/detail/229747</a><br>rancher本身的分布式块存储longhorn   <a href="http://blog.csdn.net/rancherlabs/article/details/71080450">http://blog.csdn.net/rancherlabs/article/details/71080450</a><br> rancher-ebs   <a href="http://rancher.com/docs/rancher/v1.6/zh/rancher-services/storage-service/rancher-ebs/">http://rancher.com/docs/rancher/v1.6/zh/rancher-services/storage-service/rancher-ebs/</a></p>
<h3 id="rancher网络"><a href="#rancher网络" class="headerlink" title="rancher网络"></a>rancher网络</h3><p>rancher默认创建容器使用的是rancher本身的rancher-network，与其他基于vxlan和基于gre的overalay网络不一样，rancher-network是一种基于ipsec的隧道网络，使用ipsec数据报文都是经过加密算法进行加密了的，相比于其他overlay网络更加安全，但同时加密、解密对cpu资源消耗也比较多。  </p>
<p>这就是创建容器时的托管网络  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_42.png"></p>
<p>分别介绍rancher 5种网络类型和rancher-network<br>bridge：将容器的网卡桥接到docker0，根docker的桥接模式一样;<br>container：两个容器共享一个网络栈他，共享网卡核配置信息。包括ip;<br>host: 连接到host网络的容器，共享docker 宿主机的网络，并且连hostname也是宿主机的;<br>manager:使用的就是rancher-network;<br>none：none网络就是什么都没有的网络;  </p>
<p>rancher-network<br>10.42.0.0&#x2F;16<br>IPAM<br>相比V1.2版本，使用manger网络的容器都拥有两个ip地址，一个是docker0段的一个是manger段的，V1.6采用CNI接口，这样manger网络的容器只有一个manger段的ip了。</p>
<p>网络隔离性：<br>目前使用manger网络的容器都在一个大二层，也就是没有网络的隔离。</p>
<p>rancher-agent将host的docker0也由之间的172.17.0.1改成了10.2.0.0&#x2F;16段的地址，并且每个host的docker0的ip都不一样了</p>
<p>实现原理<br>container-0 上的test-2 ping container-1上的test<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_43.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_44.png"></p>
<p>通过路由追踪发现包先给了 host上的ipsec容器，10.42.190.140为container-0上的ipsec容器，通过他将包封装，加密，通过ipsec遂道传到contain-1上的ipsec容器 10.42.208.216，然后ipsec容器在转发到目标容器。  </p>
<p>源节点包从ipsec容器到宿主机网卡，目标节点包从宿主机网卡到ipsec容器，都是Iptables 规则管理则是由 容器network-manager 组件来完成的。  </p>
<p>查看ipsec 服务的 rancher-compose.yml 可以看到，type 使用 rancher-bridge，ipam 使用 rancher-cni-ipam，bridge 网桥则复用了 docker0，有了这个配置我们甚至可以随意定义 ipsec 网络的 CIDR，如下图所示：<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_rancher_45.png"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">http://blog.csdn.net/rancherlabs/article/details/53835564    </span><br><span class="line">http://baijiahao.baidu.com/s?id=1561101273463943&amp;wfr=spider&amp;for=pc    </span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>docker-compose</title>
    <url>/2017/12/05/docker_compose/</url>
    <content><![CDATA[<h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>用来定义和管理多个容器的容器编排工具，docker-compose是docker-fig的升级版。<br>能做什么？<br>能够快速管理和创建多个容器，比如我直想起一个WordPress，在容器中也不需要自己去搭建lamp和然后安装WordPress了，只需要在docker hub找mysql和WordPress的镜像就可以，然后自己docker run，传对应的参数进去，映射端口，映射目录，很麻烦，同时非常容易出错，有了docker-compose，你只需要通过yaml语言，编写docker-compose脚本，然后就可以通过docker-compose一键是启动，关闭，删除这些容器了。脚本更加好维护和方便迁移。</p>
<p>安装docker-compose<br>可以直接通过pip 安装，因为docker-compose是基于python写的</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install docker-compose</span><br></pre></td></tr></table></figure>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/docker-compose_1.png"></p>
<p>原理<br>dockerc-compose使用不同的请求将会调用不同的python方法去处理，比如使用docker-compose run -d将会调用run方法处理，最终调用docker-daemon去完成操作，docker-compose借助docker-py来完任务，docker-py是一个使用python开发并调用docker-daemon api的docker client包。  </p>
<p>docker-compose命令</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">–-verbose：输出详细信息。</span><br><span class="line"></span><br><span class="line">-f：制定一个非docker-compose.yml命名的yaml文件。</span><br><span class="line"></span><br><span class="line">-p：设置一个项目名称，默认是当前目录的名称。</span><br><span class="line">-d：表示后台运行</span><br><span class="line">--project-directory ：设置一个项目路径默认</span><br><span class="line">build:构建镜像，docker-compose里面镜像来源有两种一种是通过Dockerfile构建，一种是通过镜像仓库pull下来，build在这里就是指调用Docker-compose的build参数，构建镜像</span><br><span class="line">config:用于检查docker-compose的yml文件语法是否有错误</span><br><span class="line">create:创建出来的容器状态为create状态。</span><br><span class="line">down：将创建出来的容器集群down掉。</span><br><span class="line">events:接收容器事件</span><br><span class="line">exec：对run的容器执行命令</span><br><span class="line">ps：输出正在运行容器</span><br><span class="line">pull:pull一个服务镜像下来 docker-compose pull server_name</span><br><span class="line">push:上传一个容器镜像 docker-compose push server_name</span><br><span class="line">stop：停止容器集群</span><br><span class="line">start：启动容器集群</span><br><span class="line">up：创建和启动一个容器</span><br><span class="line">scale:设置运行容器的数量，可以进行集群伸缩，如docker-compose scale resource_name=container_num</span><br></pre></td></tr></table></figure>

<p>例<br>下面这个docker-compose.yml默认执行docker-compose up -d只会起一个web1一个db1，web1使用centos镜像，db1使用httpd镜像，并且db1依赖web1，必须等到web1安装完才能操作。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">version: &quot;3&quot;</span><br><span class="line">services:</span><br><span class="line"> web1:</span><br><span class="line">  image: centos</span><br><span class="line">  tty: true</span><br><span class="line"> db1:</span><br><span class="line">  image: httpd</span><br><span class="line">  depends_on:</span><br><span class="line">    - web1</span><br><span class="line">  tty: true</span><br></pre></td></tr></table></figure>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/docker-compose_2.png"></p>
<p>假设web1资源不够了需要进行扩容到5个<br>执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker-compose scale web1=5</span><br></pre></td></tr></table></figure>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/docker-compose_3.png"></p>
<p>迅速扩容到5个，此时高峰期以过，不需要这么多web1容器了需要缩减到正常<br>执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker-compose scale web1=1</span><br></pre></td></tr></table></figure>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/docker-compose_4.png"></p>
<h3 id="docker-compose-yml语法解析"><a href="#docker-compose-yml语法解析" class="headerlink" title="docker-compose.yml语法解析"></a>docker-compose.yml语法解析</h3><p>详细参考</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://docs.docker.com/compose/compose-file/compose-versioning/</span><br><span class="line">https://www.cnblogs.com/freefei/p/5311294.html</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">以下面这个文件为例</span><br><span class="line">version: &quot;3&quot;</span><br><span class="line">services:</span><br><span class="line"> web1:</span><br><span class="line">  image: centos</span><br><span class="line">  tty: true</span><br><span class="line"> db1:</span><br><span class="line">  image: centos</span><br><span class="line">  depends_on:</span><br><span class="line">    - web1</span><br><span class="line">  tty: true</span><br></pre></td></tr></table></figure>

<p>version ：版本，docker-compose的版本，不同的版本所支持的语法是不一样的，所对应的docker版本也是不一样的<br>如,但新版本都是向下兼容的 。</p>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/docker-compose_5.png"></p>
<p>services：表示定义服务，是以容器为颗粒度的。<br>web1：表示服务名。<br>image：才用哪个镜像，docker-compose支持两种镜像来源，一种是从docker-hub上，或本地镜像仓库，一种是通过dockerfile构建镜像那这里就应该是build了。<br>tty：根docker run的-t一样，为容器重新分配一个伪输入终端。<br>depends_on:依赖那个服务，这里表示等待哪个服务先完成，在进行本服务的操作。  </p>
<p>其中参数  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">links:                       </span><br><span class="line">  - server_name</span><br><span class="line">  - server_name:alias_name</span><br></pre></td></tr></table></figure>
<p>链接到其他服务中的容器，使用服务名称，或别名,这样在容器中就可以通过服务名的方式联通。但在新版本docker 17.09版本中发现就算不link，docker默认也将他们信息添加到docker dns中了可以直接ping通。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">external_links</span><br><span class="line"> - container_name</span><br></pre></td></tr></table></figure>
<p>链接外部已经存在容器，非docker-compose管理的容器</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ports:</span><br><span class="line">  - &quot;xxx:xxx&quot;</span><br></pre></td></tr></table></figure>
<p>端口映射</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">volumes：</span><br><span class="line">   -/dxx/xxx:/xxx</span><br><span class="line">   -/dxx/xxxx:xxx:ro</span><br></pre></td></tr></table></figure>
<p>卷映射,加上ro表示只读模式。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">volume_from:</span><br><span class="line">     - container_name</span><br></pre></td></tr></table></figure>
<p>从另外一个服务或容器挂载他的卷。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">environment：</span><br><span class="line">  - XXXX=XXX</span><br><span class="line">  - XXXX=XXX</span><br></pre></td></tr></table></figure>
<p>设置环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">extra_hosts:</span><br><span class="line">   - &quot;www.test.com: xxx.xxx.xxx.xx&quot;</span><br></pre></td></tr></table></figure>
<p>添加此项到hosts文件</p>
<p>container_name: xxx 设置容器名  </p>
<p>hostname： xxx 设置主机名  </p>
<p>restart:always  根docker run参数一样，无论怎么样，自动拉起此容器  </p>
<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><h4 id="python-flask框架网站"><a href="#python-flask框架网站" class="headerlink" title="python flask框架网站"></a>python flask框架网站</h4><p>以docker官网例子docker-compose构建一个简单的动态网站为例<br>创建compose文件夹，并cd到目录  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir /compose</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /compose</span><br></pre></td></tr></table></figure>
<p>编写一个python文件，内容如下<br>vim app.py  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from flask import Flask</span><br><span class="line">from redis import Redis</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line">redis = Redis(host=&#x27;redis&#x27;, port=6379)</span><br><span class="line"></span><br><span class="line">@app.route(&#x27;/&#x27;)</span><br><span class="line">def hello():</span><br><span class="line">    count = redis.incr(&#x27;hits&#x27;)</span><br><span class="line">    return &#x27;Hello World! I have been seen &#123;&#125; times.\n&#x27;.format(count)</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    app.run(host=&quot;0.0.0.0&quot;, debug=True)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>其中host的redis为docker-compose里面定义的redis的service-name，因为新版本docker默认会将service_name 添加到dns所以可以不用link直接就可以通过主机名进行通信。  </p>
<p>创建requirements.txt文件用于pip程序安装  </p>
<p>vim requirements.txt  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">flask</span><br><span class="line">redis</span><br></pre></td></tr></table></figure>
<p>创建Dockerfile文件用于构建镜像<br>vim Dockerfile  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM python:3.4-alpine</span><br><span class="line">ADD . /code</span><br><span class="line">WORKDIR /code</span><br><span class="line">RUN pip install -r requirements.txt</span><br><span class="line">CMD [&quot;python&quot;, &quot;app.py&quot;]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>创建Docker-compose文件，用于构建应用集群<br>vim docker-compose.yml  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">version: &#x27;3&#x27;</span><br><span class="line">services:</span><br><span class="line">   web:</span><br><span class="line">    build: .</span><br><span class="line">    volumes:</span><br><span class="line">      - .:/code</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;5000:5000&quot;</span><br><span class="line">   redis:</span><br><span class="line">     image: &quot;redis:alpine&quot;</span><br></pre></td></tr></table></figure>

<p>开始运行<br>docker-compose up<br><img src="http://ohx02qrb8.bkt.clouddn.com/docker-compose_6.png"></p>
<p>访问浏览器<a href="http://localhost:5000,刷新一下times根着变一次。">http://localhost:5000,刷新一下times根着变一次。</a><br><img src="http://ohx02qrb8.bkt.clouddn.com/docker-compose_7.png"></p>
<p>查看容器  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker ps -a  </span><br></pre></td></tr></table></figure>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/docker-compose_8.png"></p>
<p>如果要后台运行  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/docker-compose_9.png"></p>
<p>查看服务的环境变量<br><img src="http://ohx02qrb8.bkt.clouddn.com/docker-compose_10.png"></p>
<p>通过service_name控制单个服务的开关。<br><img src="http://ohx02qrb8.bkt.clouddn.com/docker-compose_11.png"></p>
<p>需要注意的是，如果没指定网络的话，默认docker-compose会创建一个以docker-compose.yml所在文件夹名为名的bridge网络。比如我的docker-compose.yml文件是放在zabbix这个目录，那么通过docker-compose构建集群后，会自动创建一个zabbix_default网络，类型为bridge。</p>
<p>如果需要使创建的应用集群同时创建网络，使用下面例子，创建docker集群时，同时构建指定网络名称和类型、网段的docker网络。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">version: &#x27;3&#x27;</span><br><span class="line">services:</span><br><span class="line">    web:</span><br><span class="line">      build: .</span><br><span class="line">      ports:</span><br><span class="line">       - &quot;5000:5000&quot;</span><br><span class="line">      networks:</span><br><span class="line">       - test</span><br><span class="line">    redis:</span><br><span class="line">      networks:</span><br><span class="line">       - test</span><br><span class="line">      image: &quot;redis:alpine&quot;</span><br><span class="line">networks:</span><br><span class="line">  test:</span><br><span class="line">    driver: bridge</span><br><span class="line">    ipam:</span><br><span class="line">      driver: default</span><br><span class="line">      config:</span><br><span class="line">        - subnet: 172.28.0.0/16</span><br></pre></td></tr></table></figure>
<p>web和reids服务都通过network参数加入到test网络中，下面networks资源开始创建test网络，类型为bridge，ipadm驱动为默认，网段为172.28.0.0&#x2F;16。  </p>
<p>docker-compose默认没指定网络会创建一个自己的网络，然后将这些service加入进去。</p>
<p>执行docker-compose up -d<br><img src="http://ohx02qrb8.bkt.clouddn.com/docker-compose_12.png"></p>
<p>查看这个网络<br><img src="http://ohx02qrb8.bkt.clouddn.com/docker-compose_13.png"></p>
<p>如果需要加入一个已经存在的网络<br>创建网络  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker network create --driver bridge --subnet 192.168.1.0/24 --gateway 192.168.1.1 my_net  </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">version: &quot;3&quot;</span><br><span class="line">services:</span><br><span class="line"> web1:</span><br><span class="line">  image: centos</span><br><span class="line">  tty: true</span><br><span class="line">  networks:</span><br><span class="line">     - my_net</span><br><span class="line"> db1:</span><br><span class="line">  image: centos</span><br><span class="line">  depends_on:</span><br><span class="line">    - web1</span><br><span class="line">  tty: true</span><br><span class="line">  networks:</span><br><span class="line">      - my_net</span><br><span class="line"></span><br><span class="line">networks:</span><br><span class="line">  my_net:</span><br><span class="line">    external: true</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker-compose  up -d  </span><br></pre></td></tr></table></figure>
<p>在去镜像内查看就发现是192.168.1.0&#x2F;24的网段了。</p>
<h4 id="构建WordPress应用"><a href="#构建WordPress应用" class="headerlink" title="构建WordPress应用"></a>构建WordPress应用</h4><p>使用两个镜像，一个mysql，一个WordPress，WordPress已经部署好了apache和php和WordPress。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">version: &#x27;3&#x27;</span><br><span class="line">services:</span><br><span class="line">    db1:</span><br><span class="line">      image: mysql:5.7</span><br><span class="line">      volumes:</span><br><span class="line">       - /var/lib/mysql:/var/lib/mysql</span><br><span class="line">      environment:</span><br><span class="line">       - MYSQL_ROOT_PASSWORD=password</span><br><span class="line">       - MYSQL_DATABASE=wordpress</span><br><span class="line">       - MYSQL_USER=wordpress</span><br><span class="line">       - MYSQL_PASSWORD=wordpress</span><br><span class="line"></span><br><span class="line">    wordpress:</span><br><span class="line">       image: wordpress:latest</span><br><span class="line">       depends_on:</span><br><span class="line">         - db1</span><br><span class="line">       ports:</span><br><span class="line">         - 8000:80</span><br><span class="line">       environment:</span><br><span class="line">         - WORDPRESS_DB_HOST=db1</span><br><span class="line">         - WORDPRESS_DB_USER=wordpress</span><br><span class="line">         - WORDPRESS_DB_PASSWORD=wordpress</span><br><span class="line">         - WORDPRESS_DB_NAME=wordpress</span><br></pre></td></tr></table></figure>

<p>默认docker-compose出来的容器已经自动添加service_name和ip的映射了，所以这里DB_HOST直接写db1，不用links也是可以的互通的。<br>这些镜像的环境变量可以在docker hub上找到  </p>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/docker-compose_14.png"></p>
<p>构建<br>使用下面的命令，所有的服务将使用后台模式被启动  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure>

<p>访问<br><a href="http://localhost:8000/">http://localhost:8000</a><br><img src="http://ohx02qrb8.bkt.clouddn.com/docker-compose_15.png"></p>
<h4 id="构建zabbix监控"><a href="#构建zabbix监控" class="headerlink" title="构建zabbix监控"></a>构建zabbix监控</h4><p>zabbix-server分为两个部分，zabbix-server-mysql负责接收zabbix-agent的监控数据，然后将监控数据存储到mysql中，zabbix-web-nginx-mysql负责展示监控数据。</p>
<p>因为默认mysql数据库不是utf-8的编码所以需要重新构建下镜像<br>vim Dockerfile  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM mysql:5.7</span><br><span class="line">CMD [&quot;mysqld&quot;, &quot;--character-set-server=utf8&quot;, &quot;--collation-server=utf8_bin&quot;]，</span><br></pre></td></tr></table></figure>

<p>所以在mysql镜像这里没有直接用仓库的镜像，而是采用dockerfile重新构建。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">version: &#x27;3&#x27;</span><br><span class="line">services:</span><br><span class="line">    db1:</span><br><span class="line">     build: .</span><br><span class="line">     volumes:</span><br><span class="line">       - /var/lib/mysql:/var/lib/mysql</span><br><span class="line">     environment:</span><br><span class="line">       - MYSQL_ROOT_PASSWORD=password</span><br><span class="line">       - MYSQL_USER=zabbix</span><br><span class="line">       - MYSQL_PASSWORD=zabbix</span><br><span class="line">       - MYSQL_DATABASE=zabbix</span><br><span class="line"></span><br><span class="line">    zabbix-server:</span><br><span class="line">     image: zabbix/zabbix-server-mysql:ubuntu-3.4.4</span><br><span class="line">     ports:</span><br><span class="line">      - 10051:10051</span><br><span class="line">     volumes:</span><br><span class="line">       - /var/lib/mysql:/var/lib/mysql</span><br><span class="line">     environment:</span><br><span class="line">       - DB_SERVER_HOST=db1</span><br><span class="line">       - MYSQL_USER=zabbix</span><br><span class="line">       - MYSQL_PASSWORD=zabbix</span><br><span class="line">       - MYSQL_DATABASE=zabbix</span><br><span class="line"></span><br><span class="line">    zabbix-nginx-mysql:</span><br><span class="line">     image: zabbix/zabbix-web-nginx-mysql:ubuntu-3.4.4</span><br><span class="line">     depends_on:</span><br><span class="line">      - db1</span><br><span class="line">     ports:</span><br><span class="line">      - 8000:80</span><br><span class="line">     environment:</span><br><span class="line">       - DB_SERVER_HOST=db1</span><br><span class="line">       - MYSQL_USER=zabbix</span><br><span class="line">       - MYSQL_PASSWORD=zabbix</span><br><span class="line">       - ZBX_SERVER_HOST=zabbix-server</span><br><span class="line">       - PHP_TZ:Asia/Shanghai</span><br></pre></td></tr></table></figure>


<p>启动<br>docker-compose up -d<br>访问<br><a href="http://localhost:8000/">http://localhost:8000</a>  </p>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/docker-compose_16.png">  </p>
<p>参考链接<br><a href="http://wiki.jikexueyuan.com/project/docker-technology-and-combat/yaml_file.html">http://wiki.jikexueyuan.com/project/docker-technology-and-combat/yaml_file.html</a><br><a href="https://www.hi-linux.com/posts/12554.html">https://www.hi-linux.com/posts/12554.html</a><br><a href="https://docs.docker.com/compose/install/#install-compose">https://docs.docker.com/compose/install/#install-compose</a><br><a href="http://www.ywnds.com/?p=7592">http://www.ywnds.com/?p=7592</a></p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker多主机网络(flannel)</title>
    <url>/2017/12/14/docker_flannel/</url>
    <content><![CDATA[<h2 id="此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。"><a href="#此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。" class="headerlink" title="此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。"></a>此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。</h2><p>flannel<br>软件版本<br>docker：17.09<br>etcd：etcd-3.2.7-1<br>flannel：flannel-0.7.1-2<br>项目地址：<a href="https://github.com/coreos/flannel">https://github.com/coreos/flannel</a><br>项目地址：<a href="https://github.com/coreos/etcd">https://github.com/coreos/etcd</a>  </p>
<p>环境信息<br>container-1  192.168.2.110  etcd<br>container-2  192.168.2.112	docker<br>container-3  192.168.2.113  docker  </p>
<p>flannel是core os开发出的docker多host的网络解决方案，flannel为每个宿主机分配一个subnet，每个宿主机上都有一个flannel的agent端，通过这个agent端可以进行根其他宿主机网络信息的共享，创建flannel网卡生成路由信息，建立vxlan遂道,各个宿主机的网络信息存储在etcd这个key-value软件中。  </p>
<p>flannel的backend vlxan、host-gw、udp等。  </p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><h4 id="安装etcd"><a href="#安装etcd" class="headerlink" title="安装etcd"></a>安装etcd</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum install etcd -y</span><br></pre></td></tr></table></figure>
<p>配置etcd  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp /etc/etcd/etcd.conf /etc/etcd/etcd.conf.bak</span><br></pre></td></tr></table></figure>
<p>vim &#x2F;etc&#x2F;etcd&#x2F;etcd.conf</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ETCD_NAME=default                                           #节点名称</span><br><span class="line">ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;                  #数据存放位置</span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=&quot;http://0.0.0.0:2379,http://0.0.0.0:4001&quot;             #监听客户端地址</span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=&quot;http://etcd:2379,http://etcd:4001&quot;           #通知客户端地址</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>启动服务  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl start etcd</span><br></pre></td></tr></table></figure>
<p>检查服务<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_1.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_2.png"></p>
<h4 id="导入网络配置key"><a href="#导入网络配置key" class="headerlink" title="导入网络配置key"></a>导入网络配置key</h4><p>先将配置信息写到文件 flannel-config.json 中，内容为：<br>[root@container-0 ~]# cat flannel-config.json</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">  &quot;Network&quot;: &quot;10.2.0.0/16&quot;,</span><br><span class="line"></span><br><span class="line">  &quot;SubnetLen&quot;: 24,</span><br><span class="line"></span><br><span class="line">  &quot;Backend&quot;: &#123;</span><br><span class="line"></span><br><span class="line">    &quot;Type&quot;: &quot;vxlan&quot;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Network 定义该网络的 IP 池为 10.2.0.0&#x2F;16。   </p>
<p>SubnetLen 指定每个主机分配到的 subnet 大小为 24 位，即10.2.X.0&#x2F;24。  </p>
<p>Backend 为 vxlan，即主机间通过 vxlan 通信，后面我们还会讨论host-gw。  </p>
<p>将配置存入etcd  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_3.png"></p>
<p>&#x2F;docker-test&#x2F;network&#x2F;config 是此 etcd 数据项的 key，其 value 为 flannel-config.json 的内容。key 可以任意指定，这个 key 后面会作为 flanneld 的一个启动参数。执行 etcdctl get 确保设置成功。  </p>
<h4 id="测试get-value"><a href="#测试get-value" class="headerlink" title="测试get value"></a>测试get value</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_4.png"></p>
<h3 id="在container-1和container-2上执行"><a href="#在container-1和container-2上执行" class="headerlink" title="在container-1和container-2上执行"></a>在container-1和container-2上执行</h3><p>安装flannel  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum install flannel -y</span><br></pre></td></tr></table></figure>

<p>配置flannel  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp  /etc/sysconfig/flanneld /etc/sysconfig/flanneld.bak</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_5.png"></p>
<p>ip写etcd的ip  </p>
<p>启动flannel  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl enable flannel</span><br><span class="line">systemctl start flannel</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>验证可以看见flannel网卡已经出来了<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_6.png"></p>
<p>配置docker使用flannel  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat /run/flannel/subnet.env  </span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_7.png"></p>
<p>将flannel_subnet和flannel_mtu写入docker.service<br>编辑docker.service  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim /usr/lib/systemd/system/docker.service</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_8.png"></p>
<p>bip和mtu为上图cat &#x2F;run&#x2F;flannel&#x2F;subnet.env的配置项。<br>重启docker  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>
<p>docker会10.2.71.1配置到docker0上，同时生成一条路由<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_9.png"></p>
<p>同主机的docker使用docker0进行通信，跨主机的使用flannel1.1转发<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_10.png"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ps -ef|grep docker</span><br></pre></td></tr></table></figure>
<h4 id="测试连通性"><a href="#测试连通性" class="headerlink" title="测试连通性"></a>测试连通性</h4><p>flannel不会创建新的网络，会使用默认的bridge网络<br>启动容器<br>container-1</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd --name bbox1 busybox  </span><br><span class="line">````</span><br><span class="line">container-2</span><br></pre></td></tr></table></figure>
<p>docker run -itd –name bbox2 busybox  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_11.png)</span><br><span class="line"></span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_12.png)</span><br><span class="line"></span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_13.png)</span><br><span class="line"></span><br><span class="line">### 原理</span><br><span class="line">#### vxlan</span><br><span class="line">容器内网卡和物理机上一veth-xxx是一对veth-pair，同时物理机的veth-xxx挂载在docker0这个bridge上。  </span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_14.png)</span><br><span class="line"></span><br><span class="line">容器内默认路由是给10.2.71.1</span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_15.png)</span><br><span class="line"></span><br><span class="line">10.2.71.1是docker0  </span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_16.png)</span><br><span class="line"></span><br><span class="line"> 查看host路由，是10.2.0.0/16这个subnet的包会交给flannel1.1,,flannel1.1封装成udp包通过ens3，发送出去。container-2 收到包解封装，发现数据包目的地址为 10.2.36.2，根据路由表将数据包发送给 flannel1..1，并通过 docker0 到达 bbox2。  </span><br><span class="line"></span><br><span class="line"> ![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_17.png)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> ![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_18.png)</span><br><span class="line"></span><br><span class="line"> ![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_19.png)</span><br><span class="line"></span><br><span class="line">flannel并不会创建什么网桥，同一主机通过docker0连接，不同主机通过flanel1.1建立vxlan遂道连接。  </span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_20.png)</span><br><span class="line"></span><br><span class="line">#### host-gw  </span><br><span class="line">flannel支持的backend;  </span><br><span class="line">VXLAN: 使用Linux的VxLan；默认的VNI 是 1 ；默认的UDP端口是 8472;  </span><br><span class="line">host-gw: 创建IP路由的方式, 不会对数据进行封装;  </span><br><span class="line">UDP: 使用UDP 8285 端口;  </span><br><span class="line">AliVPC: 不能用于生产;  </span><br><span class="line">Alloc: 不能用于生产;  </span><br><span class="line">AWS VPC: 不能用于生产;  </span><br><span class="line">GCE: 不能用于生产;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">修改etcd  </span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_21.png)  </span><br><span class="line">导入新配置  </span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_22.png)</span><br><span class="line">重启etcd  </span><br></pre></td></tr></table></figure>
<p>systemctl restart etcd</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">重启container-1和container-2上的flannel  </span><br><span class="line">查看路由表，发现生成了一条到container-2的明细路由  </span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_23.png)</span><br><span class="line"></span><br><span class="line">mtu变成了1500  </span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_24.png)</span><br><span class="line"></span><br><span class="line">重新修改docker.service  </span><br></pre></td></tr></table></figure>
<p>vim &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.service</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">修改mtu为1500  </span><br><span class="line"></span><br><span class="line">重启docker  </span><br></pre></td></tr></table></figure>
<p>systemctl restart docker</p>
<pre><code>测试连通性  
![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/flnnel_25.png)

面对 host-gw 和 vxlan 这两种 backend 做个简单比较。    
host-gw 把每个主机都配置成网关，主机知道其他主机的 subnet 和转发地址。vxlan 则在主机间建立隧道，不同主机的容器都在一个大的网段内（比如 10.2.0.0/16）。  

虽然 vxlan 与 host-gw 使用不同的机制建立主机之间连接，但对于容器则无需任何改变，bbox1 仍然可以与 bbox2 通信。  

由于 vxlan 需要对数据进行额外打包和拆包，性能会稍逊于 host-gw。  

IPAM:  
flannel为每个主机自动分配独立的subnet，用户只需要指定一个大的地址段，每个host从这个地址段里面在细分。  

网络隔离:    
flannel都用的vxlan的vni1没有实现网络隔离。
flannel 为每个主机分配了独立的 subnet，但 flannel.1 将这些 subnet 连接起来了，相互之间可以路由。本质上，flannel 将各主机上相互独立的 docker0 容器网络组成了一个互通的大网络，实现了容器跨主机通信。flannel 没有提供隔离。  



http://ibash.cc/frontend/article/58/  
http://www.cnblogs.com/CloudMan6/p/7270551.html  
</code></pre>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>docker镜像仓库管理软件(habor）</title>
    <url>/2017/12/04/docker_habor/</url>
    <content><![CDATA[<p>项目首页<br><a href="http://vmware.github.io/harbor/#gettingHarbor">http://vmware.github.io/harbor/#gettingHarbor</a><br>软件版本<br>操作系统：centos7.2<br>harbor：1.2.2<br>docker：17.09-ce<br>环境信息<br>container-0  192.168.2.110<br>container-1   192.168.2.112<br>container-2   192.168.2.113  </p>
<h3 id="habor是什么？"><a href="#habor是什么？" class="headerlink" title="habor是什么？"></a>habor是什么？</h3><p>habor并不是镜像仓库，更确切的说是一个镜像仓库管理平台，原生的docker registry不支持用户权限管理，而habor镜像仓库也是用的原生的docker registry，只是又通过一些其他组件来进行用户和权限管理功能。<br>habor默认镜像是存储在本地文件系统，可以的支持的第三方对象存储，如Amazon s3、openstack swift、ceph radosgw。</p>
<h4 id="组件介绍"><a href="#组件介绍" class="headerlink" title="组件介绍"></a>组件介绍</h4><p>habor安装默认会启动这些组件<br>habor-administrator<br>habor的系统管理容器，可以进行habor-Server的一些系统信息的获取，如存储用量<br>harbor-db<br>负责存储habor的用户信息和项目信息<br>harbor-jobservice<br>负责habor与habor的之间项目的同步<br>harbor-log<br>负责统一管理habor日志<br>habor-ui<br>负责web端展示，token的验证和生成。<br>nginx<br>前端反向代理，<br>registry<br>docker的镜像仓库  </p>
<h4 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h4><p>来源(<a href="http://blog.csdn.net/u010278923/article/details/77941995">http://blog.csdn.net/u010278923/article/details/77941995</a>)  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_habor_1.png"></p>
<p>主要组件包括proxy，他是一个nginx前端代理，主要是分发前端页面ui访问和镜像上传和下载流量，上图中通过深蓝色先标识；ui提供了一个web管理页面，当然还包括了一个前端页面和后端API，底层使用mysql数据库；registry是镜像仓库，负责存储镜像文件，当镜像上传完毕后通过hook通知ui创建repository，上图通过红色线标识，当然registry的token认证也是通过ui组件完成；adminserver是系统的配置管理中心附带检查存储用量，ui和jobserver启动时候回需要加载adminserver的配置，通过灰色线标识；jobsevice是负责镜像复制工作的，他和registry通信，从一个registry pull镜像然后push到另一个registry，并记录job_log，上图通过紫色线标识；log是日志汇总组件，通过docker的log-driver把日志汇总到一起，通过浅蓝色线条标识。</p>
<h4 id="docker-login过程"><a href="#docker-login过程" class="headerlink" title="docker login过程"></a>docker login过程</h4><p>(<a href="http://www.sohu.com/a/67065472_115128">http://www.sohu.com/a/67065472_115128</a>)<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_habor_2.png"></p>
<p>(a) 首先，这个请求会由监听80端口的proxy容器接收到。根据预先设置的匹配规则，容器中的Nginx会将请求转发给后端的registry 容器；<br>(b) 在registry容器一方，由于配置了基于token的认证，registry会返回错误代码401，提示Docker客户端访问token服务绑定的URL。在Harbor中，这个URL指向Core Services；<br>(c) Docker 客户端在接到这个错误代码后，会向token服务的URL发出请求，并根据HTTP协议的Basic Authentication规范，将用户名密码组合并编码，放在请求头部(header)；<br>(d) 类似地，这个请求通过80端口发到proxy容器后，Nginx会根据规则把请求转发给ui容器，ui容器监听token服务网址的处理程序接收到请求后，会将请求头解码，得到用户名、密码；<br>(e) 在得到用户名、密码后，ui容器中的代码会查询数据库，将用户名、密码与mysql容器中的数据进行比对（注：ui 容器还支持LDAP的认证方式，在那种情况下ui会试图和外部LDAP服务进行通信并校验用户名&#x2F;密码)。比对成功，ui容器会返回表示成功的状态码，并用密钥生成token，放在响应体中返回给Docker 客户端。<br>至此，一次docker login 成功地完成了，Docker客户端会把步骤(c)中编码后的用户名密码保存在本地的隐藏文件中。  </p>
<p>docker push的流程<br>(<a href="http://www.sohu.com/a/67065472_115128">http://www.sohu.com/a/67065472_115128</a>)  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_habor_3.png"><br>用户登录成功后用docker push命令向Harbor 推送一个Docker image：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># docker push 192.168.2.110/library/hello-world</span><br></pre></td></tr></table></figure>
<p>(a) 首先，docker 客户端会重复login的过程，首先发送请求到registry,之后得到token 服务的地址；<br>(b) 之后，Docker 客户端在访问ui容器上的token服务时会提供额外信息，指明它要申请一个对imagelibrary&#x2F;hello-world进行push操作的token；<br>(c) token 服务在经过Nginx转发得到这个请求后，会访问数据库核实当前用户是否有权限对该image进行push。如果有权限，它会把image的信息以及push动作进行编码，并用私钥签名，生成token返回给Docker客户端；<br>(d) 得到token之后Docker客户端会把token放在请求头部，向registry发出请求，试图开始推送image。Registry 收到请求后会用公钥解码token并进行核对，一切成功后，image的传输就开始了。  </p>
<h3 id="安装habor"><a href="#安装habor" class="headerlink" title="安装habor"></a>安装habor</h3><p>container-0上安装<br>habor安装是写好的docker-compose.yml文件，所以需要先安装docker-compose  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">easy_install pip  </span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install docker-compose  </span><br></pre></td></tr></table></figure>
<p>下载habor离线安装包  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget http://harbor.orientsoft.cn/harbor-1.2.2/harbor-offline-installer-v1.2.2.tgz</span><br></pre></td></tr></table></figure>
<p>md5<br>d2af810be0554319181969835a462807<br>解压  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tar -xvf harbor-offline-installer-v1.2.2.tgz</span><br></pre></td></tr></table></figure>


<h4 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /root/habor</span><br></pre></td></tr></table></figure>

<p>vim &#x2F;root&#x2F;habor&#x2F;harbor.cfg</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hostname=192.168.2.110  //这里不能写localhost和127.0.0.1，</span><br><span class="line">ui_url_protocol = http //协议写http或https</span><br><span class="line">harbor_admin_password = 123456 //harbor admin的密码，默认为Harbor12345</span><br></pre></td></tr></table></figure>

<h4 id="执行安装"><a href="#执行安装" class="headerlink" title="执行安装"></a>执行安装</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@container-0 harbor]# ./install.sh</span><br></pre></td></tr></table></figure>
<p>它会将刚刚下载的镜像import进行，然后调用docker-compose，初始化，并启动。  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_habor_4.png">  </p>
<p>查看集群状态<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_habor_5.png">  </p>
<h4 id="web访问和基本使用"><a href="#web访问和基本使用" class="headerlink" title="web访问和基本使用"></a>web访问和基本使用</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_habor_6.png">  </p>
<p>我这里刚刚设置了用户名为admin密码为123456，如果默认密码为Harbor12345<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_habor_7.png">  </p>
<h4 id="用户管理"><a href="#用户管理" class="headerlink" title="用户管理"></a>用户管理</h4><p>创建用户<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_habor_8.png">  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_habor_9.png">  </p>
<p>创建项目<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_habor_10.png">  </p>
<p>将刚刚创建的wanshaoyuan添加成项目内，角色为项目管理员<br>用户加入对应的项目分为三种角色<br>项目管理员、开发人员、访客<br>分别对应的权限为<br>项目管理员：M(管理)、D(删除)、R(读取)、W(写入)、S(查询)<br>开发人员：R(读取)、W(写入)、S(查询)<br>访客：R(读取)、S(查询)  </p>
<p>项目管理<br>项目是一组镜像仓库的逻辑集合，habor对用户权限的控制，实上就是对这个用户在这个项目的权限控制，一个项目可以有多个项目管理员，下面的habor同步也是基于项目的。  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_habor_11.png">  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_habor_12.png">  </p>
<h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><h4 id="docker-client配置"><a href="#docker-client配置" class="headerlink" title="docker-client配置"></a>docker-client配置</h4><p>去上传镜像啦，因为habor使用的是http协议，而默认docker push镜像默认是https的所以我们需要修改下docker-client<br>container-1和container-2上操作<br>vim &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.service</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ExecStart=/usr/bin/dockerd --insecure-registry=192.168.2.110</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>

<p>在container-1上，上传镜像导habor<br>给镜像打tag<br>habor格式为  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[registry_host]:[port]/[project_name]/[封装的应用]:[版本]</span><br></pre></td></tr></table></figure>
<p>原生的registry，是用户名，habor是写刚刚创建的项目名  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[registry_host]:[port]/[username]/[封装的应用]:[版本]</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_habor_13.png"><br>push镜像  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_habor_14.png">  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_habor_15.png">  </p>
<p>在habor web端查看<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_habor_16.png">  </p>
<p>日志可以看见刚刚的push操作<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_habor_17.png">  </p>
<p>pull镜像<br>在container-2上将刚刚push的镜像pull下来<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_habor_18.png">  </p>
<h3 id="多habor的同步"><a href="#多habor的同步" class="headerlink" title="多habor的同步"></a>多habor的同步</h3><p>有多个habor服务器时可以设置镜像的同步</p>
<p>在container-1上在启动一个habor</p>
<p>安装方法同上<br>访问<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_habor_19.png">  </p>
<p>配置项目复制<br>点击进项目<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_habor_20.png">  </p>
<p>添加复制规则<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_habor_21.png"><br>container-0上查看日志，已经同步成功了<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_habor_22.png">  </p>
<p>container-1上查看，项目已经创建好了<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_habor_23.png">  </p>
<p>后面只要container-0上的镜像有更新，container-1也会自动同步  </p>
<p><a href="http://blog.csdn.net/u010278923/article/details/72468791">http://blog.csdn.net/u010278923/article/details/72468791</a><br><a href="http://www.sohu.com/a/67065472_115128">http://www.sohu.com/a/67065472_115128</a><br><a href="https://github.com/vmware/harbor/blob/master/docs/installation_guide.md">https://github.com/vmware/harbor/blob/master/docs/installation_guide.md</a><br><a href="http://www.cnblogs.com/huangjc/p/6270405.html">http://www.cnblogs.com/huangjc/p/6270405.html</a>  </p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>使用p2p技术加速容器镜像分发</title>
    <url>/2018/08/18/docker_image_p2p/</url>
    <content><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>首先p2p网络是一种分布式的去中心化的网络，在网络中每个节点的地位都是对等的，每个节点既能充当服务器，也同时能为其他节点提供服务，同时也享有其他节点提供的服务。</p>
<p>为什么需要引入p2p技术来加速镜像分发？<br>在大规模的容器集群内，镜像分发，往往需要消耗大量时间，并且会给镜像仓库带来很大的压力负担，通过p2p技术将流量分担到集群的每个节点上，这样可以大大缩短下载镜像的时间，并且能非常有效的减轻镜像仓库的压力。</p>
<h3 id="Dragonfly介绍"><a href="#Dragonfly介绍" class="headerlink" title="Dragonfly介绍"></a>Dragonfly介绍</h3><p>这里我们使用的是阿里巴巴开源的基于P2P技术的PB级文件分发系统蜻蜓(dragonfly)。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/p2p_1.png"><br>引用阿里巴巴双11技术解密内容：<br>蜻蜓整体架构分三层:第一层是 Config Service，他管理所有的 Cluster Manager，Cluster Manager 又管理所有的 Host，Host 就是终端，dfget 就是类似 wget 的一个客户端程序。<br>Config Service 主要负责 Cluster Manager 的管理、客户端节点路由、系统配 置管理以及预热服务等等。简单的说，就是负责告诉 Host，离他最近的一组 Cluster Manager 的地址列表，并定期维护和更新这份列表，使 Host 总能找到离他最近的 Cluster Manager。<br>Cluster Manager 主要的职责有两个:</p>
<ol>
<li>以被动 CDN 方式从文件源下载文件并生成一组种子分块数据;</li>
<li>构造 P2P 网络并调度每个 peer 之间互传指定的分块数据。<br>Host 上就存放着 dfget，dfget 的语法跟 wget 非常类似。<br>主要功能包括文件下 载和 P2P 共享等。</li>
</ol>
<p>原理<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/p2p_2.png"></p>
<p>两个 Host 和 CM 会组成一个 P2P 网络，首先 CM 会查看本地是否有缓存，如果没有，就会回源下载，文件当然会被分片，CM 会多线程下载这些分片，同时会将 下载的分片提供给 Host 们下载，Host 下载完一个分片后，同时会提供出来给 peer 下载，如此类推，直到所有的 Host 全部下载完。<br>本地下载的时候会将下载分片的情况记录在 metadata 里，如果突然中断了下 载，再次执行 dfget 命令，会断点续传。<br>下载结束后，还会比对 MD5，以确保下载的文件和源文件是完全一致的。蜻蜓通过 HTTP cache 协议来控制 CM 端对文件的缓存时长，CM 端当然也有自己定期 清理磁盘的能力，确保有足够的空间支撑长久的服务。<br>需要注意的是开源版的dragonfly目前没有开源config-Service。</p>
<h3 id="使用Dragonfly做docker镜像分发"><a href="#使用Dragonfly做docker镜像分发" class="headerlink" title="使用Dragonfly做docker镜像分发"></a>使用Dragonfly做docker镜像分发</h3><p>技术原理类似我们用的BT下载技术的bitorrent协议<br>cluster-manager就类似于Tracker服务器。<br>.meta类似于torrent文件<br>通过torrent文件，获取其他正在下载该文件的网址名单，根据torrent文件的网址然后连接tracker服务器，从tracker服务器获取正在下载该文件的网址名单，然后与它们取得联系，从他们那里获取文件的片端，直到整个下载完成。<br>原理（来源官方github)</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/p2p_3.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/p2p_4.png"></p>
<p>首先，docker pull 命令，会被 dfget proxy 截获。然后，由 dfget proxy 向 CM 发送调度请求，CM 在收到请求后会检查对应的下载文件是否已经被缓存到本地，如果没有被缓存，则会从Registry 中下载对应的文件，并生成种子分块数据(种子分块数据一旦生成就可以立即被使用);如果已经被缓存，则直接生成分块任务，请求者解析相应的分块任务，并从其他 peer 或者 supernode 中下载分块数据，当某个Layer的所有分块下载完成后，一个Layer也就下载完毕了，同样，当所有的Layer下载完成后，整个镜像也就下载完成了。</p>
<p>文件分块的下载</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/p2p_5.png"><br>注: 其中cluster manager即超级节点(supernode)</p>
<p>每个文件会被分成多个块在对等者(peer)间进行传输。一个peer就是一个P2P客户端。<br>超级节点会判断文件是否存在本地，如果不存在，则会将其从文件服务器下载到本地。</p>
<p>配置方法<br>主要参考官方的服务端和客户端的安装和使用<br><a href="https://github.com/alibaba/Dragonfly/tree/master/docs/zh">https://github.com/alibaba/Dragonfly/tree/master/docs/zh</a><br>软件版本：<br>docker：17.03-2<br>os：ubuntu16.04<br>Dragonfly：0.2.0<br>Harbor：1.4.0</p>
<p>环境信息</p>
<p>rke-node1：172.31.164.57<br>rke-node2：172.31.164.58<br>rke-node3：172.31.164.59<br>Harbor：172.31.164.66<br>cluster-manger：172.31.164.113</p>
<p>安装cluster-manger（<a href="https://github.com/alibaba/Dragonfly/blob/master/docs/zh/install_server.md%EF%BC%89">https://github.com/alibaba/Dragonfly/blob/master/docs/zh/install_server.md）</a><br>分为两种方式<br>1、通过docker镜像方式安装，适于用快速部署测试的环境。<br>2、源码编译安装，适用于生产环境部署<br>我们这里主要介绍docker镜像方式安装，源码方式安装参考上述链接。<br>clone 代码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/alibaba/Dragonfly.git</span><br></pre></td></tr></table></figure>

<p>进入项目目录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd Dragonfly</span><br></pre></td></tr></table></figure>

<p>编译代码，打包镜像需要本地安装java环境和maven</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./build/build.sh supernode</span><br></pre></td></tr></table></figure>
<p>获取镜像ID</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$&#123;superNodeDockerImageId&#125;=`docker image ls|grep &#x27;supernode&#x27; |awk &#x27;&#123;print $3&#125;&#x27; | head -n1`</span><br></pre></td></tr></table></figure>
<p>启动cluster-manger</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -d -p 8001:8001 -p 8002:8002 $&#123;superNodeDockerImageId&#125;</span><br></pre></td></tr></table></figure>

<p>测试验证<br>telnet 127.0.0.1 8001<br>telent 127.0.0.1 8002</p>
<p>安装client端（<a href="https://github.com/alibaba/Dragonfly/blob/master/docs/zh/install_client.md%EF%BC%89">https://github.com/alibaba/Dragonfly/blob/master/docs/zh/install_client.md）</a><br>安装客户端</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget https://github.com/alibaba/Dragonfly/raw/master/package/df-client.linux-amd64.tar.gz</span><br></pre></td></tr></table></figure>
<p>创建文件夹存放</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir /root/install</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tar xzvf df-client.linux-amd64.tar.gz -C /root/install</span><br></pre></td></tr></table></figure>
<p>设置环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim ~/.bashrc</span><br></pre></td></tr></table></figure>
<p>PATH&#x3D;$PATH:&#x2F;root&#x2F;install&#x2F;df-client</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure>
<p>验证<br>执行命令</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df-daemon -h</span><br><span class="line">Usage of df-daemon:</span><br><span class="line"> -callsystem string</span><br><span class="line">     caller name (default &quot;com_ops_dragonfly&quot;)</span><br><span class="line"> -certpem string</span><br><span class="line">     cert.pem file path</span><br><span class="line"> -dfpath string</span><br><span class="line">     dfget path (default &quot;/root/install/df-client/dfget&quot;)</span><br><span class="line"> -h	help</span><br><span class="line"> -keypem string</span><br><span class="line">     key.pem file path</span><br><span class="line"> -localrepo string</span><br><span class="line">     temp output dir of daemon (default &quot;/root/.small-dragonfly/dfdaemon/data/&quot;)</span><br><span class="line"> -notbs</span><br><span class="line">     not try back source to download if throw exception (default true)</span><br><span class="line"> -port uint</span><br><span class="line">     daemon will listen the port (default 65001)</span><br><span class="line"> -ratelimit string</span><br><span class="line">     net speed limit,format:xxxM/K</span><br><span class="line"> -registry string</span><br><span class="line">     registry addr(https://abc.xx.x or http://abc.xx.x) and must exist if df-daemon is used to mirror mode</span><br><span class="line"> -rule string</span><br><span class="line">     download the url by P2P if url matches the specified pattern,format:reg1,reg2,reg3</span><br><span class="line"> -urlfilter string</span><br><span class="line">     filter specified url fields (default &quot;Signature&amp;Expires&amp;OSSAccessKeyId&quot;)</span><br><span class="line"> -v	version</span><br><span class="line"> -verbose</span><br><span class="line">     verbose</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>使用方法<br>配置客户端连接超级节点</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/dragonfly.conf</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[node]</span><br><span class="line">address=172.31.164.113 #多台cluster-manger用逗号分隔</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>启动dfdaemon，指定镜像仓库地址，默认端口为65001</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df-daemon --registry 172.31.164.66 &amp;</span><br></pre></td></tr></table></figure>
<p>配置docker-mirror和docker http-proxy</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim /etc/docker/daemon.json</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [</span><br><span class="line">    &quot;http://127.0.0.1:65001&quot;</span><br><span class="line">],</span><br><span class="line">  &quot;insecure-registries&quot; : [</span><br><span class="line">     &quot;http://127.0.0.1:65001&quot;,</span><br><span class="line">     &quot;172.31.164.66&quot;</span><br><span class="line">]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim /lib/systemd/system/docker.service</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Environment=&quot;HTTP_PROXY=http://127.0.0.1:65001&quot;</span><br></pre></td></tr></table></figure>
<p>重启docker</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>
<p>验证查看</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker info</span><br><span class="line"></span><br><span class="line">Insecure Registries:</span><br><span class="line"> http://127.0.0.1:65001</span><br><span class="line"> 172.31.164.66</span><br><span class="line"> 127.0.0.0/8</span><br><span class="line">Registry Mirrors:</span><br><span class="line"> http://127.0.0.1:65001</span><br></pre></td></tr></table></figure>
<p>验证<br>直接将Harbor的地址改成 127.0.0.1:65001就可以拉取镜像，看看是否能拉取成功</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker pull 127.0.0.1:65001/library/front-end:30</span><br></pre></td></tr></table></figure>
<p>Harbor内公开的项目镜像拉取，不用输入镜像仓库地址拉取</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker pull library/front-end:30</span><br></pre></td></tr></table></figure>
<p>注意点：<br>1、private registry不能提供mirror的方式将流量转发到df-daemon，只能通过给docker配置http proxy的方式,原因在于docker pull project_name&#x2F;image_name:tag方式下载镜像时不会在请求头里面携带docker login时输入的账号和密码而通过docker pull registry_address&#x2F;project_name&#x2F;image_name:tag方式会在请求头内通过authorization传递login时输入的账号和密码 。</p>
<p>2、蜻蜓默认是限速20M的，取消限速的方法<a href="https://github.com/alibaba/Dragonfly/issues/38">https://github.com/alibaba/Dragonfly/issues/38</a></p>
<p>抓包分析</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcpdump -i lo port 65001  #确实有大量数据包经过</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/p2p_6.png"></p>
<p>数据查看<br>查看到大量数据分片</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ls ~/.small-dragonfly/data/</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/p2p_7.png"></p>
<p> 查看下载日志<br> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">less ~/.small-dragonfly/logs/dfclient.log</span><br></pre></td></tr></table></figure><br> 因为性能测试需要大并发，大流量环境下才能对比出差异性，所以这里性能测试结果使用阿里巴巴官方测试数据<br> <img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/p2p_8.png"></p>
<p>上图可以看出，随着下载规模的扩大，蜻蜓与 Native 模式耗时差异显著扩 大，最高可提速可以达 20 倍。在测试环境中源的带宽也至关重要，如果源的带宽是 2Gbps，提速可达 57 倍。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/p2p_9.png"></p>
<p>向 200 个节点分发 500M 的镜像，比 docker 原生模式使用更低的网络流量， 实验数据表明采用蜻蜓后，Registry 的出流量降低了 99.5% 以上;而在 1000 并发 规模下，Registry 的出流量更可以降低到 99.9% 左右。</p>
<p><a href="https://github.com/alibaba/Dragonfly">https://github.com/alibaba/Dragonfly</a></p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker多主机网络(macvlan)</title>
    <url>/2017/12/07/docker_macvlan/</url>
    <content><![CDATA[<p>docker 版本：Docker version 17.10.0-ce, build f4ffd25<br>操作系统版本：centos7.3  </p>
<p>物理网卡虚拟化技术，将一张物理网卡虚拟化出多个网卡，每个都用独立的mac地址和ip地址<br>docker用macvlan优点：1、性能好，相比其他的实现，macvlan不需要创建linux bridge、直接通过物理网卡出去</p>
<p>缺点：vlan子接口和需要自己提前划出来，ip地址使用需要自己手动去配置。  </p>
<h3 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h3><p>交换机：<br>1、创建vlan 这里使用vlan 100和vlan 200。<br>2、将两台container host连接的交换机口配置为trunk模式允许vlan 100和vlan200通过。  </p>
<p>注意<br>1、使用macvlan网络时因为不使用bridge了所有网卡都是一个真实的物理设备，能进行三层转发。</p>
<p>服务器<br>1、使用container-1和container-2的eth0连接交换机；</p>
<h3 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h3><p>container-1和container-2分别执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vconfig add eth0 100</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vconfig add eth0 200</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ip link set eth0.100 up</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ip link set eth0.200 up</span><br></pre></td></tr></table></figure>
<p>创建macvlan网络<br>在container-1和container-2上执行  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker network create -d macvlan --subnet=192.168.100.0/24 --gateway=192.168.100.1 -o parent=eth0.100 mac_net100</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker network create -d macvlan --subnet=192.168.200.0/24 --gateway=192.168.200.1 -o parent=eth0.200 mac_net200</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_macvlan_1.png">    </p>
<p>mac_net100和mac_net200本质上是独立的，为了避免ip冲突，在run时手动指定ip ;<br>container-1上运行容器</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd --name mac_net1 --ip=192.168.100.10 --network mac_net100 busybox</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd --name mac_net2 --ip=192.168.200.10 --network mac_net200 busybox</span><br></pre></td></tr></table></figure>
<p>container-2上运行容器  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd --name mac_net3 --ip=192.168.100.20 --network mac_net100 busybox</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd --name mac_net4 --ip=192.168.200.20 --network mac_net200 busybox   </span><br></pre></td></tr></table></figure>
<p>正常情况是container-1上的mac_net1能够container-2上的mac_net3通新，mac_net2能根mac_net4通信;</p>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_macvlan_2.png">    </p>
<p>1、mac_net100 ping mac_net300<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_macvlan_3.png">    </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_macvlan_4.png">    </p>
<p>2、因为macvlan的包是直接走子接口出去的，所以不用桥，也不用通过封包。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_macvlan_5.png">    </p>
<p>IPAM<br>MACVLAN需要用户自己管理subnet，自己分配地址，不同subnet通信依赖外部网关。</p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker machine(docker 多主机部署)</title>
    <url>/2017/10/30/docker_machine/</url>
    <content><![CDATA[<h2 id="此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。"><a href="#此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。" class="headerlink" title="此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。"></a>此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。</h2><h3 id="Docker-machine概念"><a href="#Docker-machine概念" class="headerlink" title="Docker machine概念"></a>Docker machine概念</h3><p>Docker machine：在一个multi-host的环境中，手动去将安装和配置docker非常繁琐，用户能够使用docker machine能够快速的安装docker环境并配置安全配置。<br>docker machine支持在不同的环境下安装配置docker host，同时还能对构建好的host进行一些简单的管理操作。<br>(1)常规linux操作系统<br>(2)虚拟化平台 ：virtualbox、vmware、kvm、hyperV等<br>(3)公有云：aws、azure等  </p>
<p>实际体验：可能因为网络原因，经常卡住不动，不如写shel脚本l、使用puppet、和ansible这些工具，可控程度高。  </p>
<p>实验环境<br>3个host<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_machine_1.png"></p>
<h3 id="安装-docker-machine"><a href="#安装-docker-machine" class="headerlink" title="安装 docker machine"></a>安装 docker machine</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -L https://github.com/docker/machine/releases/download/v0.13.0/docker-machine-`uname -s`-`uname -m` &gt;/tmp/docker-machine &amp;&amp;</span><br><span class="line">chmod +x /tmp/docker-machine &amp;&amp;</span><br><span class="line">sudo cp /tmp/docker-machine /usr/local/bin/docker-machine</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>执行docker-machine –version验证安装<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_machine_2.png"></p>
<p>安装 bash completion script，这样在 docker-manchine 管理对应的host时，命令提示符会变成对应host的name   </p>
<p>cd &#x2F;etc&#x2F;bash_completion.d<br>执行以下命令  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scripts=( docker-machine-prompt.bash docker-machine-wrapper.bash docker-machine.bash ); for i in &quot;$&#123;scripts[@]&#125;&quot;; do sudo wget https://raw.githubusercontent.com/docker/machine/v0.13.0/contrib/completion/bash/$&#123;i&#125; -P /etc/bash_completion.d; done</span><br></pre></td></tr></table></figure>

<p> 修改bashrc<br>vim ~&#x2F;.bashrc  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">source /etc/bash_completion.d/docker-machine-prompt.bash</span><br><span class="line">PS1=&#x27;[\u@\h \W$(__docker_machine_ps1)]\$ &#x27;</span><br></pre></td></tr></table></figure>

<p>创建machine<br>创建machine就是在host上自动安装和配置docker<br>docker-machine ls #显示当前通过docker-machine创建的host<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_machine_3.png"></p>
<p>创建前需要进行互信操作，实现免密码登录，docker-machine生成公钥<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_machine_4.png"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh-copy-id root@192.168.111.162</span><br><span class="line">ssh-copy-id root@192.168.111.163</span><br></pre></td></tr></table></figure>
<h3 id="开始创建"><a href="#开始创建" class="headerlink" title="开始创建"></a>开始创建</h3><p>执行  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker-machine create --driver generic --generic-ip-address=192.168.111.162 container-1</span><br></pre></td></tr></table></figure>
<p>卡住的话 docker-machine rm xxx  然后做试几次create  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">--driver 调用哪个driver，使用普通Linux就写generic ，</span><br><span class="line">--generic-ip-address 指定目标系统的ip</span><br><span class="line">container-1表示命名为contaner-1</span><br></pre></td></tr></table></figure>

<p>添加container-2  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker-machine create --driver generic --generic-ip-address=192.168.111.163 container-2</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_machine_5.png"></p>
<p>执行过程  </p>
<p>1、通过ssh登录到远程主机<br>2、安装docker<br>3、复制证书<br>4、配置docker-daemon<br>5、启动docker  </p>
<p>docker-machine ls 可以看见已经安装好的docker host了  </p>
<h3 id="docker-machine-执行的步奏"><a href="#docker-machine-执行的步奏" class="headerlink" title="docker-machine 执行的步奏"></a>docker-machine 执行的步奏</h3><p>docker-machine creat会做两步操作。<br>关于docker-machine做的安全配置参考<a href="http://www.cnblogs.com/sparkdev/p/7066789.html">http://www.cnblogs.com/sparkdev/p/7066789.html</a><br>1，安装docker，并进行配置。<br>2，生成证书保证docker服务安全。<br>我们手动按装的docker，docker daemon监听tcp端口，但没有做任何安全限制，也就是任何人想连都可以连接进来。使用docker-machine create创建的docker默认是做的tls证书加密验证的，只有安装了特定证书的client才能与docker-daemon交互。</p>
<p> &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service.d&#x2F;10-machine.conf  </p>
<p> <img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_machine_6.png"></p>
<p>在 Docker daemon 的配置文件中看到四个以 –tls 开头的参数，分别是 –tlsverify、–tlscacert、–tlscert和 –tlskey。其中的 –tlsverify 告诉 Docker daemon 需要通过 TLS 来验证远程客户端。其它三个参数分别指定了一个 pem 格式文件的路径，按照它们指定的文件路径去查看一下：  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_machine_7.png"></p>
<p>回到安装docker-machine的机器上ls &#x2F;root&#x2F;.docker&#x2F;machine&#x2F;machines&#x2F;container-1&#x2F; 和docker-daemon上的是一样的  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_machine_8.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_machine_9.png"></p>
<p>Docker Machine 在执行 create 命令的过程中，生成了一系列保证安全性的秘钥和数字证书*.pem文件。这些文件在本地和远程 Docker 主机上各存一份，本地的用于配置 Docker 客户端，远程主机上的用于配置 Docker daemon，让两边都设置 TLS 验证的标记，依此实现安全的通信。</p>
<h3 id="管理machine"><a href="#管理machine" class="headerlink" title="管理machine"></a>管理machine</h3><p>显示container-1的环境变量<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_machine_10.png"></p>
<p>执行  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">eval $(docker-machine env container-1)</span><br></pre></td></tr></table></figure>
<p>可以发现命令提示符发生了改变，因为我们前面在bashrc配置了<br>在此状态下执行所有的docker命令都相当于在container-1上执行</p>
<p>切换到container-2<br>eval $(docker-machine env container-2)  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_machine_11.png"></p>
<p>docker-machine 命令    </p>
<p>upgrad ：更新machine的docker到最新版本 docker-machine upgrade container-1 container-2<br>config：查看machine的docker daemon配置<br>stop&#x2F;restart&#x2F;restart :对host操作系统进行操作<br>scp：可以在不同machine中scp文件。  </p>
<p><a href="http://www.cnblogs.com/sparkdev/p/7066789.html">http://www.cnblogs.com/sparkdev/p/7066789.html</a><br><a href="http://www.cnblogs.com/CloudMan6/p/7237420.html">http://www.cnblogs.com/CloudMan6/p/7237420.html</a></p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker单主机网络</title>
    <url>/2017/11/03/docker_network/</url>
    <content><![CDATA[<h2 id="此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。"><a href="#此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。" class="headerlink" title="此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。"></a>此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。</h2><p>docker网络（单host上容器网络)<br>显示docker网络<br>docker network list<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_network_1.png"></p>
<p>docker run时通过–network指定网络  </p>
<h3 id="none网络"><a href="#none网络" class="headerlink" title="none网络"></a>none网络</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -it --network=none busybox</span><br></pre></td></tr></table></figure>
<p>none网络就是什么都没有的网络，挂载在这个网络的容器除了loopback没有其他任何网卡。</p>
<p>使用场景：对于一些安全性要求高且不需要联网的应用可以使用。</p>
<h3 id="host网络"><a href="#host网络" class="headerlink" title="host网络"></a>host网络</h3><p>连接到host网络的容器，共享docker 宿主机的网络，并且连hostname也是宿主机的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd  --network=hhost busybox</span><br></pre></td></tr></table></figure>

<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_network_2.png"></p>
<p>使用场景：用host网络性能比较高，容器可以直接操作宿主机网络，缺点是牺牲了灵活性，需要考虑端口冲突问题。</p>
<h3 id="bridge网络"><a href="#bridge网络" class="headerlink" title="bridge网络"></a>bridge网络</h3><p>docker使用最广泛的方式，docker安装时会创建一个docker0的linux bridge，如果不指定容器网络默认都会挂到docker0上</p>
<p>启动个容器，查看docker0<br>docker run -itd   centos_10_23  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_network_3.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_network_4.png"></p>
<p>docker 内的网卡eth0@if28和linux bridge上的vethfe788c9是一堆veth-pair</p>
<p>默认bridge配置的subnet就是172.17.0.3&#x2F;16网关为172.17.0.1配在docker0上，容器创建时会自动从172.17.0.3&#x2F;16里面抽出一个地址。</p>
<h3 id="user-define-用户自定义网络"><a href="#user-define-用户自定义网络" class="headerlink" title="user-define(用户自定义网络)"></a>user-define(用户自定义网络)</h3><p>user-define网络支持：bridge，overlay和macvlan网络驱动，macvlan和overlay属于跨宿主机的容器的通信，这里先不讨论，这里讨论使用bridge驱动创建user-define网络。</p>
<p>创建一个bridge为driver的user-define网络</p>
<p>docker network create –driver bridge my_net</p>
<p>查看网络状态<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_network_5.png"></p>
<p>inspect一下，发现新创建的网络默认subnet地址自动会从上一个开始，  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_network_6.png"></p>
<p>创建好后，多了个网桥</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_network_7.png"></p>
<p>指定网段,只需要加–subnet 和–gateway参数  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker network create --driver bridge --subnet 192.168.1.0/24 --gateway 192.168.1.1 my_net</span><br></pre></td></tr></table></figure>


<p>容器指定ip，run时加上–ip，只能是使用的–subnet的网络的容器才能使用指定ip功能<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_network_8.png"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd --network=my_net --ip 192.168.1.100 centos_10_23</span><br></pre></td></tr></table></figure>
<p>定义另外一个network网络  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker network create --subnet 192.168.2.0/24 --gateway=192.168.2.1 my_net2</span><br></pre></td></tr></table></figure>
<p>在mynet_2上创建容器  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd --network=my_net2 centos_10_23</span><br></pre></td></tr></table></figure>
<p>宿主机上查看路由<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_network_9.png"></p>
<p>开启路由转发<br>sysctl net.ipv4.ip_forward</p>
<p>在容器内测试192.168.2.2 ping 192.168.1.100，发现网关都能通就是ping不通192.168.1.100  </p>
<p>默认宿主机上的iptables DROP掉了这两个网桥的连接  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_network_10.png"></p>
<p>给my_net2的容器加入my_net网络  </p>
<p>docker network connect my_net 60cae2aee142  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_network_11.png"></p>
<p>在去ping 192.168.1.100 就ok了  </p>
<h3 id="容器和容器间的通信"><a href="#容器和容器间的通信" class="headerlink" title="容器和容器间的通信"></a>容器和容器间的通信</h3><p>3种方式：IP、Docker DNS server 、joined  </p>
<p>IP方式<br>使用docker network connect将现有容器加入到指定网络实现容器间的通信。  </p>
<p>Docker DNS server<br>Docker daemon内嵌了一个DNS Server通过Docker run –name定义的容器名进行通信，只能是user-define网络可以使用，默认的bridge不行 。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd  --name=wan_1 --network=mynet centos_10_23 </span><br><span class="line">docker run -itd  --name=wan_2 --network=my_net centos_10_23 </span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker exec -it 8549430966f7 bash  </span><br></pre></td></tr></table></figure>
<p>ping wan_1<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_network_12.png"></p>
<p>joined方式<br>两个容器共享一个网络栈他，共享网卡核配置信息。joined容器之间通过127.0.0.1直接通信  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd --name=wan1 centos_10_23</span><br><span class="line">docker run -itd --name=wan2 --network=container:wan1  centos_10_23</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_network_13.png"></p>
<p>joined合适以下场景<br>1，不同容器中的程序希望通过loopback高效快速通信，比如web-Server和app-serve。<br>2，独立的网络监控容器用来监控网络流量。</p>
<h3 id="容器访问外网"><a href="#容器访问外网" class="headerlink" title="容器访问外网"></a>容器访问外网</h3><p>容器访问外网(SNAT)<br>通过宿主机的iptables的snat，在宿主机查看  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_network_14.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_network_15.png"></p>
<p>外界访问容器<br>通过Docker-proxy<br>每一个映射端口，host都会启动一个Docker-proxy进程来处理访问容器的流量  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_network_16.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_network_17.png"></p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker多主机网络(overlay)</title>
    <url>/2017/11/17/docker_network_overlay/</url>
    <content><![CDATA[<h2 id="此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记"><a href="#此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记" class="headerlink" title="此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记"></a>此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记</h2><p>docker 版本：Docker version 17.10.0-ce, build<br>操作系统版本：centos7.3</p>
<p>Docker Libnetwork Container Network Model（CNM）阵营  </p>
<p>Docker Swarm overlay<br>Macvlan &amp; IP network drivers<br>Calico<br>Contiv（from Cisco）  </p>
<p>Docker Libnetwork的优势就是原生，而且和Docker容器生命周期结合紧密；缺点也可以理解为是原生，被Docker“绑架”。<br>Container Network Interface（CNI）阵营  </p>
<p>Kubernetes<br>Weave<br>Macvlan<br>Flannel<br>Calico<br>Contiv<br>Mesos CNI  </p>
<p>其中overlay和macvlan是docker原生就支持的。weave、flannel、calico是需要安装额外的组件才可以的。</p>
<p>overlay</p>
<h4 id="结构图"><a href="#结构图" class="headerlink" title="结构图"></a>结构图</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_1.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_2.png"></p>
<p>docker官方文档中，overlay网络是在swarm集群中配置的，但实际上，overlay网络可以独立于swarm集群实现，只需要满足以下前提条件即可。<br>1、有consul或者etcd，zookeeper的集群key-value存储服务；<br>2、组成集群的所有主机的主机名不允许重复，因为docker守护进程与consul通信时，以主机名相互区分；<br>3、所有主机都可以访问集群key-value的服务端口，按具体类型需要打开进行配置。例如docker daemon启动时增加参数–cluster-store&#x3D;consul:&#x2F;&#x2F;<consul_ip>:8500 – -cluster-advertise&#x3D;eth0:2376<br>overlay网络依赖宿主机三层网络的组播实现，需要在所有宿主机的防火墙上打开下列端口;</p>
<p>|    协议             |端口              |   说明启动后去访问8500端口<br>| —————— |:—————–:|<br>|udp    | 4789       | 容器之间流量的vxlan端口<br>|tcp&#x2F;udp | 7946      | docker守护进程的控制端口</p>
<h4 id="安装consul"><a href="#安装consul" class="headerlink" title="安装consul"></a>安装consul</h4><p>安装consul，这里通过容器的方式安装</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -d -p 8500:8500 -h consul --name consul progrium/consul -server -bootstrap</span><br></pre></td></tr></table></figure>

<p>启动后去访问8500端口  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_3.png"></p>
<p>修改container-1和container-2的docker-daemon的配置文件  </p>
<p> vim &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service.d&#x2F;10-machine.conf<br> <img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_4.png"></p>
<p>添加<br>–cluster-store&#x3D;consul:&#x2F;&#x2F;192.168.111.159:8500<br>–cluster-advertise&#x3D;eth0:2376<br>其中<br>–cluster-store：指定consul的地址<br>–cluster-advertise：告知consul自己连接的地址<br>打开consule可以看见已经连接进来了<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_5.png"></p>
<p>创建overlay网络在container-1中创建ov_net1<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_6.png"></p>
<p>docker network ls查看当前网络<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_7.png"></p>
<p>默认是一个10.0.0.0&#x2F;24的网段<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_8.png"></p>
<p>在container-2中发现这个网络同步过来了<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_9.png"></p>
<p>这是因为创建ovnet1时，container-1将overlay网络信息存在consul，container-2从consul读取了新网络的数据，后面ov_net的任何变化都会同步到container-1和container-2</p>
<h4 id="在overlay中运行容器"><a href="#在overlay中运行容器" class="headerlink" title="在overlay中运行容器"></a>在overlay中运行容器</h4><p>container-1  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd --name overlay_1 --network ov_net1 busybox</span><br></pre></td></tr></table></figure>
<p>container-2</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd --name overlay_2  --network ov_net1 busybox</span><br></pre></td></tr></table></figure>

<p>container-1上地址为<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_10.png"></p>
<p>container-2上地址为<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_11.png"></p>
<p>contaner-1 ping container-2<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_12.png"></p>
<p>container-2 ping container-1<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_13.png"></p>
<p>发现container-1和container-2都有两块网卡  </p>
<p>eth0连接的是overlay ,另外docker会创建个docker_gwbridge为使用overlay网络的容器提供上外网的能力<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_14.png"></p>
<p>inspect这个网络<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_15.png"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker network inspect 07a9d7  </span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_16.png"></p>
<p>发现地址分配记录<br>overlay隔离<br>不同overlay网络是相互隔离的<br>在创建个overlay网络  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker network create -d overlay ov_net2</span><br></pre></td></tr></table></figure>
<p>新创建的网络地址是10.0.1.0&#x2F;24<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_17.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_18.png"></p>
<p>创建主机<br>contain-1上  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd --name over_lay3 --network ov_net2 busybox</span><br></pre></td></tr></table></figure>
<p>contain-2上 </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd --name over_lay4 --network ov_net2 busybox</span><br></pre></td></tr></table></figure>

<p>ping ov_net1的机器，不通<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_19.png"></p>
<p>因为他们vni不同，vlxan就是通过vni起到网络隔离效果。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_20.png"></p>
<p>通过docker_gwbridge也是一样的<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_21.png"></p>
<h4 id="overlay原理"><a href="#overlay原理" class="headerlink" title="overlay原理"></a>overlay原理</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_22.png"></p>
<p>1、docker会为每个overlay网络创建个单独的命名空间，在这个命名空间里创建了个br0的bridge。<br>2、在这个命名空间内创建两张网卡并挂载到br0上，创建一对veth pair端口 和vxlan设备。<br>3、veth pair一端接在namespace的br0上一端接在container上。<br>4、vxlan设备用于建立vxlan tunnel，vxlan端口的vni由docker-daemon在创建时分配，具有相同vni的设备才能通信。<br>5、docker主机集群通过key&#x2F;value存储（我们这里用的是consul)共享数据，在7946端口上，相互之间通过gossip协议学习各个宿主机上运行了哪些容器。守护进程根据这些数据来在vxlan设备上生成静态MAC转发表。<br>6、vxlan设备根据静态mac转发表，通过host上的4789端口将数据发到目标节点。<br>7、根据流量包中的vxlan隧道ID，将流量转发到对端宿主机的overlay网络的网络命名空间中。<br>8、对端宿主机的overlay网络的网络命名空间中br0网桥，起到虚拟交换机的作用，将流量根据MAC地址转发到对应容器内部。  </p>
<p>查看namespace 由于容器和overlay的网络的网络命名空间文件不再操作系统默认的&#x2F;var&#x2F;run&#x2F;netns下，只能手动通过软连接的方式查看。   </p>
<p>ln -s&#x2F;var&#x2F;run&#x2F;docker&#x2F;netns &#x2F;var&#x2F;run&#x2F;netns<br>可以看见两个host上都有这个1-xx的namespace<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_23.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_24.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_25.png"></p>
<p>查看vni<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_26.png"></p>
<p>查看vxlan设备上生成的静态mac地址转发表<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_mul_net_27.png"></p>
<p>可以看见又192.168.111.162和192.168.111.163的mac地址  </p>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><p>1、由于vxlan网络与宿主机网络默认不再同一网络环境下，为了解决宿主机与容器的通信问题，docker为overlay网络中的容器额外增加了网卡eth1作为宿主机与容器通信的通道。这样在使用容器服务时，就必须根据访问性质的不同，选择不同的网卡地址，造成使用上的不便。<br>2、容器对外暴露服务仍然只能使用端口绑定的方式，外界无法简单地直接使用容器IP访问容器服务。<br>3、从上面的通信过程中来看，原生的overlay网络通信必须依赖docker守护进程及key&#x2F;value存储来实现网络通信，约束较多，容器在启动后的一段时间内可能无法跨主机通信，这对一些比较敏感的应用来说是不可靠的。  </p>
<p>container的ip都是自动分配的，如果需要静态的固定ip，怎么办？<br>在创建网络的过程中有区别  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker network create -d overlay --subnet=192.168.2.0/24   multihost</span><br><span class="line"></span><br><span class="line">docker run -d --name host1 --net=multihost --ip=192.168.2.2 centos7</span><br><span class="line"></span><br><span class="line">docker run -d --name host2 --net=multihost --ip=192.168.2.3 centos7</span><br></pre></td></tr></table></figure>



<p>IPAM<br>overlay网络中所有主机共享一个大的subnet，容器启动时会顺序分配ip，可以通过–subnet指定ip地址。</p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>docker storage-driver</title>
    <url>/2018/01/25/docker_storage_driver/</url>
    <content><![CDATA[<h3 id="作用："><a href="#作用：" class="headerlink" title="作用："></a>作用：</h3><ul>
<li>容器镜像的存储和管理。  </li>
<li>容器启动时的文件系统的准备。</li>
</ul>
<h3 id="概念："><a href="#概念：" class="headerlink" title="概念："></a>概念：</h3><p>写时复制（copy-on-write)<br>只有当快照出来的对象，它需要进行写操作时，它会从父镜像中将需要修改的块拷贝到自己的文件系统，然后在进行修改。<br>所以通过cow，容器启动拷贝只是拷贝一些元数据，所以容器启动速度特别快。<br>用时分配（allocate-on-daemand)<br>thin-pool用的就是此技术<br>当分配一个100G的空间时，并不会立刻占满这100，只是占用了一些文件的元数据，当写入数据时会根据实际的大小动态的分配,类似linux中的稀疏文件。  </p>
<h3 id="常见的storage-driver"><a href="#常见的storage-driver" class="headerlink" title="常见的storage-driver"></a>常见的storage-driver</h3><h4 id="aufs："><a href="#aufs：" class="headerlink" title="aufs："></a>aufs：</h4><p>linux上较老牌的storage_driver,基于文件级的存储driver，将多层合并成文件系统的单层表示。底层都是只读层，只有最上层文件系统可写,当写和修改文件时也是使用cow技术。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_storagedriver_1.png"></p>
<p>优点：拥有较长的历史，在大量生产环境中验证过，稳定性、可靠性较高<br>缺点：aufs对文件第一次进行修改操作时需要将整个文件复制到读写层，哪怕你只是修改文件的一小部分，这样会造成很大的性能开销。  </p>
<h4 id="overlay"><a href="#overlay" class="headerlink" title="overlay:"></a>overlay:</h4><p>与aufs设计非常类似，也是基于文件级的存储driver同时 也是 一个union filesystem，overlay主要分两层upperdir和lowerdir,其中lowerdir对应的docker中的镜像层，upperdir对应的 是容器层，merged对应的是有效的容器挂载点。  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_storagedriver_2.png"><br>优点：overlay的速度比aufsh和devicemapper快，因为aufs有很多层，而overlay只有两层。<br>overlay支持页缓存共享，多个容器访问同一个文件，可以共享一个或多个页缓存选项。  </p>
<p>缺点：1、overlay是个新生儿，还不具备在生产中使用的条件，但overlay2在centos的3.10.0-514及以上内核版本可以使用。<br>2、因为overlay也是基于文件级的存储，所以根aufs 一样对文件进行第一次修改操作时需要将整个文件拷贝到读写层但比aufs好，因为overlay只有两层，而aufs有多层。<br>3、overlay只有两层，镜像中的每一层并不对应overlay中的层，对应的是&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay&#x2F;中一个文件夹，然后用硬连接的方式将下面层文件引用到上层。<br>overlayfs的详细原理<a href="https://blog.csdn.net/luckyapple1028/article/details/78075358">https://blog.csdn.net/luckyapple1028/article/details/78075358</a>  </p>
<h4 id="overlay2："><a href="#overlay2：" class="headerlink" title="overlay2："></a>overlay2：</h4><p>overlay2：overlay的改进版，只支持4.0以上内核因为在4.0的内核上添加了Multiple lower layers in overlayfs的特性，所以overlay2可以直接造成muitiple lower layers不用像overlay一样要通过硬链接的方式(最大128层) centos的话支持3.10.0-514及以上内核版本也有此特性，所以消耗更少的inode。  </p>
<p>docker官方overlay2的PR:<br><a href="https://github.com/moby/moby/pull/22126">https://github.com/moby/moby/pull/22126</a>  </p>
<p>LINUX KERNERL 4.0 release说明：<br><a href="https://kernelnewbies.org/Linux_4.0">https://kernelnewbies.org/Linux_4.0</a>  </p>
<p>为什么overlay相比overlay2要更加占用inode？  </p>
<p>1、overlay只支持两层lowerdir和upperdir，并且只支持一个lowerdir，所以如果你的容器镜像有多层的话，层与层之前的数据共享是通过硬链接来实现的，我们也知道硬链接本身是同一个inode但不同的文件名而已，但为什么还是会大量消耗inode这里简单做的实验<br>我们在一台配置了storage-driver的机器上PULL ubuntu:18.04镜像</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master dir]# docker pull ubuntu:18.04</span><br><span class="line">18.04: Pulling from library/ubuntu</span><br><span class="line">32802c0cfa4d: Pull complete</span><br><span class="line">da1315cffa03: Pull complete</span><br><span class="line">fa83472a3562: Pull complete</span><br><span class="line">f85999a86bef: Pull complete</span><br><span class="line">Digest: sha256:48eb09a5c832172357f172593ce5e98e027814f758f6aeaef59a7fd658e50d49</span><br><span class="line">Status: Downloaded newer image for ubuntu:18.04  </span><br></pre></td></tr></table></figure>
<p>整个镜像是一个四层镜像层组成的镜像，在&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay&#x2F;下每层对应一个目录，通过tree命令可以看见每个目录内只包含一个root文件夹  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tree -L 2 /var/lib/docker/overlay/</span><br><span class="line">/var/lib/docker/overlay/</span><br><span class="line">|-- 0a9cd41c44a802a325bfc5bfdda757bced8eaf69a8d6004e5d6fcb730591ab31</span><br><span class="line">|   `-- root</span><br><span class="line">|-- 1f9c95c6642a98930a14d914ac9bcfe9535b5a604dc27851cd76266326c76ed7</span><br><span class="line">|   `-- root</span><br><span class="line">|-- 2d79f688e1bf505ef20100f7450ad7e5ea550500bd07a17b7b9b08513fc96988</span><br><span class="line">|   `-- root</span><br><span class="line">`-- e8b1fcddec600628a75964619da3d0de7310fcbd6350b0c07ddf038d71c25d8b</span><br><span class="line">    `-- root</span><br><span class="line"></span><br><span class="line">8 directories, 0 files</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这个root目录内包含的是该层独有的文件和根lowdir共享的数据硬链接，这里看见共享的数据他们本身都是通过硬链接方式连接起来的。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master overlay]# ls  -i /var/lib/docker/overlay/0a9cd41c44a802a325bfc5bfdda757bced8eaf69a8d6004e5d6fcb730591ab31/root/bin/ls</span><br><span class="line">70832997 /var/lib/docker/overlay/0a9cd41c44a802a325bfc5bfdda757bced8eaf69a8d6004e5d6fcb730591ab31/root/bin/ls</span><br><span class="line"></span><br><span class="line">[root@master overlay]# ls  -i /var/lib/docker/overlay/1f9c95c6642a98930a14d914ac9bcfe9535b5a604dc27851cd76266326c76ed7/root/bin/ls</span><br><span class="line">70832997 /var/lib/docker/overlay/1f9c95c6642a98930a14d914ac9bcfe9535b5a604dc27851cd76266326c76ed7/root/bin/ls</span><br><span class="line"></span><br><span class="line">[root@master overlay]# ls  -i /var/lib/docker/overlay/2d79f688e1bf505ef20100f7450ad7e5ea550500bd07a17b7b9b08513fc96988/root/bin/ls</span><br><span class="line">70832997 /var/lib/docker/overlay/2d79f688e1bf505ef20100f7450ad7e5ea550500bd07a17b7b9b08513fc96988/root/bin/ls</span><br><span class="line"></span><br><span class="line">[root@master overlay]# ls  -i /var/lib/docker/overlay/e8b1fcddec600628a75964619da3d0de7310fcbd6350b0c07ddf038d71c25d8b/root/bin/ls</span><br><span class="line">70832997 /var/lib/docker/overlay/e8b1fcddec600628a75964619da3d0de7310fcbd6350b0c07ddf038d71c25d8b/root/bin/ls</span><br></pre></td></tr></table></figure>

<p>但为什么overlay还是会占用大量inode呢？根本原因在于那些文件夹，每层的root目录内存放的都是完整的rootfs文件夹，但它们都是新建出来<br>的，它们inode都不一样，所以在overlay下一个容器镜像层数越多，占用的inode就越多  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_storagedriver_16.png">  </p>
<p>我们看看overlay2的方式<br>因为overlay2是支持linux kernel的Multiple lower layers in overlayfs的特性所以原生就支持多个lower所以不需要在通过硬链接方式<br>同样拉取个ubuntu:18.04的镜像  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/var/lib/docker/overlay2/</span><br><span class="line">|-- 6e599f35a3366d95287390fc00183a80bfc9a34492aaf9694836ec7b5722d0b6</span><br><span class="line">|   |-- diff</span><br><span class="line">|   |-- link</span><br><span class="line">|   |-- lower</span><br><span class="line">|   |-- merged</span><br><span class="line">|   `-- work</span><br><span class="line">|-- 6f66846fe6a29834193bf36380eb1664947f21435edd8ce8fbc9b79dea92c51b</span><br><span class="line">|   |-- diff</span><br><span class="line">|   |-- link</span><br><span class="line">|   |-- lower</span><br><span class="line">|   |-- merged</span><br><span class="line">|   `-- work</span><br><span class="line">|-- a1869d02b056d66742372c4b6d31d64f98b477590cdd76a842a653424f46710b</span><br><span class="line">|   |-- diff</span><br><span class="line">|   |-- link</span><br><span class="line">|   |-- lower</span><br><span class="line">|   |-- merged</span><br><span class="line">|   `-- work</span><br><span class="line">|-- backingFsBlockDev</span><br><span class="line">|-- da05539957d703ae81cf279c76059c947c96bd1f91fd24691b9e7630dcb13ff7</span><br><span class="line">|   |-- diff</span><br><span class="line">|   `-- link</span><br><span class="line">`-- l</span><br><span class="line">    |-- 2LYVTSJQWPVOB46BCA74GFHJ7T -&gt; ../da05539957d703ae81cf279c76059c947c96bd1f91fd24691b9e7630dcb13ff7/diff</span><br><span class="line">    |-- HEXAQW5O23XL6HHRVOMKWGAI4Z -&gt; ../6f66846fe6a29834193bf36380eb1664947f21435edd8ce8fbc9b79dea92c51b/diff</span><br><span class="line">    |-- KXIOHK4OI6PPPFZ56I3ZORNHS7 -&gt; ../6e599f35a3366d95287390fc00183a80bfc9a34492aaf9694836ec7b5722d0b6/diff</span><br><span class="line">    `-- PHQQ6RCEXR6BLZJGKBP6GYS55Q -&gt; ../a1869d02b056d66742372c4b6d31d64f98b477590cdd76a842a653424f46710b/diff</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在overlay2每层的内容都是不一样的，diff是文件系统的统一挂载点,link文件描述的是该层的标识符，lower文件描述了层与层之间的组织关系，overlay2是将底层多个lowerdir和upperdir和workdir联合挂载，形成最终的merged挂载点。  </p>
<h4 id="devicemapper："><a href="#devicemapper：" class="headerlink" title="devicemapper："></a>devicemapper：</h4><p>在linux2.6内核版本中并入内核，devicemapper将所有的镜像和容器存储在自己的虚拟块设备上，devicemapper根上面aufs和overlay不一样的是devicemapper工作在块层次上而不是文件层次上。devicemapper驱动会从配置好的thin-pool中创建一个带一个带有文件系统的块设备，所有镜像都是创建的块设备的快照，容器层是镜像层的快照，都是采用copy-on-write策略。<br>优点：<br>1、devicemapper的copy-on-write策略以64KB为最小单元，相比aufs和overlay整个文件的复制，这个效率很高了。<br>缺点：<br>2、启动n个容器就复制n分文件在内存中，对内存影响大，不合适单台host容器密度高的业务场景。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_storagedriver_3.png"></p>
<p>从上图可以看出，镜像的每一层都是它下面一层的快照，镜像是thin-pool里面创建的块设备的快照。<br>devicemapper分两种模式loop-lvm，direct-lvm，这两种模式的主要区别就是创建thin-pool的方式不一样。<br>loop-lvm是在&#x2F;var&#x2F;lib&#x2F;docker&#x2F;devicemapper&#x2F;创建data和metada两个稀疏文件通过losetup将这两个文件映射成块设备，然后通过devicemapper映射成thin-pool。  </p>
<p>direct-lvm是直接通过lvm的方式将设备创建成pv，组成两个vg分别为data和metadata，将这两个vg组成lvm的thin-pool。<br>需要注意的docker官方推荐centos和rhel生产中使用devicmapper的direct-lvm模式，不推荐loop-lvm因为loop-lvm在挂载多个容器后性能会急剧下降。</p>
<h3 id="适用场景："><a href="#适用场景：" class="headerlink" title="适用场景："></a>适用场景：</h3><p>没有好坏分只有合适分，在不同业务场景下使用不同的storage-driver。<br>这是docker官方给出的一个不同linux发行版的推荐表<br><a href="https://docs.docker.com/engine/userguide/storagedriver/selectadriver/#docker-ce">https://docs.docker.com/engine/userguide/storagedriver/selectadriver/#docker-ce</a><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_storagedriver_4.png"></p>
<p>不同storage-driver所支持的文件系统<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_storagedriver_5.png">  </p>
<p>不同的存储驱动在不同的应用场景下性能不同。例如，AUFS、Overlay、Overlay2 操作在文件级别，内存使用相对更高效，但大文件读写时，容器层会变得很大；DeviceMapper、Btrfs、ZFS 操作在块级别，适合工作在io负载高的场景；容器层数多，且写小文件频繁时，devicemapper 效率比 Overlay 更高；Btrfs、ZFS 更耗内存。<br>此图片来源七牛布道师 陈爱珍在docker.io的分享<a href="http://dockone.io/article/1513">http://dockone.io/article/1513</a>  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_storagedriver_6.png"></p>
<p>性能对比<br><a href="http://dockone.io/article/1513">http://dockone.io/article/1513</a>  </p>
<p>docker默认是使用操作系统本身默认的storage-driver，比如说你在ubuntu上安装docker那么默认就是aufs，在centos7或rhel7上部署docker默认就是overlay。<br>将centos7默认的overlay改成生产可用的devicemapper的direct-lvm模式。<br>安装device-mapper-persistent-data, lvm2包  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum install device-mapper-persistent-data, lvm2 -y    </span><br></pre></td></tr></table></figure>

<p>创建pv  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pvcreate /dev/sdb</span><br></pre></td></tr></table></figure>
<p>创建vg  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vgcreate  docker /dev/sdb</span><br></pre></td></tr></table></figure>
<p>创建lv，这里需要注意，我们分两个lv，一个是data，一个是metadata，最后要将这个两个lv组成一个thin-pool</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lvcreate --wipesignatures y -n thinpool docker -l 95%VG #使用95% VG剩余空间</span><br><span class="line">lvcreate --wipesignatures y -n thinpoolmeta docker -l 1%VG</span><br></pre></td></tr></table></figure>
<p>将lv docker&#x2F;thinpool做为thinpool ，docker&#x2F;thinpoolmeta做为pool的metadata</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lvconvert -y --zero n -c 512K --thinpool docker/thinpool  --poolmetadata docker/thinpoolmeta</span><br></pre></td></tr></table></figure>
<p>配置lvm thinpool的自动扩容  </p>
<p>vi &#x2F;etc&#x2F;lvm&#x2F;profile&#x2F;docker-thinpool.profile  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">activation &#123;</span><br><span class="line">  thin_pool_autoextend_threshold=80</span><br><span class="line">  thin_pool_autoextend_percent=20</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>thin_pool_autoextend_threshold:已用空间百分比 ，100为关闭<br>thin_pool_autoextend_percent&#x3D;20：每次扩容thinpool的容量比例，0为关闭    </p>
<p>当磁盘使用达到80%时，再扩展当前容量的20%。更新&#x2F;etc&#x2F;lvm&#x2F;profile&#x2F;docker-thinpool.profile  </p>
<p>激活此规则  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lvchange --metadataprofile docker-thinpool docker/thinpool</span><br></pre></td></tr></table></figure>

<p>对逻辑卷启用监视  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lvs -o+seg_monitor</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_storagedriver_7.png"></p>
<p>如果之前&#x2F;var&#x2F;lib&#x2F;docker有数据，建议进行备份，没有的可以直接运行。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">编辑daemon.json文件</span><br><span class="line">/etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;storage-driver&quot;: &quot;devicemapper&quot;,</span><br><span class="line">    &quot;storage-opts&quot;: [</span><br><span class="line">    &quot;dm.thinpooldev=/dev/mapper/docker-thinpool&quot;,</span><br><span class="line">    &quot;dm.use_deferred_removal=true&quot;,</span><br><span class="line">    &quot;dm.use_deferred_deletion=true&quot;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>重启docker</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>
<p>验证<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_storagedriver_8.png"></p>
<p>我们可以直接pull一个镜像看看逻辑卷使用会不会改变。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_storagedriver_9.png"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker pull centos  </span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_storagedriver_10.png"><br>在次查看发现使用率提高了<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_storagedriver_11.png"></p>
<p>扩容<br>按照传统的lvm扩容的方式扩容就可以了，假设我添加一个100G的sdc设备<br>先创建pv  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pvcreate /dev/sdc</span><br></pre></td></tr></table></figure>
<p>扩容vg  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vgextend docker /dev/sdc</span><br></pre></td></tr></table></figure>
<p>扩容thin-pool  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lvextend -l+100%FREE -n docker/thinpool</span><br></pre></td></tr></table></figure>

<p>检查<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_storagedriver_12.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_storagedriver_13.png"></p>
<p>当启动一个容器时  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd centos</span><br></pre></td></tr></table></figure>
<p>就生成了对应的块设备<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_storagedriver_14.png"></p>
<p>进入对应目录可以看见运行的容器对应的rootfs<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_storagedriver_15.png"></p>
<p>参考链接：<br><a href="http://blog.csdn.net/M2l0ZgSsVc7r69eFdTj/article/details/78153724">http://blog.csdn.net/M2l0ZgSsVc7r69eFdTj/article/details/78153724</a><br><a href="http://blog.csdn.net/vchy_zhao/article/details/70238690">http://blog.csdn.net/vchy_zhao/article/details/70238690</a><br><a href="http://blog.csdn.net/a85880819/article/details/52448654">http://blog.csdn.net/a85880819/article/details/52448654</a><br><a href="https://docs.docker.com/engine/userguide/storagedriver/selectadriver/#related-information">https://docs.docker.com/engine/userguide/storagedriver/selectadriver/#related-information</a>  </p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker多主机网络(weave)</title>
    <url>/2017/12/15/docker_weave/</url>
    <content><![CDATA[<h2 id="此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。"><a href="#此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。" class="headerlink" title="此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。"></a>此文档为翻阅cloudman的每天5分钟玩转docker技术的读书笔记。</h2><p>weave<br>软件版本<br>docker：17.09<br>weave：2.1.3<br>项目文 档：<a href="https://www.weave.works/docs/net/latest/overview/">https://www.weave.works/docs/net/latest/overview/</a><br>项目地址：<a href="https://github.com/weaveworks/weave">https://github.com/weaveworks/weave</a><br>环境信息<br>container-1  192.168.111.161<br>container-2  192.168.111.162  </p>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>weave是weaveworks开发的容器网络解决方案，weave会创建一个大的二层网络，来将各个host上的容器连接起来，容器与容器之间可以直接通信，无须nat和端口映射,weave还提供dns服务，使容器可以直接通过hostname访问。weave不需要依赖k\v服务，而是在每个主机上运行一个weave容器路由器。来根据不同的主机交换路由信息。  </p>
<p>注意点：<br>1、一个Weave网络是由一定数量的Weave节点构成的。每个Weave路由器运行在不同的宿主机上。  </p>
<p>2、Weave网络中的每个节点都有一个名字，默认是Weave网卡的地址。还有一个人类便于识别的名称，默认是主机名，也可以在Weave启动的时候指定一个名称。  </p>
<p>3、Weave路由器节点之间会建立TCP连接，通过TCP连接，进行心跳检测和交换路由信息。通过配置可以让这条链接进行加密处理。  </p>
<p>4、Weave路由器节点之间也会建立UDP连接，通过UDP连接，进行网络数据包的封装。同样也可以将UDP链接进行加密处理。</p>
<p>Weave路由器节点之间的连接(TCP连接或者UDP连接)可以穿透防火墙，意味着，Weave网络可用于跨数据中心的Docker通信。</p>
<p>5、Weave网络会在每个宿主机上创建一个网桥，每个容器通过veth pair连接到这个Weave 网桥。容器里面的veth网卡会获取到Weave网络分配给的IP地址和子网掩码。</p>
<p>6、Weave网络在不同宿主机之间路由数据的方法有两种：fast data path模式(完全工作在内核态)，sleeve模式，在这种模式中，发往非本地容器的数据会被内核捕获，然后交给用户态的Weave网络路由器处理，通过UDP发送到其他的Weave节点路由器，然后注入到内核空间，最后再转发到本地的容器中。</p>
<p>7、Weave网络路由器会学习其他节点特定的MAC地址，然后将已知的信息和拓扑结合起来。</p>
<p>8、Weave网络可以在partially connected路由数据，通过拓扑交换。比如下图中：节点1 直接连接着 节点2和节点3，如果节点1 先要发送数据到 节点4或者节点5 ，那么先必须发送到节点3。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/weave_1.png"></p>
<p>Weave 网络如何了解网络拓扑？<br>官网地址：<a href="https://www.weave.works/docs/net/latest/concepts/network-topology/">https://www.weave.works/docs/net/latest/concepts/network-topology/</a>  </p>
<p>Peers之间的拓扑交流  </p>
<p>连接到Weave网络的节点会捕获其他Peers节点的拓扑信息。Weave节点会将自己已知的拓扑，和改变的内容传递给其他节点，以便于所有的Weave Peers了解整过网络拓扑。</p>
<p>Weave节点之间的通信建立在TCP上的，有两种方法：</p>
<p>基于STP(生成树)的广播方法<br>基于邻居Gossip方法(八卦算法)<br>以下情况，拓扑信息会发送：</p>
<p>当一个连接新加入进来的时候，如果远程节点似乎是新加入到Weave网络中的，那么会发送整个网络拓扑，并且增量更新，广播节点两端的连接。<br>当一个连接被标记为已经建立，则意味着远端可以从本端接受UDP报文，然后广播一个包含本端信息的数据包。<br>当一个连接断开，一个包含本端信息的报文被广播。<br>周期性的定时器，整个拓扑信息被gossip给邻居，这是为了拓扑敏感的随机分布系统。这是为了防止由于频繁的拓扑变化，造成广播路由表过时，而使前面提到的广播没有到达所有的peers。<br>如果Peers更新拓扑信息之后，发现有一个Peer已经离线，那么就会清除掉这个Peer相关的所有的信息。</p>
<p>拓扑过期怎么办？</p>
<p>将拓扑变化信息广播给所有peers不是立即发生的。这就意味着，很有可能一个节点有过期的网络拓扑视图。</p>
<p>如果目的peer的数据包仍然可达，那么过期的拓扑可能会导致一次低效的路由选择。</p>
<p>如果过期的拓扑中目的peer不可达，那么数据包会被丢弃，对于很多协议（如TCP)，数据发送会在稍后重新尝试，在这期间拓扑信息应当被正确更新。</p>
<h3 id="weave安装"><a href="#weave安装" class="headerlink" title="weave安装"></a>weave安装</h3><p>container-1和container-2上<br>安装weave  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -L git.io/weave -o /usr/local/bin/weave</span><br><span class="line">chmod a+x /usr/local/bin/weave</span><br></pre></td></tr></table></figure>
<p>在container-1上启动weave<br>在 host1 中执行 weave launch 命令，weave的组件都是以容器启动的，weave 会从 docker hub 下载最新的 image 并启动容器。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/weave_2.png"></p>
<p>10.32.0.0&#x2F;12是weave网络使用的默认subnet，如果需要改变(所有host都要改)  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">weave launch --ipalloc-range 10.2.0.0/16</span><br></pre></td></tr></table></figure>
<p>weave 运行了三个容器：<br>weave 是主程序，负责建立 weave 网络，收发数据 ，提供 DNS 服务等。<br>weavevolumes容器提供卷存储<br>weavedb容器提供数据存储  </p>
<p>创建了一个linux bridge桥<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/weave_3.png"></p>
<p>和weave网络<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/weave_4.png"></p>
<p>container-2<br>执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">weave launch 192.168.111.161</span><br></pre></td></tr></table></figure>
<p>这里指定的container-1的地址，这样container-2和container-1才会加入到同一个网络，也可以写多个地址，格式为 ip_addr ip_addr.</p>
<p>两个host要互通，首先要确认建立了集群<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/weave_5.png"></p>
<p>测试<br>在container-1上执行<br>特别说明：声明环境变量的作用是告诉后续的docker命令都发给weave proxy进行处理。<br>声明环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">eval $(weave env)</span><br><span class="line">docker run -itd --name bbox1 busybox</span><br></pre></td></tr></table></figure>
<p>如果要恢复之前的环境，可执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">eval $(weave env --restore)</span><br></pre></td></tr></table></figure>
<p>在container-2上执行  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">eval $(weave env)</span><br><span class="line">docker run -itd --name bbox2 busybox</span><br></pre></td></tr></table></figure>

<p>bbox1 ping bbox2<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/weave_6.png"></p>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><h4 id="单台host架构图"><a href="#单台host架构图" class="headerlink" title="单台host架构图"></a>单台host架构图</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/weave_14.png"></p>
<p>bbox1和bbox2的地址为10.32.0.1&#x2F;12、10.44.0.0&#x2F;12, 子网掩码都是12位的，实际上这3个ip都位于 weave网络的10.32.0.0&#x2F;12，通过container-1和container-2上的vxlan的遂道连接在一起。  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/weave_7.png"></p>
<p>查看bbox1的网络接口  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/weave_8.png"></p>
<p>bbox1有两个网络接口eth0和ethwe，eth0桥接的是默认docker0网络。ethwe和宿主机weave网卡有关ethwe@if39表示ethwe对应编号39的interface。</p>
<p>在宿主机ip a看见了39的interface，vethwepl19152和ethwe是一对veth-pair，同时vethwep19152还挂载在weave 桥上。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/weave_9.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/weave_10.png"></p>
<p>同时挂载的还有vethwe-bridge，ip -d link查看<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/weave_11.png"></p>
<p>vethwe-bridge和vethwe-datapath是一对veth-pair，但vethwe-datapath的父设备是datapath，其中datapath是个openvswitch网桥，vxlan-6784是一个vxlan-interface，其父设备也是datapath，weave主机间是通过vxlan建立的遂道通信，等于说，veth-pair vethwe-bridge和vethwe-datapath将linux-bridge的weave桥和datapath连接在一起，其中，weave桥负责容器接入weave网络，datapath负责主机间建立vxlan隧道并收发数据。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/weave_12.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/weave_13.png"></p>
<h4 id="多host时网络架构图"><a href="#多host时网络架构图" class="headerlink" title="多host时网络架构图"></a>多host时网络架构图</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/weave_17.png"></p>
<p>1、当在container-1上的bbox1 ping container-2上bbox2时<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/weave_15.png"></p>
<p>2、将数据包给ethwe<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/weave_16.png"></p>
<p>2、ethwe和vethwep是veth-pair，所以数据包到了宿主机的weave桥上，然后给vethwe-bridge，然后通过veph-pair的另外一端vethwe-datapath到datapath，通过vxlan发送给container-2。<br>3、container-2的vxlan接口收到报文后会给datapath然后通过vethwe-datapath到weave桥收到报文后，根据目的IP地址将数据包转发给bbox2。  </p>
<h4 id="weave网络隔离"><a href="#weave网络隔离" class="headerlink" title="weave网络隔离"></a>weave网络隔离</h4><p>默认weave使用一个大的subnet(10.32.0.0&#x2F;12)所有主机容器都从这个地址空间分配ip，因为是同一个subnet，容器可以直接通信，如果要隔离，可以通过环境变量WEAVE_CIDR&#x3D;net:xxx.xxx.xxx&#x2F;24指定网段,但需要注意的是这个网段也不是随便写的，要在10.32.0.0&#x2F;12这个大子网范围内。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd -e WEAVE_CIDR=net:xxx.xxx.xxx.xxx/24 image_name  </span><br></pre></td></tr></table></figure>

<p>weave与外网连接<br>宿主机访问weave网络的容器<br>因为weave默认是一个私有的vxlan网络，默认与外部网络隔离，要想host访问，需要将host加入weave。  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/weave_18.png"></p>
<p>10.32.0.2会被分配到weave网桥上  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/weave_19.png"></p>
<p>此时就可以ping通了<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/weave_20.png"></p>
<p>如果其他主机想访问这台host上的容器，只需要添加一条默认路由将下一跳指向host的地址。<br>container-2访问container-1上的bbox1<br>只需要在container-2上添加  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ip route add 10.32.0.0/12 via 192.168.111.161</span><br></pre></td></tr></table></figure>
<p>IPAM<br>默认weave使用一个大的subnet(10.32.0.0&#x2F;12)所有主机容器都从这个地址空间分配ip一个大二层。  </p>
<p>参考链接  </p>
<p><a href="http://ibash.cc/frontend/article/59/">http://ibash.cc/frontend/article/59/</a></p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>neutron-高可用(1)--DVR</title>
    <url>/2017/07/23/dvr/</url>
    <content><![CDATA[<p>环境 fuel8.0</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>DVR简称分布式路由，M版前的neutron网络结构。<br> <img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-1.png"></p>
<p> 无论是南北流量中的1:N还是1:1还是东西流量也好都经过网络节点，这就带来一个问题了，网络节点负载很大，往往成为整个架构的性能短板，同时还很容易出现单点故障。<br>在Juno版本后可以配置DVR模式，每个compute节点上都配置了一个L3 agent，集群内的流量（东西流量）这里有个前提条件，同网段内主机在不同compute节点，在同一个compute节点就直接通过本机就能直接转发了。直接就通过tunnel到达另外一个compute节点，不在需要通过网络节点去做转发了，而配置了floatting ip的主机，直接通过本compute节点的L3 agent 通过br-ex连接外网。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-2.png"><br>进行深入前的一些基本概念的讲解：<br>路由策略<br>引用百科里的一句话<br>路由策略是一种基于目标网络进行路由更加灵活的数据包路由转发机制。应用了路由策略，路由器将通过路由图决定如何对需要路由的数据包进行处理，路由图决定了一个数据包的下一跳转发路由器。 路由策略的种类 大体上分为两种：一种是根据路由的目的地址来进行的策略称为：目的地址路由； 另一种是根据路由源地址来进行策略实施的称为：源地址路由！ 随着路由策略的发展现在有了第三种路由方式：智能均衡的策略方式！</p>
<p>使用 ip rule 操作策略路由</p>
<p>基于策略的路由比传统路由在功能上更强大，使用更灵活，它使网络管理员不仅能够根据目的地址而且能够根据报文大小、应用或IP源地址等属性来选择转发路径。<br>ip rule  查看策略路由表<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-3.png"></p>
<p>数字越小，优先级越高</p>
<p>规则0，它是优先级别最高的规则，规则规定，所有的包，都必须首先使用local表（254）进行路由。本规则不能被更改和删除。</p>
<p>规则32766，规定所有的包，使用表main进行路由。本规则可以被更改和删除。</p>
<p>规则32767，规定所有的包，使用表default进行路由。本规则可以被更改和删除。</p>
<p>在默认情况下进行路由时，首先会根据规则0在本地路由表里寻找路由，如果目的地址是本网络，或是广播地址的话，在这里就可以找到合适的路由；如果路由失败，就会匹配下一个不空的规则，在这里只有32766规则，在这里将会在主路由表里寻找路由;如果失败，就会匹配32767规则，即寻找默认路由表。如果失败，路由将失败。重这里可以看出，策略性路由是往前兼容的。</p>
<p>3232243201:	from 192.168.30.1&#x2F;24 lookup 3232243201 #这条规则的含义是规则3232243201 源ip为192.168.30.1&#x2F;24的包，使用local表3232243201 进行路由</p>
<p>linux二层网络中一些基本名词</p>
<p>TAP设备：如kvm创建虚拟机后，宿主机上的vnet0、vnet1等，是操作系统内核中的虚拟网络设备。<br>veth配对设备（veth pair）或ovs配对设备，它其实是一对网卡把一块叫veth，另外一块叫peer，当一个数据真被发送到其中一端中，veth的另外一端也会收到此帧。在虚拟化中常用veth pair来连接两个bridge，比如qbr网桥跟br-int网桥上的qvb-xxx和qvo-xxx就是一对veth pair。</p>
<h3 id="启用DVR"><a href="#启用DVR" class="headerlink" title="启用DVR"></a>启用DVR</h3><p>以下配置的前提是开启了l2_population<br>l2population原理介绍如下<br><a href="http://blog.csdn.net/cloudman6/article/details/53167522">http://blog.csdn.net/cloudman6/article/details/53167522</a></p>
<h3 id="1，安装"><a href="#1，安装" class="headerlink" title="1，安装"></a>1，安装</h3><h4 id="compute节点"><a href="#compute节点" class="headerlink" title="compute节点"></a>compute节点</h4><p>(1)安装l3-agent<br>(2)修改的内核参数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/sysctl.conf</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line">net.ipv4.conf.all.rp_filter = 0</span><br><span class="line">net.ipv4.conf.default.rp_filter = 0</span><br><span class="line"></span><br><span class="line">sysctl -p</span><br></pre></td></tr></table></figure>
<p>（3）增加一块能上外网的网卡，并创建个br-ex网桥把这块网卡桥接到br-ex上</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ovs-vsctl add-br br-ex</span><br><span class="line">ovs-vsctl add-port br-ex ethx</span><br></pre></td></tr></table></figure>
<p>(4)配置l3agent.ini</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim /etc/neutron/l3_agent.ini</span><br><span class="line"></span><br><span class="line">interface_driver =neutron.agent.linux.interface.OVSInterfaceDriver</span><br><span class="line">use_namespaces = True</span><br><span class="line">agent_mode = dvr</span><br><span class="line">````</span><br><span class="line">vim /etc/neutron/plugins/ml2/ml2_conf.ini</span><br></pre></td></tr></table></figure>
<p>[agent]<br>enable_distributed_routing&#x3D;True</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">重新启动 neutron-openvswitch-agent, netron-l3-agent ,openvswitch-switch 服务。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 网络节点</span><br></pre></td></tr></table></figure>
<p>vim &#x2F;etc&#x2F;neutron&#x2F;l3_agent.ini<br>agent_mode &#x3D; dvr_snat</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">```</span><br><span class="line">vim /etc/neutron/plugins/ml2/ml2_conf.ini</span><br><span class="line">[agent]</span><br><span class="line">enable_distributed_routing=True</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">重启neutron-server、netron-l3-agent ,openvswitch-switch 服务。</span><br></pre></td></tr></table></figure>

<p>配置成功后可以neutron agent-list 看见compute的l3-agent。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-4.png"></p>
<h3 id="原理解释"><a href="#原理解释" class="headerlink" title="原理解释"></a>原理解释</h3><p>1台control节点（server-4），2台compute（server-5、server-6）、外网为192.168.122.0&#x2F;24<br>在control节点上在控制台创建个网络，创建个路由器，设置网关，在路由器上将网络端口添加进去。在基于此网络创建个主机。<br>可以发现在control节点上创建路由器时，会同时在namespace里面创建qroute-xxx和snat-xxx两个命名空间，snat-xxx命名空间会根据创建的子网来添加接口，qg-xxx和sg-xxx，其中qg-xxx上面配置了外网ip，sg-xxx配置的是本子网可用的第一个ip（前两个为网关和dhcp）</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-5.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-6.png"></p>
<p>当该网络内其他主机需要上外网时，源ip会已postrouting形式，snat成路由器的网关，这步操作会在snat-xx命名空间内做</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-7.png"><br>在传统网络架构下，snat操作本来是应该在qroute下进行，而在DVR模式下qroute-xxx完全是连接内网的网关。</p>
<p>当创建一个实例时，会自动在实例所在的compute节点创建一个qroute-xxx 此qrouter-xxx的ip和mac都与control节点的一样。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-8.png"></p>
<p>当给实例分配一个floating_ip时，在自动在实例所在的计算节点namespace里面生成fip-xxx命名空间</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-9.png"></p>
<p>需要注意的是fip-xxx这个id是与你的public网络id是一致<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-10.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-11.png"></p>
<p>也就是说一个public网络对应一个fip-xxx namespace,在下面会解释为什么这样做<br>节点fg-xxx对应的是外网接口，fpr接口与qrouter接口一对veth pair</p>
<h3 id="DVR模式下流量的走向"><a href="#DVR模式下流量的走向" class="headerlink" title="DVR模式下流量的走向"></a>DVR模式下流量的走向</h3><h4 id="南北流量（1：N）"><a href="#南北流量（1：N）" class="headerlink" title="南北流量（1：N）"></a>南北流量（1：N）</h4><p>所谓南北流量（1：N）实际上就是指SNAT就是子网内的主机通过路由器的floatingIP去上外网<br>1，首先需要确认router已经发到compute节点，我这router叫test2<br>可以看见已经在server-6上有了</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-12.png"></p>
<p>我在vm（192.168.50.4）上面ping百度<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-13.png"></p>
<p>因为是跨网段的，所以发送给默认路由，就是网关。<br>在网关上路由策略可以看见发现，发给默认路由192.168.50.1的数据，最终会发给192.168.50.3</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-14.png"></p>
<p>那192.168.50.3在哪呢，通过前面分析，我们知道，192.168.50.3在control节点snat-xx命名空间里面<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-15.png"></p>
<p>然后在iptables里面做一次snat<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-16.png"></p>
<p>就出外网了，因为我这是KVM虚拟机里面所以还要nat一层。</p>
<p>ping public网络 192.168.122.1 然后抓包分析</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-17.png"></p>
<p>所以SNAT流程</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                    （compute节点）         vxlan    （network节点）</span><br><span class="line">vm---&gt;qr-xxx---&gt;br-int----&gt;br-tun---&gt;  br-tun-----&gt;br-int-----&gt;snat-xxx----br-ex</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="南北流量（1：1）"><a href="#南北流量（1：1）" class="headerlink" title="南北流量（1：1）"></a>南北流量（1：1）</h4><p>给主机192.168.50.4绑定一个floating_ip</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-18.png"><br>观察control节点和compute节点的namespace发现<br>control节点<br>没有发生变化，而vm所在的compute的节点的namespace发现多了一个<br>fip命名空间<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-19.png"></p>
<p>注意看这个fip的ip是我们创建的外网网络id，这个在上面已经说过了。<br>查看一个这个fip-xxx的内容</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-20.png"></p>
<p>vm绑定floating_ip后如何跟外网进行通信</p>
<p>1，vm ping外网，首先数据包丢给默认路由，qroute，<br>2，查找路由表<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-21.png"></p>
<p>命中table 16<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-22.png"></p>
<p>走rfp端口</p>
<p>3，然后进行snat</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-23.png"><br>4，然后发送给fip-xxx命名空间</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-24.png"></p>
<p>因为已经进行snat了地址变成外网的，所以这里都default route 直接给外网网关，完成</p>
<p>如下图<br>来源<a href="http://www.cnblogs.com/sammyliu/p/4713562.html">http://www.cnblogs.com/sammyliu/p/4713562.html</a></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-25.png"><br>vm绑定vip后外网DNAT进入内网主机<br>1，arp-proxy<br>外网ip要ping通内网，首先需要知道内网ip的mac，fip，并没有直接绑定在一个特定的端口是怎么知道mac的呢<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-26.png"></p>
<p>进行路由追踪发现<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-27.png"></p>
<p>arping发现mac地址为 尾数为C7:C3的</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-28.png"></p>
<p>那这个mac地址为谁的</p>
<p>竟然是fip-xxx的fg端口<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-29.png"></p>
<p>原来是fip-xxx在fg这个端口上做了arp-proxy，这样，fg既可以响应发给它自己的arp请求，也可以响应发给经过它路由的端的arp请求</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-30.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-31.png"><br>可以看见内核参数里面已经开启了</p>
<p>2，将包丢给路由器</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-32.png"><br>我们从fip-xxx ip route 可以看见包被丢给169.254.93.254这个接口了，这个接口是qroute-xxx的一个接口</p>
<p>3，qroute–xxx做DNAT<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/dvr-33.png"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">流程如下</span><br><span class="line"></span><br><span class="line">        arp欺骗           通过fpr       DNAT</span><br><span class="line">extra-----&gt;fip--xxx----&gt;qroute-----&gt; VM</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>为什么要独立出一个snat namespace来做SNAT，而不继续之前的qrouter多SNAT方案？</p>
<p>因为 当创建子网的第一台主机时，会同时在compute节点上也创建个qrouter-xxx的namespace，但computer节点上的qrouter并不做snat功能，只做当有绑定float_ip时 连接FIPnamespace中转，所以需要将SNAT功能独立出来，于是就有了SNATnamespace当然这个也只会在网络节点的namespace里面创建。</p>
<p>当虚拟机绑定fip时，为什么compute要多出一个fip namespace，不直接在qrouter上的qg直接连接br-ex？<br>是为了减少暴露在外部网络上的mac和ip地址，所以才需要有fip的出现。由于传统router只在网络节点上，数量很少，直接在传统router的qg上配floating ip和用其mac地址应答arp请求，所以没这个问题。但是dvr router由于分布到大量的compute node（每个私有网络都可能有一个router），如果再这么做，会导致有大量router的qg接口暴露在外部网络上，因此分离出fip，而fip是按照（compute node，external network）分配的，数量少很多。一个external_network就一个另外fip也可以做集中的arp proxy，不需要把floating ip真正绑到fip的接口上。同时也可以减少交换机MAC地址表的占用，减轻交换机的负担。</p>
]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title>eBPF学习摘要1(概述、理论)</title>
    <url>/2022/08/23/ebpf_1/</url>
    <content><![CDATA[<h2 id="eBPF学习摘要1-概述、理论"><a href="#eBPF学习摘要1-概述、理论" class="headerlink" title="eBPF学习摘要1(概述、理论)"></a>eBPF学习摘要1(概述、理论)</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a><a href="#%E6%A6%82%E8%BF%B0" title="概述"></a>概述</h3><p>早期内核包过滤是将网络数据包拷贝到用户态进行过滤，这样整体的包过滤性能低，后续在1992年的BSD操作系统上引入BPF包过滤技术，Linux在内核2.1.75正式引入BPF技术。网络数据包过滤可以直接在内核中执行，避免将网络数据包在用户态执行，极大提高了包过滤性能。如tcpdump工具就是利用BPF技术实现。2014年对BPF技术进行全面扩展，诞生了eBPF（extended Berkeley Packet Filter）使得BPF不仅仅是网络栈层面功能。后续iovisor 引入 BCC、bpftrace 等工具，成为 eBPF 在跟踪和排错领域的最佳实践。另外 eBPF 最重大的特性是在内核中运行沙盒程序而无需修改内核源码和重新编译内核就可以扩展内核的功能，Cilium、Katran、Falco 等一系列基于 eBPF 优化网络和安全的开源项目也逐步诞生。并且，越来越多的开源和商业解决方案开始借助 eBPF，优化其网络、安全以及观测的性能。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ebfp1_1.png"><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ebfp1_2.png"><br>图片来源：<a href="https://blog.csdn.net/eBPF_Kindling/article/details/123575619">https://blog.csdn.net/eBPF_Kindling&#x2F;article&#x2F;details&#x2F;123575619</a></p>
<p>发展历程<br>1992年：BPF全称Berkeley Packet Filter，诞生初衷提供一种内核中自定义报文过滤的手段（类汇编），提升抓包效率。（tcpdump）<br>2011年：linux kernel 3.2版本对BPF进行重大改进，引入BPF JIT，使其性能得到大幅提升。<br>2014年： linux kernel 3.15版本，BPF扩展成eBPF，其功能范畴扩展至：内核跟踪、性能调优、协议栈QoS等方面。与之配套改进包括：扩展BPF ISA指令集、提供高级语言（C）编程手段、提供MAP机制、提供Help机制、引入Verifier机制等。<br>2016年：linux kernel 4.8版本，eBPF支持XDP，进一步拓展该技术在网络领域的应用。随后Netronome公司提出eBPF硬件卸载方案。Cilium项目正式发布。<br>2018年：linux kernel 4.18版本，引入BTF，将内核中BPF对象（Prog&#x2F;Map）由字节码转换成统一结构对象，这有利于eBPF对象与Kernel版本的配套管理，为eBPF的发展奠定基础。<br>2018年：从kernel 4.20版本开始，eBPF成为内核最活跃的项目之一，新增特性包括：sysctrl hook、flow dissector、struct_ops、lsm hook、ring buffer等。场景范围覆盖容器、安全、网络、跟踪等<br>2021年：微软、Facebook、Google、Isovalent、NetFlix成立eBPF基金会，同年Cilium发布基于eBPF的Service Mesh解决方案<br>eBPF 基本架构及使用<br>参考链接：<a href="https://blog.51cto.com/dengchj/2944202">https://blog.51cto.com/dengchj/2944202</a></p>
<h3 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a><a href="#%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86" title="实现原理"></a>实现原理</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ebfp1_3.png"><br>用户态<br>1、用户编写 eBPF 程序，可以使用 eBPF 汇编或者 eBPF 特有的 C 语言来编写。<br>2、使用 LLVM&#x2F;CLang 编译器，将 eBPF 程序编译成 eBPF 字节码。<br>3、调用 bpf() 系统调用把 eBPF 字节码加载到内核。</p>
<p>内核态<br>1、当用户调用 bpf() 系统调用把 eBPF 字节码加载到内核时，内核先会对 eBPF 字节码进行安全验证。<br>2、使用 JIT（Just In Time）技术将 eBPF 字节编译成本地机器码（Native Code）。<br>3、然后根据 eBPF 程序的功能，将 eBPF 机器码挂载到内核的不同运行路径上（如用于跟踪内核运行状态的 eBPF 程序将会挂载在 kprobes 的运行路径上）。当内核运行到这些路径时，就会触发执行相应路径上的 eBPF 机器码。<br>4、通过map与用户空间程序交互</p>
<h4 id="如何保证内核安全性和优缺点"><a href="#如何保证内核安全性和优缺点" class="headerlink" title="如何保证内核安全性和优缺点"></a><a href="#%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E5%86%85%E6%A0%B8%E5%AE%89%E5%85%A8%E6%80%A7%E5%92%8C%E4%BC%98%E7%BC%BA%E7%82%B9" title="如何保证内核安全性和优缺点"></a>如何保证内核安全性和优缺点</h4><ul>
<li><p>需要特权执行：eBPF程序加载到Linux内核的进程都必须在特权模式(root)下运行，或者需要CAP_BPF功能，不受信任的程序不能加载eBPF程序</p>
</li>
<li><p>验证器：加载eBPF程序到内核后需要经过验证如有界循环、越界访问内存、使用未初始化的变量。</p>
</li>
<li><p>程序执行保护：已经加载在内核中的eBPF程序会进入read-only模式试图修改会直接crash内核。</p>
</li>
<li><p>限制内核访问范围： eBPF程序不能直接访问任意内核其他函数。必须通过eBPF helpers访问固定helpers函数。</p>
</li>
<li><p>eBPF 堆栈大小被限制在 MAX_BPF_STACK，截止到内核 Linux 5.8 版本，被设置为 512字节。</p>
</li>
<li><p>eBPF 字节码大小最初被限制为 4096 条指令，截止到内核 Linux 5.8 版本， 当前已将放宽至 100 万指令。</p>
</li>
</ul>
<p>优点：<br>1.速度和性能。 内核态进行，速度和效率高。<br>2.灵活：无需修改内核代码，即可扩展内核功能拥有无限想象空间。<br>3.低侵入性：基于eBPF实现链路追踪、服务治理等场景不需要侵入用户层。</p>
<p>缺点：<br>1.eBPF本身一些特性和能力依赖新版本内核。<br>2.学习成本高，需要对Linux Kernel和操作系统原理有深入了解。</p>
<h3 id="目前行业落地情况"><a href="#目前行业落地情况" class="headerlink" title="目前行业落地情况"></a><a href="#%E7%9B%AE%E5%89%8D%E8%A1%8C%E4%B8%9A%E8%90%BD%E5%9C%B0%E6%83%85%E5%86%B5" title="目前行业落地情况"></a>目前行业落地情况</h3><p>应用</p>
<ul>
<li><p>动态追踪：bcc、bpftrace</p>
</li>
<li><p>观测监控：Pixie、Hubble</p>
</li>
<li><p>网络：Cilium、Katran</p>
</li>
<li><p>安全：Falco、Tracee</p>
<h3 id="能解决什么问题"><a href="#能解决什么问题" class="headerlink" title="能解决什么问题"></a><a href="#%E8%83%BD%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98" title="能解决什么问题"></a>能解决什么问题</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ebfp1_4.png"></p>
</li>
</ul>
<h3 id="为什么需要eBPF能实现可观测性"><a href="#为什么需要eBPF能实现可观测性" class="headerlink" title="为什么需要eBPF能实现可观测性"></a><a href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81eBPF%E8%83%BD%E5%AE%9E%E7%8E%B0%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7" title="为什么需要eBPF能实现可观测性"></a>为什么需要eBPF能实现可观测性</h3><p>eBPF可观测性-指标采集<br>eBPF除了常规的指标监控如CPU、内存等，还可以监控细粒度的系统调用等信息，通过内核Kprobe或者Tracepoint实现;<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ebfp1_5.png"><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ebfp1_6.png"></p>
<p>eBPF可观测性-链路追踪<br>与传统APM相比，eBPF进行链路追踪不需要与业务本身进行绑定。通过拦截sock相关的send&#x2F;recv操作，解析协议头，获得进程之间的调用关系，可进一步关联Kubernetes元数据，获得容器、服务之间的调用关系;<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ebfp1_7.png"></p>
<h3 id="展望未来"><a href="#展望未来" class="headerlink" title="展望未来"></a><a href="#%E5%B1%95%E6%9C%9B%E6%9C%AA%E6%9D%A5" title="展望未来"></a>展望未来</h3><p>1、基于eBPF的服务网格<br>去除每个pod的sidecar，内核态实现服务治理（cilium1.12已实现)<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ebfp1_8.png"></p>
<p>2、基于eBPF的负载均衡器<br>利用 socket eBPF，可以在不用直接处理报文和NAT 转换的前提下，实现了负载均衡逻辑。Service网络 POD&lt;–&gt; Service &lt;–&gt; POD优化成 POD &lt;–&gt; POD，从而使Service网络性能基本等同于POD 网络。软件结构如下：<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ebfp1_9.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ebfp1_10.png"></p>
<p>3、基于eBPF的网络安全策略<br>不再依赖 iptables，不需要创建巨量的 iptables rule，从而显著降低 iptables 带来的性能影响。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ebfp1_11.png"></p>
<p>参考链接：</p>
<p><a href="https://mp.weixin.qq.com/s/Xr8ECrS_fR3aCT1vKJ9yIg">https://mp.weixin.qq.com/s/Xr8ECrS_fR3aCT1vKJ9yIg</a><br><a href="https://mp.weixin.qq.com/s?__biz=Mzg5Mjc3MjIyMA==&mid=2247544625&idx=2&sn=7ba07582e0b7fdc0ff3179f2fa2b44d4&source=41#wechat_redirect">https://mp.weixin.qq.com/s?__biz&#x3D;Mzg5Mjc3MjIyMA&#x3D;&#x3D;&amp;mid&#x3D;2247544625&amp;idx&#x3D;2&amp;sn&#x3D;7ba07582e0b7fdc0ff3179f2fa2b44d4&amp;source&#x3D;41#wechat_redirect</a><br><a href="https://blog.csdn.net/eBPF_Kindling/article/details/123575619">https://blog.csdn.net/eBPF_Kindling&#x2F;article&#x2F;details&#x2F;123575619</a><br><a href="https://www.51cto.com/article/715674.html">https://www.51cto.com/article/715674.html</a><br><a href="https://blog.csdn.net/m0_46700908/article/details/124464577?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1-124464577-blog-123575619.t5_layer_eslanding_D_0&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1-124464577-blog-123575619.t5_layer_eslanding_D_0&utm_relevant_index=2">https://blog.csdn.net/m0_46700908&#x2F;article&#x2F;details&#x2F;124464577?spm&#x3D;1001.2101.3001.6650.1&amp;utm_medium&#x3D;distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-124464577-blog-123575619.t5_layer_eslanding_D_0&amp;depth_1-utm_source&#x3D;distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-124464577-blog-123575619.t5_layer_eslanding_D_0&amp;utm_relevant_index&#x3D;2</a><br><a href="https://colobu.com/2022/05/22/use-ebpf-to-trace-rpcx-microservices/">https://colobu.com/2022/05/22/use-ebpf-to-trace-rpcx-microservices/</a><br><a href="https://arthurchiao.art/blog/ebpf-and-k8s-zh/">https://arthurchiao.art/blog/ebpf-and-k8s-zh/</a><br><a href="https://zhuanlan.zhihu.com/p/480811707">https://zhuanlan.zhihu.com/p/480811707</a><br><a href="https://github.com/mikeroyal/eBPF-Guide">https://github.com/mikeroyal/eBPF-Guide</a><br><a href="https://zhuanlan.zhihu.com/p/373090595">https://zhuanlan.zhihu.com/p/373090595</a></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>使用DockerFile构建Bare Metal镜像</title>
    <url>/2022/12/26/elemental/</url>
    <content><![CDATA[<h3 id="Mutable和Immutable介绍"><a href="#Mutable和Immutable介绍" class="headerlink" title="Mutable和Immutable介绍"></a>Mutable和Immutable介绍</h3><p>云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式API。容器技术的最大创造就是通过Dockerfile将应用打包为容器镜像，实现了不可变基础设施，标准化了应用模板。<br>在容器之前叫Mutable（可变的基础设施）在OS上部署应用，重启生效，可以随时进行修改。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/elemental-1.png"><br>容器技术就是Immutable的代表，引入容器镜像，通过Dockerfile将应用标准化打包为容器镜像，通过容器镜像启动容器，无法在容器中进行永久性修改，需要修改只能通过更新Dockerfile方式进行。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/elemental-2.png"></p>
<p>现如今Immutable理念也开始逐步从容器下沉到Bare Metal OS，通过Dockerfile构建Bare Metal镜像，实现Bare Metal OS  Immutable。</p>
<p>典型的开源项目技术Elemental项目</p>
<h3 id="Elemental概述"><a href="#Elemental概述" class="headerlink" title="Elemental概述"></a>Elemental概述</h3><p>Elemental 是一系列工具集合，主要是想通过 Kubernetes 实现集中式、完整的云原生操作系统构建和管理。</p>
<p>集群节点操作系统是通过Elemental CLI通过容器映像构建和维护的，并使用Elemental CLI安装在新主机上。<br>Elemental Operator和Rancher System Agent使Rancher Manager 能够完全控制 Elemental 集群，从在节点上安装和管理操作系统到以集中方式配置新的 K3s 或 RKE2 集群。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/elemental-3.png"></p>
<p>Elemental项目组成</p>
<ul>
<li><p>elemental-toolkit - 包括一组操作系统实用程序，可通过容器启用操作系统管理。包括 dracut 模块、引导加载程序配置、cloud-init 自定义配置服务等。</p>
</li>
<li><p>elemental-operator - 这连接到 Rancher Manager 并处理 machineRegistration 和 machineInventory CRD</p>
</li>
<li><p>elemental-register - 这通过 machineRegistrations 注册机器并通过 elemental-cli 安装</p>
</li>
<li><p>elemental-cli - 这会安装任何基于 elemental-toolkit 的衍生工具。实现OCI容器镜像构建为可在虚拟机、物理机、嵌入式设备运行的ISO镜像。</p>
</li>
<li><p>rancher-system-agent - 在已安装的系统上运行并从 Rancher Manager 获取命令在系统上安装和运行rancher-agent，注册到Rancher中。</p>
</li>
</ul>
<p>项目地址：<a href="https://github.com/rancher/elemental-toolkit">https://github.com/rancher/elemental-toolkit</a></p>
<h3 id="配置使用"><a href="#配置使用" class="headerlink" title="配置使用"></a>配置使用</h3><p>在一台装有Docker的主机上进行</p>
<p>提前准备项：</p>
<ul>
<li>一台安装了Docker的主机</li>
<li>Harbor镜像仓库</li>
<li>EXSI或物理pc、服务器用于build后的ISO测试</li>
</ul>
<p>使用Elemental-toolkit构建ISO流程</p>
<ul>
<li><p>基础base镜像发行版：<br>teal: SLE Micro for Rancher based one, shipping packages from Sle Micro 5.3.<br>green: openSUSE based one, shipping packages from OpenSUSE Leap 15.4 repositories.<br>blue: Fedora based one, shipping packages from Fedora 33 repositories<br>orange: Ubuntu based one, shipping packages form Ubuntu 20.10 repositories</p>
</li>
<li><p>自定义镜像并制作OCI Image</p>
</li>
<li><p>在装有Docker的机器启动Elemental Build<br>UEFI Boot，选择合适的实例类型<br>Clout-init userdata 初始化<br>Default user&#x2F;pass: root&#x2F;cos</p>
</li>
<li><p>升级自定义镜像<br>elemental upgrade –no-verify –reboot -d niusmallnan&#x2F;containeros:dev</p>
</li>
</ul>
<p>在安装了Docker的主机上创建&#x2F;root&#x2F;derivative目录。</p>
<p>整体目录结构</p>
<pre><code>/root/derivative/
├── Dockerfile
├── cloud-init.yaml
├── install.sh
├── installer.sh
├── k3s
├── k3s-airgap-images-amd64.tar.gz
├── manifest.yaml
├── nginx.yaml
├── overlay
│   └── iso
│       └── boot
│           └── grub2
│               └── grub.cfg
└── repositories.yaml
</code></pre>
<p>Demo架构</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/elemental-4.png"></p>
<ul>
<li>通过Elemental构建的OS中包含K3S</li>
<li>将需要部署的应用yaml放置到 &#x2F;var&#x2F;lib&#x2F;rancher&#x2F;k3s&#x2F;server&#x2F;manifests目录，K3S启动成功后会自动部署yaml启动应用。</li>
</ul>
<p>下载K3S离线镜像包和CLI文件<br><a href="https://github.com/k3s-io/k3s/releases">https://github.com/k3s-io/k3s/releases</a></p>
<p>nginx.yaml文件用于k3s启动后加载此yaml文件，模拟演示是个应用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: wanshaoyuan/nginx:v1.0</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: my-service</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  ports:</span><br><span class="line">    - port: 80</span><br><span class="line">      targetPort: 80</span><br><span class="line">      nodePort: 30007</span><br></pre></td></tr></table></figure>



<p>Dockerfile文件创建</p>
<pre><code>ARG LUET_VERSION=0.32.0
FROM quay.io/luet/base:$LUET_VERSION AS luet
FROM registry.suse.com/suse/sle-micro-rancher/5.2
ARG ARCH=amd64
ENV ARCH=$&#123;ARCH&#125;


# Copy the luet config file pointing to the upgrade repository
COPY repositories.yaml /etc/luet/luet.yaml

# Copy luet from the official images
COPY --from=luet /usr/bin/luet /usr/bin/luet
ENV LUET_NOLOCK=true
RUN luet install -y \
       toolchain/yip \
       toolchain/luet \
       utils/installer \
       system/cos-setup \
       system/immutable-rootfs \
       system/grub2-config \
       system/base-dracut-modules

RUN  mkdir /var/lib/rancher/k3s/agent/images/ -p &amp;&amp;  mkdir /var/lib/rancher/k3s/server/manifests -p
COPY install.sh /system/oem/
COPY k3s /usr/local/bin
COPY nginx.yaml /system/oem/
COPY k3s-airgap-images-amd64.tar.gz /system/oem/
RUN  chmod a+x /usr/local/bin/k3s &amp;&amp; chmod a+x /system/oem/install.sh
WORKDIR /system/oem
RUN  INSTALL_K3S_SKIP_START=&quot;true&quot; INSTALL_K3S_SKIP_ENABLE=&quot;true&quot; INSTALL_K3S_SKIP_DOWNLOAD=&quot;true&quot; sh install.sh
## System layout

# Required by k3s etc.
RUN mkdir /usr/libexec &amp;&amp; mkdir /usr/local/bin -p &amp;&amp; touch /usr/libexec/.keep

# Copy custom files
# COPY files/ /

# Copy cloud-init default configuration
COPY cloud-init.yaml /system/oem/

# Generate initrd
RUN mkinitrd

# OS level configuration
RUN echo &quot;VERSION=999&quot; &gt; /etc/os-release
RUN echo &quot;GRUB_ENTRY_NAME=derivative&quot; &gt;&gt; /etc/os-release
RUN echo &quot;welcome to our derivative&quot; &gt;&gt; /etc/issue.d/01-derivative
</code></pre>
<p>cloud-init文件创建，主要用于磁盘分区配置和登录用户名和密码配置<br>cloud-init.yaml</p>
<pre><code>name: &quot;Default settings&quot;
stages:
   initramfs:
     # Setup default hostname
     - name: &quot;Branding&quot;
       hostname: &quot;derivative&quot;
     # Setup an admin group with sudo access
     - name: &quot;Setup groups&quot;
       ensure_entities:
       - entity: |
            kind: &quot;group&quot;
            group_name: &quot;admin&quot;
            password: &quot;x&quot;
            gid: 900            
     # Setup network - openSUSE specific
     - name: &quot;Network setup&quot;
       files:
       - path: /etc/sysconfig/network/ifcfg-eth0
         content: |
                  BOOTPROTO=&#39;dhcp&#39;
                  STARTMODE=&#39;onboot&#39;                  
         permissions: 0600
         owner: 0
         group: 0
     # Setup a custom user
     - name: &quot;Setup users&quot;
       users:
       # Replace the default user name here and settings
        joe:
          # Comment passwd for no password
          passwd: &quot;joe&quot;
          shell: /bin/bash
          homedir: &quot;/home/joe&quot;
          groups:
          - &quot;admin&quot;
       #authorized_keys:
       # Replace here with your ssh keys
       # joe: 
       # - ssh-rsa ....
     # Setup sudo
     - name: &quot;Setup sudo&quot;
       files:
       - path: &quot;/etc/sudoers&quot;
         owner: 0
         group: 0
         permsisions: 0600
         content: |
            Defaults always_set_home
            Defaults secure_path=&quot;/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/bin:/usr/local/sbin&quot;
            Defaults env_reset
            Defaults env_keep = &quot;LANG LC_ADDRESS LC_CTYPE LC_COLLATE LC_IDENTIFICATION LC_MEASUREMENT LC_MESSAGES LC_MONETARY LC_NAME LC_NUMERIC LC_PAPER LC_TELEPHONE LC_ATIME LC_ALL LANGUAGE LINGUAS XDG_SESSION_COOKIE&quot;
            Defaults !insults
            root ALL=(ALL) ALL
            %admin ALL=(ALL) NOPASSWD: ALL
            @includedir /etc/sudoers.d            
       commands:
       - passwd -l root
   # Setup persistency so k3s works properly
   # See also: https://rancher.github.io/elemental-toolkit/docs/reference/immutable_rootfs/#configuration-with-an-environment-file
   rootfs.after:
    - name: &quot;Immutable Layout configuration&quot;
      environment_file: /run/cos/cos-layout.env
      environment:
        VOLUMES: &quot;LABEL=COS_OEM:/oem LABEL=COS_PERSISTENT:/var&quot;
        OVERLAY: &quot;tmpfs:25%&quot;
        RW_PATHS: &quot;/usr/local /etc /srv&quot;
        PERSISTENT_STATE_PATHS: &gt;-
          /etc/systemd
          /etc/rancher
          /etc/ssh
          /etc/iscsi 
          /etc/cni
          /home
          /opt
          /root
          /usr/libexec
          /var/log
          /var/lib/wicked
          /var/lib/longhorn
          /var/lib/cni
          /usr/local/bin
        PERSISTENT_STATE_TARGET: &gt;-
          /etc/systemd
          /etc/rancher
          /etc/ssh
          /etc/iscsi
          /etc/cni
          /home
          /opt
          /root
          /usr/libexec
          /var/log
          /var/lib/kubelet
          /var/lib/wicked
          /var/lib/longhorn
          /var/lib/cni
          /usr/local/bin
        PERSISTENT_STATE_BIND: &quot;true&quot;
   # Finally, let&#39;s start k3s when network is available, and download the SSH key from github for the joe user
   network:
     - name: &quot;Deploy cos-system&quot;
       commands:
         - elemental install /dev/sda 
         - systemctl enable k3s &amp;&amp; systemctl start  k3s
   after-install:
     - name: &quot;install k3s&quot;
       commands:
         - mount /dev/sda5 /var
         - mkdir -p  /var/lib/rancher/k3s/agent/images/  &amp;&amp; mkdir /var/lib/rancher/k3s/server/manifests -p
         - cp /system/oem/k3s-airgap-images-amd64.tar.gz  /var/lib/rancher/k3s/agent/images/
         - cp /system/oem/nginx.yaml /var/lib/rancher/k3s/server/manifests
         - reboot
</code></pre>
<p>创建manifest.yaml文件定义OS启动引导所需要文件</p>
<pre><code>iso:
  rootfs:
    - channel:system/cos
  uefi:
    - channel:live/grub2-efi-image
  image:
    - channel:live/grub2
    - channel:live/grub2-efi-image
  label: &quot;COS_LIVE&quot;

name: &quot;cOS-0&quot;

# Raw disk creation values start
raw_disk:
  x86_64:
    # which packages to install and the target to install them at
    packages:
      - name: channel:system/grub2-efi-image
        target: efi
      - name: channel:system/grub2-config
        target: root
      - name: channel:system/grub2-artifacts
        target: root/grub2
      - name: channel:recovery/cos-img
        target: root/cOS

repositories:
  - uri: quay.io/costoolkit/releases-teal
    arch: &quot;x86_64&quot;
</code></pre>
<p>创建repositories.yaml文件</p>
<pre><code>logging:
  color: false
  enable_emoji: false
general:
   debug: false
   spinner_charset: 9
repositories:
- name: &quot;cos&quot;
  description: &quot;cOS official&quot;
  type: &quot;docker&quot;
  enable: true
  cached: true
  priority: 1
  verify: false
  urls:
  - &quot;quay.io/costoolkit/releases-green&quot;
</code></pre>
<p>创建grub文件配置内核引导<br>在&#x2F;root&#x2F;derivative&#x2F;overlay&#x2F;iso&#x2F;boot&#x2F;目录创建grub2grub.cfg文件</p>
<pre><code>search --no-floppy --file --set=root /boot/kernel
set default=0
set timeout=10
set timeout_style=menu
set linux=linux
set initrd=initrd
if [ &quot;$&#123;grub_cpu&#125;&quot; = &quot;x86_64&quot; -o &quot;$&#123;grub_cpu&#125;&quot; = &quot;i386&quot; -o &quot;$&#123;grub_cpu&#125;&quot; = &quot;arm64&quot; ];then
    if [ &quot;$&#123;grub_platform&#125;&quot; = &quot;efi&quot; ]; then
        if [ &quot;$&#123;grub_cpu&#125;&quot; != &quot;arm64&quot; ]; then
            set linux=linuxefi
            set initrd=initrdefi
        fi
    fi
fi
if [ &quot;$&#123;grub_platform&#125;&quot; = &quot;efi&quot; ]; then
    echo &quot;Please press &#39;t&#39; to show the boot menu on this console&quot;
fi
set font=($root)/boot/$&#123;grub_cpu&#125;/loader/grub2/fonts/unicode.pf2
if [ -f $&#123;font&#125; ];then
    loadfont $&#123;font&#125;
fi
menuentry &quot;cOS&quot; --class os --unrestricted &#123;
    echo Loading kernel...
    $linux ($root)/boot/kernel.xz cdroot root=live:CDLABEL=COS_LIVE rd.live.dir=/ rd.live.squashimg=rootfs.squashfs console=tty1 console=ttyS0 rd.cos.disable
    echo Loading initrd...
    $initrd ($root)/boot/rootfs.xz
&#125;

if [ &quot;$&#123;grub_platform&#125;&quot; = &quot;efi&quot; ]; then
    hiddenentry &quot;Text mode&quot; --hotkey &quot;t&quot; &#123;
        set textmode=true
        terminal_output console
    &#125;
fi
</code></pre>
<p>先构建镜像</p>
<pre><code>docker build -t 172.16.1.208/library/example:v4.0 .
</code></pre>
<p>镜像要上传到镜像仓库才能build iso</p>
<pre><code>docker push 172.16.1.208/library/example:v4.0
</code></pre>
<p>构建ISO</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --rm -ti -v $(pwd):/build quay.io/costoolkit/elemental-cli:v0.0.15-ae4f000	--config-dir /build --overlay-iso /build/overlay/iso --debug build-iso -o /build 172.16.1.208/library/example:v4.0</span><br></pre></td></tr></table></figure>
<p>注：目前只支持公开的镜像仓库，不支持私有的镜像仓库<br><a href="https://github.com/rancher/elemental-cli/issues/389">https://github.com/rancher/elemental-cli/issues/389</a></p>
<p>构建完成，生成此cOS-0.iso镜像文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">o250800372/iso / -chmod 0755 -- -boot_image grub bin_path=/boot/x86_64/loader/eltorito.img -boot_image grub grub2_mbr=/tmp/elemental-iso250800372/iso//boot/x86_64/loader/boot_hybrid.img -boot_image grub grub2_boot_info=on -boot_image any partition_offset=16 -boot_image any cat_path=/boot/x86_64/boot.catalog -boot_image any cat_hidden=on -boot_image any boot_info_table=on -boot_image any platform_id=0x00 -boot_image any emul_type=no_emulation -boot_image any load_size=2048 -append_partition 2 0xef /tmp/elemental-iso250800372/iso/boot/uefi.img -boot_image any next -boot_image any efi_path=--interval:appended_partition_2:all:: -boot_image any platform_id=0xef -boot_image any emul_type=no_emulation&#x27; </span><br><span class="line">DEBU[2023-03-12T11:54:38Z] Xorriso: xorriso 1.4.6 : RockRidge filesystem manipulator, libburnia project.</span><br><span class="line"></span><br><span class="line">Drive current: -outdev &#x27;/build/cOS-0.iso&#x27;</span><br><span class="line">Media current: stdio file, overwriteable</span><br><span class="line">Media status : is blank</span><br><span class="line">Media summary: 0 sessions, 0 data blocks, 0 data, 5851m free</span><br><span class="line">xorriso : UPDATE : 623 files added in 1 seconds</span><br><span class="line">Added to ISO image: directory &#x27;/&#x27;=&#x27;/tmp/elemental-iso250800372/iso&#x27;</span><br><span class="line">xorriso : NOTE : Copying to System Area: 512 bytes from file &#x27;/tmp/elemental-iso250800372/iso/boot/x86_64/loader/boot_hybrid.img&#x27;</span><br><span class="line">xorriso : UPDATE : Writing:      24576s    6.5%   fifo 100%  buf  50%</span><br><span class="line">xorriso : UPDATE : Writing:     221184s   58.1%   fifo 100%  buf  50%  415.0xD </span><br><span class="line">ISO image produced: 380645 sectors</span><br><span class="line">Written to medium : 380656 sectors at LBA 48</span><br><span class="line">Writing to &#x27;/build/cOS-0.iso&#x27; completed successfully.</span><br></pre></td></tr></table></figure>

<p>将cOS-0.iso下载到ESXI或其他虚拟化平台也可以刻录U盘直接安装物理机。</p>
<p>配置选4c4G 60G磁盘</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/elemental-5.png"><br>加载ISO后自动分区，自动进行初始化，安装系统，完成后自动重启进入系统。</p>
<p>密码ssh账号密码joe&#x2F;joe</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/elemental-6.png"><br>在安装后的系统查看已经部署好的K3S。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/elemental-7.png"></p>
<p>查看自动部署的应用</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/elemental-8.png"><br>访问应用</p>
<p>因为整个系统都限制了修改，所以在操作系统任何目录执行修改命令都无法修改。如</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rm -rf *</span><br><span class="line"></span><br><span class="line">evice or resource busy</span><br><span class="line">rm: cannot remove &#x27;var/lib/kubelet/pods/cbf59b3a-d29a-4129-a3c9-8b79b1235104/volumes/kubernetes.io~projected/kube-api-access-zf8c5&#x27;: Device or resource busy</span><br><span class="line">rm: cannot remove &#x27;var/lib/kubelet/pods/ea697a4c-8cb8-425f-8e50-6396f5669167/volumes/kubernetes.io~projected/kube-api-access-bq66h&#x27;: Device or resource busy</span><br><span class="line">rm: cannot remove &#x27;var/lib/kubelet/pods/f18cd482-4c6f-4dd0-80fa-5fc314d3cc5b/volumes/kubernetes.io~projected/kube-api-access-8fdq7&#x27;: Device or resource busy</span><br><span class="line">rm: cannot remove &#x27;var/lib/longhorn&#x27;: Device or resource busy</span><br><span class="line">rm: cannot remove &#x27;var/lib/wicked&#x27;: Device or resource busy</span><br><span class="line">rm: cannot remove &#x27;var/log&#x27;: Device or resource busy</span><br></pre></td></tr></table></figure>


<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">touch  1</span><br><span class="line">touch: cannot touch &#x27;1&#x27;: Read-only file system</span><br></pre></td></tr></table></figure>




<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>通过Elemental实现了操作系统为不变基础设施，同时也可以将我们传统的OS带入云原生，通过Dockerfile去构建，通过CICD去统一发版维护，目前能想到的一个比较大的应用场景在于，一个是边缘场景，边缘设备操作系统批量部署安装。另外就是一些to b的客户将自己业务+容器编排和OS通过Elemental构建打包，直接到客户现场加载ISO就部署完了，开箱即用。另外OS也可以标准化，统一化管理。</p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>eBPF学习摘要3-XDP学习和理解</title>
    <url>/2022/10/17/ebpf_3/</url>
    <content><![CDATA[<h2 id="eBPF学习摘要3-XDP学习和理解"><a href="#eBPF学习摘要3-XDP学习和理解" class="headerlink" title="eBPF学习摘要3-XDP学习和理解"></a>eBPF学习摘要3-XDP学习和理解</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a><a href="#%E6%A6%82%E8%BF%B0" title="概述"></a>概述</h3><p>Linux网络数据包接收数据包的链路为Nic——&gt;rx_ring——&gt;skbuff——&gt;网络协议处理（如ip_recv)这样做的问题在于会产生大量内核态到用户态的切换过程，这会造成大量性能消耗，所以为了提升网络性能才诞生出Kernel bypass的技术如DPDK、SolarFlare技术，像DPDK就是直接饶过内核态，用户态应用直接访问网络硬件提高数据包处理效率，降低因为切换带来的损耗，但这种方式本身也存在一些缺点：1、绕开了内核态，很难与Linux操作系统中内核态本身存在的一些工具集成，一些功能需要重新开发。2、需要单独cpu核参与处理。<br>XDP（eXpress Data Path）是一个eBPF hook，可以在内核中执行eBPF程序实现对网络数据包处理，在Linux内核 4.8 版中引入。实现方式与Kernel bypass完全相反在sk_buffer之前数据包从driver出来以后就可以直接被XDP程序捕获执行，极大提升了网络数据包的处理效率。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ebfp3_1.png"></p>
<h3 id="实现方式"><a href="#实现方式" class="headerlink" title="实现方式"></a><a href="#%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F" title="实现方式"></a>实现方式</h3><p>如下图所示<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ebfp3_2.png"><br>1、数据包通过网卡,触发XDP执行<br>2、xdp程序执行读取BPF mps配置的规则对数据包执行相应的操作，通常为<br>（1）XDP_DROP：直接丢弃，不占用CPU资源，有效防止DDOS<br>（2）XDP_Allow：正常转发到内核网络栈<br>（3）XDP_REDIRECT:重定向到其他网卡，或通过AF_XDP直接发送到用户空间。<br>（4）XDP_TX:将处理后的包发给相同的网卡。</p>
<p>三种处理模式:</p>
<p>XDP在网络栈中有三个处理点：<br>offloaded模式的XDP：对于支持可编程的网卡，直接在网卡上运行XDP程序。<br>Native模式的XDP：默认模式、对于支持的网卡驱动，可以在包到达内核后立刻进行处理。（目前大部分网卡已经支持）</p>
<p>Offloaded和Native模式</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ebfp3_3.png"></p>
<p>Generic模式的XDP：网卡和驱动不支持上述两种情况的XDP时，可以在receive_skb函数此点进行处理。这个处理的位置相对靠后，在tc处理点之前，这种性能最差，一般用于测试调试模式。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ebfp3_4.png"></p>
<h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a><a href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF" title="应用场景"></a>应用场景</h3><ul>
<li>负载均衡器：通过XDP_TX和XDP_TX实现数据包的快速转发，目前很多k8s网络插件取代kube-proxy实现Service负载均衡器就是如此。Facebook的全部流量都是经过基于XDP的四层负载均衡器（katran）处理转发（<a href="https://lpc.events/event/11/contributions/950/attachments/889/1704/lpc_from_xdp_to_socket_fb.pdf%EF%BC%89">https://lpc.events/event/11/contributions/950/attachments/889/1704/lpc_from_xdp_to_socket_fb.pdf）</a></li>
<li>防火墙：Cloudflare在他们的DDoS防御L4Drop中使用用了XDP无需高CPU占用就可提供高性能丢包率（<a href="https://blog.cloudflare.com/how-to-drop-10-million-packets/%EF%BC%89">https://blog.cloudflare.com/how-to-drop-10-million-packets/）</a></li>
<li>流量监控和采样：位于内核网络栈前端，通过自定义的eBPF程序即可实现对网络流量的采样，目前很多基于eBPF的APM就是这样实现的。</li>
</ul>
<h3 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a><a href="#%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95" title="性能测试"></a>性能测试</h3><p><a href="https://people.netfilter.org/hawk/presentations/KernelRecipes2018/XDP_Kernel_Recipes_2018.pdf">https://people.netfilter.org/hawk/presentations/KernelRecipes2018/XDP_Kernel_Recipes_2018.pdf</a><br><a href="http://vger.kernel.org/lpc_net2018_talks/lpc18-xdp-future.pdf">http://vger.kernel.org/lpc_net2018_talks&#x2F;lpc18-xdp-future.pdf</a><br><a href="https://blog.cloudflare.com/how-to-drop-10-million-packets/">https://blog.cloudflare.com/how-to-drop-10-million-packets/</a><br><a href="https://blog.csdn.net/hbhgyu/article/details/109354273">https://blog.csdn.net/hbhgyu/article/details/109354273</a><br>这里面包含了对丢包性能、转发性能、DDos防御能力的测试。</p>
<h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a><a href="#%E6%80%BB%E7%BB%93%EF%BC%9A" title="总结："></a>总结：</h3><p>随着eBPF技术的持续发展，XDP能够实现DPDK相近的性能，但又更具有兼容性和灵活性，未来会得到越来越好的发展，围绕eBPF和XDP的生态软件也会越来越丰富。</p>
<p>参考链接：<br><a href="https://zhuanlan.zhihu.com/p/453005342">https://zhuanlan.zhihu.com/p/453005342</a><br><a href="https://zhuanlan.zhihu.com/p/438158551">https://zhuanlan.zhihu.com/p/438158551</a><br><a href="https://mp.weixin.qq.com/s/H9imUbdJnfj1NKdK9jtxEw">https://mp.weixin.qq.com/s/H9imUbdJnfj1NKdK9jtxEw</a><br><a href="https://zhuanlan.zhihu.com/p/321387418">https://zhuanlan.zhihu.com/p/321387418</a><br><a href="https://mp.weixin.qq.com/s/lUvxUkFg4w1X0ioktxGiHA">https://mp.weixin.qq.com/s/lUvxUkFg4w1X0ioktxGiHA</a><br><a href="https://www.seekret.io/blog/a-gentle-introduction-to-xdp/">https://www.seekret.io/blog/a-gentle-introduction-to-xdp/</a></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>ETCD集群的备份与还原</title>
    <url>/2019/01/31/etcd_backup/</url>
    <content><![CDATA[<h3 id="概述："><a href="#概述：" class="headerlink" title="概述："></a>概述：</h3><p>etcd做为一个分布式键值数据库，在Kubernetes集群中是一个非常重要的组件，用于保存Kubernetes的网络配置信息和Kubernetes对象的状态信息，一旦etcd挂了，那么也就意味着整个集群就不可用了，所以在实际情况生产环境中我们会部署多个etcd节点，组成etcd高可用集群，在etcd高可用集群中可挂节点数为(n-1)&#x2F;2个，比如你有3个节点做etcd-ha那么最多可以挂掉(3-1)&#x2F;2个节点，虽然有HA集群可以保证一定的高可用，但在实际生产环境中还是建议将etcd数据进行备份，方便在出现问题时进行还原。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/etcd_back_1.png"></p>
<p>环境信息：<br>OS：ubuntu16.04<br>docker：17.03-2<br>Kubernetes：v1.11.6<br>etcd：v3.2.18<br>Rancher：v2.1.5</p>
<p>节点信息与角色：<br>172.31.164.58   control worker etcd<br>172.31.164.59   control worker etcd<br>172.31.164.139   control worker etcd</p>
<p>操作：<br>以Rancher2.0部署的Kubernetes集群为例，其他部署工具部署的也是一样。</p>
<h3 id="etcd备份"><a href="#etcd备份" class="headerlink" title="etcd备份"></a>etcd备份</h3><p>备份步骤<br>1、 如果是使用Rancher部署的Kubernetes集群可以直接使用rancher自动备份</p>
<p>Rancherv2.1.5 在部署集群的高级选项内是可以配置etcd的自动备份的，按照配置的备份策略会进行自动备份，默认的备份策略是每12小时备份一次，备份文件的轮转周期是72小时，避免因备份文件过多占用磁盘空间.也可以根据自己实际需求进行调整。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/etcd_back_2.png"></p>
<p>备份的文件放置在master节点宿主机的&#x2F;opt&#x2F;rke&#x2F;etcd-snapshots目录下以etcd结尾的就是etcd就是备份文件，pki.bundle.tar.gz文件是对应的集群证书的备份。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/etcd_back_3.png"></p>
<p>Rancher部署的Kubernetes集群etcd是以容器化的方式运行要使用etcdctl命令有两种方式<br>1、etcd容器内执行etcdctl;docker exec -it etcd sh<br>2、或直接在宿主机上安装一个etcd不用启动，只是使用它的etcdctl命令,那可以通过<a href="https://github.com/etcd-io/etcd/releases%E4%B8%8B%E8%BD%BD%E5%AF%B9%E5%BA%94%E7%9A%84etcd%E7%89%88%E6%9C%AC%E3%80%82">https://github.com/etcd-io/etcd/releases下载对应的etcd版本。</a></p>
<p>推荐使用方式二在宿主机上安装etcd使用etcdctl执行，因为在还原时需要将etcd关闭，还是需要准备etcdctl命令<br>执行以下命令</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export ETCDCTL_API=3</span><br><span class="line">etcdctl --endpoints=https://172.31.164.58:2379 --cert=/etc/kubernetes/ssl/kube-etcd-172-31-164-58.pem --key=/etc/kubernetes/ssl/kube-etcd-172-31-164-58-key.pem --cacert=/etc/kubernetes/ssl/kube-ca.pem snapshot save  /var/lib/rancher/etcd/etcd_2019_1_31.db</span><br></pre></td></tr></table></figure>

<p>1、&#x2F;var&#x2F;lib&#x2F;rancher&#x2F;etcd这个目录是根宿主机的&#x2F;var&#x2F;lib&#x2F;etcd&#x2F;目录相映射的,所以备份在这个目录在对应的宿主机上也是能看见的。<br>2、这些证书对应文件可以直接在etcd容器内通过ps aux|more看见</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/etcd_back_4.png"></p>
<p>其中–cert-file对应–cert，–key对应–key-file –cacert对应–trusted-ca-file</p>
<p>集群方式备份根上述一样</p>
<h3 id="etcd还原"><a href="#etcd还原" class="headerlink" title="etcd还原"></a>etcd还原</h3><h4 id="单节点还原"><a href="#单节点还原" class="headerlink" title="单节点还原"></a>单节点还原</h4><p>1、将kube-api-server关闭<br>2、将etcd关闭<br>在宿主机上执行<br>将现有的etcd目录移除</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mv /var/lib/etcd /var/lib/etcd.bak</span><br></pre></td></tr></table></figure>
<p>执行还原</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export ETCDCTL_API=3</span><br><span class="line">etcdctl snapshot restore  etcd_2019_1_31.db \</span><br><span class="line">    --endpoints=172.31.164.58:2379 \</span><br><span class="line">    --name=etcd-rke-node2 \</span><br><span class="line">    --cert=/etc/kubernetes/ssl/kube-etcd-172-31-164-58.pem \</span><br><span class="line">    --key=/etc/kubernetes/ssl/kube-etcd-172-31-164-58-key.pem \</span><br><span class="line">    --cacert=/etc/kubernetes/ssl/kube-ca.pem \</span><br><span class="line">    --initial-advertise-peer-urls=https://172.31.164.58:2380 \</span><br><span class="line">    --initial-cluster-token=etcd-cluster-1 \</span><br><span class="line">    --initial-cluster=etcd-rke-node2=https://172.31.164.58:2380 \</span><br><span class="line">    --data-dir=/var/lib/etcd</span><br></pre></td></tr></table></figure>
<p>3、启动etcd</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker start etcd</span><br></pre></td></tr></table></figure>
<p>4、启动kube-apiserver</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker start kube-apiserver</span><br></pre></td></tr></table></figure>
<p>5、检查数据是否还原<br>使用kubectl查看资源对象是否已恢复或直接get all</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get all --all-namespace</span><br></pre></td></tr></table></figure>
<h4 id="HA环境下还原（多个etcd节点"><a href="#HA环境下还原（多个etcd节点" class="headerlink" title="HA环境下还原（多个etcd节点)"></a>HA环境下还原（多个etcd节点)</h4><p>将一个节点上的备份数据可以拷贝到其他全部节点，所有节点可以使用同一份数据进行恢复。</p>
<p>1、查看etcd集群信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export ETCDCTL_API=3</span><br><span class="line">etcdctl --endpoints=https://172.31.164.58:2379 --cert=/etc/kubernetes/ssl/kube-etcd-172-31-164-58.pem --key=/etc/kubernetes/ssl/kube-etcd-172-31-164-58-key.pem --cacert=/etc/kubernetes/ssl/kube-ca.pem member list</span><br><span class="line"></span><br><span class="line">2019-01-31 17:34:55.080375 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated</span><br><span class="line">abfc00bfbb85856d, started, etcd-rke-node3, https://172.31.164.59:2380, https://172.31.164.59:2379,https://172.31.164.59:4001</span><br><span class="line">cb7997809d89fa30, started, etcd-rke-node2, https://172.31.164.58:2380, https://172.31.164.58:2379,https://172.31.164.58:4001</span><br><span class="line">d3f1ec5a741e4dd1, started, etcd-rke-node4, https://172.31.164.139:2380, https://172.31.164.139:2379,https://172.31.164.139:4001</span><br></pre></td></tr></table></figure>


<p>2、关闭集群的全部api-server和etcd</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker stop etcd &amp;&amp; docker stop kube-apiserver</span><br></pre></td></tr></table></figure>

<p>3、将集群内宿主机现有的etcd目录移除</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mv /var/lib/etcd /var/lib/etcd.bak</span><br></pre></td></tr></table></figure>

<p>4、恢复数据<br>在172.31.164.58上执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export ETCDCTL_API=3</span><br><span class="line">etcdctl snapshot restore  etcd_2019_1_31.db \</span><br><span class="line">    --endpoints=172.31.164.58:2379 \</span><br><span class="line">    --name=etcd-rke-node2 \</span><br><span class="line">    --cert=/etc/kubernetes/ssl/kube-etcd-172-31-164-58.pem \</span><br><span class="line">    --key=/etc/kubernetes/ssl/kube-etcd-172-31-164-58-key.pem \</span><br><span class="line">    --cacert=/etc/kubernetes/ssl/kube-ca.pem \</span><br><span class="line">    --initial-advertise-peer-urls=https://172.31.164.58:2380 \</span><br><span class="line">    --initial-cluster-token=etcd-cluster-1 \</span><br><span class="line">    --initial-cluster=etcd-rke-node4=https://172.31.164.139:2380,etcd-rke-node3=https://172.31.164.59:2380,etcd-rke-node2=https://172.31.164.58:2380 \</span><br><span class="line">    --data-dir=/var/lib/etcd</span><br></pre></td></tr></table></figure>

<p>在172.31.164.59上执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export ETCDCTL_API=3</span><br><span class="line">etcdctl snapshot restore  etcd_2019_1_31.db \</span><br><span class="line">    --endpoints=172.31.164.59:2379 \</span><br><span class="line">    --name=etcd-rke-node3 \</span><br><span class="line">    --cert=/etc/kubernetes/ssl/kube-etcd-172-31-164-58.pem \</span><br><span class="line">    --key=/etc/kubernetes/ssl/kube-etcd-172-31-164-58-key.pem \</span><br><span class="line">    --cacert=/etc/kubernetes/ssl/kube-ca.pem \</span><br><span class="line">    --initial-advertise-peer-urls=https://172.31.164.59:2380 \</span><br><span class="line">    --initial-cluster-token=etcd-cluster-1 \</span><br><span class="line">    --initial-cluster=etcd-rke-node4=https://172.31.164.139:2380,etcd-rke-node3=https://172.31.164.59:2380,etcd-rke-node2=https://172.31.164.58:2380 \</span><br><span class="line">    --data-dir=/var/lib/etcd</span><br></pre></td></tr></table></figure>

<p>在172.31.164.139上执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export ETCDCTL_API=3</span><br><span class="line">etcdctl snapshot restore  etcd_2019_1_31.db \</span><br><span class="line">    --endpoints=172.31.164.139:2379 \</span><br><span class="line">    --name=etcd-rke-node4 \</span><br><span class="line">    --cert=/etc/kubernetes/ssl/kube-etcd-172-31-164-58.pem \</span><br><span class="line">    --key=/etc/kubernetes/ssl/kube-etcd-172-31-164-58-key.pem \</span><br><span class="line">    --cacert=/etc/kubernetes/ssl/kube-ca.pem \</span><br><span class="line">    --initial-advertise-peer-urls=https://172.31.164.139:2380 \</span><br><span class="line">    --initial-cluster-token=etcd-cluster-1 \</span><br><span class="line">    --initial-cluster=etcd-rke-node4=https://172.31.164.139:2380,etcd-rke-node3=https://172.31.164.59:2380,etcd-rke-node2=https://172.31.164.58:2380 \</span><br><span class="line">    --data-dir=/var/lib/etcd</span><br></pre></td></tr></table></figure>

<p>5、检查数据是否恢复</p>
<p>使用kubectl查看资源对象是否已恢复或直接get all</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get all --all-namespace</span><br></pre></td></tr></table></figure>

<p>以上操作也可以写成一个自动化备份的脚本，但若使用Rancher部署的集群有自动定时备份更方便。</p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Fastnetmon配置与使用</title>
    <url>/2016/12/09/fastnetmon/</url>
    <content><![CDATA[<h1 id="Fastnetmon"><a href="#Fastnetmon" class="headerlink" title="Fastnetmon"></a>Fastnetmon</h1><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>FastNetmon是一个基于多种抓包引擎来对数据包进行统计分析的DOS&#x2F;DDOS工具，可以探测和分析网络中的异常流量情况，同时，也可以对捕获的异常调用外部脚本进行处理警报啊或者进行阻断处理，全靠外部脚本是如何定义。  </p>
<ul>
<li>项目首页<a href="http://www.open-open.com/lib/view/home/143193107973">http://www.open-open.com/lib/view/home/143193107973</a>  </li>
<li>部署架构 <img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/fastnetmon.png"></li>
</ul>
<h3 id="安装和配置方法"><a href="#安装和配置方法" class="headerlink" title="安装和配置方法"></a>安装和配置方法</h3><h5 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h5><ul>
<li><p>下载自动安装脚本<br>wget <a href="https://raw.githubusercontent.com/FastVPSEestiOu/fastnetmon/master/src/fastnetmon_install.pl">https://raw.githubusercontent.com/FastVPSEestiOu/fastnetmon/master/src/fastnetmon_install.pl</a> -Ofastnetmon_install.pl</p>
</li>
<li><p>将整个项目克隆下来<br><a href="https://github.com/FastVPSEestiOu/fastnetmon">https://github.com/FastVPSEestiOu/fastnetmon</a></p>
</li>
<li><p>安装<br>注意若安装报连接错误，请将DNS改为8.8.8.8或223.5.5.5，因为下载地址都是在国外，有些DNS可能解析不够好。<br>perl fastnetmon_install.pl 启动安装脚本。</p>
</li>
</ul>
<h5 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h5><ul>
<li>配置文件，cd到你刚刚克隆的项目里面。<br>  cp src&#x2F;Fastnetmon.conf 到&#x2F;etc&#x2F;<br>  cp src&#x2F;fastnetmon_init_script_centos6 &#x2F;etc&#x2F;init.d&#x2F;fastnetmon #cp启动脚本到&#x2F;etc&#x2F;init.d&#x2F;<br>  chmod 755 &#x2F;etc&#x2F;init.d&#x2F;fastnetmon     #修改启动脚本权限<br>  cp src&#x2F;notify_about_attack.sh  &#x2F;usr&#x2F;local&#x2F;bin&#x2F;   #cp 通知脚本  </li>
<li>&#x2F;etc&#x2F;fastentmon配置项<br>  ban_time &#x3D; 1900 #对检测到攻击的ip进行多久封锁。<br>  enable_subnet_counters &#x3D; on #检测每个子网进出流量<br>  enable_connection_tracking &#x3D; on #开启攻击追踪检测，通过这个选项在日志文件里可以详细看见攻击者ip和其他一些详细情况<br>  ban_for_pps &#x3D; on，ban_for_bandwidth &#x3D; on，ban_for_flows &#x3D; on #检测的选项pps(每秒包)，bandwidth(带宽)，flows(流量)。<br>  threshold_pps &#x3D; 20000，threshold_mbps &#x3D; 1000，threshold_flows &#x3D; 3500 #监控的限定值<br>  抓包引擎选择<br>  mirrof&#x3D;off<br>  没有安装PF_RING就不要开启，不然会启动报错。<br>  pfring_sampling_ratio &#x3D; 1 #端口镜像采样率<br>  mirror_netmap &#x3D; off<br>  没有安装Netmap就不要开启，不然会会启动报错。<br>  mirror_snabbswitch&#x3D;on  #开启snabbswitch流量捕获。<br>  mirror_afpacket &#x3D;on #AF_PACKET捕获引擎开启<br>  netmap_sampling_ratio &#x3D; 1 #端口镜像抽样比<br>  pcap&#x3D;on #pcap引擎开启<br>  netflopw&#x3D;on  #使用Netflow捕获方法<br>  interfaces&#x3D;enp5s0f1 #监控的端口，我这里使用的是镜像端口,不然监控不到整个网端流量<br>  notify_script_path&#x3D;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;notify_about_attack.sh #触发脚本位置<br>  monitor_local_ip_addresses &#x3D; on #监控本地地址<br>  sort_parameter &#x3D; packets #在控制台排序单位，包<br>  max_ips_in_list &#x3D;400 #在控制台显示多少地址</li>
</ul>
<h5 id="编辑要监控的网段"><a href="#编辑要监控的网段" class="headerlink" title="编辑要监控的网段"></a>编辑要监控的网段</h5><p>  vim &#x2F;etc&#x2F;networks_list #编辑networks_list加入要监控的网段这里我加入<br>   211.156.182.0&#x2F;24 220.242.2.0&#x2F;24<br> &#x2F;etc&#x2F;init.d&#x2F;fastnetmon start #启动fastentmon，启动失败查看&#x2F;var&#x2F;log&#x2F;fastnetmon.log<br>  &#x2F;opt&#x2F;fastnetmon&#x2F;fastnetmon #打开监控控制台  </p>
<h5 id="修改监控脚本"><a href="#修改监控脚本" class="headerlink" title="修改监控脚本"></a>修改监控脚本</h5><p>1,要先外发邮件必须配置mailx</p>
<ul>
<li>安装mailx，配置SMTP vim &#x2F;etc&#x2F;mail.rc  我这里配置的是我163的邮箱<br>   set bsdcompat<br>   set from&#x3D;<a href="mailto:&#120;&#120;&#120;&#120;&#120;&#x40;&#x31;&#x36;&#51;&#46;&#99;&#x6f;&#x6d;">&#120;&#120;&#120;&#120;&#120;&#x40;&#x31;&#x36;&#51;&#46;&#99;&#x6f;&#x6d;</a> smtp&#x3D;smtp.163.com<br>  set smpt-auth-user&#x3D;<a href="mailto:&#x78;&#x78;&#x78;&#120;&#x40;&#x31;&#54;&#51;&#46;&#x63;&#111;&#x6d;">&#x78;&#x78;&#x78;&#120;&#x40;&#x31;&#54;&#51;&#46;&#x63;&#111;&#x6d;</a>  set smtp-auth-user&#x3D;<a href="mailto:&#x78;&#x78;&#x78;&#x78;&#120;&#x78;&#64;&#x31;&#x36;&#51;&#x2e;&#99;&#x6f;&#x6d;">&#x78;&#x78;&#x78;&#x78;&#120;&#x78;&#64;&#x31;&#x36;&#51;&#x2e;&#99;&#x6f;&#x6d;</a>  smtp-auth-password&#x3D;xxxxxx smtp-auth&#x3D;login</li>
<li>vim &#x2F;usr&#x2F;local&#x2F;bin&#x2F;notify_about_attack.sh #这里填要触发监控的脚本<br>   修改邮件地址为接受人。<br>cat | mail -s “FastNetMon Guard: IP $1 blocked because $2 attack with power $3 pps” $email_notify;</li>
</ul>
<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><p>使用低轨道等离子炮（Loic）进行测试这里我对我们211.156.182.143进行Tcp DDOS攻击<br>我们先把这些值调低<br>threshold_pps &#x3D; 2000，threshold_mbps &#x3D; 100，threshold_flows &#x3D; 350 #监控的限定值<br>然后重启Fastnetmon<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/fastnetmon2.png"><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/fastnetmon3.png">  </p>
<p>测试完修改回值</p>
]]></content>
      <categories>
        <category>安全</category>
      </categories>
      <tags>
        <tag>安全</tag>
      </tags>
  </entry>
  <entry>
    <title>eBPF学习摘要2—工具使用(bpftrace）</title>
    <url>/2022/09/18/ebpf_2/</url>
    <content><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a><a href="#%E6%A6%82%E8%BF%B0" title="概述"></a>概述</h3><p>bpftrace是基于eBPF实现的动态的工具，使用DSL（Domain Specific Language）编写eBPF程序，使用LLVM编译eBPF字节码，BCC与LinuxBPF系统交互。直接使用DSL编写好的脚本（类似awk语言）可以执行，无需在内核中手动编译和加载。bpftrace在内核中实现动态追踪主要是使用Kprobe探针和Tracepoints探针。使用bpftrace可以更深入的进行操作系统上问题排查，如某个函数的调用次数和延时、追踪系统OOMKILL、TCP连接丢包等。都可以自定义脚本实现。<br>另外还有一个叫BCC的项目，跟bpftrace区别是BCC可以使用高级语言开发ebfp程序，如Java、Python、Lua……</p>
<p><a href="https://github.com/iovisor/bpftrace">https://github.com/iovisor/bpftrace</a><br><a href="https://github.com/iovisor/bcc">https://github.com/iovisor/bcc</a></p>
<h3 id="安装和基础使用"><a href="#安装和基础使用" class="headerlink" title="安装和基础使用"></a><a href="#%E5%AE%89%E8%A3%85%E5%92%8C%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8" title="安装和基础使用"></a>安装和基础使用</h3><p>系统环境：<br>ubuntu：20.04<br>Kernel：5.4.0-125-generic</p>
<p>参考官网安装方式<br><a href="https://github.com/iovisor/bpftrace/blob/master/INSTALL.md">https://github.com/iovisor/bpftrace/blob/master/INSTALL.md</a><br>有各操作系统发行版的安装方式，也有基于Docker的安装方式<br>我这里为Ubuntu20.04的操作系统，先使用Ubuntu的安装方式进行安装，因为bpftrace依赖ebpf能力，对应不同的内核版本实现的功能有所差异，如4.1 版本实现了kprobes、4.7版本实现了tracepoints官方提供了环境需求检测脚本可以实现对现有环境检测<a href="https://github.com/iovisor/bpftrace/blob/master/scripts/check_kernel_features.sh">https://github.com/iovisor/bpftrace/blob/master/scripts/check_kernel_features.sh</a><br>执行后  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./check_kernel_features.sh</span><br><span class="line"></span><br><span class="line">All required features present!</span><br></pre></td></tr></table></figure>

<p>安装bpftrace  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get install -y bpftrace bpfcc-tools</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>安装完成后查看版本  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bpftrace --version</span><br><span class="line"></span><br><span class="line">bpftrace v0.9.4</span><br></pre></td></tr></table></figure>

<p>列出当前内核支持的Kprobes探针列表  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bpftrace -l &#x27;kprobe:tcp*</span><br><span class="line"></span><br><span class="line">kprobe:tcp_mmap</span><br><span class="line"></span><br><span class="line">kprobe:tcp_get_info_chrono_stats</span><br><span class="line"></span><br><span class="line">kprobe:tcp_init_sock</span><br><span class="line"></span><br><span class="line">kprobe:tcp_splice_data_recv</span><br><span class="line"></span><br><span class="line">kprobe:tcp_push</span><br><span class="line"></span><br><span class="line">kprobe:tcp_send_mss</span><br><span class="line"></span><br><span class="line">kprobe:tcp_cleanup_rbuf</span><br><span class="line"></span><br><span class="line">kprobe:tcp_set_rcvlowat</span><br><span class="line"></span><br><span class="line">kprobe:tcp_recv_timestamp</span><br><span class="line"></span><br><span class="line">kprobe:tcp_enter_memory_pressure</span><br><span class="line"></span><br><span class="line">kprobe:tcp_leave_memory_pressure</span><br><span class="line"></span><br><span class="line">kprobe:tcp_ioctl</span><br><span class="line"></span><br><span class="line">kprobe:tcp_get_info</span><br><span class="line"></span><br><span class="line">kprobe:tcp_get_md5sig_pool</span><br><span class="line"></span><br><span class="line">kprobe:tcp_set_state</span><br><span class="line"></span><br><span class="line">kprobe:tcp_shutdown</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>内核静态探针-Tracepoint  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bpftrace -l &#x27;tracepoint:*&#x27;</span><br><span class="line"></span><br><span class="line">kprobe:tcp_mmap</span><br><span class="line"></span><br><span class="line">kprobe:tcp_get_info_chrono_stats</span><br><span class="line"></span><br><span class="line">kprobe:tcp_init_sock</span><br><span class="line"></span><br><span class="line">kprobe:tcp_splice_data_recv</span><br><span class="line"></span><br><span class="line">kprobe:tcp_push</span><br><span class="line"></span><br><span class="line">kprobe:tcp_send_mss</span><br><span class="line"></span><br><span class="line">kprobe:tcp_cleanup_rbuf</span><br><span class="line"></span><br><span class="line">kprobe:tcp_set_rcvlowat</span><br><span class="line"></span><br><span class="line">kprobe:tcp_recv_timestamp</span><br><span class="line"></span><br><span class="line">kprobe:tcp_enter_memory_pressure</span><br><span class="line"></span><br><span class="line">kprobe:tcp_leave_memory_pressure</span><br><span class="line"></span><br><span class="line">kprobe:tcp_ioctl</span><br><span class="line"></span><br><span class="line">kprobe:tcp_get_info</span><br><span class="line"></span><br><span class="line">kprobe:tcp_get_md5sig_pool</span><br><span class="line"></span><br><span class="line">kprobe:tcp_set_state</span><br><span class="line"></span><br><span class="line">kprobe:tcp_shutdown</span><br><span class="line"></span><br><span class="line">tracepoint:nfsd:nfsd_compound</span><br><span class="line"></span><br><span class="line">tracepoint:nfsd:nfsd_compound_status</span><br><span class="line"></span><br><span class="line">tracepoint:nfsd:nfsd_read_start</span><br><span class="line"></span><br><span class="line">tracepoint:nfsd:nfsd_read_splice</span><br><span class="line"></span><br><span class="line">tracepoint:nfsd:nfsd_read_vector</span><br><span class="line"></span><br><span class="line">tracepoint:nfsd:nfsd_read_io_done</span><br><span class="line"></span><br><span class="line">tracepoint:nfsd:nfsd_read_done</span><br><span class="line"></span><br><span class="line">tracepoint:nfsd:nfsd_write_start</span><br><span class="line"></span><br><span class="line">tracepoint:nfsd:nfsd_write_opened</span><br><span class="line"></span><br><span class="line">tracepoint:nfsd:nfsd_write_io_done</span><br><span class="line"></span><br><span class="line">tracepoint:nfsd:nfsd_write_done</span><br><span class="line"></span><br><span class="line">tracepoint:nfsd:nfsd_read_err</span><br><span class="line"></span><br><span class="line">tracepoint:nfsd:nfsd_write_err</span><br><span class="line"></span><br><span class="line">tracepoint:nfsd:nfsd_layoutstate_alloc</span><br><span class="line"></span><br><span class="line">tracepoint:nfsd:nfsd_layoutstate_unhash</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>例如列出所有的进程打开的文件  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bpftrace -e &#x27;tracepoint:syscalls:sys_enter_openat &#123; printf(&quot;%s %s\n&quot;, comm, str(args-&gt;filename)); &#125;&#x27;</span><br><span class="line"></span><br><span class="line">kubelet /sys/fs/cgroup/memory/kubepods.slice/memory.numa_stat</span><br><span class="line"></span><br><span class="line">kubelet /sys/fs/cgroup/cpu,cpuacct/kubepods.slice/cpu.stat</span><br><span class="line"></span><br><span class="line">kubelet /sys/fs/cgroup/cpu,cpuacct/kubepods.slice/cpuacct.stat</span><br><span class="line"></span><br><span class="line">kubelet /sys/fs/cgroup/cpu,cpuacct/kubepods.slice/cpuacct.usage</span><br><span class="line"></span><br><span class="line">kubelet /sys/fs/cgroup/cpu,cpuacct/kubepods.slice/cpuacct.usage_percpu</span><br><span class="line"></span><br><span class="line">kubelet /sys/fs/cgroup/cpu,cpuacct/kubepods.slice/cpuacct.usage_all</span><br><span class="line"></span><br><span class="line">kubelet /sys/fs/cgroup/pids/kubepods.slice/pids.current</span><br><span class="line"></span><br><span class="line">kubelet /sys/fs/cgroup/pids/kubepods.slice/pids.max</span><br><span class="line"></span><br><span class="line">kubelet /sys/fs/cgroup/blkio/kubepods.slice/blkio.bfq.sectors_recursive</span><br><span class="line"></span><br><span class="line">kubelet /sys/fs/cgroup/blkio/kubepods.slice/blkio.bfq.io_serviced_recur</span><br><span class="line"></span><br><span class="line">kubelet /sys/fs/cgroup/blkio/kubepods.slice/blkio.sectors_recursive</span><br><span class="line"></span><br><span class="line">kubelet /sys/fs/cgroup/blkio/kubepods.slice/blkio.throttle.io_serviced_</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>ctrl+c暂停<br>也可以将复杂的命令写成脚本执行，默认安装完后，在&#x2F;usr&#x2F;sbin&#x2F;目录下已经集成了很多脚本</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ls /usr/sbin/|grep &quot;.*.bt&quot;</span><br><span class="line"></span><br><span class="line">bashreadline.bt</span><br><span class="line"></span><br><span class="line">biolatency.bt</span><br><span class="line"></span><br><span class="line">biosnoop.bt</span><br><span class="line"></span><br><span class="line">biostacks.bt</span><br><span class="line"></span><br><span class="line">bitesize.bt</span><br><span class="line"></span><br><span class="line">capable.bt</span><br><span class="line"></span><br><span class="line">cpuwalk.bt</span><br><span class="line"></span><br><span class="line">dcsnoop.bt</span><br><span class="line"></span><br><span class="line">ebtables</span><br><span class="line"></span><br><span class="line">ebtables-nft</span><br><span class="line"></span><br><span class="line">ebtables-nft-restore</span><br><span class="line"></span><br><span class="line">ebtables-nft-save</span><br><span class="line"></span><br><span class="line">ebtables-restore</span><br><span class="line"></span><br><span class="line">ebtables-save</span><br><span class="line"></span><br><span class="line">execsnoop.bt</span><br><span class="line"></span><br><span class="line">gethostlatency.bt</span><br><span class="line"></span><br><span class="line">killsnoop.bt</span><br><span class="line"></span><br><span class="line">loads.bt</span><br><span class="line"></span><br><span class="line">mdflush.bt</span><br><span class="line"></span><br><span class="line">naptime.bt</span><br><span class="line"></span><br><span class="line">oomkill.bt</span><br><span class="line"></span><br><span class="line">opensnoop.bt</span><br><span class="line"></span><br><span class="line">pidpersec.bt</span><br><span class="line"></span><br><span class="line">runqlat.bt</span><br><span class="line"></span><br><span class="line">runqlen.bt</span><br><span class="line"></span><br><span class="line">setuids.bt</span><br><span class="line"></span><br><span class="line">statsnoop.bt</span><br><span class="line"></span><br><span class="line">swapin.bt</span><br><span class="line"></span><br><span class="line">syncsnoop.bt</span><br><span class="line"></span><br><span class="line">syscount.bt</span><br><span class="line"></span><br><span class="line">tcpaccept.bt</span><br><span class="line"></span><br><span class="line">tcpconnect.bt</span><br><span class="line"></span><br><span class="line">tcpdrop.bt</span><br><span class="line"></span><br><span class="line">tcplife.bt</span><br><span class="line"></span><br><span class="line">tcpretrans.bt</span><br><span class="line"></span><br><span class="line">tcpsynbl.bt</span><br><span class="line"></span><br><span class="line">threadsnoop.bt</span><br><span class="line"></span><br><span class="line">vfscount.bt</span><br><span class="line"></span><br><span class="line">vfsstat.bt</span><br><span class="line"></span><br><span class="line">writeback.bt</span><br><span class="line"></span><br><span class="line">xfsdist.bt</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/iovisor/bpftrace/tree/master/tools">https://github.com/iovisor/bpftrace/tree/master/tools</a> 也存在很多脚本和测试用例。</p>
<p>比如执行tcpconnect.bt 可以参考到本机所有的TCP网络连接  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcpconnect.bt </span><br><span class="line"></span><br><span class="line">Attaching 2 probes...</span><br><span class="line"></span><br><span class="line">Tracing tcp connections. Hit Ctrl-C to end.</span><br><span class="line"></span><br><span class="line">TIME     PID      COMM             SADDR                                   SPORT  DADDR                                   DPORT </span><br><span class="line"></span><br><span class="line">23:02:09 1686607  coredns          127.0.0.1                               42216  127.0.0.1                               13429 </span><br><span class="line"></span><br><span class="line">23:02:10 1686607  coredns          127.0.0.1                               42218  127.0.0.1                               13429 </span><br><span class="line"></span><br><span class="line">23:02:11 1680193  kubelet          10.0.1.11                               34732  10.0.1.15                               13429 </span><br><span class="line"></span><br><span class="line">23:02:11 1680193  kubelet          10.0.1.11                               41570  10.0.1.244                              13429 </span><br><span class="line"></span><br><span class="line">23:02:11 1686607  coredns          127.0.0.1                               42224  127.0.0.1                               13429 </span><br><span class="line"></span><br><span class="line">23:02:11 1680193  kubelet          127.0.0.1                               55010  127.0.0.1                               13429 </span><br><span class="line"></span><br><span class="line">23:02:11 1680193  kubelet          10.0.1.11                               33098  10.0.1.145                              13429 </span><br><span class="line"></span><br><span class="line">23:02:12 1686607  coredns          127.0.0.1                               42230  127.0.0.1                               13429 </span><br><span class="line"></span><br><span class="line">23:02:13 1680193  kubelet          127.0.0.1                               34214  127.0.0.1                               13429 </span><br><span class="line"></span><br><span class="line">23:02:13 1686607  coredns          127.0.0.1                               42234  127.0.0.1                               13429</span><br></pre></td></tr></table></figure>

<p>追踪全系统范围内open()调用  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">opensnoop.bt</span><br><span class="line"></span><br><span class="line">Attaching 6 probes...</span><br><span class="line"></span><br><span class="line">Tracing open syscalls... Hit Ctrl-C to end.</span><br><span class="line"></span><br><span class="line">PID    COMM               FD ERR PATH</span><br><span class="line"></span><br><span class="line">1686817 AsyncBlockInput     2   0 /var/lib/clickhouse_storage/store/087/0872950b-6ca9-420e-8872-9</span><br><span class="line"></span><br><span class="line">1686817 AsyncBlockInput     2   0 /var/lib/clickhouse_storage/store/087/0872950b-6ca9-420e-8872-9</span><br><span class="line"></span><br><span class="line">1686817 AsyncBlockInput     2   0 /var/lib/clickhouse_storage/store/435/435debae-6288-4661-835d-e</span><br><span class="line"></span><br><span class="line">1686817 AsyncBlockInput     2   0 /var/lib/clickhouse_storage/store/435/435debae-6288-4661-835d-e</span><br><span class="line"></span><br><span class="line">1686817 AsyncBlockInput     2   0 /var/lib/clickhouse_storage/store/092/09246824-dd69-4f4c-8924-6</span><br><span class="line"></span><br><span class="line">1686817 AsyncBlockInput     2   0 /var/lib/clickhouse_storage/store/092/09246824-dd69-4f4c-8924-6</span><br><span class="line"></span><br><span class="line">1680193 kubelet             2   0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/</span><br><span class="line"></span><br><span class="line">1680193 kubelet             2   0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/</span><br><span class="line"></span><br><span class="line">1680193 kubelet             2   0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/</span><br><span class="line"></span><br><span class="line">1680193 kubelet             2   0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/</span><br><span class="line"></span><br><span class="line">1680193 kubelet             2   0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/</span><br><span class="line"></span><br><span class="line">1680193 kubelet             2   0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/</span><br><span class="line"></span><br><span class="line">1680193 kubelet             2   0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/</span><br><span class="line"></span><br><span class="line">1680193 kubelet             2   0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/</span><br><span class="line"></span><br><span class="line">1680193 kubelet             2   0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/</span><br><span class="line"></span><br><span class="line">1680193 kubelet             2   0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/</span><br><span class="line"></span><br><span class="line">1680193 kubelet             2   0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/</span><br><span class="line"></span><br><span class="line">1680193 kubelet             2   0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/</span><br><span class="line"></span><br><span class="line">1680193 kubelet             2   0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/</span><br><span class="line"></span><br><span class="line">1680193 kubelet             2   0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/</span><br><span class="line"></span><br><span class="line">1680193 kubelet             2   0 /sys/fs/cgroup/memory/kubepods.slice/kubepods-besteffort.slice/</span><br></pre></td></tr></table></figure>
<p>展示最消耗IO的进程及数据写入量</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">biotop-bpfcc</span><br><span class="line"></span><br><span class="line">20:53:20 loadavg: 0.96 1.40 1.26 2/2392 3295655</span><br><span class="line"></span><br><span class="line">PID    COMM             D MAJ MIN DISK       I/O  Kbytes  AVGms</span><br><span class="line"></span><br><span class="line">337    jbd2/vda1-8      W 252 0   vda          2   412.0   5.16</span><br><span class="line"></span><br><span class="line">1680139 etcd             W 252 0   vda         14    60.0   2.61</span><br><span class="line"></span><br><span class="line">3295622 rancher-system-  R 252 0   vda          1    56.0   2.07</span><br></pre></td></tr></table></figure>

<p>查看每个进程对应的执行命令和参数  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">execsnoop-bpfcc -T</span><br><span class="line"></span><br><span class="line">TIME     PCOMM            PID    PPID   RET ARGS</span><br><span class="line"></span><br><span class="line">21:22:05 rancher-system-  3310507 1        0 /usr/local/bin/rancher-system-agent sentinel</span><br><span class="line"></span><br><span class="line">21:22:06 cilium-cni       3310516 1680193   0 /opt/cni/bin/cilium-cni</span><br><span class="line"></span><br><span class="line">21:22:06 iptables         3310525 1680164   0 /usr/sbin/iptables -w 5 -W 100000 -S KUBE-PROXY-CANARY -t mangle</span><br><span class="line"></span><br><span class="line">21:22:06 ip6tables        3310524 1680164   0 /usr/sbin/ip6tables -w 5 -W 100000 -S KUBE-PROXY-CANARY -t mangle</span><br><span class="line"></span><br><span class="line">21:22:06 nsenter          3310526 1680193   0 /usr/bin/nsenter --net=/proc/1688099/ns/net -F -- ip -o -4 addr show dev eth0 scope global</span><br><span class="line"></span><br><span class="line">21:22:06 ip               3310526 1680193   0 /usr/sbin/ip -o -4 addr show dev eth0 scope global</span><br><span class="line"></span><br><span class="line">21:22:06 nsenter          3310527 1680193   0 /usr/bin/nsenter --net=/proc/1688099/ns/net -F -- ip -o -6 addr show dev eth0 scope global</span><br><span class="line"></span><br><span class="line">21:22:06 ip               3310527 1680193   0 /usr/sbin/ip -o -6 addr show dev eth0 scope global</span><br><span class="line"></span><br><span class="line">21:22:06 runc             3310528 1679752   0 /usr/bin/runc --version</span><br><span class="line"></span><br><span class="line">21:22:06 docker-init      3310534 1679752   0 /usr/bin/docker-init --version</span><br></pre></td></tr></table></figure>

<p>常用的一些脚本作用</p>
<ul>
<li>killsnoop.bt——追踪 kill() 系统调用发出的信号</li>
<li>tcpconnect.bt——追踪所有的 TCP 网络连接</li>
<li>pidpersec.bt——统计每秒钟（通过fork）创建的新进程</li>
<li>opensnoop.bt——追踪 open() 系统调用</li>
<li>bfsstat.bt——追踪一些 VFS 调用，按秒统计</li>
<li>bashreadline.bt——打印从所有运行shell输入的bash命令</li>
<li>tcplife.bt——追踪TCP连接生命周期</li>
<li>biotop-bpfcc——展示进程io写入</li>
</ul>
<h3 id="bpftrace执行原理"><a href="#bpftrace执行原理" class="headerlink" title="bpftrace执行原理"></a><a href="#bpftrace%E6%89%A7%E8%A1%8C%E5%8E%9F%E7%90%86" title="bpftrace执行原理"></a>bpftrace执行原理</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ebfp2_2.png"><br>用户态<br>1、用户编写 eBPF 程序，可以使用 eBPF 汇编或者 eBPF 特有的 C 语言来编写。<br>2、使用 LLVM&#x2F;CLang 编译器，将 eBPF 程序编译成 eBPF 字节码。<br>3、调用 bpf() 系统调用把 eBPF 字节码加载到内核。</p>
<p>内核态<br>1、当用户调用 bpf() 系统调用把 eBPF 字节码加载到内核时，内核先会对 eBPF 字节码进行安全验证。<br>2、使用 JIT（Just In Time）技术将 eBPF 字节编译成本地机器码（Native Code）。<br>3、然后根据 eBPF 程序的功能，将 eBPF 机器码挂载到内核的不同运行路径上（如用于跟踪内核运行状态的 eBPF 程序将会挂载在 kprobes 的运行路径上）。当内核运行到这些路径时，就会触发执行相应路径上的 eBPF 机器码。<br>4、通过map与用户空间程序交互</p>
<p>总结<br>通过bpftrace和bcc可以很形象了解ebpf特性，无需修改内核源码和重新编译内核就可以扩展内核的功能，除了像bpftrace这类追踪类软件，还有通过ebfp实现的POD安全威胁检测Falco、基于ebpf负载均衡器Katran等开源产品。另外ebpf_exporter组件也可以将自定义的ebpf执行脚本输出到Prometheus中进行监控。ebpf的生态将越来越丰富。</p>
<h3 id="Perf工具使用"><a href="#Perf工具使用" class="headerlink" title="Perf工具使用"></a><a href="#Perf%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8" title="Perf工具使用"></a>Perf工具使用</h3><p>Perf（Performance Event）是在Linux Kernel2.6集成在Linux Kernel中，主要利用CPU中PMU (Performance Monitoring Unit)和Linux中的 tracepoint实现目标取样和性能分析。Perf工具根eBPF实际上没什么关系，这里写这个工具主要是因为它本身也可以实现应用程序动态追踪，也利用到了tracepoint的能力，但与eBPF不同的是Perf是写死的能力，bpftrace基于eBPF是可以实现脚本灵活的穿插和调用。</p>
<p>安装部署<br>这里使用的操作系统是Ubuntu20.04。Kernel为5.4.0-125-generic</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get install linux-tools-common linux-tools-&quot;``(uname -r)&quot; linux-cloud-tools-&quot;``(uname -r)&quot; linux-tools-generic linux-cloud-tools-generic</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>验证版本  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">perf -v</span><br><span class="line"></span><br><span class="line">perf version 5.4.195</span><br></pre></td></tr></table></figure>
<p>采样事件  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">perf list</span><br><span class="line"></span><br><span class="line">List of pre-defined events (to be used in -e):</span><br><span class="line"></span><br><span class="line">  alignment-faults                                   [Software event]</span><br><span class="line"></span><br><span class="line">  bpf-output                                         [Software event]</span><br><span class="line"></span><br><span class="line">  context-switches OR cs                             [Software event]</span><br><span class="line"></span><br><span class="line">  cpu-clock                                          [Software event]</span><br><span class="line"></span><br><span class="line">  cpu-migrations OR migrations                       [Software event]</span><br><span class="line"></span><br><span class="line">  dummy                                              [Software event]</span><br><span class="line"></span><br><span class="line">  emulation-faults                                   [Software event]</span><br><span class="line"></span><br><span class="line">  major-faults                                       [Software event]</span><br><span class="line"></span><br><span class="line">  minor-faults                                       [Software event]</span><br><span class="line"></span><br><span class="line">  page-faults OR faults                              [Software event]</span><br><span class="line"></span><br><span class="line">  task-clock                                         [Software event]</span><br><span class="line"></span><br><span class="line">  duration_time                                      [Tool event]</span><br><span class="line"></span><br><span class="line">  msr/tsc/                                           [Kernel PMU event]</span><br><span class="line"></span><br><span class="line">  rNNN                                               [Raw hardware event descriptor]</span><br><span class="line"></span><br><span class="line">  cpu/t1=v1[,t2=v2,t3 ...]/modifier                  [Raw hardware event descriptor]</span><br><span class="line"></span><br><span class="line">   (see &#x27;man perf-list&#x27; on how to encode it)</span><br><span class="line"></span><br><span class="line">  mem:&lt;addr&gt;[/len][:access]                          [Hardware breakpoint]</span><br><span class="line"></span><br><span class="line">  alarmtimer:alarmtimer_cancel                       [Tracepoint event]</span><br><span class="line"></span><br><span class="line">  alarmtimer:alarmtimer_fired                        [Tracepoint event]</span><br><span class="line"></span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>
<p>主要分为三类：<br>Hardware Event ：通过PMU获取的硬件CPU的事件，如：cpu-cycles、缓存命中等。<br>Software Event ：软件本身的进程切换和页命中等<br>Tracepoint event：io命中率、文件系统写入速率等</p>
<p>perf top展示各个进程和函数资源占用情况，-g显示子进程，按e显示子进程函数  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">perf top -g </span><br><span class="line"></span><br><span class="line">Samples: 284K of event &#x27;cpu-clock:pppH&#x27;, 4000 Hz, Event count (approx.): 33836570425 lost: 0/0 drop: 0/0</span><br><span class="line"></span><br><span class="line">  Children      Self  Shared Object                                          Symbol</span><br><span class="line"></span><br><span class="line">-   20.27%     0.09%  perf                                                   [.] __ordered_events__flush.part.0                                                            ◆</span><br><span class="line"></span><br><span class="line">   - 2.20% __ordered_events__flush.part.0                                                                                                                                  ▒</span><br><span class="line"></span><br><span class="line">      - 2.56% deliver_event                                                                                                                                                ▒</span><br><span class="line"></span><br><span class="line">         - 3.39% hist_entry_iter__add                                                                                                                                      ▒</span><br><span class="line"></span><br><span class="line">            - 3.79% iter_add_next_cumulative_entry                                                                                                                         ▒</span><br><span class="line"></span><br><span class="line">               - 3.03% __hists__add_entry.constprop.0                                                                                                                      ▒</span><br><span class="line"></span><br><span class="line">                    3.79% hists__findnew_entry                                                                                                                             ▒</span><br><span class="line"></span><br><span class="line">               - 1.54% callchain_append                                                                                                                                    ▒</span><br><span class="line"></span><br><span class="line">                  - 2.64% append_chain_children                                                                                                                            ▒</span><br><span class="line"></span><br><span class="line">                     - 2.22% append_chain_children                                                                                                                         ▒</span><br><span class="line"></span><br><span class="line">                        - 1.73% append_chain_children                                                                                                                      ▒</span><br><span class="line"></span><br><span class="line">                           - 1.34% append_chain_children                                                                                                                   ▒</span><br><span class="line"></span><br><span class="line">                                1.07% append_chain_children                                                                                                                ▒</span><br><span class="line"></span><br><span class="line">+   20.13%     0.18%  perf                                                   [.] deliver_event                                                                             ▒</span><br><span class="line"></span><br><span class="line">+   18.62%     0.04%  perf                                                   [.] hist_entry_iter__add                                                                      ▒</span><br><span class="line"></span><br><span class="line">+   14.47%     0.80%  perf                                                   [.] iter_add_next_cumulative_entry                                                            ▒</span><br><span class="line"></span><br><span class="line">+   12.05%     0.96%  [kernel]                                               [k] do_syscall_64                                                                             ▒</span><br><span class="line"></span><br><span class="line">+    8.99%     0.00%  perf                                                   [.] process_thread                                                                            ▒</span><br><span class="line"></span><br><span class="line">+    8.93%     0.22%  [kernel]                                               [k] do_idle                                                                                   ▒</span><br><span class="line"></span><br><span class="line">+    8.83%     1.06%  [kernel]                                               [k] __softirqentry_text_start                                                                 ▒</span><br><span class="line"></span><br><span class="line">+    8.24%     6.28%  perf                                                   [.] append_chain_children                                                                     ▒</span><br><span class="line"></span><br><span class="line">+    7.11%     0.06%  perf                                                   [.] callchain_append                                                                          ▒</span><br><span class="line"></span><br><span class="line">+    5.96%     4.75%  libc-2.31.so                                           [.] pthread_attr_setschedparam                                                                ▒</span><br><span class="line"></span><br><span class="line">+    5.74%     0.25%  perf                                                   [.] __hists__add_entry.constprop.0</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>[.]：表示运行在用户态空间<br>[k]：表示运行在内核态空间</p>
<p>perf state查看程序运行情况</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">perf stat -p 1679752   按ctrl+c输出结果</span><br><span class="line"></span><br><span class="line"> Performance counter stats for process id &#x27;1679752&#x27;:</span><br><span class="line"></span><br><span class="line">            682.90 msec task-clock                #    0.066 CPUs utilized          </span><br><span class="line"></span><br><span class="line">              3154      context-switches          #    0.005 M/sec                  </span><br><span class="line"></span><br><span class="line">                36      cpu-migrations            #    0.053 K/sec                  </span><br><span class="line"></span><br><span class="line">              3275      page-faults               #    0.005 M/sec                  </span><br><span class="line"></span><br><span class="line">   &lt;not supported&gt;      cycles                                                      </span><br><span class="line"></span><br><span class="line">   &lt;not supported&gt;      instructions                                                </span><br><span class="line"></span><br><span class="line">   &lt;not supported&gt;      branches                                                    </span><br><span class="line"></span><br><span class="line">   &lt;not supported&gt;      branch-misses</span><br></pre></td></tr></table></figure>

<p>Task-clock：CPU 利用率<br>Context-switches：进程切换次数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Samples: 1K of event &#x27;block:block_rq_issue&#x27;, 1 Hz, Event count (approx.): 136 lost: 0/0 drop: 0/0</span><br><span class="line"></span><br><span class="line">  Children      Self  Trace output</span><br><span class="line"></span><br><span class="line">+   16.91%    16.91%  252,0 FF 0 () 0 + 0 [kworker/0:1H]</span><br><span class="line"></span><br><span class="line">+   15.44%    15.44%  252,0 FF 0 () 0 + 0 [kworker/7:1H]</span><br><span class="line"></span><br><span class="line">+    9.56%     9.56%  252,0 FF 0 () 0 + 0 [kworker/3:1H]</span><br><span class="line"></span><br><span class="line">+    8.09%     8.09%  252,0 FF 0 () 0 + 0 [kworker/4:1H]</span><br><span class="line"></span><br><span class="line">+    6.62%     6.62%  252,0 WS 4096 () 18340064 + 8 [etcd]</span><br><span class="line"></span><br><span class="line">+    6.62%     6.62%  252,0 WS 4096 () 18340072 + 8 [etcd]</span><br><span class="line"></span><br><span class="line">+    5.15%     5.15%  252,0 FF 0 () 0 + 0 [kworker/2:1H]</span><br><span class="line"></span><br><span class="line">+    4.41%     4.41%  252,0 FF 0 () 0 + 0 [kworker/6:1H]</span><br><span class="line"></span><br><span class="line">+    2.94%     2.94%  252,0 WS 4096 () 122005280 + 8 [etcd]</span><br><span class="line"></span><br><span class="line">     2.21%     2.21%  252,0 WS 4096 () 115952144 + 8 [etcd]</span><br><span class="line"></span><br><span class="line">     2.21%     2.21%  252,0 WS 4096 () 122005272 + 8 [etcd]</span><br><span class="line"></span><br><span class="line">+    1.47%     1.47%  252,0 FF 0 () 0 + 0 [kworker/1:1H]</span><br><span class="line"></span><br><span class="line">+    1.47%     1.47%  252,0 WS 4096 () 116164552 + 8 [etcd]</span><br><span class="line"></span><br><span class="line">     1.47%     1.47%  252,0 WS 4096 () 116173824 + 8 [etcd]</span><br><span class="line"></span><br><span class="line">+    1.47%     1.47%  252,0 WS 4096 () 122005256 + 8 [etcd]</span><br><span class="line"></span><br><span class="line">     1.47%     1.47%  252,0 WS 4096 () 122005288 + 8 [etcd]</span><br><span class="line"></span><br><span class="line">     1.47%     1.47%  252,0 WS 4096 () 122005296 + 8 [etcd]</span><br><span class="line"></span><br><span class="line">     0.74%     0.74%  252,0 FF 0 () 0 + 0 [kworker/5:1H]</span><br><span class="line"></span><br><span class="line">     0.74%     0.74%  252,0 WS 516096 () 2388520 + 1008 [jbd2/vda1-8]</span><br><span class="line"></span><br><span class="line">     0.74%     0.74%  252,0 WS 372736 () 2389528 + 728 [jbd2/vda1-8]</span><br><span class="line"></span><br><span class="line">     0.74%     0.74%  252,0 WS 4096 () 115700160 + 8 [etcd]</span><br><span class="line"></span><br><span class="line">     0.74%     0.74%  252,0 WS 4096 () 115948608 + 8 [etcd]</span><br></pre></td></tr></table></figure>
<p>对CPU事件进行检测，采样时间60s，每秒采样99个事件，采样完成后会在本地生成个perf.data文件，如果执行多次，会将上一个重命名为perf.data.old。加-p可以指定进程号输出。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">perf record -F 99 -a -g -- sleep 60</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>查看报告  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">perf report</span><br></pre></td></tr></table></figure>
<p>生成火焰图  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">下载制作火焰图工具</span><br><span class="line"></span><br><span class="line">git clone https://github.com/brendangregg/FlameGraph.git</span><br><span class="line"></span><br><span class="line">对perf.data进行解析</span><br><span class="line"></span><br><span class="line">perf script -i perf.data &amp;&gt; perf.unfold</span><br><span class="line"></span><br><span class="line">进行符号处理</span><br><span class="line"></span><br><span class="line">FlameGraph/stackcollapse-perf.pl perf.unfold &amp;&gt; perf.folded</span><br><span class="line"></span><br><span class="line">生成火焰图</span><br><span class="line"></span><br><span class="line">FlameGraph/flamegraph.pl perf.folded &gt; perf.svg</span><br></pre></td></tr></table></figure>
<p>使用chrome浏览器打开<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ebfp2_1.png"></p>
<p>火焰图怎么查看分析可参考<br><a href="https://www.infoq.cn/article/a8kmnxdhbwmzxzsytlga">https://www.infoq.cn/article/a8kmnxdhbwmzxzsytlga</a></p>
<p>参考链接<br><a href="https://github.com/iovisor/bpftrace/blob/master/docs/reference_guide.md">https://github.com/iovisor/bpftrace/blob/master/docs/reference_guide.md</a><br><a href="http://blog.nsfocus.net/bpftrace-dynamic-tracing-0828/">http://blog.nsfocus.net/bpftrace-dynamic-tracing-0828/</a><br><a href="https://www.cnblogs.com/arnoldlu/p/6241297.html">https://www.cnblogs.com/arnoldlu/p/6241297.html</a><br><a href="https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/counting-events-during-process-execution-with-perf-stat_monitoring-and-managing-system-status-and-performance">https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux&#x2F;8&#x2F;html&#x2F;monitoring_and_managing_system_status_and_performance&#x2F;counting-events-during-process-execution-with-perf-stat_monitoring-and-managing-system-status-and-performance</a></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>ETCD集群读写慢问题分析</title>
    <url>/2022/07/27/fio_etcd/</url>
    <content><![CDATA[<h3 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h3><p>1、Rancher所在local集群周期性卡顿、执行命令响应缓慢。<br>2、Rancher-server副本频繁重启。</p>
<p>3、Rancher UI空载集群切换项目，点击UI反应慢。</p>
<p>查看ETCD日志发现有大量Ready only报错和too long（xxx ms）to execute报错</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/etcd-read-only.png"></p>
<h3 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/etcd-read-only3.jpeg"></p>
<p>注：以下etcd读写流程来源腾讯云原生社区（<a href="https://blog.csdn.net/yunxiao6/article/details/108615472%EF%BC%89">https://blog.csdn.net/yunxiao6/article/details/108615472）</a><br>写数据流程（以 leader 节点为例，见上图）：</p>
<p>1、etcd 任一节点的 etcd server 模块收到 Client 写请求（如果是 follower 节点，会先通过 Raft 模块将请求转发至 leader 节点处理）。</p>
<p>2、etcd server 将请求封装为 Raft 请求，然后提交给 Raft 模块处理。</p>
<p>3、leader 通过 Raft 协议与集群中 follower 节点进行交互，将消息复制到follower 节点，于此同时，并行将日志持久化到 WAL。</p>
<p>4、follower 节点对该请求进行响应，回复自己是否同意该请求。</p>
<p>5、当集群中超过半数节点（(n&#x2F;2)+1 members ）同意接收这条日志数据时，表示该请求可以被Commit，Raft 模块通知 etcd server 该日志数据已经 Commit，可以进行 Apply。</p>
<p>6、各个节点的 etcd server 的 applierV3 模块异步进行 Apply 操作，并通过 MVCC 模块写入后端存储 BoltDB。</p>
<p>7、当 client 所连接的节点数据 apply 成功后，会返回给客户端 apply 的结果。</p>
<p>读数据流程：</p>
<p>1、etcd 任一节点的 etcd server 模块收到客户端读请求（Range 请求） 判断读请求类型，如果是串行化读（serializable）则直接进入 Apply 流程。</p>
<p>2、如果是线性一致性读（linearizable），则进入 Raft模块。</p>
<p>3、Raft模块向 leader 发出 ReadIndex 请求，获取当前集群已经提交的最新数据 Index。</p>
<p>4、等待本地 AppliedIndex 大于或等于 ReadIndex 获取的 CommittedIndex 时，进入Apply 流程。</p>
<p>5、Apply 流程：通过Key名从KV Index模块获取 Key最新的 Revision，再通过Revision从BoltDB 获取对应的Key和Value。</p>
<p>etcd 通过 WAL（预写日志）实现了内存中数据的强持久性，WAL日志受到磁盘IO 写入速度影响，fdatasync延迟也会影响etcd性能。底层ceph为分布式存储，存储多副本会进行同步，副本同步时将占用大量网络和IO资源影响性能，底层又为SAS盘，对ETCD性能影响较大。</p>
<h4 id="使用FIO模拟etcd-io写入"><a href="#使用FIO模拟etcd-io写入" class="headerlink" title="使用FIO模拟etcd io写入"></a>使用FIO模拟etcd io写入</h4><p>使用FIO模拟etcd io写入</p>
<p>安装FIO</p>
<pre><code>curl -LO https://github.com/rancherlabs/support-tools/raw/master/instant-fio-master/instant-fio-master.sh
bash instant-fio-master.sh
</code></pre>
<p>创建测试目录，对应的在&#x2F;var&#x2F;lib&#x2F;etcd目录下进行性能测试，更能直观体现</p>
<pre><code>export PATH=/usr/local/bin:$PATH
cd /var/lib/etcd
mkdir test-data
fio --rw=write --ioengine=sync --fdatasync=1 --directory=test-data --size=100m --bs=2300 --name=mytest
</code></pre>
<p>size：表示总的写入大小<br>bs：表示每次写入的大小（单位为字节）</p>
<p>为了更好的模拟实际IO写入，需要通过lsof和strace查看实际IO写入量</p>
<p>通过lsof获取etcd进程的文件描述符</p>
<pre><code>lsof -p $(pgrep etcd)|grep wal



 lsof -p $(pgrep etcd)|grep wal
etcd    21040 root    7w      REG   252,1 64000000  828705 /var/lib/rancher/etcd/member/wal/1.tmp
etcd    21040 root    8r      DIR   252,1     4096  838659 /var/lib/rancher/etcd/member/wal
etcd    21040 root   11w      REG   252,1 64000000  828702 /var/lib/rancher/etcd/member/wal/0000000000000005-000000000007016b.wal
</code></pre>
<p>11w就是写入对应的wal文件的文件描述符，通过strace查看etcd系统调用，查看实际的数据写入量。</p>
<pre><code>strace -f -p  $(pgrep etcd) -T -tt  -o test.txt
</code></pre>
<p>访问test.txt文件查找<code>write(11</code></p>
<pre><code>21064 11:23:24.438231 write(11, &quot;\25\3\0\0\0\0\0\203\10\2\20\303\240\345\252\16\32\212\6\10\0\20\2\30\306\276\34\&quot;\377\0052\337&quot;..., 840 &lt;unfinished ...&gt;
21306 11:23:24.438248 &lt;... write resumed&gt; ) = 42 &lt;0.000037&gt;
21215 11:23:24.438263 &lt;... futex resumed&gt; ) = 0 &lt;0.005978&gt;
21068 11:23:24.438277 &lt;... futex resumed&gt; ) = 1 &lt;0.000051&gt;
21064 11:23:24.438291 &lt;... write resumed&gt; ) = 840 &lt;0.000048&gt;
21306 11:23:24.438305 futex(0xc00080cf48, FUTEX_WAIT_PRIVATE, 0, NULL &lt;unfinished ...&gt;
21068 11:23:24.438319 futex(0xc0004d2148, FUTEX_WAIT_PRIVATE, 0, NULL &lt;unfinished ...&gt;
21060 11:23:24.438333 &lt;... nanosleep resumed&gt; NULL) = 0 &lt;0.000247&gt;
21060 11:23:24.438352 nanosleep(&#123;tv_sec=0, tv_nsec=20000&#125;,  &lt;unfinished ...&gt;
21215 11:23:24.438496 futex(0xc00080cf48, FUTEX_WAKE_PRIVATE, 1 &lt;unfinished ...&gt;
21064 11:23:24.438530 fdatasync(11 &lt;unfinished ...&gt;
</code></pre>
<p>可以看见文件描述符11在write完后进行了fdatasync操作通过write操作可以看见此次数据写入量为840字节，多对比几个发现范围在800-900之间，因为我的环境为单节点环境，实际数据写入量根etcd版本和集群规模有直接关系，通常情况下在2300左右，所以这里fio的bs参数设置为2300字节，模拟etcd io写入，查看延时情况。</p>
<p>测试结果</p>
<pre><code>mytest: (g=0): rw=write, bs=(R) 2300B-2300B, (W) 2300B-2300B, (T) 2300B-2300B, ioengine=sync, iodepth=1
fio-3.30-67-gdc472
Starting 1 process
mytest: Laying out IO file (1 file / 100MiB)
Jobs: 1 (f=1)
Jobs: 1 (f=1): [W(1)][100.0%][w=636KiB/s][w=283 IOPS][eta 00m:00s]
mytest: (groupid=0, jobs=1): err= 0: pid=16852: Mon Jul  4 09:46:37 2022
  write: IOPS=253, BW=569KiB/s (583kB/s)(100.0MiB/179902msec); 0 zone resets
    clat (usec): min=5, max=4377, avg=16.96, stdev=32.00
     lat (usec): min=5, max=4377, avg=17.51, stdev=32.04
    clat percentiles (usec):
     |  1.00th=[    8],  5.00th=[   10], 10.00th=[   10], 20.00th=[   11],
     | 30.00th=[   12], 40.00th=[   13], 50.00th=[   14], 60.00th=[   16],
     | 70.00th=[   18], 80.00th=[   22], 90.00th=[   29], 95.00th=[   34],
     | 99.00th=[   49], 99.50th=[   57], 99.90th=[   81], 99.95th=[   96],
     | 99.99th=[ 1369]
   bw (  KiB/s): min=   89, max=  691, per=99.97%, avg=569.10, stdev=63.60, samples=359
   iops        : min=   40, max=  308, avg=253.57, stdev=28.33, samples=359
  lat (usec)   : 10=15.39%, 20=60.57%, 50=23.19%, 100=0.81%, 250=0.03%
  lat (msec)   : 2=0.01%, 4=0.01%, 10=0.01%
  fsync/fdatasync/sync_file_range:
    sync (usec): min=1052, max=434792, avg=3923.05, stdev=3609.22
    sync percentiles (usec):
     |  1.00th=[  1237],  5.00th=[  1385], 10.00th=[  1483], 20.00th=[  1663],
     | 30.00th=[  1876], 40.00th=[  2278], 50.00th=[  4359], 60.00th=[  4752],
     | 70.00th=[  5211], 80.00th=[  5669], 90.00th=[  6325], 95.00th=[  6849],
     | 99.00th=[  8455], 99.50th=[ 12649], 99.90th=[ 22938], 99.95th=[ 23725],
     | 99.99th=[166724]
  cpu          : usr=0.33%, sys=1.60%, ctx=109419, majf=0, minf=14
  IO depths    : 1=200.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
     issued rwts: total=0,45590,0,0 short=45590,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
  WRITE: bw=569KiB/s (583kB/s), 569KiB/s-569KiB/s (583kB/s-583kB/s), io=100.0MiB (105MB), run=179902-179902msec

Disk stats (read/write):
  vda: ios=4/120187, merge=0/53744, ticks=56/185772, in_queue=9776, util=1.54%
</code></pre>
<p>主要看<br><code>fsync/fdatasync/sync_file_range:</code>项的<code> 99.00th=[  18455], 99.50th=[ 12649],</code></p>
<p>表示百分之99的sync为18455usec，对应的etcd要求写入WAL文件时百分之99的fdatasync请求必须小于 10 毫秒。<br><a href="https://etcd.io/docs/v3.4/op-guide/performance/">https://etcd.io/docs/v3.4/op-guide/performance/</a></p>
<p>参考链接：</p>
<pre><code>https://blog.happyhack.io/2021/08/05/fio-and-etcd/
https://www.suse.com/support/kb/doc/?id=000020100
https://www.ibm.com/cloud/blog/using-fio-to-tell-whether-your-storage-is-fast-enough-for-etcd
https://blog.csdn.net/yunxiao6/article/details/108615472
</code></pre>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>flexvolume简介</title>
    <url>/2019/02/07/flexvolume/</url>
    <content><![CDATA[<h3 id="概述："><a href="#概述：" class="headerlink" title="概述："></a>概述：</h3><p>在实际使用中，我们的Kubernetes集群需要与外部存储进行对接，这时需要使用使用存储驱动，通过存储驱动去对接存储，Kubernetes有内置一些驱动能对接常见存储如nfs、ceph-rbd、ceph-fs、iscsi等，当我们需要对接的存储内置不支持时，这时我们就需要使用扩展存储驱动的方式去对接，在Kubernetes中自定义扩展存储驱动方式有两种一种是通过flexvolume另外一种是CSI。  </p>
<p>flexvolume是在Kubernetes1.2版本中存在于Kubernetes集群Kubernetes1.8版本正式GA的，用户将编写好的插件的二进制可执行文件放置到宿主机的指定目录即可正常调用使用。二进制可执行文件可以使用任意开发语言开发，但需要定义好几个指定接口<br>接口如下：<br>init：kubelet&#x2F;kube-controller-manager 初始化存储插件时调用，插件需要返回是否需要要 attach 和 detach 操作<br>attach：将存储卷挂载到 Node 上<br>detach：将存储卷从 Node 上卸载<br>waitforattach： 等待 attach 操作成功（超时时间为 10 分钟）<br>isattached：检查存储卷是否已经挂载<br>mountdevice：将设备挂载到指定目录中以便后续 bind mount 使用<br>unmountdevice：将设备取消挂载<br>mount：将存储卷挂载到指定目录中<br>umount：将存储卷取消挂载  </p>
<p>Flexvolume从v1.8开始支持动态检测驱动程序的能力，初始化和更新驱动生效只需要重启kubelet即可生效。  </p>
<h3 id="flexvolume驱动开发的几个关键点"><a href="#flexvolume驱动开发的几个关键点" class="headerlink" title="flexvolume驱动开发的几个关键点"></a>flexvolume驱动开发的几个关键点</h3><p>flexvolume的开发主要注意几点：<br>1、flexvolume插件的详细日志在kubelet的日志中，kubectl看的只是pod运行的状态。<br>2、插件脚本中任何echo到前台的语句都要注释掉，返回给kubelet的只能是标准的json格式。<br>3、如果需要调试可以通过echo将输出的调试信息&gt;&gt;重定向到外部文本。  </p>
<p>这里以官方的LVM例子和两种常见的Kubernetes集群部署方式为例两种讲解一下flexvolume驱动的使用<br>将下例驱动从github中clone下来。<br><a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/flexvolume">https://github.com/kubernetes/examples/tree/master/staging/volumes/flexvolume</a></p>
<p>在进行验证前请保证测试机器有一块空闲的磁盘。<br>在使用flexvolume时需要配置kubelet参数–enable-controller-attach-detach&#x3D;false   </p>
<h3 id="RKE部署集群方式"><a href="#RKE部署集群方式" class="headerlink" title="RKE部署集群方式"></a>RKE部署集群方式</h3><p>这里以LVM为例，其他插件类式</p>
<h4 id="创建lvm"><a href="#创建lvm" class="headerlink" title="创建lvm"></a>创建lvm</h4><p>创建pv  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pvcreate /dev/sdx</span><br></pre></td></tr></table></figure>
<p>创建vg  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vgcreate /dev/sdx vg0</span><br></pre></td></tr></table></figure>
<p>创建LV  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lvcreate -L 19G vg0 //根据实际情况修改命令</span><br></pre></td></tr></table></figure>
<h4 id="修改rke部署Kubernetes集群yaml文件"><a href="#修改rke部署Kubernetes集群yaml文件" class="headerlink" title="修改rke部署Kubernetes集群yaml文件"></a>修改rke部署Kubernetes集群yaml文件</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubernetes_version: v1.11.3-rancher1-1</span><br><span class="line">services:</span><br><span class="line">    kubelet:</span><br><span class="line">       extra_args:</span><br><span class="line">        enable-controller-attach-detach: false</span><br><span class="line">       extra_binds:</span><br><span class="line">        - &quot;/usr/libexec/kubernetes/kubelet-plugins:/var/lib/kubelet/volumeplugins&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>主要有两点：  </p>
<ul>
<li><p>配置enable-controller-attach-detach: false 这样是直接让kubelet去attache设备到pod不然会报错Volume has not been added to the list of VolumesInUse in the node’s volume status for volume.表示该节点的附加&#x2F;分离操作仅委派给controller-manager</p>
</li>
<li><p>因为rke部署的Kubernetes集群kubelet是容器化的所以需要将存储插件目录映射到宿主机上。</p>
</li>
</ul>
<h4 id="将flexvolume驱动从github上clone下来并修改"><a href="#将flexvolume驱动从github上clone下来并修改" class="headerlink" title="将flexvolume驱动从github上clone下来并修改"></a>将flexvolume驱动从github上clone下来并修改</h4><p><a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/flexvolume">https://github.com/kubernetes/examples/tree/master/staging/volumes/flexvolume</a>  </p>
<p>在deploy目录创建drivers文件夹，将父目录的lvm驱动复制到drivers文件夹，整体目录结构如下  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">|-- deploy</span><br><span class="line">|   |-- deploy.sh</span><br><span class="line">|   |-- Dockerfile</span><br><span class="line">|   |-- drivers</span><br><span class="line">|   |    -- lvm</span><br><span class="line">|   |-- ds.yaml</span><br><span class="line">|   `-- README.md</span><br><span class="line">|-- dummy</span><br><span class="line">|-- dummy-attachable</span><br><span class="line">|-- nfs</span><br><span class="line">|-- lvm</span><br><span class="line">|-- nginx-dummy-attachable.yaml</span><br><span class="line">|-- nginx-dummy.yaml</span><br><span class="line">|-- nginx-lvm.yaml</span><br><span class="line">|-- nginx-nfs.yaml</span><br><span class="line">|-- nginx.yaml</span><br><span class="line">`-- README.md</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>修改deploy.sh（主要为了生成的文件夹名为k8s~lvm，pod调用驱动时更加清晰）<br>改成如下<br>VENDOR&#x3D;${VENDOR:-k8s}<br>DRIVER&#x3D;${DRIVER:-lvm}  </p>
<p>生成驱动镜像<br>切换到deploy目录生成驱动部署镜像  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker build -t image_name:tag .</span><br></pre></td></tr></table></figure>
<p>修改ds.yaml文件<br>Image修改为我们刚刚生成镜像名和tag<br>Hostpath修改为  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hostPath:</span><br><span class="line">            # TODO Change to the Flexvolume plugin directory of your cluster.</span><br><span class="line">            path: /usr/libexec/kubernetes/kubelet-plugins/</span><br></pre></td></tr></table></figure>
<p> 这个的作用主要是部署这个daemonset后会自动将驱动放到这个目录，这样就不用我们一台一台主要去复制了，另外修改启动也直接改这个yaml那全部节点也都会自动修改。这个目录是根我们刚刚在rke目录根kubelet映射那个目录是一样的，因为kubelet是容器化的要将这个目录映射到kubelet容器内。</p>
<p>部署这个daemonset</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f ds.yaml</span><br></pre></td></tr></table></figure>
<p>检查驱动  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get ds</span><br><span class="line">NAME      DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span><br><span class="line">flex-ds         1                    1              1                   1                        1                      &lt;none&gt;          9d</span><br></pre></td></tr></table></figure>

<p>在宿主机上<br>检查是否生成k8s~lvm驱动文件夹  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ls /usr/libexec/kubernetes/kubelet-plugins/</span><br><span class="line">k8s~lvm</span><br></pre></td></tr></table></figure>
<p>测试<br>使用如下测试案例</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: nginx</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: test</span><br><span class="line">      mountPath: /data</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 80</span><br><span class="line">  volumes:</span><br><span class="line">  - name: test</span><br><span class="line">    flexVolume:</span><br><span class="line">      driver: &quot;k8s/lvm&quot;</span><br><span class="line">      fsType: &quot;ext4&quot;</span><br><span class="line">      options:</span><br><span class="line">        volumeID: &quot;lvol0&quot;</span><br><span class="line">        size: &quot;19.00g&quot;</span><br><span class="line">        volumegroup: &quot;vg0&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>driver就是我们刚刚那个插件目录的k8s<del>lvm这里将</del>换成&#x2F;.</li>
<li>格式化文件系统，这里推荐ext4，xfs在大环境下驱动有问题，需要改驱动代码，参考：<a href="https://www.jianshu.com/p/3ecc7f72cf9f">https://www.jianshu.com/p/3ecc7f72cf9f</a></li>
<li>VolumeId：就是我们刚刚创建的lv的名字，若不记得lvs则显示</li>
<li>Volumegroup：就是vg的名字，vgs可以看见</li>
</ul>
<p>创建测试应用  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod</span><br><span class="line">NAME            READY     STATUS    RESTARTS   AGE</span><br><span class="line">flex-ds-5qnwj   1/1       Running   0          4m</span><br></pre></td></tr></table></figure>
<p>成功运行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df -h查看挂载</span><br><span class="line">/dev/mapper/vg0-lvol0   19G   45M   18G   1% /var/lib/kubelet/plugins/kubernetes.io/flexvolume/k8s/lvm/mounts/test</span><br></pre></td></tr></table></figure>
<h3 id="kubeadm部署集群方式"><a href="#kubeadm部署集群方式" class="headerlink" title="kubeadm部署集群方式"></a>kubeadm部署集群方式</h3><p>1、创建lvm方式同上。  </p>
<p>2、配置kubelet的–enable-controller-attach-detach&#x3D; false参数<br>编辑vim &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;kubelet.service.d&#x2F;10-kubeadm.conf<br>在Environment选项后面加上–enable-controller-attach-detach&#x3D;false参数<br>重启kubelet使kubelet生效  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure>

<p>剩下部署方式根rke一样  </p>
<h4 id="修改flexvolumeq驱动"><a href="#修改flexvolumeq驱动" class="headerlink" title="修改flexvolumeq驱动"></a>修改flexvolumeq驱动</h4><p>官方lvm驱动案例，功能太弱了，需要提前将pv、vg、lv这些创建好，我们可以做一些修改，比如增加自动创建LV卷的功能，这样我们就不需要将LV卷一个个提前创建好，可以更省事。<br>通过对驱动程序分析<br>attach函数主要负责块设备检测，如果在yaml中定义的块不存在则直接退出  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">attach() &#123;</span><br><span class="line">	JSON_PARAMS=$1</span><br><span class="line">	SIZE=$(echo $1 | jq -r &#x27;.size&#x27;)</span><br><span class="line"></span><br><span class="line">	DMDEV=$(getdevice)</span><br><span class="line">	if [ ! -b &quot;$&#123;DMDEV&#125;&quot; ]; then</span><br><span class="line">		err &quot;&#123;\&quot;status\&quot;: \&quot;Failure\&quot;, \&quot;message\&quot;: \&quot;Volume $&#123;VOLUMEID&#125; does not exist\&quot;&#125;&quot;</span><br><span class="line">		exit 1</span><br><span class="line">	fi</span><br><span class="line">	log &quot;&#123;\&quot;status\&quot;: \&quot;Success\&quot;, \&quot;device\&quot;:\&quot;$&#123;DMDEV&#125;\&quot;&#125;&quot;</span><br><span class="line">	exit 0</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>domountdevice函数主要负责进行格式化块设备，和mount操作  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">domountdevice() &#123;</span><br><span class="line">	MNTPATH=$1</span><br><span class="line">	DMDEV=$2</span><br><span class="line">	FSTYPE=$(echo $3|jq -r &#x27;.[&quot;kubernetes.io/fsType&quot;]&#x27;)</span><br><span class="line"></span><br><span class="line">	if [ ! -b &quot;$&#123;DMDEV&#125;&quot; ]; then</span><br><span class="line">		err &quot;&#123;\&quot;status\&quot;: \&quot;Failure\&quot;, \&quot;message\&quot;: \&quot;$&#123;DMDEV&#125; does not exist\&quot;&#125;&quot;</span><br><span class="line">		exit 1</span><br><span class="line">	fi</span><br><span class="line"></span><br><span class="line">	if [ $(ismounted) -eq 1 ] ; then</span><br><span class="line">		log &quot;&#123;\&quot;status\&quot;: \&quot;Success\&quot;&#125;&quot;</span><br><span class="line">		exit 0</span><br><span class="line">	fi</span><br><span class="line"></span><br><span class="line">	VOLFSTYPE=`blkid -o udev $&#123;DMDEV&#125; 2&gt;/dev/null|grep &quot;ID_FS_TYPE&quot;|cut -d&quot;=&quot; -f2`</span><br><span class="line">	if [ &quot;$&#123;VOLFSTYPE&#125;&quot; == &quot;&quot; ]; then</span><br><span class="line">		mkfs -t $&#123;FSTYPE&#125; $&#123;DMDEV&#125; &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">		if [ $? -ne 0 ]; then</span><br><span class="line">			err &quot;&#123; \&quot;status\&quot;: \&quot;Failure\&quot;, \&quot;message\&quot;: \&quot;Failed to create fs $&#123;FSTYPE&#125; on device $&#123;DMDEV&#125;\&quot;&#125;&quot;</span><br><span class="line">			exit 1</span><br><span class="line">		fi</span><br><span class="line">	fi</span><br><span class="line"></span><br><span class="line">	mkdir -p $&#123;MNTPATH&#125; &amp;&gt; /dev/null</span><br><span class="line"></span><br><span class="line">	mount $&#123;DMDEV&#125; $&#123;MNTPATH&#125; &amp;&gt; /dev/null</span><br><span class="line">	if [ $? -ne 0 ]; then</span><br><span class="line">		err &quot;&#123; \&quot;status\&quot;: \&quot;Failure\&quot;, \&quot;message\&quot;: \&quot;Failed to mount device $&#123;DMDEV&#125; at $&#123;MNTPATH&#125;\&quot;&#125;&quot;</span><br><span class="line">		exit 1</span><br><span class="line">	fi</span><br><span class="line">	log &quot;&#123;\&quot;status\&quot;: \&quot;Success\&quot;&#125;&quot;</span><br><span class="line">	exit 0</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>如果要实现自动基于VG创建LV卷，在自动挂载，那需要在格式化和mount前完成，所以需要我们把这个功能在attach函数中实现  </p>
<pre><code>attach() &#123;
    JSON_PARAMS=$1
    VOLUMEID=$(echo $&#123;JSON_PARAMS&#125; | jq -r &#39;.volumeID&#39;)
    VG=$(echo $&#123;JSON_PARAMS&#125; | jq -r &#39;.volumegroup&#39;)
    THINPOOL=$(echo $&#123;JSON_PARAMS&#125; | jq -r &#39;.thinpool | select(type == &quot;string&quot;)&#39;)
    SIZE=$(echo $&#123;JSON_PARAMS&#125; | jq -r &#39;.size&#39;)
    DMDEV=$(getdevice)
    if [ ! -b &quot;$&#123;DMDEV&#125;&quot; ]; then
        LVCREATE_OPTS=&quot;-n $&#123;VOLUMEID&#125; -L $&#123;SIZE&#125; $&#123;VG&#125;&quot;
        lvcreate $&#123;LVCREATE_OPTS&#125; 2&gt;&amp;1 &gt; /dev/null
        if [ &quot;$?&quot; != &quot;0&quot; ]; then
            err &quot;&#123;\&quot;status\&quot;: \&quot;Failure\&quot;, \&quot;message\&quot;: \&quot;Could not attach $&#123;VG&#125;/$&#123;VOLUMEID&#125;\&quot;&#125;&quot;
            exit 1
        fi
    fi
    log &quot;&#123;\&quot;status\&quot;: \&quot;Success\&quot;, \&quot;device\&quot;:\&quot;$&#123;DMDEV&#125;\&quot;&#125;&quot;
    exit 0
&#125;
</code></pre>
<p>最终实现如上。  </p>
<p>参考链接  </p>
<p><a href="https://kubernetes.feisky.xyz/cha-jian-kuo-zhan/volume/flex-volume">https://kubernetes.feisky.xyz/cha-jian-kuo-zhan/volume/flex-volume</a>  </p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernete</tag>
      </tags>
  </entry>
  <entry>
    <title>Gitlab-CI使用</title>
    <url>/2020/11/27/gitlab_ci/</url>
    <content><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>GitLab CI &#x2F; CD是GitLab的一部分,gitlab 8.0版本开始新增的功能，是用Ruby和Go语言编写的。根我们通常的CI系统不一样通常的是一个master-slave架构，即使没有slave，master一样可以做CI，slave只是做为一个压力分担功能，gitlab是gitlab-server本身是不执行的，是通过api与GitLab Runner交互让gitlab-runner去执行CI。</p>
<p>GitLab Runner是一个go语言编写程序，它可以运行在任何可以运行go环境的平台上(二进制包、docker、k8s)</p>
<p>易用性方面<br>同样也是Everything as docker<br>Gitlab CI 整个流程和 Drone 以及流行的 Travis CI 都是比较类似的，通过在项目中添加一个 .gitlab-ci.yml 的配置文件，配置文件中描述构建流水线来执行任务，对不同编程语言的编译通过不同的docker image实现。<br>让CI工作所需的步骤可归纳为<br>添加.gitlab-ci.yml到存储库的根目录<br>配置一个Runner</p>
<p>Runner分两种类型<br>Specific Runners （独享的runner）<br>Shared Runners（共享的runner）</p>
<p>软件版本</p>
<p>gitlab：GitLab Community Edition 13.7.0<br>Kubernetes：1.17.4</p>
<h3 id="部署gitlab-runner"><a href="#部署gitlab-runner" class="headerlink" title="部署gitlab-runner"></a>部署gitlab-runner</h3><h4 id="创建测试项目代码和Dockerfile"><a href="#创建测试项目代码和Dockerfile" class="headerlink" title="创建测试项目代码和Dockerfile"></a>创建测试项目代码和Dockerfile</h4><p>在gitlab上创建一个项目名为go-server，编写个应用<br>server.go</p>
<pre><code>package main

import (
    &quot;fmt&quot;
    &quot;log&quot;
    &quot;net/http&quot;
)

func hello(w http.ResponseWriter, r *http.Request) &#123;
    fmt.Fprintf(w, &quot;Hello World&quot;)
&#125;

func main() &#123;
    http.HandleFunc(&quot;/&quot;, hello)
    if err := http.ListenAndServe(&quot;:8080&quot;, nil); err != nil &#123;
        log.Fatal(err)
    &#125;
&#125;
</code></pre>
<p>创建Dockerfile<br>Dockerfile</p>
<pre><code>FROM golang
WORKDIR /go
ADD server /go
CMD [&quot;./server&quot;]
</code></pre>
<p>deployment.yaml</p>
<p>部署到Kubernetes的yaml文件</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: go-demo
  labels:
    app: go-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: go-demo
  template:
    metadata:
      labels:
        app: go-demo
    spec:
      containers:
      - name: go-demo
        image: 172.16.1.31/library/go-server-demo:IMAGE_TAG
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: go-demo
spec:
  type: NodePort
  selector:
    app: go-demo
  ports:
    - port: 8080
      targetPort: 8080
      nodePort: 30007
</code></pre>
<h4 id="在Kubernetes集群中部署gitlab-runner"><a href="#在Kubernetes集群中部署gitlab-runner" class="headerlink" title="在Kubernetes集群中部署gitlab-runner"></a>在Kubernetes集群中部署gitlab-runner</h4><p>整体流程<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gitlab-ci-5.png"></p>
<ul>
<li>先执行代码构建，构建出制品</li>
<li>基于制品构建应用的容器镜像</li>
<li>部署到Kubernetes集群中</li>
</ul>
<p>在gitlab项目页申请runner连接gitlab的地址和token，注意保存</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gitlab-ci-1.png"></p>
<p>git clone部署runner到Kubernetes中的helm文件</p>
<pre><code>git clone https://gitlab.com/gitlab-org/charts/gitlab-runner.git
</code></pre>
<p>修改values.yaml 以下几个地方：</p>
<pre><code>gitlabUrl:   //填写在项目页申请的gitlab地址和token
runnerRegistrationToken:
rbac:
  create: true //启用RBAC

runners:
  # runner configuration, where the multi line strings is evaluated as
  # template so you can specify helm values inside of it.
  #
  # tpl: https://helm.sh/docs/howto/charts_tips_and_tricks/#using-the-tpl-function
  # runner configuration: https://docs.gitlab.com/runner/configuration/advanced-configuration.html
  config: |
    [[runners]]
      [runners.kubernetes]
        namespace = &quot;&#123;&#123;.Release.Namespace&#125;&#125;&quot;
        image = &quot;ubuntu:20.04&quot;
        privileged = true
  tags: &quot;k8s-runner&quot;//执行CI任务时可以通过tags匹配对应的runner，这里指定为k8s-runner
</code></pre>
<p>通过helm部署runner到Kubernetes中</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">helm install --namespace default gitlab-runner gitlab-runner/</span><br></pre></td></tr></table></figure>

<p>查看是否部署成功，主要是看POD是否正常启动</p>
<pre><code>kubectl get pod 
NAME                                          READY   STATUS    RESTARTS   AGE
gitlab-runner-gitlab-runner-b5744f77c-jwnjk   1/1     Running   0          5d
</code></pre>
<p>默认是部署一个副本，如果需要多个runner实例，扩容对应的Deployment副本数即可。<br>同样是刚刚项目CI&#x2F;CD设置页可以看见已经可用的runner了<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gitlab-ci-2.png"></p>
<p>在gitlab-ci中创建环境变量用于存放一些连接认证信息环境变量<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gitlab-ci-3.png"></p>
<p>将对应的信息填写到的对应的环境变量中，包括连接镜像仓库的帐号和密码和连接Kubernetes集群的config文件</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gitlab-ci-3.png"></p>
<p>KUBECONFIG文件进行base64编码存入，为了防止重定向后格式损坏</p>
<pre><code> base64  /root/.kube/config -w0
</code></pre>
<h3 id="测试案例"><a href="#测试案例" class="headerlink" title="测试案例"></a>测试案例</h3><p>编写.gitlab-ci.yaml</p>
<pre><code>stages:
  - package
  - docker_build
  - deploy_k8s
build_job:
  image: golang:alpine
  stage: package
  tags:
    - k8s-runner
  script:
    - go build server.go
  artifacts:
    paths:
    - /builds/root/go-server
docker_build_job:
  image: docker:19.03.0
  services:
    - name: docker:19.03.0-dind
      command: [&quot;--insecure-registry=0.0.0.0/0&quot;,&quot;--registry-mirror=https://vqgjby9l.mirror.aliyuncs.com&quot;]
  variables:
    DOCKER_HOST: tcp://127.0.0.1:2375
    DOCKER_TLS_CERTDIR: &quot;&quot;
  stage: docker_build
  tags:
    - k8s-runner
  script:
    - docker info
    - docker login -u $REGISTRY_USERNAME -p $REGISTRY_PASSWORD 172.16.1.31
    - docker build -t 172.16.1.31/library/go-server-demo:$CI_PIPELINE_ID .
    - docker push 172.16.1.31/library/go-server-demo:$CI_PIPELINE_ID
deploy_k8s_job:
  image: bitnami/kubectl:1.17
  stage: deploy_k8s
  tags:
    - k8s-runner
  script:
    - echo $KUBECONFIG |base64 -d &gt; config
    - sed -i &quot;s/IMAGE_TAG/$CI_PIPELINE_ID/g&quot; deployment.yaml
    - cat deployment.yaml
    - kubectl apply -f deployment.yaml --kubeconfig config
</code></pre>
<p>注：<br>1、因为Docker这边我们需要配置insecure-Registry需要这里通过Service的方式配置，所以这里在stage启用的是安装docker-cli的镜像，然后在server里面启用的是docker-dind的镜像，连接通过tcp连接使用<br>这里需要注意新版docker需要使用DOCKER_TLS_CERTDIR参数设置为空，不然无法连接。</p>
<p>2、kubeconfig文件使用base64编码主要为了重定向后格式不会造成损坏。</p>
<p>3、因为gitlab-ci每个阶段都是启动一个容器去执行构建任务，所以在每个阶段产生的制品如何给下个阶段使用是一个很大问题，这里可以配置artifacts参数将对应需要保留的制品保存到下个阶段。</p>
<p>4、构建的缓存配置，可以参考以下链接<br><a href="https://docs.gitlab.com/runner/configuration/advanced-configuration.html">https://docs.gitlab.com/runner/configuration/advanced-configuration.html</a></p>
<p>5、Docker镜像构建这里使用的是docker-dind的方式，实际上也可以直接透传主机的docker.sock文件到POD内使用，但不是特别安全，也可以使用一些其他解决方案如kaniko</p>
<p>上传后gitlab-ci会自动执行cicd部署到Kubernetes集群中</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gitlab-ci-4.png"></p>
<p>自动部署完成，查看Kubernetes集群中部署对象</p>
<pre><code>kubectl get pod 

NAME                                          READY   STATUS    RESTARTS   AGE
go-demo-7f697958d4-7fbcd                      1/1     Running   0          32h
</code></pre>
<p>访问节点30007端口</p>
<pre><code>curl 192.168.0.6:30007
Hello World
</code></pre>
<h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>优点：</p>
<ul>
<li>与gitlab集成度非常高</li>
<li>不需要单独部署有gitlab&gt;&#x3D;8.0 就能直接使用</li>
<li>runner支持Autoscale</li>
<li>UI可视化，可操作性强，可针对但个流程进行重复执行及报表展示</li>
<li>CI完全对应你这个代码库，每个项目对应自己CI</li>
</ul>
<p>缺点：</p>
<ul>
<li>没有插件，对接第三方系统需要自己实现</li>
<li>只能支持gitlab代码仓库</li>
</ul>
]]></content>
      <categories>
        <category>CI/CD</category>
      </categories>
      <tags>
        <tag>CI/CD</tag>
      </tags>
  </entry>
  <entry>
    <title>GPU互联方式</title>
    <url>/2023/11/03/gpu-1/</url>
    <content><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a><a href="#%E6%A6%82%E8%BF%B0" title="概述"></a>概述</h3><p>随着AI大模型的深入发展，越来越多用户需要将大量GPU投入到环境中进行AI训练，AI训练本质就是利用一堆GPU做并行计算，训练、推理。计算方式有数量并行（将训练的数据拆成不同的子集分给不同的GPU去做运算）、模型并行（把模型中神经网络的不同层拆分给不同GPU计算）、张量并行（把同一层张量拆分成不同小块给不同GPU计算）。无论哪种方式都需要将GPU间大量数据交互，对网络要求是高带宽、低延时、无拥塞、无丢包。</p>
<h3 id="同服务器内GPU间连接"><a href="#同服务器内GPU间连接" class="headerlink" title="同服务器内GPU间连接"></a><a href="#%E5%90%8C%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%86%85GPU%E9%97%B4%E8%BF%9E%E6%8E%A5" title="同服务器内GPU间连接"></a>同服务器内GPU间连接</h3><h4 id="PCIE连接"><a href="#PCIE连接" class="headerlink" title="PCIE连接"></a><a href="#PCIE%E8%BF%9E%E6%8E%A5" title="PCIE连接"></a>PCIE连接</h4><p>购买的单块GPU卡，直接插入服务器的PCIE插槽，GPU通过PCIE通道实现GPU和CPU互联，PCIE连接最大的问题是整体速率太低，不满足当前AI大模型的需求，当前最高的PCIE5.0和Nvlink4.0相比都会存在7倍的差异。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gpu1-1.jpg"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gpu1-2.jpg"></p>
<p>图片来源：<code>https://www.sohu.com/a/747247345_121865302#:~:text=%E7%9B%B8%E6%AF%94%E4%BA%8EPCIe%EF%BC%8CNVLink,%E5%A5%BD%E7%9A%84%E6%80%A7%E8%83%BD%E5%92%8C%E6%95%88%E7%8E%87%E3%80%82&amp;text=%E7%AE%80%E8%80%8C%E8%A8%80%E4%B9%8B%EF%BC%8CPCIe,%E5%88%86%E5%88%AB%E6%9C%89%E5%93%AA%E4%BA%9B%E4%BC%98%E5%8A%A3%E5%8A%BF%EF%BC%9F）</code></p>
<p>PCIE合适场景：<br>1、单卡性能能满足业务需求，可以直接单卡透传场景。</p>
<h4 id="Nvlink连接"><a href="#Nvlink连接" class="headerlink" title="Nvlink连接"></a><a href="#Nvlink%E8%BF%9E%E6%8E%A5" title="Nvlink连接"></a>Nvlink连接</h4><p>PCIE存在带宽瓶颈，并且只能实现两两GPU互联，NVLink技术使GPU无需通过PCIe总线即可访问远程GPU内存，整体性能比PCIE高，并且结合Nvswitch可以实现八卡互联。</p>
<p>需要实现2-8个GPU互联，统一整合提供给业务用，需要SXM接口板卡，SXM规格GPU主要用在DGX服务器（目前只能从NVIDIA购买）上，另外一类就是合作伙伴设计的HGX板的服务器上。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gpu1-3.jpg"><br>如何将这么多GPU连接起来呢？通过NVLINK连接实现高带宽传输<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gpu1-4.jpg"></p>
<table>
<thead>
<tr>
<th>PCIe版本</th>
<th>PCIe 1.0</th>
<th>PCIe 2.0</th>
<th>PCIe 3.0</th>
<th>PCIe 4.0</th>
<th>PCIe 5.0</th>
</tr>
</thead>
<tbody><tr>
<td>发布时间</td>
<td>2003</td>
<td>2007</td>
<td>2010</td>
<td>2017</td>
<td>2019</td>
</tr>
<tr>
<td>编码方式</td>
<td>8b&#x2F;10b</td>
<td>8b&#x2F;10b</td>
<td>128b&#x2F;130b</td>
<td>128b&#x2F;130b</td>
<td>128b&#x2F;130b</td>
</tr>
<tr>
<td>信号速率（GT&#x2F;S）</td>
<td>2.5</td>
<td>5</td>
<td>8</td>
<td>16</td>
<td>32</td>
</tr>
<tr>
<td>X16带宽（GB&#x2F;S）</td>
<td>8</td>
<td>16</td>
<td>32</td>
<td>64</td>
<td>128</td>
</tr>
</tbody></table>
<p>第四代NVLINK带宽，例如单个 NVIDIA H100 Tensor Core GPU 支持多达 18 个 NVLink 连接，总带宽为 900 GB&#x2F;s，是 PCIe 5.0 带宽的 7 倍。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gpu1-5.jpg"><br>NVLINK提供的两个GPU卡之间的互联，如果需要多卡互联需要使用NVSwitch，比如一台DGX服务器里面的8张H800 GPU<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gpu1-6.jpg"></p>
<p>如下图所示，每个H100 GPU 连接到4个NVLink交换芯片，GPU之间的NVLink带宽达到900 GB&#x2F;s。同时，每个H100 SXM GPU 也通过 PCIe连接到CPU，因此8个GPU中的任何一个计算的数据都可以送到CPU。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gpu1-7.jpg"><br>Nvlink合适场景：<br>1、单卡算力满足不了业务需求，需要多卡互联场景。</p>
<h4 id="跨节点互联"><a href="#跨节点互联" class="headerlink" title="跨节点互联"></a><a href="#%E8%B7%A8%E8%8A%82%E7%82%B9%E4%BA%92%E8%81%94" title="跨节点互联"></a>跨节点互联</h4><h5 id="RDMA概述"><a href="#RDMA概述" class="headerlink" title="RDMA概述"></a><a href="#RDMA%E6%A6%82%E8%BF%B0" title="RDMA概述"></a>RDMA概述</h5><p>训练超大模型需要多机多卡，需要将多个训练任务进行切分到不同卡上进行分布式训练，这里面涉及模型切分和卡间通信，主流的并行训练方式有数据并行、模型并行、张量并行、流水线并行等方式。所以对集群网络有很高要求，需要低延时、高带宽。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gpu1-8.jpg"></p>
<p>AI大模型GPU训练需要的网络带宽需要至少100Gbps~400Gbps，实现方式只能通过RDMA网络（Remote Direct Memory Access）实现。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gpu1-9.jpg"><br>从数据传输过程可以看出，数据在服务器的Buffer中进行了多次复制，并且需要在操作系统中添加或卸载TCP和IP头。这些操作不仅增加了数据传输延迟，而且消耗了大量的CPU资源，无法满足高性能计算的要求。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gpu1-10.jpg"><br>RDMA可以绕过操作系统内核，直接访问到另外一台服务器内存，减少中间层，提高整体转发效率，降低延时。<br>RDMA与传统TCP网络相比带来的价值<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gpu1-11.jpg"></p>
<h4 id="RDMA的核心价值："><a href="#RDMA的核心价值：" class="headerlink" title="RDMA的核心价值："></a><a href="#RDMA%E7%9A%84%E6%A0%B8%E5%BF%83%E4%BB%B7%E5%80%BC%EF%BC%9A" title="RDMA的核心价值："></a>RDMA的核心价值：</h4><p>内存零拷贝（Zero Copy）：RDMA应用程序可以绕过内核网络栈直接进行数据传输，不需要将应用程序从用户态内存空间拷贝到内核网络栈内核空间。<br>内核旁路（Kernel bypass）：直接从NIC到达用户态内存，减少了CPU从内核拷贝到用户态的过程。<br>CPU offload：应用程序可以直接访问远程主机内存降低远程主机中CPU的消耗。</p>
<h4 id="RDMA实现："><a href="#RDMA实现：" class="headerlink" title="RDMA实现："></a><a href="#RDMA%E5%AE%9E%E7%8E%B0%EF%BC%9A" title="RDMA实现："></a>RDMA实现：</h4><p>Infiniband：Mellanox主导的一项技术，后续被NVIDIA收购，完全区别于传统以太网，有自己独立的协议栈、需要独立的网卡、线缆、网络设备支持，整体成本较高，目前IB主推速率200Gbps-HDR和400Gbps-EDR。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gpu1-12.jpg"><br>Roce：基于 Ethernet的RDMA由IBTA提出，分为两个版本，Rocev1和RoceV2，V1版本没有继承以太网的网络层所以没有IP字段，无法被路由和跨网段，基本上没有应用场景，V2版本基于UDP使用了以太网的网络层，通过PFC（基于优先级的流量控制），ECN（显式拥塞通知）以及DCQCN（Data Center Quantized Congestion Notification）等技术对传统以太网络改造，实现无损以太网络，以确保零丢包。</p>
<p>iWARP：基于TCP协议需要实现，在TCP之上构建DDP（Data Placement Protocol）实现零拷贝的功能。</p>
<p>Roce和iWARP都只需要网卡支持即可，交换机可以正常使用以太网交换机，Rocev2的DCQCN算法还需要交换机支持RED（Random early detection）和ECN（Explicit Congestion Notification）功能</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gpu1-13.jpg"></p>
<h3 id="GPU池化方案"><a href="#GPU池化方案" class="headerlink" title="GPU池化方案"></a><a href="#GPU%E6%B1%A0%E5%8C%96%E6%96%B9%E6%A1%88" title="GPU池化方案"></a>GPU池化方案</h3><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a><a href="#%E6%A6%82%E5%BF%B5" title="概念"></a>概念</h4><p>GPU池化主要用于将GPU资源如CPU和内存资源池化一样，关键点在于按需调用，动态伸缩，用完释放。GPU池化能解决的问题有：1、GPU资源利用不均匀。2、远程调用GPU。3、多种异构GPU的统一支持。<br>AI领域用户对GPU的调用链路如下：<br>1、用户app为业务层主要运行用户的训练或推理任务。<br>2、Framework框架层主要深度学习框架pytorch、TensorFlow等<br>3、CUDA Runtime及周边生态库，如cudart、cublas、cudnn、cufft、cusparse等<br>4、CUDA User Driver：用户态CUDA Driver如cuda、nvml等<br>5、CUDA kernel Driver：内核态CUDA Driver如nvidia.ko和驱动<br>6、GPU卡硬件</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gpu1-14.jpg"><br>目前GPU池化方案基本上通过在CUDA Runtime&#x2F;Driver层拦截API实现。<br>GPU 池化也必须以同时满足故障隔离和算力隔离的方案作为基础。</p>
<h4 id="业内方案"><a href="#业内方案" class="headerlink" title="业内方案"></a><a href="#%E4%B8%9A%E5%86%85%E6%96%B9%E6%A1%88" title="业内方案"></a>业内方案</h4><p>Bitfusion<br>VMware旗下的Bitfusion有Server端和Client端。<br>Server端部署在带GPU的物理服务器中，server端用于将GPU虚拟化提供给多个业务使用，<br>Client端部署在实际需要使用GPU资源的业务节点上，Client端会将业务对GPU的需求拦截，然后通过网络传输给Bitfusion Server，计算完成后再返回结果。可以基于开源的cuda-hook代码实现：<a href="https://github.com/Bruce-Lee-LY/cuda_hook">https://github.com/Bruce-Lee-LY/cuda_hook</a><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gpu1-15.jpg"></p>
<p>实现方法：<br>Client端实现CUDA Driver，拦截全部对GPU的请求通过网络转发到Server端进行处理，server端完成后在返回给到app。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gpu1-16.jpg"></p>
<p>国内趋动科技Orion X解决方案<br>与Bitfusion比较类型，通过在业务侧部署Client端，拦截对CUDA Driver和请求转发到Server端进行处理。组件能力如下：<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gpu1-17.jpg"></p>
<ul>
<li>Orion Controller：负责整个GPU资源池的资源管理。其响应Orion Client的vGPU请求，并从GPU资源池中为Orion Client端的CUDA应用程序分配并返回Orion vGPU资源。</li>
<li>Orion Server：负责GPU资源化的后端服务程序，部署在每一个CPU以及GPU节点上，接管本机内的所有物理GPU。当Orion Client端应用程序运行时，通过Orion Controller的资源调度，建立和Orion Server的连接。Orion Server为其应用程序的所有CUDA调用提供一个隔离的运行环境以及真实GPU硬件算力。</li>
<li>Orion Client：模拟了NVidia CUDA的运行库环境，为CUDA程序提供了API接口兼容的全新实现。通过和Orion其他功能组件的配合，为CUDA应用程序虚拟化了一定数量的虚拟GPU（Orion vGPU）。使用CUDA动态链接库的CUDA应用程序可以通过操作系统环境设置，使得一个CUDA应用程序在运行时由操作系统负责链接到Orion Client提供的动态链接库上。由于Orion Client模拟了NVidia CUDA运行环境，因此CUDA应用程序可以透明无修改地直接运行在Orion vGPU之上。</li>
</ul>
<p>最大问题<br>底层依赖NVIDIA-MPS方案，将多个进程上的kernel发送到MPS server或者直接发送到GPU上计算，避免了多进程在GPU上context的频繁切换。缺点是故障率较高，特别是故障在进程间扩散一般是不能容忍的。</p>
<h4 id="框架实现"><a href="#框架实现" class="headerlink" title="框架实现"></a><a href="#%E6%A1%86%E6%9E%B6%E5%AE%9E%E7%8E%B0" title="框架实现"></a>框架实现</h4><p>DDP（Distributed Data Parallelism）<br>使用Pytorch框架的业务可以使用DDP实现多机多卡训练，提示GPU利用率。<br>PyTorch的DDP利用了数据并行和模型并行两种策略。在数据并行中，数据被划分成多个子集，并在不同的GPU上进行训练。这种策略的优势在于实现简单，但当数据集非常大时，可能会因为数据划分不均导致训练结果不一致。模型并行是将模型的不同部分分别放在不同的GPU上训练，这种策略可以避免数据划分的问题，但实现起来更为复杂。</p>
<p>参考链接：<br><a href="https://mp.weixin.qq.com/s/GYiZk3Fgqqse6YfAfvmX7g">https://mp.weixin.qq.com/s/GYiZk3Fgqqse6YfAfvmX7g</a><br><a href="https://www.nvidia.cn/data-center/nvlink/#:~:text=NVLink%20%E6%98%AF%E4%B8%80%E7%A7%8DGPU,%E5%A4%9A%E5%AF%B9%E5%A4%9AGPU%20%E9%80%9A%E4%BF%A1%E3%80%82">https://www.nvidia.cn/data-center/nvlink/#:~:text&#x3D;NVLink%20%E6%98%AF%E4%B8%80%E7%A7%8DGPU,%E5%A4%9A%E5%AF%B9%E5%A4%9AGPU%20%E9%80%9A%E4%BF%A1%E3%80%82</a><br><a href="https://www.sdnlab.com/25923.html">https://www.sdnlab.com/25923.html</a><br><a href="https://aijishu.com/a/1060000000133430">https://aijishu.com/a/1060000000133430</a></p>
]]></content>
      <categories>
        <category>GPU</category>
      </categories>
      <tags>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>GPU算力理解和规划</title>
    <url>/2024/08/09/gpu_power/</url>
    <content><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>做AI训练和推理场景中，主要看GPU的FLOPS（每秒浮点运算次数）衡量集群的算力能力单位为PFLOPS，也可以简称为P也上目前很多在建的智算中心通常会说新建的这个算力中心提供的算力上多少P，单卡用TFLOPS（Tera flops 每秒1万亿次浮点运算）</p>
<p>一个MFLOPS（megaFLOPS）等于10^6 FLOPS；<br>一个GFLOPS（gigaFLOPS）等于10^9 FLOPS；<br>一个TFLOPS（teraFLOPS）等于10^12 FLOPS；<br>一个PFLOPS（petaFLOPS）等于10^15 FLOPS；<br>一个EFLOPS（exaFLOPS）等于10^18 FLOPS。</p>
<p>以H100为例，可以看到在不同类型GPU卡下的性能指标<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gpu2-1.png"><br>SXM对应PCIE除了在显存带宽上存在差距，在不同精度下的性能也存在差异，所以这也是需要注意的在同型号卡不同接口类型存在的性能差异。<br>*表示采用稀疏技术</p>
<h3 id="精度单位"><a href="#精度单位" class="headerlink" title="精度单位"></a>精度单位</h3><p>在上图中可以看见存在FP64、FP32、TF32、FP16、INT8等这些精度单位，不同精度对应的模型训练效果占用存储空间和训练时间都会存在不同。<br>图片对应的精度带用Tensor Core的意思是支持专用硬件Tensor Core进行运算加速和混合精度训练的。</p>
<p>Tensor Core有两大优势：<br>优势一：性能增强<br>Tensor Core是NVIDIA在Volta架构引入的当时Tensor Core只为FP16进行优化，在Hopper架构 Tensor Core扩展了 TF32、FP64、FP16 和 INT8 精度，将性能提升3倍。</p>
<p>优势二：实现混合精度</p>
<p>通过Tensor Core可以实现混合精度将累加和累乘混合一起，比如使用半精度来加速矩阵乘法，使用单精度或双精度数据来修正结果，对应的可参考：<br><a href="https://blog.csdn.net/bestpasu/article/details/134098651">https://blog.csdn.net/bestpasu/article/details/134098651</a></p>
<p>FP64：双精度浮点数，占用64位（8字节）存储空间，主要用于大规模科学计算、工程计算等需要高精度计算的场景。<br>FP32：单精度浮点数，占用32位（4字节）存储空间<br>TF32 ：英伟达提出的代替FP32的单精度浮点格式，占用19位，指数位数值范围与FP32一样都是8位<br>BFLOAT16：用于半精度矩阵乘法计算的浮点数格式，占用16位存储空间，相对于FP16在保持存储空间相同的情况下能够提高运算精度和效率。<br>FP16：半精度浮点数占用16位（2字节）存储空间，通常用于模型训练过程中参数和梯度计算。<br>FP8：8位（1字节）存储空间，通常用于训练和推理场景，相比INT8， FP8 有更宽的动态范围， 更能精准捕获 LLM 中参数的数值分布<br>INT8 ：8位整数，通常用于模型训练完成后进行量化，从高精度浮点数，转换为低精度整型数，主要用于减少模型的大小和计算复杂性，同时尽可能减少精度损失的一种优化手段。</p>
<p>根据英伟达官网的表述，AI训练场景为缩短训练时间，主要使用BF16、FP8、TF32 和FP16；AI推理厂家为在低延迟下实现高吞吐量，主要使用TF32、BF16、FP16、FP8 和INT8；HPC（高性能计算）为实现在所需的高准确性下进行科学计算的功能，主要使用FP64。<br>（来自韭研公社APP）</p>
<h3 id="稀疏计算和稠密计算"><a href="#稀疏计算和稠密计算" class="headerlink" title="稀疏计算和稠密计算"></a>稀疏计算和稠密计算</h3><p>稀疏算力是指计算过程中，数据存储和传输中存在大量空缺或零值的计算方式。在稀疏算力中，数据通常以矩阵的形式存在，其中大部分元素为0。稀疏算力在处理大规模稀疏数据时具有很高的效率。</p>
<p>稠密算力是指计算过程中，数据存储和传输中不存在大量空缺或零值的计算方式。在稠密算力中，数据通常以矩阵的形式存在，其中大部分元素不为0。稠密算力在处理大规模稠密数据时具有很高的效率。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gpu3-3.png"></p>
<p>应用场景：</p>
<p>稀疏算力：稀疏算力在图像处理、信号处理、推荐系统等领域具有广泛的应用。</p>
<p>稠密算力：稠密算力在科学计算、机器学习、深度学习、智驾等领域具有广泛的应用。</p>
<h3 id="算力规划计算"><a href="#算力规划计算" class="headerlink" title="算力规划计算"></a>算力规划计算</h3><h4 id="GPGPU卡数规划"><a href="#GPGPU卡数规划" class="headerlink" title="GPGPU卡数规划"></a>GPGPU卡数规划</h4><p>所需GPU卡数量 &#x3D; 总算力需求 &#x2F; 单卡算力</p>
<p>以1000P算力需求为例，使用H100-SXM机型，计算对应的卡数</p>
<p>通常用FP16精度为例，H100，一张H100，BF16稀疏算力为1979TF，对应1.979&#x2F;1000≈1.9P，8卡对应约为16P。<br>1000&#x2F;16&#x3D;63台，考虑到设计的便捷性，通常以64台作为推荐数量，对应的稠密算力，性能减半，则对应64*2&#x3D;128台。</p>
<p>稠密算力大约等于稀疏算力的一半,所以说H100，一卡对应1p通常是说稠密算力。</p>
<p>所需GPU卡数量：稀疏算力：64<em>8&#x3D;512块卡。稠密算力对应：128</em>8&#x3D;1024卡。</p>
<h4 id="根据模型参数量规划算力"><a href="#根据模型参数量规划算力" class="headerlink" title="根据模型参数量规划算力"></a>根据模型参数量规划算力</h4><p>训练场景：<br>总算力&#x3D;6 * token数 * 模型参数</p>
<p>注：<br>6是训练过程中前向传播、反向传播两个步骤，共计 2 次浮点运算。因此对于每个 token、每个模型参数，需要进行 3 × 2 flops &#x3D; 6 次浮点运算</p>
<p>这是一个经验公式，表示对于每一个 token，进行一次完整的前向和反向传播大约需要 6 倍于模型参数数量的浮点运算量。</p>
<p>以LLama3 65B，1.4T数据量为例，计算H100 SXM需要的卡数和耗时，Llama属于 采用的是稠密（Dense）模型，65B的参数都激活了。非MoE模型，MoE模型需要额外考虑激活的参数量。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gpu3-1.png"></p>
<p>总算力需求：6<em>1.4T</em>65B&#x3D;5.46*10^23 FLOPS</p>
<p>以H100为例BF16稀疏计算对应1.9PFLOPS，稠密计算约为1PFLOPS，GPU实际利用率按百分之50计算，约0.5PFLOPS，假设使用2048卡规模</p>
<p>2048<em>0.5≈1024PFLOPS，PFLOPS换算FLOPS需要</em>10^15</p>
<p>耗时&#x3D; (5.46<em>10^23)&#x2F;(1024</em>10^15)≈7天</p>
<p>使用A100话单卡BF16 Tensor Core的算力为312 TFLOPS，2048张卡吞吐约为319P</p>
<p>耗时&#x3D; 5.46<em>10^23)&#x2F;(319</em>10^15)≈21天</p>
<p>另外对于大模型需要进行分布式训练还需要考虑卡间通信带宽</p>
<h4 id="显存需求计算"><a href="#显存需求计算" class="headerlink" title="显存需求计算"></a>显存需求计算</h4><h5 id="推理场景显存（全参微调）"><a href="#推理场景显存（全参微调）" class="headerlink" title="推理场景显存（全参微调）"></a>推理场景显存（全参微调）</h5><p><code>推理显存需求=模型参数显存占用+KV Cache显存占用</code></p>
<p>Llama3-7B为例</p>
<table>
<thead>
<tr>
<th>data type</th>
<th>bytes per parameter</th>
</tr>
</thead>
<tbody><tr>
<td>fp32</td>
<td>4 bytes</td>
</tr>
<tr>
<td>fp16</td>
<td>2 bytes</td>
</tr>
<tr>
<td>bf16</td>
<td>2 bytes</td>
</tr>
<tr>
<td>int8</td>
<td>1 bytes</td>
</tr>
<tr>
<td>int4</td>
<td>0.5 bytes</td>
</tr>
</tbody></table>
<p>模型参数显存：</p>
<p>7b参数对应fp16需要的显存为<br>2*7b&#x3D;14G</p>
<p>注：2为fp16对应的bytes</p>
<p>KV Cache占用显存<br>模型推理过程中，模型一次生成一个token，然后使用之前生成的token作为输入来预测下一个token。<br>每次生成新的token时，模型需要重新计算新的Q、K、V，并基于它们计算Attention权重。然而，之前生成的K、V在当前解码过程中是可以重复利用的，为了加快推理速度，可以将之前计算好的K、V存储在缓存中，这就是KV Cache，它们存储在GPU显存中，从而节省计算时间。</p>
<p>memory&#x3D;BatchSize<em>SeqLength</em>hiddensize<em>layers</em>2*dtype<br>如LLama3-7b</p>
<ol>
<li><p>Hidden Size (隐藏层大小)：</p>
<p> •	4096：LLaMA 7B 的隐藏层大小为 4096，这表示每个 token 通过 transformer 层时的向量维度。</p>
</li>
<li><p>Sequence Length (序列长度)：</p>
<p> •	2048 tokens：默认的最大序列长度为 2048 tokens。这是模型在一次前向传播中能够处理的最大 token 数。</p>
</li>
<li><p>Batch Size (批量大小)：</p>
<p> •	Batch Size 是可调参数，根据可用的显存和任务需求来选择。在训练或推理时，批量大小可以不同。常见批量大小为 1、8、16 等，但具体值取决于显存和硬件资源。</p>
</li>
<li><p>Number of Layers (层数)：</p>
<p> •	32 层：LLaMA 7B 模型有 32 层 transformer 层，每一层负责进行一轮 token 的上下文理解。</p>
</li>
</ol>
<p>memory&#x3D;1<em>2048</em>4096<em>32</em>2*2≈1G</p>
<p>这个与batchsize大小有关，这里设置的1，也与用户并发有关，还有输入输出的序列长度，只是做个参考</p>
<p>参考：<a href="https://mp.weixin.qq.com/s/7p-UMOv075OHp0dF5M63hw">https://mp.weixin.qq.com/s/7p-UMOv075OHp0dF5M63hw</a></p>
<p>实际推理侧落地也会使用MQA和GQA技术进行优化</p>
<p>实际对应的模型都会有对应的性能测试报告，在对应的精度情况下显存占用情况和如Qwen的<br><a href="https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html">https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html</a></p>
<p>快速计算方法：<br>8bit量化模型：参数量1B 占用 1G 显存以上。<br>比如：<br>8bit量化 7B模型，显存占用 7G 以上<br>4bit量化 7B模型，显存占用 3.5G 以上<br>float16 7B模型，显存占用 14G 以上</p>
<h5 id="训练场景显存（全参数训练）"><a href="#训练场景显存（全参数训练）" class="headerlink" title="训练场景显存（全参数训练）"></a>训练场景显存（全参数训练）</h5><p>完整的训练当前都是采用混合精度训练方法，显存需求与以下参数有关</p>
<p>1、模型参数：模型本身的占用<br>2、梯度参数：训练过程中梯度更新<br>3、优化器参数：使用不同优化器不一样，通常以Adam为例<br>4、激活值占用：用于存储前向计算时的激活值，模型的每层都会产生中间激活值，这些激活值在反向传播时会被用来计算梯度，因此需要在内存中保存，激活值和batch_size以及seq_length相关，实际训练的时候激活值对显存的占用会很大。注：激活值（中间计算结果）是以 float32（32位浮点数）格式存储的，每个浮点数占用 4字节。</p>
<p>其中模型参数、梯度参数、优化器参数为静态占用，激活值参数为动态占用，先不考虑</p>
<p>N为模型参数量比如LLAMA3-7B</p>
<p>1、模型参数：全精度训练（FP32）的权重需要 4 * N 字节显存。混合精度训练需要 6N 字节，因为 FP16 和 FP32 的权重要各存一份。</p>
<p>2、梯度参数：占用 4N 字节，因为梯度始终以 FP32 精度保存。</p>
<p>3、优化器参数。取决于优化器的类型。以常用的 Adam 优化器为例，训练过程中需要分别存梯度和梯度平方的移动平均，对每个参数存2个状态，因此需要占用 8N 字节显存。</p>
<p>4、激活值显存占用：显存大致是 batch size x 层数 x 序列长度 x每层输出维度 x 4 字节。</p>
<p>假设：batch size 为 32，模型为12层，输入序列长度为 1024，模型的每层输出维度为 4096。</p>
<p>占用显存为32<em>12</em>1024<em>4096</em>4&#x3D;6GB</p>
<p>在混合精度训练时，以上三项总共需要 6N + 4N + 8N &#x3D; 18N 字节，以 7B 模型为例，约为 126G。<br>加上激活值显存占用6GB&#x3D;132G</p>
<p>实际在训练中，会使用多卡并行的分布式训练使用ZeRO技术进行显存优化，现在也集成到DeepSpeed库中了。另外当前也很多场景也都是使用PEFT（微调技术）进行部分参数训练比如使用Lora和QLoRA进行训练。这个可以在对应的微调框架内如LLaMA-Factory（<code>https://github.com/hiyouga/LLaMA-Factory</code>）Unsloth查看</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gpu3-2.png"></p>
<p><a href="https://llm-system-requirements.streamlit.app/">https://llm-system-requirements.streamlit.app/</a></p>
<p><a href="https://github.com/hiyouga/LLaMA-Factory">https://github.com/hiyouga/LLaMA-Factory</a></p>
<p>总结：</p>
<p>也可以使用huggingface官方的计算工具<br><a href="https://huggingface.co/spaces/hf-accelerate/model-memory-usage">https://huggingface.co/spaces/hf-accelerate/model-memory-usage</a></p>
<p>参考链接：<br><a href="https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html">https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html</a></p>
<p><a href="https://mp.weixin.qq.com/s/7p-UMOv075OHp0dF5M63hw">https://mp.weixin.qq.com/s/7p-UMOv075OHp0dF5M63hw</a></p>
<p><a href="https://github.com/hiyouga/LLaMA-Factory">https://github.com/hiyouga/LLaMA-Factory</a><br><a href="https://gpumap.com/moxing/38887.html">https://gpumap.com/moxing/38887.html</a></p>
]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>容器云平台GPU资源的应用</title>
    <url>/2018/05/26/gpu_user/</url>
    <content><![CDATA[<h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>阿里云创建GPU计算型，规格型号为GPU计算型GN5，对应NVIDA的GPU型号为Nvidia P100 GPU</p>
<h4 id="先决条件"><a href="#先决条件" class="headerlink" title="先决条件"></a>先决条件</h4><ul>
<li>主机安装gcc</li>
<li>bios禁用禁用secure boot，也就是设置为disable</li>
<li>如果没有禁用secure boot,会导致NVIDIA驱动安装失败，或者不正常。</li>
<li>禁用nouveau<br>打开编辑配置文件：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo gedit /etc/modprobe.d/blacklist.conf</span><br><span class="line">在最后一行添加：</span><br><span class="line"></span><br><span class="line">blacklist nouveau</span><br></pre></td></tr></table></figure></li>
<li>安装使用docker-ce:19.03</li>
</ul>
<h3 id="安装NVIDIA驱动"><a href="#安装NVIDIA驱动" class="headerlink" title="安装NVIDIA驱动"></a>安装NVIDIA驱动</h3><p>阿里云启动机器时也可以选择自动安装驱动，这里为了方便连接使用手动方式。  </p>
<p>linux上安装NVIDIA驱动有两种方式</p>
<p>方式一：通过执行二进制脚本安装</p>
<p>方式二：通过安装cuda的rpm包的方式安装</p>
<p>查看GPU型号  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> lspci|grep NV</span><br><span class="line">00:08.0 3D controller: NVIDIA Corporation GP100GL [Tesla P100 PCIe 16GB] (rev a1)</span><br></pre></td></tr></table></figure>




<p>选择对应的型号下载gpu驱动<br><a href="https://www.nvidia.cn/Download/index.aspx?lang=cn">https://www.nvidia.cn/Download/index.aspx?lang=cn</a><br><img src="https://pic.downk.cc/item/5eb03b66c2a9a83be5ff0559.jpg"><br>安装gcc  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum install gcc* -y</span><br></pre></td></tr></table></figure>
<p>下载驱动</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget https://cn.download.nvidia.cn/tesla/440.64.00/nvidia-driver-local-repo-rhel7-440.64.00-1.0-1.x86_64.rpm</span><br></pre></td></tr></table></figure>

<p>若下载慢，也可以通过以下链接下载  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://v2.fangcloud.com/share/cec9fca23cb2fe56b2d9d0732f</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rpm -ihv nvidia-driver-local-repo-rhel7-440.64.00-1.0-1.x86_64.rpm</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum install cuda-driver -y</span><br></pre></td></tr></table></figure>
<p>重启节点  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">reboot</span><br></pre></td></tr></table></figure>


<p>查看gpu</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nvidia-smi </span><br><span class="line">Sat Apr 25 00:32:30 2020       </span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |</span><br><span class="line">|-------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|===============================+======================+======================|</span><br><span class="line">|   0  Tesla P100-PCIE...  Off  | 00000000:00:08.0 Off |                    0 |</span><br><span class="line">| N/A   24C    P0    26W / 250W |      0MiB / 16280MiB |      4%      Default |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">                                                                               </span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                       GPU Memory |</span><br><span class="line">|  GPU       PID   Type   Process name                             Usage      |</span><br><span class="line">|=============================================================================|</span><br><span class="line">|  No running processes found                                                 |</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>



<h3 id="配置docker使用GPU"><a href="#配置docker使用GPU" class="headerlink" title="配置docker使用GPU"></a>配置docker使用GPU</h3><p>docker要使用GPU，需要将docker的runtime替换为NVIDIA的docker-runtime，替换方法如下<br><img src="https://pic.downk.cc/item/5eb04772c2a9a83be50c1bd9.jpg"></p>
<h4 id="安装NVIDIA-docker2"><a href="#安装NVIDIA-docker2" class="headerlink" title="安装NVIDIA-docker2"></a>安装NVIDIA-docker2</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">distribution=$(. /etc/os-release;echo $ID$VERSION_ID)</span><br><span class="line">curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | \</span><br><span class="line">sudo tee /etc/yum.repos.d/nvidia-docker.repo</span><br></pre></td></tr></table></figure>
<p>更新yum源</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">DIST=$(sed -n &#x27;s/releasever=//p&#x27; /etc/yum.conf)</span><br><span class="line">DIST=$&#123;DIST:-$(. /etc/os-release; echo $VERSION_ID)&#125;</span><br><span class="line">sudo rpm -e gpg-pubkey-f796ecb0</span><br><span class="line">sudo gpg --homedir /var/lib/yum/repos/$(uname -m)/$DIST/*/gpgdir --delete-key f796ecb0</span><br><span class="line">sudo gpg --homedir /var/lib/yum/repos/$(uname -m)/latest/nvidia-docker/gpgdir --delete-key f796ecb0</span><br><span class="line">sudo gpg --homedir /var/lib/yum/repos/$(uname -m)/latest/nvidia-container-runtime/gpgdir --delete-key f796ecb0</span><br><span class="line">sudo gpg --homedir /var/lib/yum/repos/$(uname -m)/latest/libnvidia-container/gpgdir --delete-key f796ecb0</span><br><span class="line">sudo yum update</span><br></pre></td></tr></table></figure>
<p>安装NVIDIA-docker2</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo yum install nvidia-docker2 -y</span><br></pre></td></tr></table></figure>

<h4 id="修改默认RUNTIME"><a href="#修改默认RUNTIME" class="headerlink" title="修改默认RUNTIME"></a>修改默认RUNTIME</h4><p>安装后会自动替换原有的daemon.json文件，需要重新修改替换，将默认的runtime替换为NVIDIA-container-runtime</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tee  /etc/docker/daemon.json &lt;&lt; EOF</span><br><span class="line">&#123;</span><br><span class="line">    &quot;default-runtime&quot;: &quot;nvidia&quot;,</span><br><span class="line">    &quot;registry-mirrors&quot;: [&quot;https://vqgjby9l.mirror.aliyuncs.com&quot;],</span><br><span class="line">    &quot;runtimes&quot;: &#123;</span><br><span class="line">        &quot;nvidia&quot;: &#123;</span><br><span class="line">            &quot;path&quot;: &quot;nvidia-container-runtime&quot;,</span><br><span class="line">            &quot;runtimeArgs&quot;: []</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">   &quot;log-opts&quot;: &#123;</span><br><span class="line">      &quot;max-size&quot;: &quot;100m&quot;,</span><br><span class="line">      &quot;max-file&quot;: &quot;3&quot;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<p>重启docker</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart docker</span><br></pre></td></tr></table></figure>

<p>执行docker info检查</p>
<p><img src="https://pic.downk.cc/item/5eb047ebc2a9a83be50c91af.png"></p>
<h4 id="测试docker调用"><a href="#测试docker调用" class="headerlink" title="测试docker调用"></a>测试docker调用</h4><p>使用一个简单的训练任务容器测试是否能正常通过docker调用GPU</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd wanshaoyuan/pytorch:v1.0</span><br></pre></td></tr></table></figure>
<p>查看GPU状态，有将正常进程调度到GPU上</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nvida-smi </span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gpu_2.png"></p>
<h3 id="k8s配置对接"><a href="#k8s配置对接" class="headerlink" title="k8s配置对接"></a>k8s配置对接</h3><p>docker对接成功后就可以正常使用容器应用调用GPU资源了，但此时kubernetes还是无法发现GPU资源对象，需要在在kubernetes中安装k8s-device-plugin插件这样kubelet就能正常上报GPU资源信息<br>安装k8s-device-plugin  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta6/nvidia-device-plugin.yml</span><br></pre></td></tr></table></figure>
<p>安装完成后会在kube-system命名空间中以daemonset方式部署nvidia-device-plugin<br>此时通过kubectl查看节点上报资源信息可以看见GPU资源对象了</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl describe node/xxx</span><br></pre></td></tr></table></figure>
<p><img src="https://pic.downk.cc/item/5eb04872c2a9a83be50d1978.png"></p>
<p>在kubernetes中运行训练任务进行测试<br><img src="https://pic.downk.cc/item/5eb048a6c2a9a83be50d48e2.png"></p>
<p>默认情况下采用独占模式，在资源配置中设置gpu资源对象<br><img src="https://pic.downk.cc/item/5eb048c7c2a9a83be50d641b.png"></p>
<p>kubernetes会将workload自动调度到有GPU的节点上<br>在此查看GPU资源使用信息，调度成功。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/gpu_2.png"><br>参考文档：<br><a href="https://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html#redhat-x86_64">https://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html#redhat-x86_64</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>容器应用优雅关闭</title>
    <url>/2021/05/26/graceful_close/</url>
    <content><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>优雅关闭：  在关闭前，执行正常的关闭过程，释放连接和资源，如我们操作系统执行shutdown。  </p>
<p>目前业务系统组件众多，互相之间调用关系也比较复杂，一个组件的下线、关闭会涉及到多个组件<br>对于任何一个线上应用，如何保证服务更新部署过程中从应用停止到重启恢复服务这个过程中不影响正常的业务请求，这是应用开发运维团队必须要解决的问题。传统的解决方式是通过将应用更新流程划分为手工摘流量、停应用、更新重启三个步骤，由人工操作实现客户端不对更新感知。这种方式简单而有效，但是限制较多：不仅需要使用借助网关的支持来摘流量，还需要在停应用前人工判断来保证在途请求已经处理完毕。   </p>
<p>同时，在应用层也有一些保障应用优雅停机的机制，目前Tomcat、Spring Boot、Dubbo等框架都有提供相关的内置实现，如SpringBoot 2.3内置graceful shutdown可以很方便的直接实现优雅停机时的资源处理，同时一个普通的Java应用也可以基于Runtime.getRuntime().addShutdownHook()来自定义实现，它们的实现原理都基本一致，通过等待操作系统发送的SIGTERM信号，然后针对监听到该信号做一些处理动作。优雅停机是指在停止应用时，执行的一系列保证应用正常关闭的操作。这些操作往往包括等待已有请求执行完成、关闭线程、关闭连接和释放资源等，优雅停机可以避免非正常关闭程序可能造成数据异常或丢失，应用异常等问题。优雅停机本质上是JVM即将关闭前执行的一些额外的处理代码。</p>
<h3 id="现状分析"><a href="#现状分析" class="headerlink" title="现状分析"></a>现状分析</h3><p>现阶段，业务容器化后业务启动是通过shell脚本启动业务，对应的在容器内PID为1的进程为shell进程但shell 程序不转发signals，也不响应退出信号。所以在容器应用中如果应用容器中启动 shell，占据了 pid&#x3D;1  的位置，那么就无法接收k8s发送的SIGTERM信号，只能等超时后被强行杀死了。</p>
<h3 id="案例分析"><a href="#案例分析" class="headerlink" title="案例分析"></a>案例分析</h3><p>go开发的一个Demo  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">    &quot;fmt&quot;</span><br><span class="line">    &quot;os&quot;</span><br><span class="line">    &quot;os/signal&quot;</span><br><span class="line">    &quot;syscall&quot;</span><br><span class="line">    &quot;time&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func main()  &#123;</span><br><span class="line">    c := make(chan os.Signal)</span><br><span class="line">    signal.Notify(c, syscall.SIGTERM, syscall.SIGINT)</span><br><span class="line">    go func() &#123;</span><br><span class="line">        for s := range c &#123;</span><br><span class="line">            switch s &#123;</span><br><span class="line">            case syscall.SIGINT, syscall.SIGTERM:</span><br><span class="line">                fmt.Println(&quot;退出&quot;, s)</span><br><span class="line">                ExitFunc()</span><br><span class="line">            default:</span><br><span class="line">                fmt.Println(&quot;other&quot;, s)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    fmt.Println(&quot;进程启动...&quot;)</span><br><span class="line">    time.Sleep(time.Duration(200000)*time.Second)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func ExitFunc()  &#123;</span><br><span class="line">    fmt.Println(&quot;正在退出...&quot;)</span><br><span class="line">    fmt.Println(&quot;执行清理...&quot;)</span><br><span class="line">    fmt.Println(&quot;退出完成...&quot;)</span><br><span class="line">    os.Exit(0)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>代码参考：<a href="https://www.jianshu.com/p/ae72ad58ecb6">https://www.jianshu.com/p/ae72ad58ecb6</a></p>
<p>1、Signal.Notify会监听括号内指定的信号，若没有指定，则监听所有信号。<br>2、通过switch对监听到信号进行判断，如果是SININT和SIGTERM则条用Exitfunc函数执行退出。  </p>
<h4 id="SHELL模式和CMD模式带来的差异性"><a href="#SHELL模式和CMD模式带来的差异性" class="headerlink" title="SHELL模式和CMD模式带来的差异性"></a>SHELL模式和CMD模式带来的差异性</h4><p>编写应用Dockerfile文件</p>
<p>概述<br>在Dockerfile中CMD和ENTRYPOINT用来启动应用，有shell模式和exec模式，对应的使用shell模式，PID为1的进程为shell，使用exec模式PID为1的进程为业务本身。<br>SHELL模式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM golang as builder</span><br><span class="line">WORKDIR /go/</span><br><span class="line">COPY app.go    .</span><br><span class="line">RUN go build app.go</span><br><span class="line">FROM ubuntu</span><br><span class="line">WORKDIR /root/</span><br><span class="line">COPY --from=builder /go/app .</span><br><span class="line">CMD ./app</span><br></pre></td></tr></table></figure>
<p>构建镜像</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker build -t app:v1.0-shell ./</span><br></pre></td></tr></table></figure>

<p>运行查看</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker exec -it app-shell ps aux</span><br><span class="line">USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND</span><br><span class="line">root           1  0.7  0.0   2608   548 pts/0    Ss+  03:22   0:00 /bin/sh -c ./</span><br><span class="line">root           6  0.0  0.0 704368  1684 pts/0    Sl+  03:22   0:00 ./app</span><br><span class="line">root          24  0.0  0.0   5896  2868 pts/1    Rs+  03:23   0:00 ps aux</span><br></pre></td></tr></table></figure>
<p>可以看见PID为1的进程是sh进程</p>
<p>此时执行docker stop，业务进程是接收不到SIGTERM信号的，要等待一个超时时间后被KILL</p>
<p>日志没有输出SIGTERM关闭指令</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker stop app-shell</span><br><span class="line">app-shell</span><br><span class="line"></span><br><span class="line">docker logs app-shell</span><br><span class="line">进程启动...</span><br></pre></td></tr></table></figure>



<p>EXEC模式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM golang as builder</span><br><span class="line">WORKDIR /go/</span><br><span class="line">COPY app.go    .</span><br><span class="line">RUN go build app.go</span><br><span class="line">FROM ubuntu</span><br><span class="line">WORKDIR /root/</span><br><span class="line">COPY --from=builder /go/app .</span><br><span class="line">CMD [&quot;./app&quot;]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>构建镜像</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker build -t app:v1.0-exec ./</span><br></pre></td></tr></table></figure>

<p>运行查看</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker exec -it app-exec ps aux</span><br><span class="line">USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND</span><br><span class="line">root           1  2.0  0.0 703472  1772 pts/0    Ssl+ 03:33   0:00 ./app</span><br><span class="line">root          14  0.0  0.0   5896  2908 pts/1    Rs+  03:34   0:00 ps aux</span><br></pre></td></tr></table></figure>
<p>可以看见PID为1的进程是应用进程</p>
<p>此时执行docker stop，业务进程是可以接收SIGTERM信号的，会优雅退出</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker stop app-exec</span><br><span class="line">app-exec</span><br><span class="line"></span><br><span class="line">docker logs app-exec</span><br><span class="line">进程启动...</span><br><span class="line">退出 terminated</span><br><span class="line">正在退出...</span><br><span class="line">执行清理...</span><br><span class="line">退出完成...</span><br></pre></td></tr></table></figure>



<p>注意：<br>1、以下测试在ubuntu做为应用启动base镜像测试成功，在alpine做为应用启动base镜像时shell模式和exec模式都一样，都是应用进程为PID 1的进程。</p>
<h4 id="直接启动应用和通过脚本启动区别"><a href="#直接启动应用和通过脚本启动区别" class="headerlink" title="直接启动应用和通过脚本启动区别"></a>直接启动应用和通过脚本启动区别</h4><p>在实际生产环境中，因为应用启动命令后会接很多启动参数，所以通常我们会使用一个启动脚本来启动应用，方便我们启动应用。对应的在容器内PID为1的进程为shell进程但shell 程序不转发signals，也不响应退出信号。所以在容器应用中如果应用容器中启动 shell，占据了 pid&#x3D;1  的位置，那么就无法接收k8s发送的SIGTERM信号，只能等超时后被强行杀死了。<br>启动脚本<br>start.sh</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &gt; start.sh&lt;&lt; EOF </span><br><span class="line">#!/bin/sh</span><br><span class="line">sh -c /root/app</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM golang as builder</span><br><span class="line">WORKDIR /go/</span><br><span class="line">COPY app.go    .</span><br><span class="line">RUN go build app.go</span><br><span class="line">FROM alpine</span><br><span class="line">WORKDIR /root/</span><br><span class="line">COPY --from=builder /go/app .</span><br><span class="line">ADD start.sh /root/</span><br><span class="line">CMD [&quot;/bin/sh&quot;,&quot;/root/start.sh&quot;]</span><br></pre></td></tr></table></figure>


<p>构建应用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker build -t app:v1.0-script ./</span><br></pre></td></tr></table></figure>
<p>查看</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker exec -it app-script ps aux</span><br><span class="line">PID   USER     TIME  COMMAND</span><br><span class="line">1     root     0:00  /bin/sh /root/start.sh</span><br><span class="line">6     root     0:00  /root/app</span><br><span class="line">19    root     0:00  ps aux</span><br></pre></td></tr></table></figure>

<p>docker stop关闭应用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker stop app-script</span><br></pre></td></tr></table></figure>

<p>是登待超时后被强行KILL</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker logs app-script</span><br><span class="line">进程启动...</span><br></pre></td></tr></table></figure>

<h3 id="容器应用优雅关闭方案介绍"><a href="#容器应用优雅关闭方案介绍" class="headerlink" title="容器应用优雅关闭方案介绍"></a>容器应用优雅关闭方案介绍</h3><h4 id="方案介绍"><a href="#方案介绍" class="headerlink" title="方案介绍"></a>方案介绍</h4><p>正常的优雅停机可以简单的认为包括两个部分：<br>-	应用：应用自身需要实现优雅停机的处理逻辑，确保处理中的请求可以继续完成，资源得到有效的关闭释放，等等。针对应用层，不管是Java应用还是其他语言编写的应用，其实现原理基本一致，都提供了类似的监听处理接口，根据规范要求实现即可。<br>-	平台：平台层要能够将应用从负载均衡中去掉，确保应用不会再接受到新的请求连接，并且能够通知到应用要进行优雅停机处理。在传统的部署模式下，这部分工作可能需要人工处理，但是在K8s容器平台中，K8s的Pod删除默认就会向容器中的主进程发送优雅停机命令，并提供了默认30s的等待时长，若优雅停机处理超出30s以后就会强制终止。同时，有些应用在容器中部署时，并不是通过容器主进程的形式进行部署，那么K8s也提供了PreStop的回调函数来在Pod停止前进行指定处理，可以是一段命令，也可以是一个HTTP的请求，从而具备了较强的灵活性。<br>通过以上分析，理论上应用容器化部署以后仍然可以很好的支持优雅停机，甚至相比于传统方式实现了更多的自动化操作，本文档后面会针对该方案进行详细的方案验证。</p>
<ul>
<li>容器应用中第三方Init：在构建应用中使用第三方init如tini或dumb-init</li>
</ul>
<p>方案一：<br>通过k8s的prestop参数调用容器内进程关闭脚本，实现优雅关闭。</p>
<p>方案二：<br>通过第三方init进程传递SIGTERM到进程中。</p>
<h3 id="方案验证"><a href="#方案验证" class="headerlink" title="方案验证"></a>方案验证</h3><h4 id="方案一：通过k8s-Prestop参数调用"><a href="#方案一：通过k8s-Prestop参数调用" class="headerlink" title="方案一：通过k8s Prestop参数调用"></a>方案一：通过k8s Prestop参数调用</h4><p>在前面脚本启动的dockerfile基础上，定义一个优雅关闭的脚本，通过k8s-prestop在关闭POD前调用优雅关闭脚本，实现pod优雅关闭。</p>
<p>启动脚本<br>start.sh</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &gt; start.sh&lt;&lt; EOF </span><br><span class="line">#!/bin/sh</span><br><span class="line">./app</span><br></pre></td></tr></table></figure>
<p>stop.sh<br>优雅关闭脚本</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/sh</span><br><span class="line">ps -ef|grep app|grep -v grep|awk &#x27;&#123;print $1&#125;&#x27;|xargs kill -15</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM golang as builder</span><br><span class="line">WORKDIR /go/</span><br><span class="line">COPY app.go    .</span><br><span class="line">RUN go build app.go</span><br><span class="line">FROM alpine</span><br><span class="line">WORKDIR /root/</span><br><span class="line">COPY --from=builder /go/app .</span><br><span class="line">ADD start.sh /root/</span><br><span class="line">CMD [&quot;/bin/sh&quot;,&quot;/root/start.sh&quot;]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>构建镜像</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker build -t app:v1.0-prestop ./</span><br></pre></td></tr></table></figure>

<p>通过yaml部署到k8s中</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: app-prestop</span><br><span class="line">  labels:</span><br><span class="line">    app: prestop</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: prestop</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: prestop</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: prestop</span><br><span class="line">        image: 172.16.1.31/library/app:v1.0-prestop</span><br><span class="line">        lifecycle:</span><br><span class="line">          preStop:</span><br><span class="line">            exec:</span><br><span class="line">              command:</span><br><span class="line">              - sh</span><br><span class="line">              - /root/stop.sh</span><br></pre></td></tr></table></figure>


<p>查看POD日志，然后删除pod副本</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod </span><br><span class="line">NAME                            READY   STATUS    RESTARTS   AGE</span><br><span class="line">app-prestop-847f5c4db8-mrbqr    1/1     Running   0          73s</span><br></pre></td></tr></table></figure>

<p>查看日志</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl logs app-prestop-847f5c4db8-mrbqr -f</span><br><span class="line">进程启动...</span><br></pre></td></tr></table></figure>

<p>另外窗口删除POD</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl logs app-prestop-847f5c4db8-mrbqr -f</span><br><span class="line">进程启动...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">退出 terminated</span><br><span class="line">正在退出...</span><br><span class="line">执行清理...</span><br><span class="line">退出完成...</span><br></pre></td></tr></table></figure>
<p>可以看见执行了Prestop脚本进行优雅关闭。<br>同样的可以将yaml文件中的Prestop脚本取消进行对比测试可以发现就会进行强制删除。  </p>
<h4 id="方案二：shell脚本修改为exec执行"><a href="#方案二：shell脚本修改为exec执行" class="headerlink" title="方案二：shell脚本修改为exec执行"></a>方案二：shell脚本修改为exec执行</h4><p>修改<code>start.sh</code>脚本</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/sh</span><br><span class="line">exec ./app</span><br></pre></td></tr></table></figure>
<p>shell中添加一个 exec 即可让应用进程替代当前shell进程,可将SIGTERM信号传递到业务层，让业务实现优雅关闭。  </p>
<p>可使用上面例子，进行修改测试。</p>
<h4 id="方案三：通过第三init工具启动"><a href="#方案三：通过第三init工具启动" class="headerlink" title="方案三：通过第三init工具启动"></a>方案三：通过第三init工具启动</h4><p>使用dump-init或tini做为容器的主进程，在收到退出信号的时候，会将退出信号转发给进程组所有进程。，主要适用应用本身无关闭信号处理的场景。docker –init本身也是集成的tini。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM golang as builder</span><br><span class="line">WORKDIR /go/</span><br><span class="line">COPY app.go    .</span><br><span class="line">RUN go build app.go</span><br><span class="line">FROM alpine</span><br><span class="line">WORKDIR /root/</span><br><span class="line">COPY --from=builder /go/app .</span><br><span class="line">ADD start.sh tini /root/</span><br><span class="line">RUN chmoad a+x start.sh &amp;&amp; apk add --no-cache tini</span><br><span class="line">ENTRYPOINT [&quot;/sbin/tini&quot;, &quot;--&quot;]</span><br><span class="line">CMD [&quot;/root/tini&quot;, &quot;--&quot;, /root/start.sh&quot;]</span><br></pre></td></tr></table></figure>

<p>构建镜像</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker build -t app:v1.0-tini ./</span><br></pre></td></tr></table></figure>
<p>测试运行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd --name app-tini app:v1.0-tini</span><br></pre></td></tr></table></figure>

<p>查看日志</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker logs app-tini</span><br><span class="line"></span><br><span class="line">进程启动...</span><br></pre></td></tr></table></figure>

<p>发现容器快速停止了，但没有输出应用关闭和清理的日志</p>
<p>后面查阅相关资料发现</p>
<p>使用tini或dump-init做为应用启动的主进程。<br>tini和dumb-init会将关闭信号向子进程传递，但不会等待子进程完全退出后自己在退出。而是传递完后直接就退出了。</p>
<p>相关issue：<br><a href="https://github.com/krallin/tini/issues/180">https://github.com/krallin/tini/issues/180</a></p>
<p>后面又查到另外一个第三方的组件smell-baron能实现等待子进程优雅关闭后在关闭本身功能。 但这个项目本身热度不是特别高，并且有很久没有维护了。 </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM golang as builder</span><br><span class="line">WORKDIR /go/</span><br><span class="line">COPY app.go    .</span><br><span class="line">RUN go build app.go</span><br><span class="line">FROM ubuntu</span><br><span class="line">WORKDIR /root/</span><br><span class="line">COPY --from=builder /go/app .</span><br><span class="line">ADD start.sh /root/</span><br><span class="line">ADD smell-baron /bin/smell-baron</span><br><span class="line">RUN chmod a+x /bin/smell-baron  &amp;&amp; chmod a+x start.sh</span><br><span class="line">ENTRYPOINT [&quot;/bin/smell-baron&quot;]</span><br><span class="line">CMD [&quot;/root/start.sh&quot;]</span><br></pre></td></tr></table></figure>

<p>构建镜像</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker build -t app:v1.0-smell-baron ./</span><br></pre></td></tr></table></figure>
<p>测试</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd --name app-smell-baron app:v1.0-smell-baron</span><br><span class="line"></span><br><span class="line">docker stop  app-smell-baron</span><br><span class="line"></span><br><span class="line">进程启动...</span><br><span class="line">退出 terminated</span><br><span class="line">正在退出...</span><br><span class="line">执行清理...</span><br><span class="line">退出完成...</span><br></pre></td></tr></table></figure>


<h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>1、对于容器化应用启动命令建议使用EXEC模式。<br>2、对于应用本身代码层面已经实现了优雅关闭的业务，但有shell启动脚本，容器化后部署到k8s上建议使方案一和方案二。<br>3、对于应用本身代码层面没有实现优雅关闭的业务，建议使用方案三。  </p>
<p>项目地址：<br><a href="https://github.com/insidewhy/smell-baron">https://github.com/insidewhy/smell-baron</a><br><a href="https://github.com/Yelp/dumb-init">https://github.com/Yelp/dumb-init</a><br><a href="https://github.com/krallin/tini">https://github.com/krallin/tini</a>  </p>
]]></content>
      <categories>
        <category>应用上云</category>
      </categories>
      <tags>
        <tag>应用上云</tag>
      </tags>
  </entry>
  <entry>
    <title>Habor高可用的实现方式</title>
    <url>/2019/12/22/habor_ha/</url>
    <content><![CDATA[<h3 id="Harbor架构"><a href="#Harbor架构" class="headerlink" title="Harbor架构"></a>Harbor架构</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/harbor_ha_1.png" alt="image"><br>图片来源harbor官网  </p>
<h4 id="组件介绍："><a href="#组件介绍：" class="headerlink" title="组件介绍："></a>组件介绍：</h4><p>proxy: 通过nginx服务器来做反向代理</p>
<p>registry: docker官方的镜像存储服务。</p>
<p>ui: 前端UI入口</p>
<p>adminserver: 作为Harbor工程的配置数据管理器使用</p>
<p>DB: 存放用户信息、项目信息、用户权限和安全漏洞库，早期使用Mysql，目前采用Postgres。   </p>
<p>job services: 通过状态机的形式将镜像复制到远程Harbor实例。镜像删除同样也可以被同步到远程Harbor实例中。</p>
<p>log: 运行rsyslogd的容器，主要用于收集其他容器的日志</p>
<p>Notary： 通过镜像签名方式方式，用于确保镜像来源真实性。</p>
<h4 id="需要实现高可用的组件"><a href="#需要实现高可用的组件" class="headerlink" title="需要实现高可用的组件"></a>需要实现高可用的组件</h4><p>Redis：用于存储session会话信息。</p>
<p>Postgres：用于存储用户信息，项目信息，用户权限信息和安全漏洞库。  </p>
<p>Registry：实际镜像管理组件。  </p>
<h4 id="双主HA模式"><a href="#双主HA模式" class="headerlink" title="双主HA模式"></a>双主HA模式</h4><p>通过共享存储实现高可用<br>Harbor默认全部数据存储在&#x2F;data目录中，所以通过共享存储实际上就是将data目录挂载到共享存储中，目前常用方式是使用一些开源的分布式文件系统存储如cephfs，glusterfs或放置在对象存储S3中。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/harbor_ha_2.png">  </p>
<ul>
<li>底层使用共享存储用于保证镜像数据可靠性。</li>
<li>将postgres独立出来，通过pgpool-II实现高可用集群。 </li>
<li>外部通过LB+keepalived方式转发到两个harbor中实现统一入口。</li>
</ul>
<p>优点：</p>
<ul>
<li>由于多Harbor实例共享存储，因此可以保持数据是实时一致的；</li>
</ul>
<p>缺点：</p>
<ul>
<li>门槛高，需要具备共享存储；</li>
<li>需要搭建和维护postgres集群，维护成本高；</li>
</ul>
<h4 id="主备模式"><a href="#主备模式" class="headerlink" title="主备模式"></a>主备模式</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/harbor_ha_3.png">  </p>
<ul>
<li>底层使用共享存储用于保证镜像和postgres的数据可靠性。</li>
<li>外部通过在两个Habor间通过Keepalived维持VIP，保证统一入口，但一个宕机后可以通过VIP漂移方式在将请求迅速转发到另外一个可用节点。</li>
</ul>
<p>优点：</p>
<ul>
<li>架构简单，部署两个harbor，只需要通过共享存储和keepalived就可以实现HA；</li>
</ul>
<h2 id="缺点：-只能实现主备，没有全主，每次只能有一个harbor可以对外提供服务；"><a href="#缺点：-只能实现主备，没有全主，每次只能有一个harbor可以对外提供服务；" class="headerlink" title="缺点：- 只能实现主备，没有全主，每次只能有一个harbor可以对外提供服务； "></a>缺点：<br>- 只能实现主备，没有全主，每次只能有一个harbor可以对外提供服务； </h2><h3 id="基于镜像复制方式实现高可用"><a href="#基于镜像复制方式实现高可用" class="headerlink" title="基于镜像复制方式实现高可用"></a>基于镜像复制方式实现高可用</h3><p>但没有共享存储时可以使用Harbor本身的镜像同步方式  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/harbor_ha_4.png">  </p>
<ul>
<li>Harbor使用底层使用各自的本地存储；  </li>
<li>两个Harbor之间配置双向同步镜像策略；</li>
</ul>
<p>优点：  </p>
<ul>
<li>架构简单，只需要部署两个Harbor，不需要通过其他额外组件。</li>
<li>配置文件，只需要配置两个Harbor间的镜像同步。</li>
</ul>
<p>缺点：</p>
<ul>
<li>只能实现主备，没有全主，每次只能有一个harbor可以对外提供服务； </li>
<li>但其中一台宕机后，镜像push请求到另外一台harbor后，待另外一台harbor恢复后需要手工执行一下同步策略进行同步。  </li>
<li>只能同步镜像，用户和用户权限信息无法进行同步，因为这些都保存到postgres数据库中，所以需要通过导表方式进行同步用户和权限。</li>
<li>若VIP进行频繁飘移，容易造成两边postgres数据不一致。</li>
</ul>
<h4 id="导表方式如下："><a href="#导表方式如下：" class="headerlink" title="导表方式如下："></a>导表方式如下：</h4><h5 id="需要导出以下表："><a href="#需要导出以下表：" class="headerlink" title="需要导出以下表："></a>需要导出以下表：</h5><table>
<thead>
<tr>
<th>表名</th>
<th>用途</th>
</tr>
</thead>
<tbody><tr>
<td>harbor_user</td>
<td>用户信息表</td>
</tr>
<tr>
<td>project</td>
<td>项目信息表</td>
</tr>
<tr>
<td>project_member</td>
<td>项目成员表</td>
</tr>
<tr>
<td>project_metadata</td>
<td>项目元数据表，数据主要为项目创建和更新时间</td>
</tr>
<tr>
<td>repository</td>
<td>项目镜像信息元数据</td>
</tr>
</tbody></table>
<p>假设有A节点和B节点两个Harbor，目前vip在A节点上，需要将A节点以下表导出同步到B节点上，若后续VIP漂移到B节点上后也需要将B节点上数据重新同步到A节点。  </p>
<h4 id="在主节点上操作："><a href="#在主节点上操作：" class="headerlink" title="在主节点上操作："></a>在主节点上操作：</h4><h5 id="备份registry数据库"><a href="#备份registry数据库" class="headerlink" title="备份registry数据库"></a>备份registry数据库</h5><p>进入 harbor-db容器  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pg_dump -d registry -f /var/lib/postgresql/registry.sql</span><br></pre></td></tr></table></figure>


<h5 id="导出原表"><a href="#导出原表" class="headerlink" title="导出原表"></a>导出原表</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pg_dump -t harbor_user registry &gt; /var/lib/postgresql/harbor_user.sql</span><br><span class="line">pg_dump -t project_member registry &gt; /var/lib/postgresql/project_member.sql</span><br><span class="line">pg_dump -t project registry &gt; /var/lib/postgresql/project.sql</span><br><span class="line">pg_dump -t project_metadata registry &gt; /var/lib/postgresql/project_metadata.sql</span><br><span class="line">pg_dump -t repository registry &gt;/var/lib/postgresql/repository.sql</span><br><span class="line">pg_dump -t oidc_user registry &gt;/var/lib/postgresql/oidc_user.sql</span><br></pre></td></tr></table></figure>

<h5 id="将主节点导出的表文件发送到备节点上"><a href="#将主节点导出的表文件发送到备节点上" class="headerlink" title="将主节点导出的表文件发送到备节点上"></a>将主节点导出的表文件发送到备节点上</h5><h4 id="在备节点操作"><a href="#在备节点操作" class="headerlink" title="在备节点操作"></a>在备节点操作</h4><p>进入 harbor-db容器执行  </p>
<h5 id="备份registry数据库-1"><a href="#备份registry数据库-1" class="headerlink" title="备份registry数据库"></a>备份registry数据库</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pg_dump -d registry -f /var/lib/postgresql/registry.sql</span><br></pre></td></tr></table></figure>


<h5 id="删除原表（解决导入时外键依赖问题）"><a href="#删除原表（解决导入时外键依赖问题）" class="headerlink" title="删除原表（解决导入时外键依赖问题）"></a>删除原表（解决导入时外键依赖问题）</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">psql registry -c &quot;drop table project_metadata;&quot;</span><br><span class="line">psql registry -c &quot;drop table project;&quot;</span><br><span class="line">psql registry -c &quot;drop table oidc_user;&quot;</span><br><span class="line">psql registry -c &quot;drop table harbor_user;&quot;</span><br><span class="line">psql registry -c &quot;drop table project_member;&quot;</span><br><span class="line">psql registry -c &quot;drop table repository;&quot;</span><br></pre></td></tr></table></figure>


<h5 id="导入"><a href="#导入" class="headerlink" title="导入"></a>导入</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">psql registry -f /tmp/harbor/harbor_user.sql </span><br><span class="line">psql registry -f /tmp/harbor/project.sql</span><br><span class="line">psql registry -f /tmp/harbor/project_metadata.sql </span><br><span class="line">psql registry -f /tmp/harbor/oidc_user.sql</span><br><span class="line">psql registry -f /tmp/harbor/project_member.sql </span><br><span class="line">psql registry -f /tmp/harbor/repository.sql </span><br></pre></td></tr></table></figure>


<p>查看harbor用户和项目成员信息，同步即可  </p>
<h3 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h3><ul>
<li>若VIP进行频繁飘移，容易造成两边postgres数据不一致。</li>
</ul>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>容器镜像签名</title>
    <url>/2020/02/22/harbor_notary/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Notary 是一个用于建立内容之间信任的平台。在容器镜像中可以对镜像进行加密签名用来保证镜像件来源和镜像内容防篡改。<br>软件版本</p>
<table>
<thead>
<tr>
<th>软件</th>
<th>版本</th>
</tr>
</thead>
<tbody><tr>
<td>Harbor</td>
<td>1.10.1</td>
</tr>
<tr>
<td>Docker</td>
<td>18.09.9</td>
</tr>
</tbody></table>
<h2 id="Harbor打开Notary"><a href="#Harbor打开Notary" class="headerlink" title="Harbor打开Notary"></a>Harbor打开Notary</h2><p>Harbor部署时可以选择启用notray，需要Harbor使用Https模式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./install --with-notary</span><br></pre></td></tr></table></figure>

<p>部署完后会在harbor中启用<br>notary-server-photon和notary-signer-photon组件，服务监听4443端口</p>
<p><img src="https://cdn.rawgit.com/docker/notary/27469f01fe244bdf70f34219616657b336724bc3/docs/images/metadata-sequence.svg"></p>
<p>新建个notary项目配置内容信任，阻止没有签名的镜像下载<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/harbor_notary_1.png"></p>
<p>上传镜像<br>默认不启用docker的内容信任参数上传镜像</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker push 172.16.1.31/notary/hyperkube:v1.15.5-rancher1</span><br></pre></td></tr></table></figure>

<p>此时在Harbor中看见镜像是未签名状态</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/harbor_notary_2.png"></p>
<p>因为项目开启了仅允许通过认证的镜像下载，所以未签名的镜像无法pull下来。</p>
<h2 id="docker开启镜像签名"><a href="#docker开启镜像签名" class="headerlink" title="docker开启镜像签名"></a>docker开启镜像签名</h2><p>使用以下两个环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export DOCKER_CONTENT_TRUST=1</span><br><span class="line">export DOCKER_CONTENT_TRUST_SERVER=&quot;https://172.16.1.31:4443&quot;</span><br></pre></td></tr></table></figure>
<p>DOCKER_CONTENT_TRUST&#x3D;1：表示开启Docker内容信任模式<br>DOCKER_CONTENT_TRUST_SERVER：指定认证服务器，其实就是对应harbor notary服务的地址和端口</p>
<p>若Harbor使用自签名证书，则需要将对应的ca证书放置到<br>主机的~&#x2F;.docker&#x2F;tls&#x2F;172.16.1.31:4443&#x2F;目录，证书名为ca.crt。</p>
<p>172.16.1.31:443为你的notary节点ip和端口</p>
<p>原理就是使用目录下ca证书进行镜像的签发密钥和验证。</p>
<p>继续push镜像</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker push 172.16.1.31/notary/hyperkube:v1.15.5-rancher1</span><br><span class="line">The push refers to repository [172.16.1.31/notary/hyperkube]</span><br><span class="line">2ea80d1d4b24: Layer already exists</span><br><span class="line">3892a0baf222: Layer already exists</span><br><span class="line">39b3ac6d96e9: Layer already exists</span><br><span class="line">7bbae4dddb88: Layer already exists</span><br><span class="line">a00defcfe869: Layer already exists</span><br><span class="line">2ab0ae805c74: Layer already exists</span><br><span class="line">43a8fe7d2382: Layer already exists</span><br><span class="line">3f6a6f542637: Layer already exists</span><br><span class="line">5ba3be777c2d: Layer already exists</span><br><span class="line">v1.15.5-rancher1: digest: sha256:19c919ef37b634919bd08429ee1fde0f3d8ed83c7e75929f7e259ba35f88d9b7 size: 2211</span><br><span class="line">Signing and pushing trust metadata</span><br><span class="line">You are about to create a new root signing key passphrase. This passphrase</span><br><span class="line">will be used to protect the most sensitive key in your signing system. Please</span><br><span class="line">choose a long, complex passphrase and be careful to keep the password and the</span><br><span class="line">key file itself secure and backed up. It is highly recommended that you use a</span><br><span class="line">password manager to generate the passphrase and keep it safe. There will be no</span><br><span class="line">way to recover this key. You can find the key in your config directory.</span><br><span class="line">Enter passphrase for new root key with ID cd80889:</span><br></pre></td></tr></table></figure>

<p>输入密码对镜像进行签名，后续镜像上传都是使用此密码进行镜像签署。<br>根密钥的密码<br>在 ~&#x2F;.docker&#x2F;trust 目录中生成一个根密钥<br>镜像签署密钥的密码<br>在 ~&#x2F;.docker&#x2F;trust 目录中生成一个镜像签署密钥</p>
<p>上传成功后在harbor中查看</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/harbor_notary_3.png"></p>
<p>查看镜像的签名</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker trust inspect 172.16.1.31/notary/hyperkube:v1.15.5-rancher1</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker trust inspect 172.16.1.31/notary/hyperkube:v1.15.5-rancher1</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;Name&quot;: &quot;172.16.1.31/notary/hyperkube:v1.15.5-rancher1&quot;,</span><br><span class="line">        &quot;SignedTags&quot;: [</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;SignedTag&quot;: &quot;v1.15.5-rancher1&quot;,</span><br><span class="line">                &quot;Digest&quot;: &quot;19c919ef37b634919bd08429ee1fde0f3d8ed83c7e75929f7e259ba35f88d9b7&quot;,</span><br><span class="line">                &quot;Signers&quot;: [</span><br><span class="line">                    &quot;Repo Admin&quot;</span><br><span class="line">                ]</span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        &quot;Signers&quot;: [],</span><br><span class="line">        &quot;AdministrativeKeys&quot;: [</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;Name&quot;: &quot;Root&quot;,</span><br><span class="line">                &quot;Keys&quot;: [</span><br><span class="line">                    &#123;</span><br><span class="line">                        &quot;ID&quot;: &quot;4d8fcd2d60d87115dff15e43a649ff0c95fa26f7fd9cf1756ce4775c1a0f94c8&quot;</span><br><span class="line">                    &#125;</span><br><span class="line">                ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;Name&quot;: &quot;Repository&quot;,</span><br><span class="line">                &quot;Keys&quot;: [</span><br><span class="line">                    &#123;</span><br><span class="line">                        &quot;ID&quot;: &quot;7254ee1ad59f87b21cd1af6ec847f95a41b84702cd745e8eb9c47abadb5145e5&quot;</span><br><span class="line">                    &#125;</span><br><span class="line">                ]</span><br><span class="line">            &#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<h3 id="镜像拉取"><a href="#镜像拉取" class="headerlink" title="镜像拉取"></a>镜像拉取</h3><p>1、若节点docker配置了docker内容信任</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export DOCKER_CONTENT_TRUST=1</span><br><span class="line">export DOCKER_CONTENT_TRUST_SERVER=&quot;https://172.16.1.31:4443&quot;</span><br></pre></td></tr></table></figure>

<p>拉取镜像需要进行证书签名比对，若harbor使用的是自签名证书，此时需要节点~&#x2F;.docker&#x2F;tls&#x2F;上放置了CA证书</p>
<p>若节点没有配置docker内容信任，则拉取镜像时不需要做签名验证。</p>
<p>2、Harbor项目开启了只允许信任镜像下载，则未签名的镜像无法下载</p>
<h3 id="镜像删除"><a href="#镜像删除" class="headerlink" title="镜像删除"></a>镜像删除</h3><p>签名了的镜像无法直接通过Harbor进行删除，需要在notary中将镜像签名去除</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">notary -s https://172.16.1.31:4443 -d ~/.docker/trust --tlscacert /root/.docker/tls/172.16.1.31\:4443/ca.crt  remove -p 172.16.1.31/notary/hyperkube v1.15.5-rancher1 signed</span><br></pre></td></tr></table></figure>

<p>注意点：<br>1、tag和镜像是用空格分开的<br>2、notary命令可以通过<a href="https://github.com/theupdateframework/notary/releases%E4%B8%8B%E8%BD%BD">https://github.com/theupdateframework/notary/releases下载</a></p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker安装</title>
    <url>/2017/10/23/install_docker/</url>
    <content><![CDATA[<p>centos7安装docker<br>官方推荐centos7.3系统<br>安装依赖<br>yum install -y yum-utils device-mapper-persistent-data  lvm2</p>
<p>添加docker yum源  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum-config-manager  --add-repo https://download.docker.com/linux/centos/docker-ce.repo</span><br></pre></td></tr></table></figure>
<p>版本区别：<br>docker-ce-edge：类似开发测试版，功能比较新，但不一定稳定，edge是每月更新一次。</p>
<p>docker-ce-stable：稳定版，每3个月更新一次。</p>
<p>开启docker-ce-edge更新(默认是关闭的)<br>yum-config-manager –enable docker-ce-edge<br>yum-config-manager –disable docker-ce-edge</p>
<p>安装docker  </p>
<p>yum install docker-ce -y</p>
<p>启动docker  </p>
<p>systemctl start docker  </p>
<p>测试docker是否安装成功,运行个httpd 镜像  </p>
<p>docker run -d -p 80:80 httpd</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_install_1.png"></p>
<p>-d:以后台方式运行<br>-p：端口映射，源端口:目标端口  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_install_2.png"></p>
<p>docker hub在国外，速度太慢了，使用国内的镜像源daocloud</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://a58c8480.m.daocloud.io</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>重启docker 进程<br>systemctl restart docker  </p>
<p>运行mysql  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --name first-mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=4435448 -d mysql</span><br></pre></td></tr></table></figure>

<p>-e设置环境变量<br>这里使用-e设置环境变量MYSQL_ROOT_PASSWORD</p>
<p>连接方法<br>mysql -h 10.211.55.5 -P3306 -uroot -p4435448</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/docker_install_3.png"></p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Helm(一)(介绍+安装+简单使用)</title>
    <url>/2018/09/09/helm_1/</url>
    <content><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><h4 id="什么是helm？"><a href="#什么是helm？" class="headerlink" title="什么是helm？"></a>什么是helm？</h4><p>helm是kubernetes里面的包管理工具，那什么包管理工具呢？其实就类似于我们用的centos的yum，ubuntu的apt一样能够快速安装、卸载、更新软件，最重要的一点是能够解决软件安装的依赖问题。</p>
<h4 id="helm能帮助我们解决什么问题？"><a href="#helm能帮助我们解决什么问题？" class="headerlink" title="helm能帮助我们解决什么问题？"></a>helm能帮助我们解决什么问题？</h4><p>在kubernetes中，部署应用是一件非常麻烦的事情，首先你得学会kubernetes中的各种概念，deployment，service，pv，pvc……，其次你还得会写yaml文件，通过yaml文件把这些资源对象整合起来，所以这就产生了一个很大问题了，这些yaml文件的管理和执行的先后顺序怎么解决。举个简单例子，WordPress这个博客应用，主要用到k8s哪些资源对象，首先通过deployment部署应用，通过service提供统一入口，如果需要数据持久化的话还需要创建pv和pvc，还需要secret配置WordPress的帐号密码，mariadb的帐号密码，而且部署时还有依赖关系，WordPress数据是存储在mariadb里面的，所以需要mariadb先启动就绪后在启动WordPress，那helm能如何解决这些问题？首先helm可以把这些资源对象封装在一起做成一个软件包统一发包，这个软件包里面就可以解决掉服务与访问之间的依赖问题，其次helm能更加高效的共享和重用访问，比如这个mariadb，如果就通过yaml文件访问部署，那这里里面很多配置参数都写死了，如果另外一个应用通过这个yaml部署mariadb，就需要改里面的配置参数，通过helm能把这些参数做成变量的形式，通过传递变量的方式来达到复用的效果。总结如下 1、helm能通过软件包的方式将这些资源对象统一管理起来。2、helm降低了应用配置和部署的复杂性。3、helm时应用程序可以多次复用。</p>
<h4 id="helm的概念"><a href="#helm的概念" class="headerlink" title="helm的概念"></a>helm的概念</h4><p>helm：命令行工具也是helm的客户端，主要用本地chart开发，chart仓库管理，与Tiller连接，release信息仓库。<br>chart：helm的软件包，类似于yum和apt里面的软件安装包，chart里面内含了kubernetes对象的配置模块，参数，依赖关系和说明文档。<br>release：是chart运行的实例，比如你通过helm将memcache这个chart部署到了这个kubernetes集群，那这个部署好的memcache就是一个release。<br>tiller：helm是一个c&#x2F;s架构程序，tiller是helm的服务端，用于接收helm客户端的请求它与kube-apiserver交互，负责基于chart创建release和删除和release状态追踪。<br>Repoistory：软件仓库，就类似于yum源和apt源一样。chart就是存在这里面。</p>
<h3 id="helm安装"><a href="#helm安装" class="headerlink" title="helm安装"></a>helm安装</h3><p>环境信息：<br>os：ubuntu16.04<br>docker：17.03<br>kubernetes：1.11.2<br>helm：2.9.1</p>
<p>不同操作系统有不同安装方法<a href="https://github.com/helm/helm/releases%EF%BC%8C%E6%88%91%E8%BF%99%E9%87%8C%E4%BD%BF%E7%94%A8ubuntu%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%EF%BC%8C%E7%9B%B4%E6%8E%A5%E4%B8%8B%E8%BD%BD%E5%AF%B9%E5%BA%94%E7%9A%84%E5%8C%85%E5%B0%B1%E5%A5%BD%E4%BA%86">https://github.com/helm/helm/releases，我这里使用ubuntu操作系统，直接下载对应的包就好了</a></p>
<h4 id="下载helm"><a href="#下载helm" class="headerlink" title="下载helm"></a>下载helm</h4><p>会发现下载不下来，因为在google那边</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget  https://kubernetes-helm.storage.googleapis.com/helm-v2.9.1-linux-amd64.tar.gz</span><br></pre></td></tr></table></figure>
<p>翻墙下载</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://kubernetes-helm.storage.googleapis.com/helm-v2.9.1-linux-amd64.tar.gz</span><br></pre></td></tr></table></figure>
<p>解压</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tar -xvf helm-v2.9.1-linux-amd64.tar.gz</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd linux-amd64/</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mv helm  /usr/bin/</span><br></pre></td></tr></table></figure>
<h4 id="安装Tiller"><a href="#安装Tiller" class="headerlink" title="安装Tiller"></a>安装Tiller</h4><p>执行helm init</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">helm init</span><br><span class="line">Creating /root/.helm</span><br><span class="line">Creating /root/.helm/repository</span><br><span class="line">Creating /root/.helm/repository/cache</span><br><span class="line">Creating /root/.helm/repository/local</span><br><span class="line">Creating /root/.helm/plugins</span><br><span class="line">Creating /root/.helm/starters</span><br><span class="line">Creating /root/.helm/cache/archive</span><br><span class="line">Creating /root/.helm/repository/repositories.yaml</span><br><span class="line">Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com</span><br><span class="line">Error: Looks like &quot;https://kubernetes-charts.storage.googleapis.com&quot; is not a valid chart repository or cannot be reached: Get https://kubernetes-charts.storage.googleapis.com/index.yaml: net/http: TLS handshake timeout</span><br></pre></td></tr></table></figure>
<p>helm init  在缺省配置下， Helm 会利用 “gcr.io&#x2F;kubernetes-helm&#x2F;tiller” 镜像在Kubernetes集群上安装配置 Tiller；并且利用 “<a href="https://kubernetes-charts.storage.googleapis.com/">https://kubernetes-charts.storage.googleapis.com</a>“ 作为缺省的 stable repository 的地址。由于在国内可能无法访问 “gcr.io”, “storage.googleapis.com”<br>所以这里我们换成国内阿里的repo</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.9.1 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts</span><br></pre></td></tr></table></figure>
<p>该命令会在当前目录下创建helm文件夹即~&#x2F;.helm，并且通过Kubernetes Deployment 部署tiller. 检查Tiller是否成功安装：<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/helm_base_1.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/helm_base_2.png"></p>
<h4 id="授权"><a href="#授权" class="headerlink" title="授权"></a>授权</h4><p>如果k8s集群打开的RBAC的话需要给tiller用ServiceAccount运行并且给这个ServiceAccount配置clusterrolebinding，因为tiller需要直接根kube-apiserver交互需要权限</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create serviceaccount --namespace kube-system tiller</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl patch deploy --namespace kube-system tiller-deploy -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;template&quot;:&#123;&quot;spec&quot;:&#123;&quot;serviceAccount&quot;:&quot;tiller&quot;&#125;&#125;&#125;&#125;&#x27;</span><br></pre></td></tr></table></figure>
<p>验证<br>执行helm version<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/helm_base_3.png"></p>
<p>查看当前仓库可用的chart</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">helm search</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/helm_base_4.png"></p>
<h4 id="部署应用测试"><a href="#部署应用测试" class="headerlink" title="部署应用测试"></a>部署应用测试</h4><p>部署mariadb<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/helm_base_5.png"></p>
<p>执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">helm install  --name mariadb --set &quot;persistence.enabled=false&quot; stable/mariadb</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/helm_base_6.png"></p>
<p>这里配置persistence.enabled&#x3D;false是因为我环境里面没有pvc所以把持久化存储关了，将chart内的这个变量设置为false<br>部署完成<br>查看是否部署成功<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/helm_base_7.png"></p>
<p>删除release</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">helm del release_name</span><br></pre></td></tr></table></figure>

<p>直接通过helm del删除release，会把对应的kubernetes资源对象删除，但release记录还是会保留下来，这时如果要还原可以直接执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">helm rollback release_name revision</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/helm_base_8.png"></p>
<p>虽然rollback能执行成功并且，资源也重启创建回来了，但是状态会变成PENDING_ROLLBACK,看github的issue是说rollback只能用于现有的部署，已经删除了release回滚状态会报错<br><a href="https://github.com/helm/helm/issues/3722">https://github.com/helm/helm/issues/3722</a><br>如果需要删除时，彻底删干净，连同release记录也删除，需要添加额外参数–purge</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">helm del --purge release_name</span><br></pre></td></tr></table></figure>

<h3 id="helm原理"><a href="#helm原理" class="headerlink" title="helm原理"></a>helm原理</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/helm_base_9.png"><br>图片来源cloudman《每天5分钟玩转kubernetes》<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/helm_base_10.png"></p>
<p>创建release<br>1、helm客户端从本地tar文件或repo里面解析出chart信息。<br>2、helm客户端将解析出来的信息通过grpc传给tiller。<br>3、tiller根据传来的信息生成release，然后将install release请求直接传递给kube-apiserver。</p>
<p>删除release<br>1、helm客户端从本地tar文件或repo里面解析出chart信息。<br>2、helm客户端将解析出来的信息通过grpc传给tiller。<br>3、tiller将delete release请求直接传递给kube-apiserver。</p>
<p>更新release<br>1、helm客户端将需要更新的chart的release名称chart结构和value信息传给tiller。<br>2、tiller将收到的信息生成新的release，并同时更新这个release的history。<br>3、tiller将新的release传递给kubernetes-apiserver进行更新。</p>
<p><a href="https://www.cnblogs.com/CloudMan6/p/8970314.html">https://www.cnblogs.com/CloudMan6/p/8970314.html</a><br><a href="https://docs.helm.sh/">https://docs.helm.sh</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>ironic-简介</title>
    <url>/2017/05/08/ironic/</url>
    <content><![CDATA[<h1 id="ironic简介"><a href="#ironic简介" class="headerlink" title="ironic简介"></a>ironic简介</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>openstack中对物理机管理的组件，通过ironic可以给物理机，上电、下电、重启，自动安装操作系统，根据设定的规则模板，进行自动化配置。ironic是I版开始进入孵化，j版与nova进行集成、K版正式release,ironic原本是mirantis开发的，后面贡献到社区。部署虚机和部署物理机对底层nova调用来创建虚机，但对nova-compute的底层的hypevisor是不一样的调用虚机底层调用的驱动可能是libvirtd、vmwareapi.VMwareVCDriver等，而物理机底层驱动是ironic。</p>
<h2 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h2><p>目前有些服务在虚机上运行达不到理想的性能，仅管openstack拥有trove、sahara、magnum等组件用于支持数据库平台、大数据平台、容器平台、但毕竟是在虚拟机里面部署服务，性能远远不够。但直接部署在物理机上又不好管理，此时ironic应运而生了。ironic可以解决物理机的添加、删除、电源管理、安装部署、未来可能能支持自动部署数据库、大数据平台、容器平台。</p>
<h2 id="项目构成"><a href="#项目构成" class="headerlink" title="项目构成"></a>项目构成</h2><p>ironic： 包含ironic-api 和ironic-conductor进程<br>python-ironicclinet: python clinet and CLI<br>ironic-python-agent: 一个运行在deployment ramdisk中的Python程序，用于执行一系列部署动作<br>pyghmi: 一个python的IPMI库，可以代替IPMItool<br>ironic-inspector: 硬件自检工具<br>ironic-lib： ironic的通用库函数<br>ironic-webclinet ：web console<br>ironic-ui：ironic的horizon插件<br>bifrost：一套只运行Ironic的Ansible脚本   </p>
<h2 id="ironic组件"><a href="#ironic组件" class="headerlink" title="ironic组件"></a>ironic组件</h2><p>ironic-api：负责接收请求，并且将请求传递给ironic-conductor<br>ironic-conductor：ironic中唯一一个能根数据库进行交互的组件，负责接收ironic-api的请求，然后根据请求执行相应的，创建、开机、关机、删除操作<br>ironic-python-agent：部署裸机时，pxe启动进入的一个bootstrap镜像，此镜像是用社区diskimage-builder工具制做，镜像内安装了ironic-python-agent，进入系统后与ironic-conductor进行交互， 将本地磁盘以iscsi方式挂载到控制节点，控制节点将系统dd到裸机磁盘上  </p>
<h2 id="ironic用到的技术"><a href="#ironic用到的技术" class="headerlink" title="ironic用到的技术"></a>ironic用到的技术</h2><p>PXE+tftp+dhcp<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ironic-1.png"></p>
<p>IPMI<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ironic-2.png"></p>
<p>iscsi<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ironic-3.png"></p>
<h2 id="裸机部署原理"><a href="#裸机部署原理" class="headerlink" title="裸机部署原理"></a>裸机部署原理</h2><p>1，nova boot启动一个实例，nova-api将请求通过rabbitmq，传给nova-conductor，在传给nova-scheduler。<br>2，nova-scheduler根据传递进来的flavor和image筛选出合适的计算节点，然后将请求传递到对应的计算节点。<br>3，nova-compute与ironic通信，调用ironic-api。<br>4，ironic-api将请求下发给ironic-conductor，ironic-ductor调用glance下载bootstrap镜像。<br>在tftp-serer目录生成pxelinux配置文件，并将bootstrap镜像放到tftp目录。<br>5，ironic-conductor通过调用ipmi driver将物理机开机，并设置以pxe的方式启动。<br>6，物理服务器启动后，通过pxe加载了bootstrap镜像启动后，镜像内ironic-python-agent 启动，与ironic-ductor进行交互，将本地磁盘通过iscsi方式挂载到控制节点，控制节点通过dd的方式将用户选择的系统写入到磁盘中。<br>7，写入完毕后ironic将调用ipmi driver对物理服务器进行重启操作。<br>8，重新启动后主机，初次进入系统时，运行镜像内的cloud-init进行初始化，配置hostname和密码。<br>这里需要注意，使用的镜像，需要提前在 &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;里面将网卡的配置文件提交准备好不然开机，网卡会up不起来。centos7需要修改grub参数，将网卡名以ethx显示，不然以设备名显示，配置文件不好写。</p>
<h2 id="流程图如下"><a href="#流程图如下" class="headerlink" title="流程图如下"></a>流程图如下</h2><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ironic-4.png"></p>
<p>需要注意的是<br>镜像<br>ironic有两种镜像一种是部署时的bootstrap镜像，这种镜像是内含了，ironic-python-agent，启动后会主动与ironic-conductor进行交互。  </p>
<p>另外一种是用户需要安装到裸机的镜像，这种镜像是通过bootstrap，通过iscsi将磁盘挂载到控制节点dd写入的。   </p>
<h2 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h2><p>在Newton版前，ironic都是不支持多租户，都是在一个flat网络上，互相之间是没有隔离关系的。</p>
<p><a href="https://www.ibm.com/developerworks/cn/cloud/library/cl-cn-virtualboxironic/index.html">https://www.ibm.com/developerworks/cn/cloud/library/cl-cn-virtualboxironic/index.html</a><br><a href="http://www.cnblogs.com/menkeyi/p/6063551.html">http://www.cnblogs.com/menkeyi/p/6063551.html</a><br><a href="https://docs.openstack.org/developer/ironic/liberty/deploy/user-guide.html">https://docs.openstack.org/developer/ironic/liberty/deploy/user-guide.html</a><br><a href="https://wiki.openstack.org/wiki/Ironic">https://wiki.openstack.org/wiki/Ironic</a>  </p>
]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title>Istio(二)通过Istio实现服务治理</title>
    <url>/2019/08/31/istio-2/</url>
    <content><![CDATA[<h3 id="环境信息："><a href="#环境信息：" class="headerlink" title="环境信息："></a>环境信息：</h3><table>
<thead>
<tr>
<th>组件</th>
<th>版本</th>
</tr>
</thead>
<tbody><tr>
<td>Kubernetes</td>
<td>1.15.5</td>
</tr>
<tr>
<td>Istio</td>
<td>1.4.2</td>
</tr>
<tr>
<td>helm</td>
<td>2.16.1</td>
</tr>
</tbody></table>
<h3 id="服务治理"><a href="#服务治理" class="headerlink" title="服务治理"></a>服务治理</h3><h4 id="sidecar的注入"><a href="#sidecar的注入" class="headerlink" title="sidecar的注入"></a>sidecar的注入</h4><p>前面提到，通过ServiceMesh进行微服务治理和传统的SDK的区别在于，ServiceMesh主要是依靠在应用POD中附加透明代理方式实现。下面介绍下istio是如何实现透明代理注入的。  </p>
<p>sidecar的注入方式有两种：<br>1、手动方式注入<br>手动注入其实就是修改deployment的pod  template添加另外一个sidecar容器  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">istioctl kube-inject -f samples/sleep/sleep.yaml | kubectl apply -f -</span><br></pre></td></tr></table></figure>

<p>2、使用自动方式注入<br>原理：<br>Kubernetes 调用时，admissionregistration.k8s.io&#x2F;v1beta1会进行配置。会在带有 istio-injection&#x3D;enabled 标签的命名空间中选择 Pod进行sidercar注入<br>配置sidecar自动注入是使用了mutating webhook admission controller实现的，在Kubernetes1.9+的版本才实现此功能,需要对pod实现此功能需要给对应的namespace打上istio-injection&#x3D;enabled 的标签  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl label namespace default istio-injection=enabled</span><br></pre></td></tr></table></figure>

<p>查看是否生效  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get namespace -L istio-injection</span><br><span class="line">NAME           STATUS   AGE   ISTIO-INJECTION</span><br><span class="line">default         Active   20h   enabled</span><br><span class="line">istio-system    Active   19h   </span><br><span class="line">kube-public     Active   20h   </span><br><span class="line">kube-system     Active   20h   </span><br></pre></td></tr></table></figure>


<p>这样就会在 Pod 创建时触发 Sidecar 的注入过程了。<br>运行测试容器  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f samples/sleep/sleep.yaml</span><br></pre></td></tr></table></figure>
<p>查看POD内是否有两个container</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod </span><br><span class="line">NAME                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">sleep-86cf99dfd6-ffmct   2/2     Running   0          13m</span><br></pre></td></tr></table></figure>
<p>查看POD的详细信息，发现多了个istio-proxy的container及其配置信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl describe pod/sleep-86cf99dfd6-ffmct</span><br></pre></td></tr></table></figure>




<p>需要关闭自动注入功能，只需要将这个标签去掉即可  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl label namespace default istio-injection-</span><br></pre></td></tr></table></figure>

<p>然后新建的POD就不带sidercar容器了</p>
<h4 id="概念讲解"><a href="#概念讲解" class="headerlink" title="概念讲解"></a>概念讲解</h4><p>Gateway：  服务入口流量控制，可与VirtualService绑定然后通过Istio规则来控制流量，早期使用kubernetes的ingress，后因为ingress的功能原因，又独立弄了套istio-Gateway，Istio Gateway 只配置四层到六层的功能，通常也会绑定VirtualServer实现七层转发规则。  </p>
<p>VirtualService：  定义转发到后端应用的路由规则，比如流量拆分百分比。  </p>
<p>DestinationRule：所定义了经过VirtualService处理之后的流量的访问策略。如定义负载均衡配置、连接池大小以及外部检测（用于在负载均衡池中对不健康主机进行识别和驱逐）配置。  </p>
<h4 id="bookinfo应用部署"><a href="#bookinfo应用部署" class="headerlink" title="bookinfo应用部署"></a>bookinfo应用部署</h4><p>以官方微服务应用Bookinfo为例演示istio的应用治理能力<br>bookinfo 应用程序显示的有关书籍的信息，类似于在线书店的单个商品。应用页面上显示的是书籍的描述、书籍详细信息（ISBN，页数等）以及书评。</p>
<p>bookinfo 应用一共包含四个微服务：Productpage、Details、Reviews、Ratings。</p>
<p>Productpage： 使用 Python 开发，负责展示书籍的名称和书籍的简介。<br>Details： 使用 Ruby 开发，负责展示书籍的详细信息。<br>Reviews： 使用 Java开发，负责显示书评（Reviews分为3个版本分别对应V1、V2、V3对应的显示就是没有星星，黑星星、红星星。<br>Ratings： 使用 Node.js 开发,负责显示书籍的评星。  </p>
<p>部署bookinfo  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get deployment</span><br><span class="line">NAME             READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">details-v1       1/1     1            1           13m</span><br><span class="line">productpage-v1   1/1     1            1           13m</span><br><span class="line">ratings-v1       1/1     1            1           13m</span><br><span class="line">reviews-v1       1/1     1            1           13m</span><br><span class="line">reviews-v2       1/1     1            1           13m</span><br><span class="line">reviews-v3       1/1     1            1           13m</span><br></pre></td></tr></table></figure>


<h3 id="通过Istio实现bookinfo服务治理"><a href="#通过Istio实现bookinfo服务治理" class="headerlink" title="通过Istio实现bookinfo服务治理"></a>通过Istio实现bookinfo服务治理</h3><h4 id="通过istio-Gateway暴露bookinfo"><a href="#通过istio-Gateway暴露bookinfo" class="headerlink" title="通过istio-Gateway暴露bookinfo"></a>通过istio-Gateway暴露bookinfo</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line">apiVersion: networking.istio.io/v1alpha3</span><br><span class="line">kind: Gateway</span><br><span class="line">metadata:</span><br><span class="line">  name: bookinfo-gateway</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    istio: ingressgateway # 关联istio-system namespace下istio-ingressgateway</span><br><span class="line">  servers:</span><br><span class="line">  - port:</span><br><span class="line">      number: 80</span><br><span class="line">      name: http</span><br><span class="line">      protocol: HTTP</span><br><span class="line">    hosts:</span><br><span class="line">    - &quot;*&quot;</span><br><span class="line">---</span><br><span class="line">apiVersion: networking.istio.io/v1alpha3</span><br><span class="line">kind: VirtualService</span><br><span class="line">metadata:</span><br><span class="line">  name: bookinfo</span><br><span class="line">spec:</span><br><span class="line">  hosts:</span><br><span class="line">  - &quot;*&quot;</span><br><span class="line">  gateways:</span><br><span class="line">  - bookinfo-gateway #关联Gateway对象</span><br><span class="line">  http:</span><br><span class="line">  - match:</span><br><span class="line">    - uri:</span><br><span class="line">        exact: /productpage #精确匹配</span><br><span class="line">    - uri:</span><br><span class="line">        prefix: /static #前缀匹配  </span><br><span class="line">    - uri:</span><br><span class="line">        exact: /login</span><br><span class="line">    - uri:</span><br><span class="line">        exact: /logout</span><br><span class="line">    - uri:</span><br><span class="line">        prefix: /api/v1/products</span><br><span class="line">    route:</span><br><span class="line">    - destination:</span><br><span class="line">        host: productpage.default.svc.cluster.local #对应kubernetes的Service，注意使用服务的短名称时Istio会根据规则所在的命名空间来处理这一名称，而非服务所在的命名空间。</span><br><span class="line">        port:</span><br><span class="line">          number: 9080</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p>访问istio-gateway对应的nodeport端口</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get svc/istio-ingressgateway  -n istio-system</span><br><span class="line">NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                                                                                      AGE</span><br><span class="line">istio-ingressgateway   NodePort   10.43.187.106   &lt;none&gt;        15020:30485/TCP,80:31380/TCP,443:31390/TCP,31400:31400/TCP,15029:31464/TCP,15030:31827/TCP,15031:30522/TCP,15032:31907/TCP,15443:32151/TCP   5h38m</span><br></pre></td></tr></table></figure>

<p>如访问：<a href="http://172.16.1.6:31380/productpage%EF%BC%8C">http://172.16.1.6:31380/productpage，</a><br>出现bookinfo页</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_2_1.png"></p>
<p>左侧评Reviews分为3个版本分别对应V1、V2、V3对应的显示就是没有星星，黑星星、红星星。可以通过刷新页面进行切换显示  </p>
<p>V1 空星星显示页  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_2_3.png"></p>
<p>V2 黑星星显示页  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_2_4.png"></p>
<p>V3 红星星显示页</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_2_2.png">  </p>
<p>通过kiali查看流量拓扑<br>创建kiali登录的帐号和密码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create secret generic kiali-secret -n istio-system --from-literal &quot;username=admin&quot; --from-literal &quot;passphrase=admin&quot;</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_2_6.png">  </p>
<h4 id="动态请求路由"><a href="#动态请求路由" class="headerlink" title="动态请求路由"></a>动态请求路由</h4><p>配置路由请求策略将全部访问流量指向reviews v1版本</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line">apiVersion: networking.istio.io/v1alpha3</span><br><span class="line">kind: VirtualService</span><br><span class="line">metadata:</span><br><span class="line">  name: reviews</span><br><span class="line">spec:</span><br><span class="line">  hosts:</span><br><span class="line">  - reviews</span><br><span class="line">  http:</span><br><span class="line">  - route:</span><br><span class="line">    - destination:</span><br><span class="line">        host: reviews.default.svc.cluster.local</span><br><span class="line">        subset: v1</span><br><span class="line">        </span><br><span class="line">---</span><br><span class="line">apiVersion: networking.istio.io/v1alpha3</span><br><span class="line">kind: DestinationRule</span><br><span class="line">metadata:</span><br><span class="line">  name: reviews</span><br><span class="line">spec:</span><br><span class="line">  host: reviews.default.svc.cluster.local</span><br><span class="line">  subsets:</span><br><span class="line">  - name: v1</span><br><span class="line">    labels:</span><br><span class="line">      version: v1</span><br><span class="line">  - name: v2</span><br><span class="line">    labels:</span><br><span class="line">      version: v2</span><br><span class="line">  - name: v3</span><br><span class="line">    labels:</span><br><span class="line">      version: v3</span><br><span class="line">EOF</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>DestinationRule内定义与后端服务的关联，这里的v1、v2、v3对应的label实际上就是对应kubernetes的workload的label</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">kubectl get deployment --show-labels</span><br><span class="line"></span><br><span class="line">NAME             READY   UP-TO-DATE   AVAILABLE   AGE   LABELS</span><br><span class="line">ratings-v1       1/1     1            1           23h   app=ratings,version=v1</span><br><span class="line">reviews-v1       1/1     1            1           23h   app=reviews,version=v1</span><br><span class="line">reviews-v2       1/1     1            1           23h   app=reviews,version=v2</span><br><span class="line">reviews-v3       1/1     1            1           23h   app=reviews,version=v3</span><br></pre></td></tr></table></figure>

<p>然后在去访问bookinfo页面发现Book Reviews处怎么刷新都还是空白<br>打开Kiali可以看见绿色代表流量走向的箭头已经指向reviews v1，对应的是<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_2_3.png"></p>
<p>查看kiali查看流量拓扑，流量都指向V1</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_2_7.png">  </p>
<h4 id="实现灰度发布"><a href="#实现灰度发布" class="headerlink" title="实现灰度发布"></a>实现灰度发布</h4><p>在这里实现两种灰度发布的方式 </p>
<p>方式一：通过http的head将请求路由到不同的后端服务上<br>比如我们这里将用户标识进行请求路由，比如将用户名为jason的请求路由到Reviews service v2的版本上其他用户还是轮询访问  </p>
<p>清空之前定义的规则  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl delete VirtualService/reviews</span><br></pre></td></tr></table></figure>

<p>然后定义新规则  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line">apiVersion: networking.istio.io/v1alpha3</span><br><span class="line">kind: VirtualService</span><br><span class="line">metadata:</span><br><span class="line">  name: reviews</span><br><span class="line">spec:</span><br><span class="line">  hosts:</span><br><span class="line">    - reviews</span><br><span class="line">  http:</span><br><span class="line">  - match:</span><br><span class="line">    - headers:</span><br><span class="line">        end-user:</span><br><span class="line">          exact: jason</span><br><span class="line">    route:</span><br><span class="line">    - destination:</span><br><span class="line">        host: reviews</span><br><span class="line">        subset: v2</span><br><span class="line">  - route:</span><br><span class="line">    - destination:</span><br><span class="line">        host: reviews</span><br><span class="line">        subset: v1</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<p>规则定义在请求头的end-user字段为Jason时将请求转发到Reviews-server v2版本<br>在UI上通过sign in登录输入用户名jason，不用输入密码刷新页面可以看见<br>无论怎么刷新都还是黑星星<br>退出登录或用其他用户名登录访问都还是Reviews-server v1版本(没星星)   </p>
<p>方式二：将访问流量分比例，比如百分之80流量访问Reviews-server v1版本，百分之20流量访问Reviews-server v2版本<br>先将刚才demo的规则清除 </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl delete virtualservice/reviews</span><br></pre></td></tr></table></figure>
<p>定义新的路由规则</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line">apiVersion: networking.istio.io/v1alpha3</span><br><span class="line">kind: VirtualService</span><br><span class="line">metadata:</span><br><span class="line">  name: reviews</span><br><span class="line">spec:</span><br><span class="line">  hosts:</span><br><span class="line">    - reviews</span><br><span class="line">  http:</span><br><span class="line">  - route:</span><br><span class="line">    - destination:</span><br><span class="line">        host: reviews</span><br><span class="line">        subset: v1</span><br><span class="line">      weight: 80</span><br><span class="line">    - destination:</span><br><span class="line">        host: reviews</span><br><span class="line">        subset: v2</span><br><span class="line">      weight: 20</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<p>不断刷新页面，你会发现Review只会显示没有星星和黑星星，并且黑星星的显示次数比没星星要少很多。<br>查看kiali看见流量走向Reviews-server v1和v2版本</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_2_8.png"></p>
<h4 id="熔断"><a href="#熔断" class="headerlink" title="熔断"></a>熔断</h4><p>在微服务中，服务被拆成一个个服务模块，各个服务模块之间有大量的调用，为了保证不会因为某个服务访问流量过载而影响其他服务导致大规模集群故障，所以通过熔断机制来保证服务的可靠性。</p>
<p>熔断的实现根我们日常生活中电路中的保险开关是一样的，通过熔断器对应用服务进行检测，当下游服务因访问压力过大而响应变慢或失败，上游服务为了保护系统整体的可用性，可以暂时切断对下游服务的调用。</p>
<p>熔断器配置：在时间窗口内，接口调用超时比率达到一个阈值，会开启熔断。进入熔断状态后，后续对该服务接口的调用会被拒绝，直接进入服务降级模式。</p>
<p>熔断恢复：当经过了对应的时间之后，服务将从熔断状态恢复过来，再次接受调用。</p>
<p>通过istio实现熔断<br>启动httpbin测试应用<br>httpbin是一个web服务，我们将它是部署到kubernetes中然后通过fortio模拟请求访问</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">kubectl apply -f samples/httpbin/httpbin.yaml</span><br></pre></td></tr></table></figure>
<p>创建断路器  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl create -f -</span><br><span class="line">apiVersion: networking.istio.io/v1alpha3</span><br><span class="line">kind: DestinationRule</span><br><span class="line">metadata:</span><br><span class="line">  name: httpbin</span><br><span class="line">spec:</span><br><span class="line">  host: httpbin</span><br><span class="line">  trafficPolicy:</span><br><span class="line">    connectionPool:</span><br><span class="line">      tcp:</span><br><span class="line">        maxConnections: 1</span><br><span class="line">      http:</span><br><span class="line">        http1MaxPendingRequests: 1</span><br><span class="line">        maxRequestsPerConnection: 1</span><br><span class="line">    outlierDetection:</span><br><span class="line">      consecutiveErrors: 1</span><br><span class="line">      interval: 1s</span><br><span class="line">      baseEjectionTime: 3m</span><br><span class="line">      maxEjectionPercent: 100</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<p>在本步骤中，我们可以理解为Istio的熔断功能主要是通过在链接池中加入上述三个参数：<br>·     MaxConnections定义了到目标主机的 HTTP1&#x2F;TCP 最大连接数；<br>·     http1MaxPendingRequests定义了针对一个目标的 HTTP 请求的最大排队数量；<br>·     maxRequestsPerConnection定义了对某一后端的请求中，一个连接内能够发出的最大请求数量。如果将这一参数设置为 1 则会禁止 keep alive 特性。</p>
<p>接下来创建一个客户端，用来向后端服务发送请求，观察是否会触发熔断策略。这里要使用一个简单的负载测试客户端，名字叫 fortio。这个客户端可以控制连接数量、并发数以及发送 HTTP 请求的延迟。这里我们会把给客户端也进行 Sidecar 的注入，以此保证 Istio 对网络交互的控制：  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f samples/httpbin/sample-client/fortio-deploy.yaml</span><br></pre></td></tr></table></figure>
<p>接下来就可以登入客户端 Pod 并使用 Fortio 工具来调用 httpbin。-curl 参数表明只调用一次</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FORTIO_POD=$(kubectl get pod | grep fortio | awk &#x27;&#123; print $1 &#125;&#x27;)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl exec -it $FORTIO_POD  -c fortio /usr/bin/fortio -- load -curl  http://httpbin:8000/get</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">HTTP/1.1 200 OK</span><br><span class="line">server: envoy</span><br><span class="line">date: Sun, 02 Feb 2020 08:49:27 GMT</span><br><span class="line">content-type: application/json</span><br><span class="line">content-length: 371</span><br><span class="line">access-control-allow-origin: *</span><br><span class="line">access-control-allow-credentials: true</span><br><span class="line">x-envoy-upstream-service-time: 3</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;args&quot;: &#123;&#125;, </span><br><span class="line">  &quot;headers&quot;: &#123;</span><br><span class="line">    &quot;Content-Length&quot;: &quot;0&quot;, </span><br><span class="line">    &quot;Host&quot;: &quot;httpbin:8000&quot;, </span><br><span class="line">    &quot;User-Agent&quot;: &quot;fortio.org/fortio-1.3.1&quot;, </span><br><span class="line">    &quot;X-B3-Parentspanid&quot;: &quot;ea1d6bd8e1ac8621&quot;, </span><br><span class="line">    &quot;X-B3-Sampled&quot;: &quot;0&quot;, </span><br><span class="line">    &quot;X-B3-Spanid&quot;: &quot;cef333d2e32ce6b4&quot;, </span><br><span class="line">    &quot;X-B3-Traceid&quot;: &quot;d89ae2b658f513b3ea1d6bd8e1ac8621&quot;</span><br><span class="line">  &#125;, </span><br><span class="line">  &quot;origin&quot;: &quot;127.0.0.1&quot;, </span><br><span class="line">  &quot;url&quot;: &quot;http://httpbin:8000/get&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这表明我们创建的客户端已经成功与服务端进行了一次通信。<br>在kiali上可以看见httpbin连接路由图。请求都是返回正常的状态码为200</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_2_9.png"></p>
<p>验证熔断<br>在上面的熔断设置中指定了 maxConnections&#x3D;1 以及 http1MaxPendingRequests&#x3D;1。这意味着如果超过了一个连接同时发起请求，Istio 就会熔断，阻止后续的请求或连接。我们不妨尝试通过并发2个连接(-c 2)发送20个请求数(-n 20)来看一下结果。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl exec -it $FORTIO_POD  -c fortio /usr/bin/fortio -- load -c 2 -qps 0 -n 20 -loglevel Warning http://httpbin:8000/get</span><br></pre></td></tr></table></figure>


<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">Sockets used: 17 (for perfect keepalive, would be 2)</span><br><span class="line">Code 200 : 3 (15.0 %)</span><br><span class="line">Code 503 : 17 (85.0 %)</span><br><span class="line">Response Header Sizes : count 20 avg 34.5 +/- 82.13 min 0 max 230 sum 690</span><br><span class="line">Response Body/Total Sizes : count 20 avg 226.3 +/- 159.6 min 153 max 601 sum 4526</span><br><span class="line">All done 20 calls (plus 0 warmup) 5.768 ms avg, 344.2 qps</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>基本上所有的请求都发送成功了。明明我们设置的最大连接数是1，而我们模拟了两个并发连接，理论上应该只有一半的请求能成功才对，难道熔断没有成功？这里别忘了我们还设置了http1MaxPendingRequests&#x3D;1，正如在前文中介绍的，这个参数的功能类似于为最大连接数提供了一级缓存，所以虽然我们的最大连接数是1，但是因为这个参数也为1，所以两个并发连接的请求都可以发送成功。</p>
<p>接下来我们需要修改请求连接数，将连接数提高到3(-c 3)，请求数不变</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl exec -it $FORTIO_POD  -c fortio /usr/bin/fortio -- load -c 3 -qps 0 -n 20 -loglevel Warning http://httpbin:8000/get</span><br></pre></td></tr></table></figure>
<p>­<br>这时候可以观察到熔断行为生效了  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line"></span><br><span class="line">Code 200 : 13 (65.0 %)</span><br><span class="line">Code 503 : 7 (35.0 %)</span><br></pre></td></tr></table></figure>
<p>有百分之65的请求通过了，百分之35的请求被拒绝了</p>
<p>重复执行上述测试命令后，进入服务降级模式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">Sockets used: 20 (for perfect keepalive, would be 3)</span><br><span class="line">Code 503 : 20 (100.0 %)</span><br></pre></td></tr></table></figure>

<h4 id="黑白名单"><a href="#黑白名单" class="headerlink" title="黑白名单"></a>黑白名单</h4><p>定义个黑名单，禁止Review-v3访问后端ratings服务，所以就不会显示红星星</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line">apiVersion: &quot;config.istio.io/v1alpha2&quot;</span><br><span class="line">kind: handler</span><br><span class="line">metadata:</span><br><span class="line">  name: denyreviewsv3handler</span><br><span class="line">spec:</span><br><span class="line">  compiledAdapter: denier</span><br><span class="line">  params:</span><br><span class="line">    status:</span><br><span class="line">      code: 7</span><br><span class="line">      message: Not allowed</span><br><span class="line">---</span><br><span class="line">apiVersion: &quot;config.istio.io/v1alpha2&quot;</span><br><span class="line">kind: instance</span><br><span class="line">metadata:</span><br><span class="line">  name: denyreviewsv3request</span><br><span class="line">spec:</span><br><span class="line">  compiledTemplate: checknothing</span><br><span class="line">---</span><br><span class="line">apiVersion: &quot;config.istio.io/v1alpha2&quot;</span><br><span class="line">kind: rule</span><br><span class="line">metadata:</span><br><span class="line">  name: denyreviewsv3</span><br><span class="line">spec:</span><br><span class="line">  match: destination.labels[&quot;app&quot;] == &quot;ratings&quot; &amp;&amp; source.labels[&quot;app&quot;]==&quot;reviews&quot; &amp;&amp; source.labels[&quot;version&quot;] == &quot;v3&quot;</span><br><span class="line">  actions:</span><br><span class="line">  - handler: denyreviewsv3handler</span><br><span class="line">    instances: [ denyreviewsv3request ]</span><br><span class="line">EOF</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>重点关注<code>match: destination.labels[&quot;app&quot;] == &quot;ratings&quot; &amp;&amp; source.labels[&quot;app&quot;]==&quot;reviews&quot; &amp;&amp; source.labels[&quot;version&quot;] == &quot;v3&quot;</code><br>这句配置，定义了源和目标，源为label为version&#x3D;&#x3D;v3的workload，目标为label为app&#x3D;&#x3D;reviews的workload。应用后去访问bookinfo发现,不断刷新，访问到v3时你会发现如下图显示。 </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_2_11.png"></p>
<p><a href="https://jimmysong.io/istio-handbook/concepts/traffic-management-basic.html">https://jimmysong.io/istio-handbook/concepts/traffic-management-basic.html</a><br><a href="https://www.servicemesher.com/istio-handbook/best-practices/how-to-implement-ingress-gateway.html">https://www.servicemesher.com/istio-handbook/best-practices/how-to-implement-ingress-gateway.html</a></p>
]]></content>
      <categories>
        <category>ServiceMesh</category>
      </categories>
      <tags>
        <tag>ServiceMesh</tag>
      </tags>
  </entry>
  <entry>
    <title>Jenkins对接sonarqube</title>
    <url>/2018/06/11/jenkins&amp;sonar/</url>
    <content><![CDATA[<h3 id="概述："><a href="#概述：" class="headerlink" title="概述："></a>概述：</h3><p>sonar是一个开源的代码质量分析检测平台，通过插件机制，能支持多种开发语言的代码质量检测，如java、php、ruby等，支持自定义代码扫描规则，同时sonar也支持对接常见的CI平台如Jenkins等，支持CI平台将代码重代码仓库拉取下来后直接调用sonar进行代码扫描，并将扫描结果进行汇总分类，汇制表图。</p>
<h3 id="环境信息："><a href="#环境信息：" class="headerlink" title="环境信息："></a>环境信息：</h3><p>system：ubuntu:16.04<br>Jenkins：2.89.4<br>java：jdk1.8.161<br>rancher：2.0.2<br>kubernetes：1.10.1</p>
<h3 id="sonar安装"><a href="#sonar安装" class="headerlink" title="sonar安装"></a>sonar安装</h3><p>sonar安装这里直接使用ranche2.0内置的应用商店安装的，当然你也可以通过其他方式安装<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkin_sonar_1.png"></p>
<p>默认部署出来的访问是通过ingress，为了方便直接改成NodePort<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_sonar_2.png"></p>
<p>部署成功<br>默认会部署4个pod另外两个test直接删了，留这两个sonarqube和sonar-postgresql就可以<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_sonar_3.png"></p>
<p>通过hostip:nodeport方式访问sonar,比如我这里nodeport是31963</p>
<h3 id="sonar配置"><a href="#sonar配置" class="headerlink" title="sonar配置"></a>sonar配置</h3><p>访问(默认帐号密码是admin&#x2F;admin)<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_sonar_4.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_sonar_5.png"><br>如果需要中文直接安装插件就好<br>administrator—-&gt;Marketplace<br>搜索Chinese—-安装</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_sonar_6.png"><br>安装java语言插件<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_sonar_7.png"></p>
<p>申请token<br>administrator—&gt;security—&gt;user—&gt;token<br>保存生成token<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_sonar_8.png"></p>
<h3 id="Jenkins配置"><a href="#Jenkins配置" class="headerlink" title="Jenkins配置"></a>Jenkins配置</h3><p>安装插件<br>系统设置—&gt;插件管理<br>安装SonarQube Scanner for Jenkins<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_sonar_9.png"></p>
<p>配置<br>系统设置—&gt;配置—&gt;SonarQube servers<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_sonar_10.png"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Name：SonarQube</span><br><span class="line">Server URL ：sonar-server</span><br><span class="line">server authentication token ： 刚刚sonar生成的token</span><br><span class="line">Version of sonar-maven-plugin	:5.3 or higher</span><br></pre></td></tr></table></figure>
<p> 配置Sonar-Scanner<br> sonar-Scanner用于Jenkins扫描可以选自动安装也可以手动选择应该安装好了的，将路径填上去。这里我们选自动安装<br> 系统管理—&gt;全局工具配置<br> 配置JAVA_HOME<br> <img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_sonar_11.png"></p>
<p> <img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_sonar_12.png"></p>
<p>测试<br>freestyle</p>
<p>sudoers java路径<br>Jenkins创建freestyle<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_sonar_13.png"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sonar.projectKey=Test #sonar那显示project-key</span><br><span class="line">sonar.projectName=Test #sonar那显示project名字</span><br><span class="line">sonar.projectVersion=1.0 ##sonar那显示project版本</span><br><span class="line">sonar.sources=src</span><br><span class="line">sonar.java.binaries=/var/lib/jenkins/workspace/piggymetrics/gateway/target/classes</span><br><span class="line">sonar.language=java</span><br><span class="line">sonar.sourceEncoding=UTF-8</span><br></pre></td></tr></table></figure>
<p>执行构建<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_sonar_14.png"></p>
<p>pipline</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">node&#123;</span><br><span class="line"></span><br><span class="line">    stage(&#x27;git clone&#x27;)&#123;</span><br><span class="line">        //check CODE</span><br><span class="line">        git credentialsId: &#x27;ab35a358-9743-4a65-afd2-58a56a3e8f29&#x27;, url: &#x27;http://172.31.164.58:1080/wanshaoyuan/piggymetrics-account-service.git&#x27;</span><br><span class="line">    &#125;</span><br><span class="line">    stage(&#x27;SonarQube analysis&#x27;) &#123;</span><br><span class="line">        def sonarqubeScannerHome = tool name: &#x27;SonarQube Scanner&#x27;</span><br><span class="line"></span><br><span class="line">        withSonarQubeEnv(&#x27;SonarQube&#x27;) &#123;</span><br><span class="line">            sh &quot;$&#123;sonarqubeScannerHome&#125;/bin/sonar-scanner&quot;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>pipline必须在对应的代码库的根目录创建sonar-project.properties<br>添加以下内容</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sonar.projectKey=piggymetrics-auth-service:0.0.1</span><br><span class="line"># this is the name and version displayed in the SonarQube UI. Was mandatory prior to SonarQube 6.1.</span><br><span class="line">sonar.projectName=piggymetrics-auth-service</span><br><span class="line">sonar.projectVersion=0.0.1</span><br><span class="line"># Path is relative to the sonar-project.properties file. Replace &quot;\&quot; by &quot;/&quot; on Windows.</span><br><span class="line"># This property is optional if sonar.modules is set.</span><br><span class="line">sonar.sources=src</span><br><span class="line">sonar.java.binaries=/var/lib/jenkins/workspace/piggymetrics-auth-service/target/classes</span><br><span class="line">sonar.java.source=1.8</span><br><span class="line">sonar.java.target=1.8</span><br><span class="line">sonar.language=java</span><br><span class="line"># Encoding of the source code. Default is default system encoding</span><br><span class="line">sonar.sourceEncoding=UTF-8</span><br></pre></td></tr></table></figure>
<p>执行构建<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_sonar_15.png"></p>
<p>最终在sonarQue看见<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_sonar_16.png"></p>
<p>测试代码可以直接使用测试</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://github.com/sqshq/PiggyMetrics</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>CI/CD</category>
      </categories>
      <tags>
        <tag>CI/CD</tag>
      </tags>
  </entry>
  <entry>
    <title>Istio(三)通过Rancher使用Istio</title>
    <url>/2019/10/01/istio-3/</url>
    <content><![CDATA[<h3 id="环境信息："><a href="#环境信息：" class="headerlink" title="环境信息："></a>环境信息：</h3><table>
<thead>
<tr>
<th>组件</th>
<th>版本</th>
</tr>
</thead>
<tbody><tr>
<td>Kubernetes</td>
<td>1.15.5</td>
</tr>
<tr>
<td>Istio</td>
<td>1.4.3</td>
</tr>
<tr>
<td>Rancher</td>
<td>2.3.4</td>
</tr>
</tbody></table>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>Rancher2.3版本已经集成Istio了，通过Rancher2.3可以非常简单方便和直观的对Istio规则进行配置和管理。</p>
<h3 id="部署Rancher"><a href="#部署Rancher" class="headerlink" title="部署Rancher"></a>部署Rancher</h3><p>参考官方手册，进行单节点部署，或HA部署。</p>
<p><a href="https://docs.rancher.cn/rancher2x/quick-start.html">https://docs.rancher.cn/rancher2x/quick-start.html</a></p>
<h3 id="启用Istio"><a href="#启用Istio" class="headerlink" title="启用Istio"></a>启用Istio</h3><p>进入Rancher后在对集群工具选项中启用Istio。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_3_1.png">  </p>
<p>默认是没有启用Istio-gateway，可以自行启用<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_3_2.png">  </p>
<p>启用Istio后Rancher会在System项目下部署Istio组件<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_3_3.png"> </p>
<p>在对应项目的资源下可以看见已经启用的Istio，在这可以配置Istio规则对应图表的查看<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_3_4.png">  </p>
<h3 id="测试使用"><a href="#测试使用" class="headerlink" title="测试使用"></a>测试使用</h3><p>继续以Bookinfo为例，通过Rancher UI配置对应的规则<br>部署bookinfo </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml</span><br></pre></td></tr></table></figure>
<p>在对应的default命名空间启用sidecar自动注入</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_3_5.png">  </p>
<p>部署bookinfo </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml</span><br></pre></td></tr></table></figure>
<p>因为本次没有启用Istio-Gateway，所以直接将bookinfo的productpage通过nodeport方式暴露出来</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl patch svc productpage -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&#x27;</span><br></pre></td></tr></table></figure>


<p>访问</p>
<p><a href="http://172.16.1.6:32632/productpage">http://172.16.1.6:32632/productpage</a></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_2_1.png"></p>
<p>流量监控查看，这个主要从prometheus处获取</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_3_7.png">  </p>
<p>内嵌kiali监控图<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_3_8.png">  </p>
<p>其他对应的如grafana、kiali、jaeger、prometheus的访问界面可以直接点击左上方图标即可进入。  </p>
<h4 id="实现动态请求路由"><a href="#实现动态请求路由" class="headerlink" title="实现动态请求路由"></a>实现动态请求路由</h4><p>配置路由请求策略将全部访问流量指向reviews v1版本<br>定义destinationrule配置对应的版本和label关联<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_3_9.png">  </p>
<p>创建VirtualService服务，定义http路由访问V1版本，权重设置100</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_3_10.png"><br>访问<br><a href="http://172.16.1.6:32632/productpage%E4%B8%8D%E6%96%AD%E5%88%B7%E6%96%B0%E9%A1%B5%E9%9D%A2%E5%8F%AF%E4%BB%A5%E7%9C%8B%E8%A7%81%E4%B8%80%E7%9B%B4%E6%98%BE%E7%A4%BAv1%E7%89%88%E6%9C%AC">http://172.16.1.6:32632/productpage不断刷新页面可以看见一直显示v1版本</a></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_2_3.png"></p>
<h4 id="实现灰度发布"><a href="#实现灰度发布" class="headerlink" title="实现灰度发布"></a>实现灰度发布</h4><p>在这里实现两种灰度发布的方式</p>
<p>方式一：通过http的head将请求路由到不同的后端服务上<br>比如我们这里将用户标识进行请求路由，比如将用户名为jason的请求路由到Reviews service v2的版本上其他用户还是轮询访问</p>
<p>删除之前定义的VirtualService规则</p>
<p>定义新规则<br>根据http的头部headers配置其匹配规则</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_3_11.png">  </p>
<p>创建两个http路由，一个定义匹配规则，规则为基于http的头的end-user，精确匹配为jason则，访问到reviews v2版本，当http的头的end-user字段非jason时访问V1版本。</p>
<p>规则定义在请求头的end-user字段为Jason时将请求转发到Reviews-server v2版本<br>在UI上通过sign in登录输入用户名jason，不用输入密码刷新页面可以看见<br>无论怎么刷新都还是黑星星<br>退出登录或用其他用户名登录访问都还是Reviews-server v1版本(没星星)</p>
<p>方式二：将访问流量分比例，比如百分之80流量访问Reviews-server v1版本，百分之20流量访问Reviews-server v2版本</p>
<p>删除刚刚定义的virtualService规则，建立新规则，在http路由中添加另外一个hosts，设置流量访问权重，V1版本权重为80，V2版本权重为20</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_3_12.png">  </p>
<p>不断刷新页面，你会发现Review只会显示没有星星和黑星星，并且黑星星的显示次数比没星星要少很多。 查看kiali看见流量走向Reviews-server v1和v2版本</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_3_13.png">  </p>
<h4 id="熔断配置"><a href="#熔断配置" class="headerlink" title="熔断配置"></a>熔断配置</h4><p>部署httpbin</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f samples/httpbin/httpbin.yaml</span><br></pre></td></tr></table></figure>

<p>创destinationrule断路器  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/istio_3_14.png"> </p>
<p>定义连接池<br>HTTP1最大等待请求设置为1<br>每个连接的HTTP最大请求设置为1<br>TCP最大连接数设置为1</p>
<p>异常检测<br>基本驱逐时间设置为1<br>连续错误设置为1<br>时间间隔为1s<br>最大驱逐百分比为100%</p>
<p>部署fortio进行测试  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f samples/httpbin/sample-client/fortio-deploy.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FORTIO_POD=$(kubectl get pod | grep fortio | awk &#x27;&#123; print $1 &#125;&#x27;)</span><br></pre></td></tr></table></figure>
<p>执行fortio用3个连接20个请求进行测试</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl exec -it $FORTIO_POD  -c fortio /usr/bin/fortio -- load -c 3 -qps 0 -n 20 -loglevel Warning http://httpbin:8000/get</span><br></pre></td></tr></table></figure>
<p>多连接几次，可以看见服务降级生效，请求都被拒绝了。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt; 0.001 &lt;= 0.00147963 , 0.00123982 , 100.00, 3</span><br><span class="line"># target 50% 0.00072548</span><br><span class="line"># target 75% 0.000921566</span><br><span class="line"># target 90% 0.00115988</span><br><span class="line"># target 99% 0.00144765</span><br><span class="line"># target 99.9% 0.00147643</span><br><span class="line">Sockets used: 20 (for perfect keepalive, would be 3)</span><br><span class="line">Code 503 : 20 (100.0 %)</span><br></pre></td></tr></table></figure>

<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>通过rancher可以相对简单的把Istio用起来，但实际上Istio本身的配置复杂对于大部分使用者还是有一定门槛，相信在后期的版本中应该能得到一定解决。</p>
]]></content>
      <categories>
        <category>ServiceMesh</category>
      </categories>
      <tags>
        <tag>ServiceMesh</tag>
      </tags>
  </entry>
  <entry>
    <title>Jenkins介绍和使用</title>
    <url>/2018/05/01/jenkins_1/</url>
    <content><![CDATA[<h3 id="Jenkins安装"><a href="#Jenkins安装" class="headerlink" title="Jenkins安装"></a>Jenkins安装</h3><h4 id="常见安装方式"><a href="#常见安装方式" class="headerlink" title="常见安装方式"></a>常见安装方式</h4><p>Jenkins安装可以直接参考官方文档<br><a href="https://www.jenkins.io/download/">https://www.jenkins.io/download/</a><br>Jenkins软件主要分为两个大版本<br>Stable版和Regular releases 版本，最大区别在于Stable版，每隔12周会从Stable版本里面选出LTS (长期支持) 版本作为该时间段的稳定版本。Regular releases 版本是每周更新。<br>支持以下几种常见安装方式：<br>1、对应操作系统发行版软件安装包方式安装<br>2、War包安装<br>3、容器化安装<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-1.png"></p>
<h4 id="安装Jenkins"><a href="#安装Jenkins" class="headerlink" title="安装Jenkins"></a>安装Jenkins</h4><p>这里使用Docker安装，能够快速拉起，解决依赖问题<br>在一台装有Docker的主机上执行以下命令</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -d --name jenkins -u root -p 8080:8080 -p 50000:50000 -v /var/jenkins_home:/var/jenkins_home -v /usr/bin/docker:/usr/bin/docker -v /var/run/docker.sock:/var/run/docker.sock  jenkins/jenkins:lts-jdk11</span><br></pre></td></tr></table></figure>

<p>查看初始化密码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker logs jenkins --tail 100</span><br></pre></td></tr></table></figure>

<p>访问主机的8080端口<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_2.png"></p>
<p>安装方式标准安装和自定义安装，标准安装主要进行一些标准插件的安装，自定义安装就是自己可以选择插件进行安装。离线模式可以选自定义安装，这里我们选择标准安装。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_3.png"></p>
<p>设置管理员用户名和密码<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_5.png"><br>进入界面<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-2.png"></p>
<h3 id="插件管理"><a href="#插件管理" class="headerlink" title="插件管理"></a>插件管理</h3><h4 id="在线安装插件"><a href="#在线安装插件" class="headerlink" title="在线安装插件"></a>在线安装插件</h4><p>这边我们以gitlab做对接所以需要安装gitlab插件为例，操作任何<br>系统管理—&gt;管理插件—–&gt;可选插件—-&gt;gitlab plugin<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_7.png"><br>默认在线方式安装是通过境外的Jenkins插件服务器进行下载，网络质量会比较差，为了更好的下载速度，可以配置使用境内插件地址。<br>Manager jenkins &gt; manage plugins &gt; advanced &gt; upgrade url<br>重新填写 清华大学的Jenkins Plugin URL : <a href="https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/current/update-center.json">https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/current/update-center.json</a><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-3.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_7.png"></p>
<p>安装完成后可以在已安装插件这里看见安装完后的gitlab-plugin<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-4.png"></p>
<h4 id="离线安装插件"><a href="#离线安装插件" class="headerlink" title="离线安装插件"></a>离线安装插件</h4><p>对于无法联网环境，可以通过离线插件方式进行安装，Jenkins插件之间本身是有依赖的，需要先找到对应的插件所依赖的其他插件，一同下载导入即可。<br>通过系统管理-插件管理-可选插件-过滤框输入Maven Integration-选择Maven Integration-点击安装   可以看见这个插件所需要的依赖插件<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-5.png"></p>
<p>去<a href="https://plugins.jenkins.io/">https://plugins.jenkins.io/</a> 下载这些插件（.hpi）<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-6.png"></p>
<p>将hpi文件导入到Jenkins<br>系统管理-插件管理-高级-选择.hpi文件-上传</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-7.png"></p>
<p>另外一种方式，在外网可通环境下使用容器方式部署，将插件安装完后，直接将此容器重新做为镜像，到离线环境中使用，这样新启动的Jenkins里面就包含了这些插件。  </p>
<h4 id="创建一个简单的编译任务"><a href="#创建一个简单的编译任务" class="headerlink" title="创建一个简单的编译任务"></a>创建一个简单的编译任务</h4><p>创建构建任务<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-8.png"></p>
<p>选择FreeStyle Project<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-9.png"></p>
<p>构建方式选择执行shell<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-10.png"></p>
<p>输入shell脚本</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">echo ”Hello world“</span><br></pre></td></tr></table></figure>
<p>开始构建<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-11.png"></p>
<p>构建完成后，在左下角可以看见对应的构建记录<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-12.png"></p>
<p>点击控制台输出查看详细情况<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-13.png"></p>
<p>这就是一个还没有对接任何代码仓库，简单的编译任务。  </p>
<h3 id="Jenkins权限管理"><a href="#Jenkins权限管理" class="headerlink" title="Jenkins权限管理"></a>Jenkins权限管理</h3><p>通过权限控制，使项目组对应的成员只能看见本项目下构建任务，如前端项目组成员只能看见前端项目下的构建任务，后端项目组成员，只能看见后端项目下的构建任务<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-14.png"></p>
<h4 id="安装Role-based-插件"><a href="#安装Role-based-插件" class="headerlink" title="安装Role-based 插件"></a>安装Role-based 插件</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-15.png"></p>
<h4 id="修改全局安全设置"><a href="#修改全局安全设置" class="headerlink" title="修改全局安全设置"></a>修改全局安全设置</h4><p>进入 系统管理-&gt;全局安全配置 中将 访问控制 里 授权策略 配置为 Role-Based Strategy：<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-16.png"></p>
<h4 id="创建用户"><a href="#创建用户" class="headerlink" title="创建用户"></a>创建用户</h4><p>Manager Jenkins——&gt;Manager users——&gt; 新建用户</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-17.png"></p>
<h4 id="在-Jenkins-中创建两个视图-backend、frontend-并且在两个视图中创建相应的任务："><a href="#在-Jenkins-中创建两个视图-backend、frontend-并且在两个视图中创建相应的任务：" class="headerlink" title="在 Jenkins 中创建两个视图 backend、frontend 并且在两个视图中创建相应的任务："></a>在 Jenkins 中创建两个视图 backend、frontend 并且在两个视图中创建相应的任务：</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-18.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-19.png"></p>
<h4 id="创建角色"><a href="#创建角色" class="headerlink" title="创建角色"></a>创建角色</h4><p>下面将创建一个拥有全局可读权限的全局角色来保证全部用户拥有全部可读权限，然后再创建Item组角色，利用通配符方式匹配对应项目前缀，这样用户拥有“全局可读”+“匹配项目读写”权限。</p>
<p>进入 系统管理-&gt;Manage and Assign Roles 可以看到 角色管理 与 角色分配：<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-20.png"></p>
<p>设置全局之读角色<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-21.png"></p>
<p>添加item角色<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-22.png"></p>
<p>在 Item roles 一栏添加组角色 group_backend、group_frontend 两个角色，并且设置：</p>
<p>角色 group_backend 只能看见以 backend.* 开头的视图与项目。<br>角色 group_frontend 只能看见以 frontend.* 开头的视图与项目。<br>设置完成后再设置两个角色都勾选全部权限，确保两个角色拥有该组的全部权限。</p>
<p>Pattern 里面输入通配符，一般是项目前缀，经常以 “xxxx.*” 命名，“xxxx” 为前缀，“.*”为匹配任意名称。</p>
<h4 id="分配角色"><a href="#分配角色" class="headerlink" title="分配角色"></a>分配角色</h4><p>进入Manage and Assign Roles——&gt;Assign Roles选项卡</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-23.png"></p>
<p>将frontend-user1分配到group_frontend角色，backend-user分配到group_backend角色</p>
<p>登录frontend-user1和backend-user查看分配结果<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-24.png"><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins1-25.png"></p>
]]></content>
      <categories>
        <category>CI/CD</category>
      </categories>
      <tags>
        <tag>CI/CD</tag>
      </tags>
  </entry>
  <entry>
    <title>Jenkins与外部系统集成</title>
    <url>/2021/10/14/jenkins_Docking_external_system/</url>
    <content><![CDATA[<h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>软件版本</p>
<table>
<thead>
<tr>
<th>软件</th>
<th>版本</th>
</tr>
</thead>
<tbody><tr>
<td>gitlab</td>
<td>14.3.0</td>
</tr>
<tr>
<td>Jenkins</td>
<td>2.303.1</td>
</tr>
<tr>
<td>Harbor</td>
<td>1.10.2</td>
</tr>
<tr>
<td>Sonar</td>
<td>9.1</td>
</tr>
<tr>
<td>Nexus</td>
<td>3.35.0-02</td>
</tr>
<tr>
<td>ArgoCD</td>
<td>2.1.3</td>
</tr>
</tbody></table>
<h4 id="部署gitlab"><a href="#部署gitlab" class="headerlink" title="部署gitlab"></a>部署gitlab</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --detach --hostname 10.8.242.28 --publish 443:443 --publish 80:80 --publish 1022:22 --name gitlab --restart always --volume /srv/gitlab/config:/etc/gitlab --volume /srv/gitlab/logs:/var/log/gitlab --volume /srv/gitlab/data:/var/opt/gitlab gitlab/gitlab-ce:12.10.3-ce.0</span><br></pre></td></tr></table></figure>

<p>替换hostname为实际节点外网IP</p>
<h4 id="部署Harbor"><a href="#部署Harbor" class="headerlink" title="部署Harbor"></a>部署Harbor</h4><p>Harbor部署与管理<br>部署前先修改docker<br>编辑docker</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim /etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line"> &quot;insecure-registries&quot; : [&quot;0.0.0.0/0&quot;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>重启docker</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl restart docker </span><br></pre></td></tr></table></figure>

<p>安装docker-compose</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -L https://github.com/docker/compose/releases/download/1.24.1/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose</span><br><span class="line">chmod +x /usr/local/bin/docker-compose</span><br></pre></td></tr></table></figure>
<p>下载harbor  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://github.com/goharbor/harbor/releases/download/v1.10.2/harbor-online-installer-v1.10.2.tgz</span><br></pre></td></tr></table></figure>


<p>配置harbo.yaml  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hostname: 172.31.48.86 //修改为实际节点IP</span><br></pre></td></tr></table></figure>

<p>屏蔽https配置  </p>
<p>安装harbor </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./install.sh --with-clair</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker-compose  ps</span><br><span class="line">      Name                     Command                  State                 Ports          </span><br><span class="line">---------------------------------------------------------------------------------------------</span><br><span class="line">clair               /docker-entrypoint.sh            Up (healthy)   6060/tcp, 6061/tcp       </span><br><span class="line">harbor-core         /harbor/start.sh                 Up (healthy)                            </span><br><span class="line">harbor-db           /entrypoint.sh postgres          Up (healthy)   5432/tcp                 </span><br><span class="line">harbor-jobservice   /harbor/start.sh                 Up                                      </span><br><span class="line">harbor-log          /bin/sh -c /usr/local/bin/ ...   Up (healthy)   127.0.0.1:1514-&gt;10514/tcp</span><br><span class="line">harbor-portal       nginx -g daemon off;             Up (healthy)   80/tcp                   </span><br><span class="line">nginx               nginx -g daemon off;             Up (healthy)   0.0.0.0:80-&gt;80/tcp       </span><br><span class="line">redis               docker-entrypoint.sh redis ...   Up             6379/tcp                 </span><br><span class="line">registry            /entrypoint.sh /etc/regist ...   Up (healthy)   5000/tcp                 </span><br><span class="line">registryctl         /harbor/start.sh                 Up (healthy)        </span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>访问<a href="http://node_ip/">http://node_ip</a></p>
<p>admin&#x2F;Harbor12345<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/harbor_1.png"></p>
<h4 id="创建测试项目"><a href="#创建测试项目" class="headerlink" title="创建测试项目"></a>创建测试项目</h4><p>spring-petclinic官方示例项目地址：<a href="https://projects.spring.io/spring-petclinic/">https://projects.spring.io/spring-petclinic/</a></p>
<p>本次实践针对Spring官方提供的spring-petclinic示例项目进行容器化部署，该项目采用Spring Boot + Thymeleaf开发，数据库可使用MySQL、H2等，本实践为操作方便直接使用内置的H2数据库。</p>
<p>注意：由于本实践采用的是H2内置数据库，所以每个应用实例的数据独立，也使得应用变成了有状态应用，而生产的最佳实践应该是数据采用外部存储，且应用采用无状态方式部署。</p>
<p>国内clone地址：<a href="https://gitee.com/wanshaoyuan/spring-petclinic.git">https://gitee.com/wanshaoyuan/spring-petclinic.git</a></p>
<p>将此项目clone后上传到私有的gitlab中.</p>
<h3 id="与Gitlab集成"><a href="#与Gitlab集成" class="headerlink" title="与Gitlab集成"></a>与Gitlab集成</h3><h4 id="安装gitlab插件"><a href="#安装gitlab插件" class="headerlink" title="安装gitlab插件"></a>安装gitlab插件</h4><p><img src="https://pic.downk.cc/item/5ebd382ac2a9a83be5766ac4.jpg"></p>
<h4 id="Gitlab中申请AccessToken"><a href="#Gitlab中申请AccessToken" class="headerlink" title="Gitlab中申请AccessToken"></a>Gitlab中申请AccessToken</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-2.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_11.png"></p>
<p><img src="https://pic.downk.cc/item/5ebd3952c2a9a83be57762d6.jpg"></p>
<p>将申请成功的token保存好  </p>
<h4 id="配置Jenkins对接gitlab"><a href="#配置Jenkins对接gitlab" class="headerlink" title="配置Jenkins对接gitlab"></a>配置Jenkins对接gitlab</h4><p><img src="https://pic.downk.cc/item/5ebd39b2c2a9a83be577b003.jpg"><br>添加凭证<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ci_14.png"><br>测试连接</p>
<p><img src="https://pic.downk.cc/item/5ebd3b2dc2a9a83be5792b5e.jpg"></p>
<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><p>读取gitlab中项目spring-petclinic项目中pom.xml文件</p>
<p><img src="https://pic.downk.cc/item/5ebd3ddac2a9a83be57c2e59.jpg"></p>
<p><img src="https://pic.downk.cc/item/5ebd3f39c2a9a83be57dc753.jpg"></p>
<p>配置连接gitlab私有项目的密钥可以用ssh密钥也可以使用账号密码<br><img src="https://pic.downk.cc/item/5ebd3efcc2a9a83be57d86c1.jpg"></p>
<p><img src="https://pic.downk.cc/item/5ebd3f92c2a9a83be57e211a.jpg"><br>分支处修改为main分支<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-4.png"></p>
<p>构建<br><img src="https://pic.downk.cc/item/5ebd3fc1c2a9a83be57e4fc9.jpg"></p>
<p>去cat这个文件输出内容<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-3.png"></p>
<p>执行立即构建<br><img src="https://pic.downk.cc/item/5ebd407fc2a9a83be57f28ba.jpg"></p>
<p>输出结果为实际我们的pom.xml的文件内容  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-5.png"></p>
<h3 id="与Kubernetes集成构建分布式动态编译环境"><a href="#与Kubernetes集成构建分布式动态编译环境" class="headerlink" title="与Kubernetes集成构建分布式动态编译环境"></a>与Kubernetes集成构建分布式动态编译环境</h3><h4 id="安装Kubernetes插件"><a href="#安装Kubernetes插件" class="headerlink" title="安装Kubernetes插件"></a>安装Kubernetes插件</h4><p>Jenkins与Kubernetes集成实现动态Slave Pod，需要安装Kubernetes插件：</p>
<ul>
<li>kubernetes</li>
</ul>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-6.png"></p>
<h4 id="安装Kubernetes-Continuous-Deploy插件"><a href="#安装Kubernetes-Continuous-Deploy插件" class="headerlink" title="安装Kubernetes Continuous Deploy插件"></a>安装Kubernetes Continuous Deploy插件</h4><p>Jenkins访问kubernetes需要依赖于kubeconfig，为支持kubeconfig类型的凭据配置，需要安装Kubernetes Continuous Deploy插件：</p>
<ul>
<li>Kubernetes Continuous Deploy</li>
</ul>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-7.png"></p>
<h4 id="配置Kubernetes集群"><a href="#配置Kubernetes集群" class="headerlink" title="配置Kubernetes集群"></a>配置Kubernetes集群</h4><p>配置 系统管理—&gt;系统设置—&gt;新增一个云<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-8.png"><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-9.png"></p>
<p>配置Jenkins URL，这里可以不配置api-server地址和证书key，连接kubernetes，所以默认会去读取放在JENKINS_HOME的.kube&#x2F;目录的kubeconfig文件，用于连接集群。我这里是通过安装包的方式安装的Jenkins HOME在&#x2F;var&#x2F;lib&#x2F;jenkins&#x2F;目录，如果是通过容器方式启动，将kubeconfig文件直接放~&#x2F;.kube&#x2F;目录。<br>保存到Jenkins主机的config文件中</p>
<p>复制粘贴到Jenkins容器内的~&#x2F;.kube&#x2F;config文件中</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker exec -it jenkins mkdir /root/.kube/</span><br><span class="line">docker cp config  jenkins:/root/.kube/config</span><br></pre></td></tr></table></figure>
<p>注意：<br>此方式Jenkins容器重启后，会将目录重新初始化覆盖掉，kubeconfig文件，生产环境可以直接挂载。</p>
<h4 id="验证Pipeline流水线"><a href="#验证Pipeline流水线" class="headerlink" title="验证Pipeline流水线"></a>验证Pipeline流水线</h4><p>以上Jenkins与Kubernetes的集成配置就基本完成了，下面在正式为Spring Petclinic应用创建Pipeline之前，先简单测试下Jenkins与Kubernetes集成Pipeline流水线是否正常。</p>
<ul>
<li>新建一个流水线类型的任务test-hello-pipeline</li>
</ul>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-10.png"></p>
<ul>
<li>准备流水线测试脚本</li>
</ul>
<figure class="highlight groovy"><table><tr><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">  agent &#123;</span><br><span class="line">    kubernetes &#123;</span><br><span class="line">      cloud <span class="string">&#x27;Kubernetes&#x27;</span></span><br><span class="line">      namespace <span class="string">&#x27;default&#x27;</span></span><br><span class="line">      yaml <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">apiVersion: v1</span></span><br><span class="line"><span class="string">kind: Pod</span></span><br><span class="line"><span class="string">spec:</span></span><br><span class="line"><span class="string">  containers:</span></span><br><span class="line"><span class="string">    - name: busybox</span></span><br><span class="line"><span class="string">      image: busybox</span></span><br><span class="line"><span class="string">      command:</span></span><br><span class="line"><span class="string">      - sleep</span></span><br><span class="line"><span class="string">      args:</span></span><br><span class="line"><span class="string">      - infinity</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  stages &#123;</span><br><span class="line">    stage(<span class="string">&#x27;Test&#x27;</span>) &#123;</span><br><span class="line">      steps &#123;</span><br><span class="line">        container(<span class="string">&#x27;busybox&#x27;</span>) &#123;</span><br><span class="line">            sh <span class="string">&quot;echo &#x27;hello world&#x27;&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>以上是一个简单的声明式pipeline，利用busybox镜像输出<code>hello world</code>字符串。</p>
<ul>
<li>添加流水线脚本</li>
</ul>
<p>把测试脚本添加到任务的流水线脚本框中：<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-13.png"></p>
<ul>
<li>保存流水线，并执行构建</li>
</ul>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-14.png"></p>
<ul>
<li>查看JOB运行结果</li>
</ul>
<p>在Kubernetes中可以看到Jenkins自动创建了Pod来执行任务，任务执行完成以后，Pod自动删除。</p>
<p>Jenkins中查看下构建的控制台输出，正常输出了<code>hello world</code>：</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-12.png"></p>
<p>验证结果表明，Jenkins与Kubernetes配置成功，Pipeline运行正常。</p>
<h3 id="Sonar-Qube对接实现代码质量扫描"><a href="#Sonar-Qube对接实现代码质量扫描" class="headerlink" title="Sonar-Qube对接实现代码质量扫描"></a>Sonar-Qube对接实现代码质量扫描</h3><p><img src="https://docs.sonarqube.org/latest/images/dev-cycle.png"><br><img src="https://docs.sonarqube.org/9.1/images/SQ-instance-components.png"></p>
<h4 id="安装sonarqube"><a href="#安装sonarqube" class="headerlink" title="安装sonarqube"></a>安装sonarqube</h4><p>初始化</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">helm repo add sonarqube https://SonarSource.github.io/helm-chart-sonarqube</span><br><span class="line">helm repo update</span><br><span class="line">kubectl create namespace sonarqube</span><br></pre></td></tr></table></figure>
<p>helm安装sonarqube</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">helm install  sonarqube  --namespace sonarqube  sonarqube/sonarqube --set postgresql.persistence.enabled=false</span><br></pre></td></tr></table></figure>
<p>注意：这里为了快速部署没有设置postgresql的持久化存储，有数据丢失风险，生产环境postgresql建议设计HA或持久化存储。<br>设置为NodePort对外暴露</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl patch svc sonarqube-sonarqube -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&#x27; -n sonarqube </span><br></pre></td></tr></table></figure>
<p>查看NodePort端口</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl  get svc -n sonarqube</span><br><span class="line">NAME                            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">sonarqube-postgresql            ClusterIP   10.110.40.18    &lt;none&gt;        5432/TCP         4m41s</span><br><span class="line">sonarqube-postgresql-headless   ClusterIP   None            &lt;none&gt;        5432/TCP         4m41s</span><br><span class="line">sonarqube-sonarqube             NodePort    10.106.78.100   &lt;none&gt;        9000:30005/TCP   4m41s</span><br></pre></td></tr></table></figure>
<p>查看启动成功</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> kubectl get pod -n sonarqube</span><br><span class="line">NAME                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">sonarqube-postgresql-0   1/1     Running   0          4m7s</span><br><span class="line">sonarqube-sonarqube-0    1/1     Running   0          4m7s</span><br></pre></td></tr></table></figure>

<p>访问节点的30005端口</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-15.png"></p>
<p>默认密码admin&#x2F;admin</p>
<p>如果需要中文直接安装插件就好<br>administrator—-&gt;Marketplace<br>搜索Chinese—-安装<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_sonar_6.png"></p>
<p>生成token<br>申请token<br>administrator—&gt;security—&gt;user—&gt;token<br>保存生成token<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_sonar_8.png"></p>
<h4 id="Jenkins配置"><a href="#Jenkins配置" class="headerlink" title="Jenkins配置"></a>Jenkins配置</h4><p>安装插件<br>系统设置—&gt;插件管理<br>安装SonarQube Scanner for Jenkins</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-16.png"></p>
<p>配置插件<br>配置sonarQube-server<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-17.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-18.png"></p>
<p>Server URL填写sonarqube的地址</p>
<p>Server authentication token填写刚刚创建的token，这里创建一个密钥<br>类型为Secret Text。Secret填写token详细信息，ID为此secret的名称<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-19.png"></p>
<p>配置sonarQUbe-agent<br>系统管理-&gt;全局工具配置——&gt;SonarQube Scanner<br>此处配置为自动安装<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-20.png"></p>
<h4 id="FreeStyle风格任务下配置SonarQube"><a href="#FreeStyle风格任务下配置SonarQube" class="headerlink" title="FreeStyle风格任务下配置SonarQube"></a>FreeStyle风格任务下配置SonarQube</h4><p>以上面的test-gitlab项目的spring-petclinic为例<br>先执行maven构建出class文件，在进行扫描,因为sonarQube扫描的对象是.class而不是.java文件。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-25.png"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -i -v /var/jenkins_home/workspace/:/tmp  maven:3.6-jdk-8 mvn -f /tmp/spring-petclinic/pom.xml clean package -DskipTests</span><br></pre></td></tr></table></figure>
<p>在构建阶段添加”Execute SonarQube Scanner”</p>
<p>输入以下内容</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sonar.projectKey=test</span><br><span class="line">sonar.projectName=test</span><br><span class="line">sonar.projectVersion=1.0</span><br><span class="line">sonar.sources=src</span><br><span class="line">sonar.java.binaries=target/classes</span><br><span class="line">sonar.language=java</span><br><span class="line">sonar.sourceEncoding=UTF-8</span><br></pre></td></tr></table></figure>

<p>注：<br>sonar.projectKey&#x3D;Test #sonar那显示project-key<br>sonar.projectName&#x3D;Test #sonar那显示project名字<br>sonar.projectVersion&#x3D;1.0 ##sonar那显示project版本<br>sonar.sources&#x3D;src #指定要扫描的源码目录。<br>sonar.java.binaries&#x3D;target&#x2F;classes  #指定java文件编译后class文件目录。<br>sonar.language&#x3D;java #只扫描的语言。<br>sonar.sourceEncoding&#x3D;UTF-8 #指定源码的编码格式，一般都会去指定为UTF-8。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-21.png"></p>
<p>执行构建<br>Jenkins处查看<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-22.png"></p>
<p>sonar处查看<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-23.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-24.png"></p>
<h4 id="Jenkins-Pipeline风格任务下配置SonarQube"><a href="#Jenkins-Pipeline风格任务下配置SonarQube" class="headerlink" title="Jenkins-Pipeline风格任务下配置SonarQube"></a>Jenkins-Pipeline风格任务下配置SonarQube</h4><p>使用Pipeline流水线，需要在添加以下步骤</p>
<p>1、在对应的代码库的根目录创建sonar-project.properties  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sonar.projectKey=test2</span><br><span class="line">sonar.projectName=test2</span><br><span class="line">sonar.projectVersion=1.0</span><br><span class="line">sonar.sources=src</span><br><span class="line">sonar.java.binaries=target/classes</span><br><span class="line">sonar.java.source=1.8</span><br><span class="line">sonar.java.target=1.8</span><br><span class="line">sonar.language=java</span><br><span class="line">sonar.sourceEncoding=UTF-8</span><br></pre></td></tr></table></figure>

<p>Pipeline中添加以下步骤</p>
<p>Pipeline中添加以下步骤</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">stage(&#x27;SonarQube analysis&#x27;) &#123;</span><br><span class="line">      steps &#123;</span><br><span class="line">        script &#123;</span><br><span class="line">        def sonarqubeScannerHome = tool name: &#x27;SonarQubeScanner&#x27;</span><br><span class="line">            withSonarQubeEnv(&#x27;sonar&#x27;) &#123;</span><br><span class="line">            sh &quot;$&#123;sonarqubeScannerHome&#125;/bin/sonar-scanner&quot;</span><br><span class="line">         &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>注：<br>1、SonarQubeScanner为全局工具配置中的SonarQube Scanner的配置名称。<br>2、withSonarQubeEnv配置的sonar变量为全局——&gt;系统配置sonar-server的配置名称</p>
<p>清空workspace</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rm -rf /var/jenkins_home/workspace/spring-petclinic</span><br></pre></td></tr></table></figure>
<h3 id="Sonattype-Nexus"><a href="#Sonattype-Nexus" class="headerlink" title="Sonattype-Nexus"></a>Sonattype-Nexus</h3><p>Nexus是开源的制品库，可以用来存储一些代码构建后的制品如jar包，npm包和docker镜像等。也可以将存放制品后的仓库做为私服，供给给后面需要内网编译的软件使用。</p>
<h4 id="部署安装"><a href="#部署安装" class="headerlink" title="部署安装"></a>部署安装</h4><p>软件版本：3.35.0-02<br>本次部署为了更加方便和快捷，采用Docker方式部署<br>创建目录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir /var/nexus-data &amp;&amp; chown -R 200 /var/nexus-data</span><br></pre></td></tr></table></figure>
<p>Docker运行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -d -p 8081:8081 --name nexus -v /var/nexus-data:/nexus-data sonatype/nexus3</span><br></pre></td></tr></table></figure>

<p>初始账号和密码访问<br>账号：admin<br>密码：  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat /var/nexus-data/admin.password</span><br></pre></td></tr></table></figure>
<p>创建仓库<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-45.png"></p>
<p>仓库分为三种类型，proxy、group、hosted。<br>Proxy：Repository是代理仓库，可以配置上游仓库地址，如阿里云仓库地址。当本地仓库不到时，去向配置的上游仓库查找。<br>hosted：供本地使用的本地仓库。<br>group：仓库组，将多个仓库合成一个组，查找jar包时，会按照仓库组中的仓库顺序下载jar包。</p>
<p>这里创建两个名为spring-petclinic-releases、spring-petclinic-snapshots，类型为hotsted的Maven2仓库。</p>
<p>releases库主要用于存储正式版的制品，snapshots存储持续集成过程中产生的制品。  </p>
<p>这里可以根据情况进行修改为release或snapshots<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-46.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-47.png"></p>
<p>maven处配置<br>在spring-petclinic目录下创建conf&#x2F;settings.xml文件用于存放连接Nexus3的凭证信息，正常可以在maven_home或~&#x2F;.m2&#x2F;目录有这文件。因为这里是使用Docker进行构建编译，所以这里直接与业务代码放置在一起。<br>settings.xml文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot;</span><br><span class="line">          xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class="line">          xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;pluginGroups&gt;</span><br><span class="line">  &lt;/pluginGroups&gt;</span><br><span class="line">  &lt;proxies&gt;</span><br><span class="line">  &lt;/proxies&gt;</span><br><span class="line"> &lt;servers&gt;</span><br><span class="line">     &lt;server&gt;</span><br><span class="line">      &lt;id&gt;releases&lt;/id&gt;</span><br><span class="line">      &lt;username&gt;账号&lt;/username&gt;</span><br><span class="line">      &lt;password&gt;密码&lt;/password&gt;</span><br><span class="line">     &lt;/server&gt;</span><br><span class="line">     &lt;server&gt;</span><br><span class="line">      &lt;id&gt;snapshots&lt;/id&gt;</span><br><span class="line">      &lt;username&gt;账号&lt;/username&gt;</span><br><span class="line">      &lt;password&gt;密码&lt;/password&gt;</span><br><span class="line">     &lt;/server&gt;</span><br><span class="line">  &lt;/servers&gt;</span><br><span class="line">&lt;/settings&gt;</span><br></pre></td></tr></table></figure>

<p>关闭https检测，因为Nexus3使用的是http方式对外暴露所以需要关闭maven构建时强行要求https链接</p>
<p><code>src/checkstyle/nohttp-checkstyle.xml</code><br>注释<br><code>&lt;module name=&quot;io.spring.nohttp.checkstyle.check.NoHttpCheck&quot;/&gt;</code><br>注释后：<br><code>&lt;!-- &lt;module name=&quot;io.spring.nohttp.checkstyle.check.NoHttpCheck&quot;/&gt; --&gt;</code></p>
<p>pom.xml文件添加以下内容</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;distributionManagement&gt;</span><br><span class="line">        &lt;repository&gt;</span><br><span class="line">            &lt;!--id的名字可以任意取，但是在setting文件中的属性&lt;server&gt;的ID与这里一致--&gt;</span><br><span class="line">            &lt;id&gt;releases&lt;/id&gt;</span><br><span class="line">            &lt;!--指向仓库类型为host(宿主仓库）的储存类型为Release的仓库--&gt;</span><br><span class="line">            &lt;url&gt;http://172.16.0.195:8081/repository/mspring-petclinic-releases/&lt;/url&gt;</span><br><span class="line">        &lt;/repository&gt;</span><br><span class="line">        &lt;snapshotRepository&gt;</span><br><span class="line">            &lt;id&gt;snapshots&lt;/id&gt;</span><br><span class="line">            &lt;!--指向仓库类型为host(宿主仓库）的储存类型为Snapshot的仓库--&gt;</span><br><span class="line">            &lt;url&gt;http://172.16.0.195:8081/repository/spring-petclinic-snapshots/&lt;/url&gt;</span><br><span class="line">        &lt;/snapshotRepository&gt;</span><br><span class="line">  &lt;/distributionManagement&gt;</span><br></pre></td></tr></table></figure>


<p>执行编译</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -i -v /root/spring-petclinic/:/tmp  maven:3.6-jdk-8 mvn -f /tmp/pom.xml --settings /tmp/conf/settings.xml clean deploy </span><br></pre></td></tr></table></figure>

<p>编译完成上传成功后<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-48.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-49.png"></p>
<p>在spring-petclinic-snapshots仓库内可见上传来的jar包，对应的jar包后面也接上了对应的时间戳，方便进行分类。  </p>
<p>如果要上传到release仓库，将pom.xml中的<code>&lt;version&gt;2.5.0-SNAPSHOT&lt;/version&gt;</code>中的<code>-SNAPSHOT</code>字段删除就表示为正式版。  </p>
<h3 id="ArgoCD集成实现CD端对接"><a href="#ArgoCD集成实现CD端对接" class="headerlink" title="ArgoCD集成实现CD端对接"></a>ArgoCD集成实现CD端对接</h3><h4 id="编写并上传部署spring-petclinic的yaml文件和Dockerfile文件"><a href="#编写并上传部署spring-petclinic的yaml文件和Dockerfile文件" class="headerlink" title="编写并上传部署spring-petclinic的yaml文件和Dockerfile文件"></a>编写并上传部署spring-petclinic的yaml文件和Dockerfile文件</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: spring-petclinic-0-0-1</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: spring-petclinic</span><br><span class="line">      version: 0.0.1</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: spring-petclinic</span><br><span class="line">        version: 0.0.1</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: spring-petclinic</span><br><span class="line">        image: registry.cn-shenzhen.aliyuncs.com/yedward/spring-petclinic:0.0.1</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            memory: 2Gi</span><br><span class="line">            cpu: 1</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">        livenessProbe:</span><br><span class="line">          failureThreshold: 3</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /actuator/health/liveness</span><br><span class="line">            port: 8080</span><br><span class="line">            scheme: HTTP</span><br><span class="line">          initialDelaySeconds: 30</span><br><span class="line">          periodSeconds: 5</span><br><span class="line">          successThreshold: 1</span><br><span class="line">          timeoutSeconds: 2</span><br><span class="line">        readinessProbe:</span><br><span class="line">          failureThreshold: 3</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /actuator/health/readiness</span><br><span class="line">            port: 8080</span><br><span class="line">            scheme: HTTP</span><br><span class="line">          initialDelaySeconds: 30</span><br><span class="line">          periodSeconds: 5</span><br><span class="line">          successThreshold: 2</span><br><span class="line">          timeoutSeconds: 2</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: spring-petclinic-svc-0-0-1</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: spring-petclinic</span><br><span class="line">    version: 0.0.1</span><br><span class="line">  ports:</span><br><span class="line">  - port: 8080</span><br><span class="line">    targetPort: 8080</span><br><span class="line">  type: NodePort</span><br></pre></td></tr></table></figure>
<p>将yaml中的镜像地址改为实际的镜像仓库地址和项目名称。<br>Dockerfile</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM registry.cn-shenzhen.aliyuncs.com/yedward/openjdk:8-jre-slim</span><br><span class="line"># 企业实际场景中应该通过USER指定以非root用户运行</span><br><span class="line">USER appuser</span><br><span class="line">EXPOSE 8080</span><br><span class="line">COPY target/*.jar /app/</span><br><span class="line">WORKDIR /app</span><br><span class="line">CMD java -jar -Xms1024m -Xmx1024m /app/spring-petclinic.jar</span><br></pre></td></tr></table></figure>
<p>上传到gitlab中的spring-petclinic项目中</p>
<h4 id="部署ArgoCD"><a href="#部署ArgoCD" class="headerlink" title="部署ArgoCD"></a>部署ArgoCD</h4><p>单节点部署<br>使用官网快速部署</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create namespace argocd</span><br><span class="line">kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml</span><br></pre></td></tr></table></figure>
<p>部署完后产生以下服务</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">NAME                                      READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/argocd-application-controller-0       1/1     Running   0          5d6h</span><br><span class="line">pod/argocd-dex-server-74588646d-sz9g8     1/1     Running   0          2d2h</span><br><span class="line">pod/argocd-redis-5ccdd9d4fd-csthm         1/1     Running   1          5d6h</span><br><span class="line">pod/argocd-repo-server-5bbb8bdf78-mxkv7   1/1     Running   0          18h</span><br><span class="line">pod/argocd-server-789fb45964-82mzx        1/1     Running   0          18h</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NAME                            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE</span><br><span class="line">service/argocd-dex-server       ClusterIP   10.43.180.172   &lt;none&gt;        5556/TCP,5557/TCP,5558/TCP   5d6h</span><br><span class="line">service/argocd-metrics          ClusterIP   10.43.184.97    &lt;none&gt;        8082/TCP                     5d6h</span><br><span class="line">service/argocd-redis            ClusterIP   10.43.4.233     &lt;none&gt;        6379/TCP                     5d6h</span><br><span class="line">service/argocd-repo-server      ClusterIP   10.43.9.45      &lt;none&gt;        8081/TCP,8084/TCP            5d6h</span><br><span class="line">service/argocd-server           NodePort    10.43.48.239    &lt;none&gt;        80:31320/TCP,443:31203/TCP   5d6h</span><br><span class="line">service/argocd-server-metrics   ClusterIP   10.43.149.186   &lt;none&gt;        8083/TCP                     5d6h</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/argocd-dex-server    1/1     1            1           5d6h</span><br><span class="line">deployment.apps/argocd-redis         1/1     1            1           5d6h</span><br><span class="line">deployment.apps/argocd-repo-server   1/1     1            1           5d6h</span><br><span class="line">deployment.apps/argocd-server        1/1     1            1           5d6h</span><br><span class="line"></span><br><span class="line">NAME                                            DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/argocd-dex-server-74588646d     1         1         1       5d6h</span><br><span class="line">replicaset.apps/argocd-redis-5ccdd9d4fd         1         1         1       5d6h</span><br><span class="line">replicaset.apps/argocd-repo-server-5bbb8bdf78   1         1         1       5d6h</span><br><span class="line">replicaset.apps/argocd-server-789fb45964        1         1         1       5d6h</span><br><span class="line"></span><br><span class="line">NAME                                             READY   AGE</span><br><span class="line">statefulset.apps/argocd-application-controller   1/1     5d6h</span><br></pre></td></tr></table></figure>
<p>使用NodePort方式为对外暴露</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl patch svc argocd-server -n argocd -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&#x27;</span><br></pre></td></tr></table></figure>
<p>访问dashboard<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_1.png"></p>
<p>默认帐号为admin，密码通过secret获取</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=&quot;&#123;.data.password&#125;&quot; | base64 -d</span><br></pre></td></tr></table></figure>

<h4 id="配置ArgoCD"><a href="#配置ArgoCD" class="headerlink" title="配置ArgoCD"></a>配置ArgoCD</h4><p>配置对接gitlab<br>setting——&gt;Repositories-&gt;Connect repo using HTTPS<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-26.png"></p>
<p>如果对应的git是私有库，pull需要帐号密码则需要在argo设置中配置repo connect</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_10.png"></p>
<p>填写对应的帐号密码，如果是自签名证书需要将CA附上<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/argocd_11.png"></p>
<p>创建Project<br>setting——&gt;Projects<br>项目是argocd中的管理对象，也与之对应的发布权限相关联。<br>创建项目，并配置DESTINATIONS，能够发布到哪些集群和命名空间<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-28.png"></p>
<p>创建Application<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-29.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-30.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-31.png"></p>
<p>创建完后<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-32.png"></p>
<p>点击sync会自动将yaml文件部署到k8s集群中。<br>可以在Kubernetes集群中查看到</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod </span><br><span class="line">NAME                                      READY   STATUS    RESTARTS   AGE</span><br><span class="line">spring-petclinic-0-0-1-6695b96956-xx9nw   1/1     Running   0          28m</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-36.png"></p>
<h4 id="Harbor中创建对应项目"><a href="#Harbor中创建对应项目" class="headerlink" title="Harbor中创建对应项目"></a>Harbor中创建对应项目</h4><p>在harbor中创建spring-petclinic项目<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-33.png"></p>
<h4 id="gitlab-Webhook配置"><a href="#gitlab-Webhook配置" class="headerlink" title="gitlab Webhook配置"></a>gitlab Webhook配置</h4><p>当前Jenkins进行CI构建还是基于手动点击运行，可以配置基于gitlab的触发事件进行调用，如push、merge、tag push等事件触发回调Jenkins自动执行CI<br>jenkins处打开项目触发器<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-35.png"><br>生成连接Secret token保存下来<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-41.png"><br>Gitlab配置：<br>root登录后，需要开放安全配置，允许本地local网络连接<br>在menu选择admin——&gt;settings-——&gt;Network——&gt;Outbound requests</p>
<p>勾选</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Allow requests to the local network from web hooks and services  </span><br><span class="line">Allow requests to the local network from system hooks</span><br></pre></td></tr></table></figure>
<p>将Jenkins的ip添加到白名单中，保存。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-42.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-40.png"></p>
<p>项目——&gt;setting——&gt;webhooks<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-43.png"></p>
<p>填写Jenkins对应的回调地址和token</p>
<p>点击Test settings即可在Jenkins处看见已经开始的构建任务。</p>
<p>保存配置<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-44.png"></p>
<p>test这里选择基于事件回调。查看Jenkins处是否开始自动执行任务。</p>
<h4 id="Jenkins配置-1"><a href="#Jenkins配置-1" class="headerlink" title="Jenkins配置"></a>Jenkins配置</h4><p>Argo是检查到yaml文件变化会进行自动发布到k8s中，那么我们只需要在Jenkins中增加修改和上传yaml阶段即可。</p>
<p>完整的构建阶段shell<br>编译阶段shell</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -i -v /var/jenkins_home/workspace/:/tmp  maven:3.6-jdk-8 mvn -f /tmp/spring-petclinic/pom.xml clean package -DskipTests</span><br></pre></td></tr></table></figure>

<p>代码扫描阶段</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sonar.projectKey=test</span><br><span class="line">sonar.projectName=test</span><br><span class="line">sonar.projectVersion=1.0</span><br><span class="line">sonar.sources=src</span><br><span class="line">sonar.java.binaries=target/classes</span><br><span class="line">sonar.language=java</span><br><span class="line">sonar.sourceEncoding=UTF-8</span><br></pre></td></tr></table></figure>
<p>镜像构建阶段</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker login -u useradmin -p password harbor_ip</span><br><span class="line">docker build -t harbor_ip/spring-petclinic/spring-petclinic:$BUILD_NUMBER .</span><br><span class="line">docker push harbor_ip/spring-petclinic/spring-petclinic:$BUILD_NUMBER </span><br></pre></td></tr></table></figure>
<p>注：<br>1、这里使用Jenkins内置BUILD_NUMBER号为镜像tag，跟Jenkins的CI号是匹配的。<br>2、将上传镜像的账号密码修改为实际的账号密码。 </p>
<p>发布更新部署yaml阶段</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone http://username:password@1.13.173.7/root/spring-petclinic.git</span><br><span class="line">git config --global user.email &quot;root@example.com&quot;</span><br><span class="line">git config --global user.name &quot;root&quot;</span><br><span class="line">git remote set-url origin http://username:password@1.13.173.7/root/spring-petclinic.git</span><br><span class="line">sed -i &quot;s/spring-petclinic:.*/spring-petclinic:$BUILD_NUMBER/g&quot; spring-petclinic/deployment.yaml</span><br><span class="line">cd spring-petclinic/</span><br><span class="line">git add deployment.yaml</span><br><span class="line">git commit -m &quot;update yaml&quot;</span><br><span class="line">git push origin main</span><br></pre></td></tr></table></figure>

<p>在配置一个构建后删除操作，避免构建后缓存影响下次构建<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-37.png"></p>
<p>执行构建，构建成功后查看对应的k8s中的部署的业务镜像版本号是否与实际应用部署的环境变量相同。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-38.png"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl describe pod/spring-petclinic-0-0-1-699954b589-h7n58</span><br><span class="line"></span><br><span class="line">Events:</span><br><span class="line">  Type    Reason     Age   From               Message</span><br><span class="line">  ----    ------     ----  ----               -------</span><br><span class="line">  Normal  Scheduled  57m   default-scheduler  Successfully assigned default/spring-petclinic-0-0-1-7969df6996-dn2cc to rke-node2</span><br><span class="line">  Normal  Pulled     57m   kubelet            Container image &quot;1.13.173.7:8080/spring-petclinic/spring-petclinic:15&quot; already present on machine</span><br><span class="line">  Normal  Created    57m   kubelet            Created container spring-petclinic</span><br><span class="line">  Normal  Started    57m   kubelet            Started container spring-petclinic</span><br></pre></td></tr></table></figure>


<p>访问节点ip+spring-petclinic服务暴露出来的NodePort端口<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins2-39.png"></p>
<p>这是一个宠物医院的管理系统，可以通过此页面进行宠物管理。</p>
<h3 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h3><p>Jenkins内置环境变量<br>直接访问${YOUR_JENKINS_HOST}&#x2F;env-vars.html即可</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">BUILD_NUMBER， 唯一标识一次build，例如23；</span><br><span class="line">BUILD_ID，基本上等同于BUILD_NUMBER，但是是字符串，例如2011-11-15_16-06-21；</span><br><span class="line">JOB_NAME， job的名字，例如JavaHelloWorld；</span><br><span class="line">BUILD_TAG，作用同BUILD_ID,BUILD_NUMBER,用来全局地唯一标识一此build，例如jenkins-JavaHelloWorld-23；</span><br><span class="line">EXECUTOR_NUMBER， 例如0；</span><br><span class="line">NODE_NAME，slave的名字，例如MyServer01；</span><br><span class="line">NODE_LABELS，slave的label，标识slave的用处，例如JavaHelloWorld MyServer01；</span><br><span class="line">JAVA_HOME， java的home目录，例如C:\Program Files (x86)\Java\jdk1.7.0_01；</span><br><span class="line">WORKSPACE，job的当前工作目录，例如c:\jenkins\workspace\JavaHelloWorld；</span><br><span class="line">HUDSON_URL = JENKINS_URL， jenkins的url，例如http://localhost:8000/ ；</span><br><span class="line">BUILD_URL，build的url 例如http://localhost:8000/job/JavaHelloWorld/23/；</span><br><span class="line">JOB_URL， job的url，例如http://localhost:8000/job/JavaHelloWorld/；</span><br><span class="line">SVN_REVISION，svn 的revison， 例如4；</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>CI/CD</category>
      </categories>
      <tags>
        <tag>CI/CD</tag>
      </tags>
  </entry>
  <entry>
    <title>Jenkins对接kubernetes实现slave动态创建</title>
    <url>/2018/08/25/jenkins_dymaic_slave/</url>
    <content><![CDATA[<h3 id="概述："><a href="#概述：" class="headerlink" title="概述："></a>概述：</h3><p>单个Jenkins很难满足生产环境中多个项目同时编译的需求，容易造成队列阻塞，所以这个时候需要通过Jenkins-slave的方式将编译分担到slave上，但传统的直接通过几台服务器专门做slave，浪费资源，还容易造成编译环境的不一致导致编译失败。后面有Jenkins有推出过基于docker的Jenkins-slave，虽然可以解决上述问题，但所以在这里我们可以充分利用kubernetes的功能，将Jenkins-slave以pod的方式运行在kubernetes集群内，将任务分发给这些Jenkins-slave pod，然后执行任务，任务执行完以后，在自动释放掉这些pod，能够很好的利用资源。</p>
<p>版本：<br>os：ubuntu16.04<br>kubernetes：1.10.5<br>Jenkins：2.89.4  （通过deb包方式安装)</p>
<h3 id="配置方法"><a href="#配置方法" class="headerlink" title="配置方法"></a>配置方法</h3><p>Jenkins安装kubernetes插件<br>系统管理—&gt;管理插件—&gt;搜索Kubernetes plugin安装</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_dymaic_slave_1.png"><br>配置<br>系统管理—&gt;系统设置—&gt;新增一个云</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_dymaic_slave_2.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_dymaic_slave_3.png"></p>
<p>配置Jenkins URL，这里我们没有配置api-server地址和证书key，连接kubernetes，所以默认会去读取放在JENKINS_HOME的.kube&#x2F;目录的kubeconfig文件，用于连接集群。我这里是通过安装包的方式安装的Jenkins HOME在&#x2F;var&#x2F;lib&#x2F;jenkins&#x2F;目录，如果是通过容器方式启动，将kubeconfig文件直接放~&#x2F;.kube&#x2F;目录。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_dymaic_slave_4.png"><br>配置pod template<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_dymaic_slave_5.png"></p>
<p>名字可以随便写，注意这个namespace和labels，需要规划好namespace默认不写就是default labels是用于后面编译的项目调用这个kubernetes pod template时用的。</p>
<p>配置container template<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_dymaic_slave_6.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_dymaic_slave_7.png"></p>
<p>注意这个pod的名字叫jnlp，用来覆盖默认容器，如果改成其他名字的话，那这个pod会启动两个container，都去连接master<br>配置Arguments to pass to the command的用处是定义多个Jenkins-agent container时通过${computer.jnlpmac} ${computer.name}，做为jnlp的启动参数，用来区分不同container。<br>配置cpu、memory资源request，因为可使用资源太低的节点上运行jnlp容易出错。</p>
<p>保存配置<br>打开Jenkins-slave连接master的端口，这里定义为50000<br>系统管理—&gt;全局安全配置—&gt;agents<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_dymaic_slave_8.png"></p>
<p>验证<br>创建个freestyle<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_dymaic_slave_9.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_dymaic_slave_10.png"></p>
<p>特别需要注意这个label Expression，要根我们刚刚定义的kubernetes-template配置的一样，这样做的意义是什么，因为在实际使用环境中，不同的项目由不同的开发语言构成，也需要不同的编译环境，所以会配置多种语言环境的jnlp镜像，通过label去调用合适的kubernetes-template</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_dymaic_slave_11.png"></p>
<p>这里我们配置一个最简单的shell，输出一串字符。<br>点击立刻构建<br>在kubernetes集群内就看见这个jnlp-slave已经被创建出来了。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_dymaic_slave_12.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_dymaic_slave_13.png"></p>
<p>查看本次编译结果，编译成功<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_dymaic_slave_14.png"></p>
<p>pipline用法</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">podTemplate(label: &#x27;jnlp-slave&#x27;, cloud: &#x27;kubernetes&#x27;) &#123;</span><br><span class="line">    node(&#x27;jnlp-slave&#x27;) &#123;</span><br><span class="line">        stage(&#x27;Run shell&#x27;) &#123;</span><br><span class="line">            sh &#x27;echo hello world&#x27;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>总结：<br>通过jenkins-kubernetes-plugin能实现对Jenkins slave的弹性部署和回收，能非常有效的利用资源，在实际使用的过程中，我们会根据不同的编程去构建一个对应的jnlp，或在一个jnlp镜像内包含全部编程语言。</p>
<p>参考链接<br><a href="https://github.com/jenkinsci/docker-jnlp-slave">https://github.com/jenkinsci/docker-jnlp-slave</a><br><a href="https://github.com/jenkinsci/kubernetes-plugin/blob/master/README.md">https://github.com/jenkinsci/kubernetes-plugin/blob/master/README.md</a></p>
]]></content>
      <categories>
        <category>CI/CD</category>
      </categories>
      <tags>
        <tag>CI/CD</tag>
      </tags>
  </entry>
  <entry>
    <title>Jenkins-pipeline讲解和使用</title>
    <url>/2021/10/14/jenkins_pipeline/</url>
    <content><![CDATA[<h3 id="什么是Jenkins-Pipeline"><a href="#什么是Jenkins-Pipeline" class="headerlink" title="什么是Jenkins-Pipeline"></a>什么是Jenkins-Pipeline</h3><p>Pipeline是一套运行在Jenkins上的工作流框架，2.X版本Jenkins的核心功能，主要是将一个大的工作流分拆成多个独立的功能模块，实现单个任务难以完成的复杂流程编排和可视化。<br>Jenkins Pipeline也是实现CICD As file的一个重要工具，将Pipeline编写成Jenkinsfile与业务代码一起存放。</p>
<p>Pipeline支持两种语法：<br>1、声明式语法</p>
<p>Jenkins新加入的语法规则在Jenkinsfile固定的关键字之内，所采用的语法风格大多与shell类似，这种风格更加符合日常的阅读习惯，也更简单，以后我都将采用这种方式进行介绍以及深入。</p>
<p>2、脚本式语法<br>不是shell脚本形式，而是基于Groovy语言的语法风格，学习成本相对较高</p>
<p>建议直接使用声明式语法清晰简单明了，合适大部分人入门</p>
<h3 id="Pipeline和FreeStyle对比"><a href="#Pipeline和FreeStyle对比" class="headerlink" title="Pipeline和FreeStyle对比"></a>Pipeline和FreeStyle对比</h3><p>| |  灵活方式 | 显示形式  |<br>|—|—|—|—|<br>| FreeStyle  | 图形化操作，合适入门操作，后期流程多后，不易于快速快速构建  | 只有统一日志展示，没有完整阶段流程信息展示  |   |<br>| Pipeline | 结构化代码语法，易于阅读和管理，可以实现CICD as Code  |  阶段流程信息展示清晰，每个阶段构建时间和对应的构建日志清晰可读 |   |</p>
<h3 id="Jenkins-Pipline语法介绍"><a href="#Jenkins-Pipline语法介绍" class="headerlink" title="Jenkins-Pipline语法介绍"></a>Jenkins-Pipline语法介绍</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins3-11.png"></p>
<p><img src="http://t.eryajf.net/imgs/2021/09/f126d7d667e038cd.jpg"><br>图片来源:<a href="https://wiki.eryajf.net/pages/3298.html#_1-%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D%E3%80%82">https://wiki.eryajf.net/pages/3298.html#_1-%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D%E3%80%82</a></p>
<ul>
<li>Aagent: 一个 Aagent 就是一个 Jenkins执行 Step 的具体运行环境。</li>
<li>Stage: 表示Pipeline的一个阶段，如clone code阶段，Build阶段。一个Pipeline中至少需要一个Stage。</li>
<li>Step: 表示实际的执行步骤，小到执行一个 Shell 脚本，大到构建一个 Docker 镜像，由各类 Jenkins Plugin 提供，当插件扩展Pipeline DSL 时，通常意味着插件已经实现了一个新的步骤，在Stage下有且只能有一个step。</li>
</ul>
<h4 id="environment"><a href="#environment" class="headerlink" title="environment"></a>environment</h4><p>环境变量，可以定义在全局变量或者步骤中的局部变量，取决于所定义位置。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">   agent any</span><br><span class="line">   environment &#123;</span><br><span class="line">       DISABLE_AUTH = &#x27;true&#x27;               </span><br><span class="line">   &#125;</span><br><span class="line">   stages &#123;</span><br><span class="line">       stage(“Build”) &#123;</span><br><span class="line">           steps &#123;</span><br><span class="line">               echo env.DISABLE_AUTH</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行结果，会输出对应的环境变量。</p>
<h4 id="options"><a href="#options" class="headerlink" title="options"></a>options</h4><p>选项，定义流水线运行时的配置选项。如历史构建记录数量保留，超时时间等操作。以下例子定义重试次数.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">    agent any</span><br><span class="line">    options &#123;</span><br><span class="line">        retry(3)</span><br><span class="line">    &#125;</span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(&#x27;Example&#x27;) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">               sh &quot;dwdwe&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写一个无法执行的命令，可以通过Console Output看见会Retrying 3次才停止。  </p>
<h4 id="parameters"><a href="#parameters" class="headerlink" title="parameters"></a>parameters</h4><p>参数，为流水线运行时设置相关的参数，不需要在UI界面上额外定义。<br>出现在Pipeline块内，并且只有一次。<br>常用参数：<br>string：字符串类型的参数，例如： <code>parameters &#123; string(name: &#39;DEPLOY_ENV&#39;, defaultValue: &#39;staging&#39;, description: &#39;&#39;) &#125;</code><br>文本: 一个text参数，可以包含多行，例如： <code>parameters &#123; text(name: &#39;DEPLOY_TEXT&#39;, defaultValue: &#39;One\nTwo\nThree\n&#39;, description: &#39;&#39;) &#125;</code><br>booleanParam: 一个布尔参数，例如： <code>parameters &#123; booleanParam(name: &#39;DEBUG_BUILD&#39;, defaultValue: true, description: &#39;&#39;) &#125;</code><br>choice： 选择参数，<code>例如： parameters &#123; choice(name: &#39;CHOICES&#39;, choices: [&#39;one&#39;, &#39;two&#39;, &#39;three&#39;], description: &#39;&#39;) &#125;</code></p>
<p>password: 在 Jenkins 参数化构建 UI 提供一个暗文密码输入框，所有需要脱敏的信息，都可以通过这个参数来配置。<br><code> parameters &#123; password(name: &#39;PASSWORD&#39;, defaultValue: &#39;SECRET&#39;, description: &#39;A secret password&#39;) &#125;</code></p>
<p>注意：<br>这种声明定义之后，需要手动构建一次，然后才会自动落位到配置好的参数化构建中了。<br>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">    agent any</span><br><span class="line">    parameters &#123;</span><br><span class="line">        string(name: &#x27;PERSON&#x27;, defaultValue: &#x27;Mr Jenkins&#x27;, description: &#x27;Who should I say hello to?&#x27;)</span><br><span class="line"></span><br><span class="line">        text(name: &#x27;BIOGRAPHY&#x27;, defaultValue: &#x27;&#x27;, description: &#x27;Enter some information about the person&#x27;)</span><br><span class="line"></span><br><span class="line">        booleanParam(name: &#x27;TOGGLE&#x27;, defaultValue: true, description: &#x27;Toggle this value&#x27;)</span><br><span class="line"></span><br><span class="line">        choice(name: &#x27;CHOICE&#x27;, choices: [&#x27;One&#x27;, &#x27;Two&#x27;, &#x27;Three&#x27;], description: &#x27;Pick something&#x27;)</span><br><span class="line"></span><br><span class="line">        password(name: &#x27;PASSWORD&#x27;, defaultValue: &#x27;SECRET&#x27;, description: &#x27;Enter a password&#x27;)</span><br><span class="line">    &#125;</span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(&#x27;Example&#x27;) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">                echo &quot;Hello $&#123;params.PERSON&#125;&quot;</span><br><span class="line"></span><br><span class="line">                echo &quot;Biography: $&#123;params.BIOGRAPHY&#125;&quot;</span><br><span class="line"></span><br><span class="line">                echo &quot;Toggle: $&#123;params.TOGGLE&#125;&quot;</span><br><span class="line"></span><br><span class="line">                echo &quot;Choice: $&#123;params.CHOICE&#125;&quot;</span><br><span class="line"></span><br><span class="line">                echo &quot;Password: $&#123;params.PASSWORD&#125;&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在执行Build时会多出这些选项<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins3-12.png"></p>
<h4 id="post"><a href="#post" class="headerlink" title="post"></a>post</h4><p>运行后处理，当流水线完成后根据配置的条件做一些动作，如：构建失败后邮件通知。<br>条件：<br>always：无论怎么样总是执行。<br>changed: 当前Pipeline状态与先前不一致情况下执行。<br>failuer: 失败情况下。<br>success: 成功情况下。<br>unstable: 不稳定情况下，Pipeline状态标识为黄色。<br>aborted: Pipeline中止情况下。<br>cleanup: 无论怎么样，执行目录清理</p>
<p>例如：<br>无论怎么样，将执行情况邮件发送。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">post&#123;</span><br><span class="line">         always&#123;</span><br><span class="line">         mail to : ‘team@example.com’,</span><br><span class="line">                   subject:”Pipeline statue:$&#123;currentBuild.fullDisplayName&#125;”,</span><br><span class="line">                   body:”The execution result$&#123;env.Build_url&#125;”</span><br><span class="line"></span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>定义三种执行状态，此时shell命令无法执行，输出always和success输出信息。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">    agent any</span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(&#x27;Example&#x27;) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">               sh  &#x27;dwd&#x27;</span><br><span class="line">            &#125;</span><br><span class="line">        </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    post &#123; </span><br><span class="line">        always &#123; </span><br><span class="line">            echo &#x27;already exec&#x27;</span><br><span class="line">        &#125;</span><br><span class="line">        success &#123;</span><br><span class="line">            echo &#x27;exec success &#x27;</span><br><span class="line">        &#125;</span><br><span class="line">        failure &#123;</span><br><span class="line">            echo &#x27;exec failure&#x27;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h4 id="tool"><a href="#tool" class="headerlink" title="tool"></a>tool</h4><p>构建工具，获取通过自动安装或手动安装工具的环境变量，支持maven、jdk、gradle，工具的名称必须预先在Jenkins的系统设置-&gt;全局工具配置中定义。<br>例如在Jenkins—&gt;Global Tool Configuration中添加工具对应的环境变量，然后在项目中引用。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">    agent any</span><br><span class="line">    tools &#123;</span><br><span class="line">        maven &#x27;maven&#x27; </span><br><span class="line">    &#125;</span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(&#x27;Example&#x27;) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">                sh &#x27;mvn --version&#x27;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="input"><a href="#input" class="headerlink" title="input"></a>input</h4><p>交互输入，在流水线执行各个阶段的时候，由人工确认是否继续执行。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pipeline&#123;</span><br><span class="line">    agent any</span><br><span class="line">    environment&#123;</span><br><span class="line">    approvalMap = &#x27;&#x27;</span><br><span class="line">    &#125;</span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(&#x27;pre deploy&#x27;)&#123;</span><br><span class="line">            steps&#123;</span><br><span class="line">                script&#123;</span><br><span class="line">                    approvalMap = input (</span><br><span class="line">                        message: &#x27;发布到哪个环境？&#x27;,</span><br><span class="line">                        ok: &#x27;确定&#x27;,</span><br><span class="line">                        parameters:[</span><br><span class="line">                            choice(name: &#x27;ENV&#x27;,choices: &#x27;test\npre\nprod&#x27;,description: &#x27;发布到什么环境？&#x27;),</span><br><span class="line">                            string(name: &#x27;username&#x27;,defaultValue: &#x27;&#x27;,description: &#x27;输入用户名&#x27;)</span><br><span class="line">                        ],</span><br><span class="line">                        submitter: &#x27;admin&#x27;,</span><br><span class="line">                    )</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        stage(&#x27;deploy&#x27;)&#123;</span><br><span class="line">            steps&#123;</span><br><span class="line">                echo &quot;操作者是 $&#123;approvalMap[&#x27;username&#x27;]&#125;&quot;</span><br><span class="line">                echo &quot;发布到什么环境 $&#123;approvalMap[&#x27;ENV&#x27;]&#125;&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>点击构建，程序将会在input的步骤停住，等待用户进行相应输入和选择。<br>message ：必填。页面展示信息</p>
<p>ok：input表单上“ok”按钮的可选文本。</p>
<p>submitter：允许提交此input选项的用户或外部组名列表，用逗号分隔。默认允许任何用户。</p>
<p>例子结果如图所示：<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins3-13.png"></p>
<h4 id="when"><a href="#when" class="headerlink" title="when"></a>when</h4><p>条件判断，允许流水线根据给定的条件决定是否应该执行阶段，when 指令必须包含至少一个条件.<br>内置条件<br>branch：分支匹配，如<code>when &#123; branch &#39;release-v2.5&#39; &#125;</code><br>environment：环境变量匹配如<code>when &#123; environment name: &#39;DEPLOY_TO&#39;, value: &#39;production&#39; &#125;</code>    </p>
<p>not：当嵌套条件为false时执行stage。必须包含至少一个条件例如：<code>when &#123; not &#123; branch &#39;master&#39; &#125; &#125;</code><br>allOf：当所有嵌套条件都为真时，执行舞台。必须包含至少一个条件。例如：<code>when &#123; allOf &#123; branch &#39;master&#39;; environment name: &#39;DEPLOY_TO&#39;, value: &#39;production&#39; &#125; &#125;</code><br>anyOf: 当至少一个嵌套条件为真时执行。必须至少包含一个条件。例如：<code>when &#123; anyOf &#123; branch &#39;master&#39;; branch &#39;staging&#39; &#125; &#125;</code></p>
<p>以上例子支持通配符配置，如<code>when &#123; branch &#39;release-v2.*&#39; &#125;</code><br>例子:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">    agent any</span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(&#x27;Example&#x27;) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">               git branch: &#x27;main&#x27;, url: &#x27;https://gitee.com/wanshaoyuan/spring-petclinic.git&#x27;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        stage(&#x27;deploy to proc&#x27;)&#123;</span><br><span class="line">           when &#123;</span><br><span class="line">             branch &#x27;main&#x27;</span><br><span class="line">           &#125;</span><br><span class="line">            steps&#123;</span><br><span class="line">              echo &quot;deploy to proc env&quot;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>执行后因为是clone main分支，将输出deploy to proc env。</p>
<h4 id="parallel"><a href="#parallel" class="headerlink" title="parallel"></a>parallel</h4><p>默认Pipeline是串行，可以通过parallel配置并行构建，阶段可以在他们内部声明多嵌套阶段, 它们将并行执行，一个阶段只能有一个 steps 或 parallel的阶段。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">    agent any</span><br><span class="line"></span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(&#x27;one&#x27;) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">               echo &quot;stage1&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        stage(&#x27;two&#x27;) &#123;</span><br><span class="line">            failFast true</span><br><span class="line">            parallel &#123;</span><br><span class="line">                stage(&#x27;并行1&#x27;) &#123;</span><br><span class="line">                  steps &#123;</span><br><span class="line">                    echo &quot;并行一&quot;</span><br><span class="line">                  &#125;</span><br><span class="line">            &#125;</span><br><span class="line">                stage(&#x27;并行2&#x27;) &#123;</span><br><span class="line">                  steps &#123;</span><br><span class="line">                    echo &quot;并行二&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注:添加failFast true到包含parallel的stage中，其中一个失败时中止所有parallel内的stage。<br>本阶段会执行多个步骤。<br> 通过BlueOcean查看Pipeline效果图<br> <img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins3-15.png"></p>
<h4 id="script"><a href="#script" class="headerlink" title="script"></a>script</h4><p>脚本标签，需要执行一些系统命令语法。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">    agent any</span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(&#x27;Example&#x27;) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">               sh &#x27;date&#x27;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过sh执行对应的shell命令。  </p>
<h4 id="trigger"><a href="#trigger" class="headerlink" title="trigger"></a>trigger</h4><p>触发器，设置构建触发器，比如根据周期计划定时构建。<br>cron定时构建<br>根Linux内crontab对应的，分、时、日、月、周。这里例子定义每分钟执行一次。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">    agent any</span><br><span class="line">    triggers&#123;</span><br><span class="line">      cron(&#x27;* * * * 1&#x27;)</span><br><span class="line">    &#125;</span><br><span class="line">   </span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(&#x27;cat文件&#x27;)&#123;</span><br><span class="line">            steps&#123;</span><br><span class="line">                sh &#x27;&#x27;&#x27;cat README.md&#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">     &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>注意点：<br>H关键字为Hash，表示当前设置的时间跨度范围内随机一值例如<br><code>triggers&#123; cron(&#39;H/15 * * * *&#39;) &#125;</code><br>每15分钟执行一次，可能在 :07,:22,:37,:52执行</p>
<h4 id="withCredentials"><a href="#withCredentials" class="headerlink" title="withCredentials"></a>withCredentials</h4><p>将secret与变量对应起来。在jenkins中创建的密钥，在Pipeline中希望通过变量方式引用，可以通过withCredentials进行。</p>
<p>例子<br>在全局凭据中创建个Username with password的凭证，输入用户名和密码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">    agent any</span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(&#x27;部署到测试环境&#x27;)&#123;</span><br><span class="line">            steps&#123;</span><br><span class="line">                 withCredentials([usernamePassword(credentialsId: &#x27;harbor_account&#x27;, passwordVariable: &#x27;password&#x27;, usernameVariable: &#x27;username&#x27;)]) &#123;</span><br><span class="line">                 sh &#x27;echo $username&#x27;</span><br><span class="line">               &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">     &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>执行构建后会输出密钥中的用户名<br>注：<br>usernamePassword：withCredentials的类型，相应的还支持sshUserPrivateKey(ssh key)和certificate（证书）。<br>credentialsId：Jenkins中对应的配置名称。<br>passwordVariable： 密码项目转成对应的变量。<br>usernameVariable：  用户名项转换成对应的变量。  </p>
<p>SSH User Private Key 示例</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">withCredentials(bindings: [sshUserPrivateKey(credentialsId: &#x27;jenkins-ssh-key-for-abc&#x27;, \</span><br><span class="line">                                             keyFileVariable: &#x27;SSH_KEY_FOR_ABC&#x27;, \</span><br><span class="line">                                             passphraseVariable: &#x27;&#x27;, \</span><br><span class="line">                                             usernameVariable: &#x27;&#x27;)]) &#123;</span><br><span class="line">  // some block</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Certificate 示例</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">withCredentials(bindings: [certificate(aliasVariable: &#x27;&#x27;, \</span><br><span class="line">                                       credentialsId: &#x27;jenkins-certificate-for-xyz&#x27;, \</span><br><span class="line">                                       keystoreVariable: &#x27;CERTIFICATE_FOR_XYZ&#x27;, \</span><br><span class="line">                                       passwordVariable: &#x27;XYZ-CERTIFICATE-PASSWORD&#x27;)]) &#123;</span><br><span class="line">  // some block</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>参考： <a href="https://www.jenkins.io/zh/doc/book/pipeline/jenkinsfile/#usernames-and-passwords">https://www.jenkins.io/zh/doc/book/pipeline/jenkinsfile/#usernames-and-passwords</a></p>
<h3 id="BlueOcean使用"><a href="#BlueOcean使用" class="headerlink" title="BlueOcean使用"></a>BlueOcean使用</h3><p>Jenkins针对pipeline提供了全新的Blue Ocean界面，可以清晰的查看流水线的执行情况：</p>
<p>安装blueocean插件<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins3-14.png"></p>
<p>安装完以后在对应的项目处会多出一个打开Blue Ocean按钮，<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins3-16.png"></p>
<p>界面美化后的扁平化风格，可以通过此界面，进行一些常规的配置和操作<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins3-17.png"></p>
<p>阶段性Pipeline重跑<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins3-18.png"></p>
<p>日志统一下载<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins3-19.png"></p>
<h3 id="Jenkinsfile"><a href="#Jenkinsfile" class="headerlink" title="Jenkinsfile"></a>Jenkinsfile</h3><p>目前主流大部分CI工具都支持Pipeline as Code，就是将整CI流程通过代码方式实现，然后将对应的代码和业务代码放置在一起，对应的CI工具在拉取业务代码后可以直接解析CI的流程代码进行执行。Jenkins也是支持这种方式的，通过将写好的Pipeline写在Jenkinsfile中存放在代码仓库中，Jenkins配置读取指定目录的Jenkinsfile文件即可。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins3-20.png"></p>
<h3 id="实例Demo最佳实践"><a href="#实例Demo最佳实践" class="headerlink" title="实例Demo最佳实践"></a>实例Demo最佳实践</h3><p>完整CICD步骤<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins3-2.png"></p>
<p>1、clone 源码。<br>2、编译源码。<br>3、编译后进行代码扫描和编译后可执行文件扫描。<br>4、将编译后生成的制品上传到制品库。<br>5、镜像构建，将构建后的镜像上传到Harbor。<br>6、更新Gitlab中的部署文件。<br>7、触发ArgoCD同步。手动或自动部署到Kubernetes环境中。</p>
<h4 id="Jenkins创建连接账号"><a href="#Jenkins创建连接账号" class="headerlink" title="Jenkins创建连接账号"></a>Jenkins创建连接账号</h4><p>因为要进行镜像上传和修改gitlab中的部署yaml，需要进行修改。<br>系统管理——&gt;Manage Credentials——&gt;创建全局域的认证信息<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins3-3.png"></p>
<p>添加凭据<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins3-4.png"></p>
<p>类型为Username with password<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins3-5.png"></p>
<p>创建id为harbor_account和gitlab_account的凭证用于Pipeline连接。</p>
<h4 id="代码库配置连接SonarQube信息"><a href="#代码库配置连接SonarQube信息" class="headerlink" title="代码库配置连接SonarQube信息"></a>代码库配置连接SonarQube信息</h4><p>使用Pipeline流水线，需要在添加以下步骤</p>
<p>1、在对应的代码库的根目录创建sonar-project.properties文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sonar.projectKey=test2</span><br><span class="line">sonar.projectName=test2</span><br><span class="line">sonar.projectVersion=1.0</span><br><span class="line">sonar.sources=src</span><br><span class="line">sonar.java.binaries=target/classes</span><br><span class="line">sonar.java.source=1.8</span><br><span class="line">sonar.java.target=1.8</span><br><span class="line">sonar.language=java</span><br><span class="line">sonar.sourceEncoding=UTF-8</span><br></pre></td></tr></table></figure>

<p>Pipeline中添加以下步骤</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">stage(&#x27;SonarQube analysis&#x27;) &#123;</span><br><span class="line">      steps &#123;</span><br><span class="line">        script &#123;</span><br><span class="line">        def sonarqubeScannerHome = tool name: &#x27;SonarQubeScanner&#x27;</span><br><span class="line">            withSonarQubeEnv(&#x27;sonar&#x27;) &#123;</span><br><span class="line">            sh &quot;$&#123;sonarqubeScannerHome&#125;/bin/sonar-scanner&quot;</span><br><span class="line">         &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>注：<br>1、SonarQubeScanner为全局工具配置中的SonarQube Scanner的配置名称。<br>2、withSonarQubeEnv配置的sonar变量为全局——&gt;系统配置sonar-server的配置名称</p>
<h4 id="Pipeline创建"><a href="#Pipeline创建" class="headerlink" title="Pipeline创建"></a>Pipeline创建</h4><p>创建个名称为spring-petclini的Pipeline<br>配置构建触发器<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins3-6.png"></p>
<p>贴入以下Pipeline 代码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">  agent &#123;</span><br><span class="line">    kubernetes &#123;</span><br><span class="line">      cloud &#x27;kubernetes&#x27;</span><br><span class="line">      namespace &#x27;default&#x27;</span><br><span class="line">      yaml &quot;&quot;&quot;</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: git</span><br><span class="line">      image: alpine/git:v2.26.2</span><br><span class="line">      command:</span><br><span class="line">        - cat</span><br><span class="line">      tty: true</span><br><span class="line">    - name: maven</span><br><span class="line">      image: maven:3.6.3-openjdk-8</span><br><span class="line">      command:</span><br><span class="line">        - cat</span><br><span class="line">      tty: true</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - mountPath: /root/.m2/repository</span><br><span class="line">          name: jenkins-maven-m2-pvc</span><br><span class="line">    - name: docker</span><br><span class="line">      image: docker:19.03-dind</span><br><span class="line">      command:</span><br><span class="line">        - cat</span><br><span class="line">      tty: true</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - mountPath: /var/run/docker.sock</span><br><span class="line">          name: docker-sock</span><br><span class="line">    - name: helm-kubectl</span><br><span class="line">      image: registry.cn-shenzhen.aliyuncs.com/yedward/helm-kubectl:3.3.1-1.18.8</span><br><span class="line">      command:</span><br><span class="line">        - cat</span><br><span class="line">      tty: true</span><br><span class="line">  volumes:</span><br><span class="line">    - name: jenkins-maven-m2-pvc</span><br><span class="line">      persistentVolumeClaim:</span><br><span class="line">        claimName: jenkins-maven-m2-pvc</span><br><span class="line">    - name: docker-sock</span><br><span class="line">      hostPath:</span><br><span class="line">        path: /var/run/docker.sock</span><br><span class="line">        type: &quot;&quot;</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  stages &#123;</span><br><span class="line">    stage(&#x27;Clone&#x27;) &#123;</span><br><span class="line">      steps &#123;</span><br><span class="line">        container(&#x27;git&#x27;) &#123;</span><br><span class="line">          git branch: &#x27;main&#x27;, credentialsId: &#x27;gitlab&#x27;, url: &#x27;http://172.16.1.184/root/spring-petclinic.git&#x27;        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    stage(&#x27;Build&#x27;) &#123;</span><br><span class="line">      steps &#123;</span><br><span class="line">        container(&#x27;maven&#x27;) &#123;</span><br><span class="line">          sh &#x27;mvn clean package -DskipTests&#x27;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    stage(&#x27;SonarQube analysis&#x27;) &#123;</span><br><span class="line">      steps &#123;</span><br><span class="line">        script &#123;</span><br><span class="line">        def sonarqubeScannerHome = tool name: &#x27;SonarQubeScanner&#x27;</span><br><span class="line">            withSonarQubeEnv(&#x27;sonar&#x27;) &#123;</span><br><span class="line">            sh &quot;$&#123;sonarqubeScannerHome&#125;/bin/sonar-scanner&quot;</span><br><span class="line">         &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    stage(&#x27;Publish&#x27;) &#123;</span><br><span class="line">      steps &#123;</span><br><span class="line">        container(&#x27;docker&#x27;) &#123;</span><br><span class="line">            withCredentials([usernamePassword(credentialsId: &#x27;harbor_account&#x27;, passwordVariable: &#x27;USERPWD&#x27;, usernameVariable: &#x27;USERNAME&#x27;)]) &#123;</span><br><span class="line">                sh &#x27;echo &quot;$USERPWD&quot; | docker login --username=&quot;$USERNAME&quot; 172.16.1.31 --password-stdin&#x27;</span><br><span class="line">                sh &#x27;docker build -t 172.16.1.31/spring-petclinic/spring-petclinic:$BUILD_NUMBER .&#x27;</span><br><span class="line">                sh &#x27;docker push 172.16.1.31/spring-petclinic/spring-petclinic:$BUILD_NUMBER&#x27;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    stage(&#x27;Deploy&#x27;) &#123;</span><br><span class="line">      steps &#123;</span><br><span class="line">        container(&#x27;git&#x27;) &#123;</span><br><span class="line">            withCredentials([usernamePassword(credentialsId: &#x27;gitlab_account&#x27;, passwordVariable: &#x27;USERPWD&#x27;, usernameVariable: &#x27;USERNAME&#x27;)]) &#123;</span><br><span class="line">                sh &#x27;git config --global user.email &quot;root@example.com&quot;&#x27;</span><br><span class="line">                sh &#x27;git config --global user.name &quot;root&quot;&#x27;</span><br><span class="line">                sh &#x27;git remote set-url origin http://$USERNAME:$USERPWD@172.16.1.184/root/spring-petclinic.git&#x27;</span><br><span class="line">                sh &#x27;sed -i &quot;s/spring-petclinic:.*/spring-petclinic:$BUILD_NUMBER/g&quot; deployment.yaml&#x27;</span><br><span class="line">                sh &#x27;git add deployment.yaml&#x27;</span><br><span class="line">                sh &#x27;git commit -m &quot;update yaml&quot;&#x27;</span><br><span class="line">                sh &#x27;git push origin main&#x27;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意点：<br>1、clone阶段如果是私有代码仓库，需要配置凭证，可以通过流水线语法生成对应的执行命令。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins3-6.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins3-7.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins3-8.png"></p>
<p>输入对应的git地址和分支，选择gitlab密钥，生成对应的执行代码</p>
<p>执行<br>修改spring-petclinic项目的代码注释掉主页的图片，提交代码，触发自动CICD查看效果。</p>
<p>注释掉首页小狗图片<br><code>spring-petclinic/src/main/resources/templates/welcome.html</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;!--&lt;img class=&quot;img-responsive&quot; src=&quot;../static/resources/images/pets.png&quot; th:src=&quot;@&#123;/resources/images/pets.png&#125;&quot;/&gt;--&gt;</span><br></pre></td></tr></table></figure>
<p>重新提交代码<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins3-9.png"></p>
<p>查看已经没有对应logo<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Jenkins3-10.png"></p>
<p>参考链接：<br><a href="https://www.jenkins.io/doc/book/pipeline/syntax/">https://www.jenkins.io/doc/book/pipeline/syntax/</a><br><a href="https://wiki.eryajf.net/pages/3298.html">https://wiki.eryajf.net/pages/3298.html</a></p>
]]></content>
      <categories>
        <category>CI/CD</category>
      </categories>
      <tags>
        <tag>CI/CD</tag>
      </tags>
  </entry>
  <entry>
    <title>自定义Jenkins-slave镜像</title>
    <url>/2018/10/23/jenkins_slave/</url>
    <content><![CDATA[<h3 id="概述："><a href="#概述：" class="headerlink" title="概述："></a>概述：</h3><p>在前面章节，我们提到过通过Jenkins对接Kubernetes能实现对Jenkins slave的弹性部署和回收，能非常有效的利用资源，在实际使用的过程中，我们会根据不同的编程语言去构建一个对应的jnlp，或在一个jnlp镜像内包含全部编程语言。下面讲解一下如何定制自己的jnlp镜像。</p>
<p>软件版本：<br>os：ubuntu 16.04<br>Kubernetes：1.12.0<br>jenkins：2.89.4</p>
<h3 id="制做方法："><a href="#制做方法：" class="headerlink" title="制做方法："></a>制做方法：</h3><p>基于标准操作系统base镜像构建，我这里使用debian，当然也可以使用centos，ubuntu，alpine等其他镜像（以java语言环境为例，构建maven编译环境，其他开发语言类似)</p>
<p>1、安装配置JAVA环境和maven编译环境，下载jdk，maven<br><a href="https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html">https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a><br>下载Linux x64 tar包，解压到Dockerfile目录</p>
<p><a href="http://mirror.bit.edu.cn/apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz">http://mirror.bit.edu.cn/apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz</a><br>解压tar包，到Dockerfile目录</p>
<p>2、安装配置Jenkins-slave<br><a href="https://github.com/jenkinsci/docker-jnlp-slave/blob/master/jenkins-slave">https://github.com/jenkinsci/docker-jnlp-slave/blob/master/jenkins-slave</a><br>下载jenkins-slave启动脚本，并放置到 &#x2F;usr&#x2F;local&#x2F;bin&#x2F;目录</p>
<p>3、安装slave.jar包<br>slave.jar包主要用于Jenkins-slave去连接Jenkins-server所用<br><a href="https://repo.jenkins-ci.org/public/org/jenkins-ci/main/remoting/3.16/remoting-3.16.jar">https://repo.jenkins-ci.org/public/org/jenkins-ci/main/remoting/3.16/remoting-3.16.jar</a><br>下载slave.jar包并放置到&#x2F;usr&#x2F;share&#x2F;jenkins&#x2F;目录</p>
<p>4、安装配置docker-in-docker<br>下载docker二进制包<br><a href="https://download.docker.com/linux/static/stable/x86_64/">https://download.docker.com/linux/static/stable/x86_64/</a><br>docker-17.03.2-ce.tgz<br>解压后，放到Dockerfile目录</p>
<p>5、下载kubectl二进制包<br><a href="https://dl.k8s.io/v1.11.3/kubernetes-client-linux-amd64.tar.gz">https://dl.k8s.io/v1.11.3/kubernetes-client-linux-amd64.tar.gz</a></p>
<p>解压后，放到Dockerfile目录</p>
<p>6、放置连接Kubernetes集群的kubeconfig文件<br>拷贝连接集群的kubeconfig文件，重命名为config</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mv kubeconfig config</span><br></pre></td></tr></table></figure>
<p>基于以下Dockerfile构建镜像</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM debian:9</span><br><span class="line">USER root</span><br><span class="line">ARG AGENT_WORKDIR=/home/jenkins/agent</span><br><span class="line">RUN mkdir -p /home/jenkins/.jenkins &amp;&amp; mkdir -p $&#123;AGENT_WORKDIR&#125;</span><br><span class="line">COPY jdk1.8.0_181 /usr/local/jdk1.8.0_181</span><br><span class="line">COPY maven3.3.9 /usr/local/maven3.3.9</span><br><span class="line">COPY jenkins-slave /usr/local/bin/jenkins-slave</span><br><span class="line">COPY slave.jar /usr/share/jenkins/</span><br><span class="line">COPY docker /usr/local/docker</span><br><span class="line">COPY kubectl /usr/bin/</span><br><span class="line">COPY config /root</span><br><span class="line">ENV PATH=/usr/local/jdk1.8.0_181/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/maven3.3.9/bin:/usr/local/docker</span><br><span class="line">RUN chmod +x /usr/local/bin/jenkins-slave</span><br><span class="line">ENV HOME /home/jenkins</span><br><span class="line">RUN chmod 755 /usr/share/jenkins &amp;&amp; chmod 644 /usr/share/jenkins/slave.jar</span><br><span class="line">USER root</span><br><span class="line">ENV AGENT_WORKDIR=$&#123;AGENT_WORKDIR&#125;</span><br><span class="line">WORKDIR /home/jenkins</span><br><span class="line">ENTRYPOINT [&quot;jenkins-slave&quot;]</span><br></pre></td></tr></table></figure>

<p>测试用例<br><a href="https://github.com/wanshaoyuan/pipeline-exemple.git">https://github.com/wanshaoyuan/pipeline-exemple.git</a></p>
<p>测试的Jenkins的pipeline<br>注意：deployment.yaml里面应用部署在sock-shop这个命名空间内，需要提前创建好。<br>jenkins-pipeline</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">podTemplate(label: &#x27;jnlp-slave&#x27;, cloud: &#x27;kubernetes&#x27;) &#123;</span><br><span class="line">    node(&#x27;jnlp-slave&#x27;) &#123;</span><br><span class="line">    stage(&#x27;git clone&#x27;)&#123;</span><br><span class="line">        //clone CODE根据实际情况修改</span><br><span class="line">        git credentialsId: &#x27;ba80cc36-77b1-42d0-8838-f834de346fe1&#x27;, url: &#x27;https://github.com/wanshaoyuan/pipeline-exemple.git&#x27;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    stage(&#x27;code build&#x27;)&#123;</span><br><span class="line">        sh &#x27;&#x27;&#x27;/usr/local/maven3.3.9/bin/mvn -DskipTests package -f /home/jenkins/workspace/test-jnlp2&#x27;&#x27;&#x27;</span><br><span class="line">    &#125;</span><br><span class="line">    stage(&#x27;image build&#x27;)&#123;</span><br><span class="line">        sh &#x27;&#x27;&#x27;echo $BUILD_ID</span><br><span class="line">              GROUP=172.31.164.66/library COMMIT=$BUILD_ID /home/jenkins/workspace/test-jnlp2/scripts/build.sh&#x27;&#x27;&#x27;</span><br><span class="line">    &#125;</span><br><span class="line">    stage(&#x27;upload&#x27;)&#123;</span><br><span class="line">        //上传镜像</span><br><span class="line">        sh &#x27;&#x27;&#x27;docker login 172.31.164.66 -u=admin -p=123456</span><br><span class="line">        docker push 172.31.164.66/library/shipping:$BUILD_ID&#x27;&#x27;&#x27;</span><br><span class="line">    &#125;</span><br><span class="line">    stage(&#x27;deploy&#x27;)&#123;</span><br><span class="line">        //执行部署脚本</span><br><span class="line">       sh &#x27;sed -i &quot;s/172.31.164.66\\/library\\/shipping:.*/172.31.164.66\\/library\\/shipping:$BUILD_ID/g&quot; /home/jenkins/workspace/test-jnlp2/deployment.yml&#x27;</span><br><span class="line">       sh &#x27;kubectl apply -f /home/jenkins/workspace/test-jnlp2/deployment.yml --kubeconfig /root/config&#x27;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>配置jnlp映射宿主机的docker.sock文件,不然无法实现容器镜像build<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_slave_3.png"></p>
<p>执行完毕<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_slave_1.png"></p>
<p>最后访问集群30001</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get svc -n sock-shop</span><br><span class="line">NAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">carts          ClusterIP   10.43.23.102    &lt;none&gt;        80/TCP         1d</span><br><span class="line">carts-db       ClusterIP   10.43.229.245   &lt;none&gt;        27017/TCP      1d</span><br><span class="line">catalogue      ClusterIP   10.43.133.136   &lt;none&gt;        80/TCP         1d</span><br><span class="line">catalogue-db   ClusterIP   10.43.156.93    &lt;none&gt;        3306/TCP       1d</span><br><span class="line">front-end      NodePort    10.43.130.209   &lt;none&gt;        80:30001/TCP   1d</span><br><span class="line">orders         ClusterIP   10.43.212.179   &lt;none&gt;        80/TCP         1d</span><br><span class="line">orders-db      ClusterIP   10.43.178.3     &lt;none&gt;        27017/TCP      1d</span><br><span class="line">payment        ClusterIP   10.43.205.54    &lt;none&gt;        80/TCP         1d</span><br><span class="line">queue-master   ClusterIP   10.43.178.73    &lt;none&gt;        80/TCP         1d</span><br><span class="line">rabbitmq       ClusterIP   10.43.233.8     &lt;none&gt;        5672/TCP       1d</span><br><span class="line">shipping       ClusterIP   10.43.198.185   &lt;none&gt;        80/TCP         1d</span><br><span class="line">user           ClusterIP   10.43.213.139   &lt;none&gt;        80/TCP         1d</span><br><span class="line">user-db        ClusterIP   10.43.94.66     &lt;none&gt;        27017/TCP      1d</span><br></pre></td></tr></table></figure>

<p><a href="http://host_ip:30001/">http://host_ip:30001</a></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/jenkins_slave_2.png"></p>
]]></content>
      <categories>
        <category>CI/CD</category>
      </categories>
      <tags>
        <tag>CI/CD</tag>
      </tags>
  </entry>
  <entry>
    <title>K3S安装</title>
    <url>/2019/10/28/k3s_install/</url>
    <content><![CDATA[<h1 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h1><p>|软件名称 | 软件版本|<br>| :—-: | :—-:  | :—-:<br>| k3s|  v0.9.1 |</p>
<h1 id="安装k3s"><a href="#安装k3s" class="headerlink" title="安装k3s"></a>安装k3s</h1><h2 id="在线安装"><a href="#在线安装" class="headerlink" title="在线安装"></a>在线安装</h2><p>执行在线安装脚本，会自动识别环境，拉取镜像，安装k3s<br>安装server端：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -sfL https://get.k3s.io | sh -</span><br></pre></td></tr></table></figure>
<p>安装agent端：<br>获取连接server端的token  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/var/lib/rancher/k3s/server/node-token  </span><br><span class="line">k3s agent -s https://master_ip:6443  -t  token  </span><br></pre></td></tr></table></figure>
<p>agent连接server端</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -sfL https://get.k3s.io | K3S_URL=https://myserver:6443 K3S_TOKEN=XXX sh -</span><br></pre></td></tr></table></figure>
<p>验证  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">k3s kubectl get node</span><br><span class="line"></span><br><span class="line">NAME     STATUS   ROLES    AGE   VERSION</span><br><span class="line">ubuntu-server   Ready    master   86m   v1.15.4-k3s.1</span><br><span class="line">ubuntu-agent    Ready     worker   86m   v1.15.4-k3s.1</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>卸载<br>执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/usr/local/bin/k3s-uninstall.sh 或k3s-agent-uninstall.sh </span><br></pre></td></tr></table></figure>



<p>注意：<br>在线安装有pause镜像会去k8s.gcr.io仓库拉取，在国内网络环境连接会受阻。  </p>
<h2 id="离线安装"><a href="#离线安装" class="headerlink" title="离线安装"></a>离线安装</h2><p>准备下载离线包<br>在github，k3s release页下载对应的离线镜像</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://github.com/rancher/k3s/releases/</span><br></pre></td></tr></table></figure>

<p>根据机器类型，下载对应的文件  </p>
<table>
<thead>
<tr>
<th>文件</th>
<th>作用</th>
<th>机器类型</th>
</tr>
</thead>
<tbody><tr>
<td>k3s</td>
<td>k3s 可执行文件</td>
<td>x86 CPU架构64位系统</td>
</tr>
<tr>
<td>k3s-arm64</td>
<td>k3s可执行文件</td>
<td>ARM CPU架构 64位系统</td>
</tr>
<tr>
<td>k3s-armhf</td>
<td>k3s可执行文件</td>
<td>ARM CPU架构 32位系统</td>
</tr>
<tr>
<td>k3s-airgap-images-amd64.tar</td>
<td>离线镜像包</td>
<td>x86 CPU架构64位系统</td>
</tr>
<tr>
<td>k3s-airgap-images-arm64.tar</td>
<td>离线镜像包</td>
<td>ARM CPU架构 64位系统</td>
</tr>
<tr>
<td>k3s-airgap-images-arm.tar</td>
<td>离线镜像包</td>
<td>ARM CPU架构 32位系统</td>
</tr>
</tbody></table>
<p>选择对应的操作系统版本，下载对应的k3s可执行文件和离线镜像包。  </p>
<p>注：<br>官方的Raspbian Buster Lite系统仍然是32位。</p>
<p>需要提前在打开操作系统cgroup</p>
<p>ubuntu18.04方法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo vi /boot/firmware/cmdline.txt</span><br><span class="line"></span><br><span class="line">cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory</span><br></pre></td></tr></table></figure>
<p> Raspbian Buster Lite系统方法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /boot/cmdline.txt</span><br><span class="line">cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory</span><br></pre></td></tr></table></figure>
<p>重启生效。  </p>
<p>创建镜像存储目录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir -p /var/lib/rancher/k3s/agent/images/</span><br></pre></td></tr></table></figure>

<p>拷贝离线镜像件  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo cp ./k3s-airgap-images-$ARCH.tar /var/lib/rancher/k3s/agent/images/</span><br></pre></td></tr></table></figure>
<p>拷贝可执行文到&#x2F;usr&#x2F;bin目录，并配置可执行权限</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp k3sxxx /usr/bin/k3s</span><br><span class="line">chmoad a+x /usr/bin/k3s</span><br></pre></td></tr></table></figure>

<p>启动 k3s会自动解压并导入镜像文件  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">k3s server</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rm /etc/rancher/k3s/k3s.yaml </span><br><span class="line">rm /var/lib/rancher/ -rf </span><br></pre></td></tr></table></figure>
<p>查看集群</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">k3s  kubectl get node</span><br><span class="line">NAME     STATUS   ROLES    AGE   VERSION</span><br><span class="line">ubuntu   Ready    master   14s   v1.15.4-k3s.1</span><br></pre></td></tr></table></figure>
<p>agent连接<br>在server端获取agent连接server的token</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat   /var/lib/rancher/k3s/server/node-token</span><br></pre></td></tr></table></figure>



<h2 id="常见命令"><a href="#常见命令" class="headerlink" title="常见命令"></a>常见命令</h2><p>列出当前集群镜像  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">k3s crictl images</span><br></pre></td></tr></table></figure>
<p>列出当前集群容器  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">k3s crictl ps -a </span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">k3s crictl ps </span><br></pre></td></tr></table></figure>
<p>查看pod占用cpu 内存资源  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">k3s crictl stats</span><br></pre></td></tr></table></figure>


<h2 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h2><h3 id="不使用containerd直接docker"><a href="#不使用containerd直接docker" class="headerlink" title="不使用containerd直接docker"></a>不使用containerd直接docker</h3><p>安装docker后</p>
<p>将离线镜像导入</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker load &lt; k3s-airgap-images-arm64.tar </span><br></pre></td></tr></table></figure>
<p>执行k3s server</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">k3s server --docker</span><br></pre></td></tr></table></figure>

<h3 id="使用systemd启动k3s-server"><a href="#使用systemd启动k3s-server" class="headerlink" title="使用systemd启动k3s server"></a>使用systemd启动k3s server</h3><p>创建systemd文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim /etc/systemd/system/k3s.service</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=Lightweight Kubernetes</span><br><span class="line">Documentation=https://k3s.io</span><br><span class="line">After=network-online.target</span><br><span class="line">[Service]</span><br><span class="line">Type=$&#123;SYSTEMD_TYPE&#125;</span><br><span class="line">EnvironmentFile=$&#123;FILE_K3S_ENV&#125;</span><br><span class="line">ExecStartPre=-/sbin/modprobe br_netfilter</span><br><span class="line">ExecStartPre=-/sbin/modprobe overlay</span><br><span class="line">ExecStart=$&#123;BIN_DIR&#125;/k3s \\</span><br><span class="line">    $&#123;CMD_K3S_EXEC&#125;</span><br><span class="line">KillMode=process</span><br><span class="line">Delegate=yes</span><br><span class="line">LimitNOFILE=infinity</span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br><span class="line">TasksMax=infinity</span><br><span class="line">TimeoutStartSec=0</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5s</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure>

<p>根据实际情况修改ExecStar,server用server对应的命令，agent用agent对应指令。</p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>k3s集群部署和使用cilium网络插件</title>
    <url>/2020/08/16/k3s_install_cilium/</url>
    <content><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>cilium都近期比较火的一个网络插件，它通过将ebpf技术引入网络插件中用于满足容器工作负载的新可伸缩性，安全性和可见性要求。</p>
<h3 id="前置条件"><a href="#前置条件" class="headerlink" title="前置条件"></a>前置条件</h3><p>ebpf依赖内核技术，所以需要保证以下内核版本<br>1 、kernel版本 &gt;&#x3D; 4.9.17</p>
<table>
<thead>
<tr>
<th>软件</th>
<th>版本</th>
</tr>
</thead>
<tbody><tr>
<td>k3s</td>
<td>v1.18.6+k3s1</td>
</tr>
<tr>
<td>cilium</td>
<td>1.8</td>
</tr>
</tbody></table>
<h3 id="部署k3s"><a href="#部署k3s" class="headerlink" title="部署k3s"></a>部署k3s</h3><p>采用两节点部署k3s，节点一角色为master+worker，节点二角色为worker</p>
<p>部署master</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -sfL https://docs.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh -s  - --write-kubeconfig ~/.kube/config --write-kubeconfig-mode 666  --docker --no-flannel</span><br></pre></td></tr></table></figure>



<p>获取连接master的token信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat /var/lib/rancher/k3s/server/node-token</span><br></pre></td></tr></table></figure>

<p>部署worker</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -sfL https://docs.rancher.cn/k3s/k3s-install.sh| INSTALL_K3S_EXEC=&#x27;--docker --no-flannel&#x27;  INSTALL_K3S_MIRROR=cn K3S_URL=https://master-ip:6443 K3S_TOKEN=xxx sh  -</span><br></pre></td></tr></table></figure>


<p>查看节点部署</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> kubectl get node</span><br><span class="line">NAME                      STATUS     ROLES    AGE   VERSION</span><br><span class="line">rke-k3s-node1.novalocal   NotReady   master   24m   v1.18.6+k3s1</span><br><span class="line">rke-k3s-node2             NotReady   &lt;none&gt;   10s   v1.18.6+k3s1</span><br></pre></td></tr></table></figure>
<p>此时因为没有部署网络插件所以节点状态为NotReady状态</p>
<p>所有节点执行挂载bpf文件系统</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo mount bpffs -t bpf /sys/fs/bpf</span><br></pre></td></tr></table></figure>


<h3 id="方式一：快速部署cilium网络插件"><a href="#方式一：快速部署cilium网络插件" class="headerlink" title="方式一：快速部署cilium网络插件"></a>方式一：快速部署cilium网络插件</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f https://raw.githubusercontent.com/cilium/cilium/v1.8/install/kubernetes/quick-install.yaml</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>查看组件状态</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod -n kube-system</span><br><span class="line">NAME                                     READY   STATUS      RESTARTS   AGE</span><br><span class="line">cilium-p2c2p                             1/1     Running     0          28m</span><br><span class="line">cilium-operator-868c78f7b5-bvgrl         1/1     Running     0          28m</span><br><span class="line">coredns-8655855d6-9nvn9                  1/1     Running     0          27m</span><br><span class="line">local-path-provisioner-6d59f47c7-k2454   1/1     Running     0          21m</span><br><span class="line">metrics-server-7566d596c8-k4dzz          1/1     Running     0          21m</span><br><span class="line">helm-install-traefik-cmlzv               0/1     Completed   3          21m</span><br><span class="line">svclb-traefik-wz685                      2/2     Running     0          20m</span><br><span class="line">traefik-758cd5fc85-t2mzc                 1/1     Running     0          8m49s</span><br><span class="line">cilium-operator-868c78f7b5-tjm27         1/1     Running     0          2m49s</span><br><span class="line">svclb-traefik-n2b8m                      2/2     Running     0          71s</span><br><span class="line">cilium-97j69                             1/1     Running     0          91s</span><br></pre></td></tr></table></figure>

<p>测试网络连通性</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/cilium/cilium/v1.8/examples/kubernetes/connectivity-check/connectivity-check.yaml</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>主要测试二层网络联通性和三层网络联通性  ，状态都为running表示正常<br>cilium默认跨主机网络通信是通过VXLAN实现的</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod</span><br><span class="line">NAME                                                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod-to-a-6cf58894b7-lh9pz                                1/1     Running   0          5m11s</span><br><span class="line">pod-to-b-intra-node-77b485d996-6bm8d                     1/1     Running   0          5m10s</span><br><span class="line">host-to-b-multi-node-clusterip-c7557d4f8-89lj6           1/1     Running   0          5m11s</span><br><span class="line">echo-b-659766fb56-c54fm                                  1/1     Running   0          5m11s</span><br><span class="line">echo-a-5f555bbc8b-fck6k                                  1/1     Running   0          5m12s</span><br><span class="line">pod-to-a-l3-denied-cnp-74b9566cc7-ct4kg                  1/1     Running   0          5m11s</span><br><span class="line">echo-b-host-65d7db76d8-kv277                             1/1     Running   0          5m11s</span><br><span class="line">host-to-b-multi-node-headless-5dfcdf9b76-wsz7m           1/1     Running   0          5m11s</span><br><span class="line">pod-to-b-intra-node-nodeport-56975db6c7-sqwq8            1/1     Running   0          5m11s</span><br><span class="line">pod-to-b-multi-node-clusterip-75f5c78f68-4ljhc           1/1     Running   0          5m10s</span><br><span class="line">pod-to-b-multi-node-headless-5df88f9bd4-gfjws            1/1     Running   0          5m10s</span><br><span class="line">pod-to-b-multi-node-nodeport-55b9769455-dv6pv            1/1     Running   0          5m10s</span><br><span class="line">pod-to-a-allowed-cnp-5898f7d8c9-bjch7                    1/1     Running   0          5m11s</span><br><span class="line">pod-to-a-external-1111-5779fb7cb9-qf8mq                  0/1     Running   4          5m9s</span><br><span class="line">pod-to-external-fqdn-allow-google-cnp-74466b4c6f-sfchn   0/1     Running   3          5m9s</span><br></pre></td></tr></table></figure>

<p>删除测试用例</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl delete -f https://raw.githubusercontent.com/cilium/cilium/v1.8/examples/kubernetes/connectivity-check/connectivity-check.yaml</span><br></pre></td></tr></table></figure>




<h3 id="方式二：Helm部署方式cilium网络插件"><a href="#方式二：Helm部署方式cilium网络插件" class="headerlink" title="方式二：Helm部署方式cilium网络插件"></a>方式二：Helm部署方式cilium网络插件</h3><p>因为cilium1.8开始hubble功能集成到Cilium的helm-chart中了，所以在1.8版本cilium需要使用hubble需要在chart中启用，部署方式分两种</p>
<p>方式一：Cilium + cilium-etcd-operator</p>
<p>方式二：cilium+自带etcd</p>
<p>因为k3s默认使用SQLite3做为后端数据库所以这里使用方式一方式进行安装</p>
<p>hubble主要用于<br>下载helm客户端</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget https://get.helm.sh/helm-v3.3.0-linux-amd64.tar.gz</span><br></pre></td></tr></table></figure>
<p>添加cilium的repo</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">helm repo add cilium https://helm.cilium.io/</span><br></pre></td></tr></table></figure>

<p>部署cilium和hubble</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">helm install cilium cilium/cilium --version 1.8.2 \</span><br><span class="line">   --namespace kube-system \</span><br><span class="line">   --set global.etcd.enabled=true \</span><br><span class="line">   --set global.etcd.managed=true \</span><br><span class="line">   --set global.hubble.enabled=true \</span><br><span class="line">   --set global.hubble.listenAddress=&quot;:4244&quot; \</span><br><span class="line">   --set global.hubble.metrics.enabled=&quot;&#123;dns,drop,tcp,flow,port-distribution,icmp,http&#125;&quot; \</span><br><span class="line">   --set global.hubble.relay.enabled=true \</span><br><span class="line">   --set global.hubble.ui.enabled=true</span><br></pre></td></tr></table></figure>



<p>使用NodePort方式对外暴露</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl patch svc hubble-ui  -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&#x27; -n kube-system  </span><br></pre></td></tr></table></figure>


<p>访问hubble-UI  </p>
<p>通过hubble可以非常清晰看到各个应用的互相连接关系和访问信息。  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/hubble-1.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/hubble-2.png"></p>
<h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>整体部署和使用下来功能是非常强大，目前看社区都有往ebpf方向走，包括向calico目前也有个ebpf的模式，通过ebpf可以提升整体网络的性能和细粒化网络流量的控制和可视化，后续将以这个为方向深入理解。但带来的问题就是对于排错对比与calico-bgp或flannel-vxlan会更加复杂，在部署后出现过一次大面积瘫痪情况，没有找到是k3s问题和cilium问题，后面将集群和cilium重新部署就好了。</p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes下CICD工具的选型</title>
    <url>/2018/12/29/k8s_cicd/</url>
    <content><![CDATA[<h3 id="什么是CI"><a href="#什么是CI" class="headerlink" title="什么是CI?"></a>什么是CI?</h3><p>持续集成(Continuous integration)频繁的将代码提交然后集成到主干。</p>
<h3 id="什么CD"><a href="#什么CD" class="headerlink" title="什么CD?"></a>什么CD?</h3><p>持续交付（Continuous Delivery)<br>持续交付是在CI的基础上，将集成到主干的代码，产出的可部署软件版本，部署到类生产环境进行测试验证，没问题在手动部署到生产环境。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_1.png"><br>持续部署（Continuous Deployment)<br>在持续交付基础上，部署到生产环境是自动操作的。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_2.png"></p>
<p>最终<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_3.png"></p>
<h3 id="为什么要做CI-CD"><a href="#为什么要做CI-CD" class="headerlink" title="为什么要做CI&#x2F;CD?"></a>为什么要做CI&#x2F;CD?</h3><ul>
<li>提高效率，减少人工操作。</li>
<li>能够快速确定新代码和原有代码能否集成。</li>
<li>降低风险，快速发现错误。</li>
<li>加速软件发布周期。</li>
</ul>
<h3 id="需要什么？"><a href="#需要什么？" class="headerlink" title="需要什么？"></a>需要什么？</h3><p>1、具有版本管理功能的代码仓库管理系统（svn，gitlab，github)<br>2、CI工具（jenkins，gitlab-ci，drone)<br>3、自动化测试<br>4、自动化部署工具（ansible、puppet，saltstack）</p>
<h3 id="基于容器实现CICD的优势"><a href="#基于容器实现CICD的优势" class="headerlink" title="基于容器实现CICD的优势"></a>基于容器实现CICD的优势</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_4.png"></p>
<p>持续集成与持续交付的难点在于如何屏蔽不同语言、不同框架、不同系统之间的持续集成与持续交付流程的差异性。</p>
<ul>
<li>标准化打包方式，解决应用部署依赖问题。</li>
<li>抹平环境差异</li>
<li>提升软件交付和迭代效率，还能避免交付内容不同导致的人为错误。</li>
</ul>
<h3 id="CICD工具"><a href="#CICD工具" class="headerlink" title="CICD工具"></a>CICD工具</h3><p>常见：</p>
<ul>
<li>Drone</li>
<li>GitlabCI</li>
<li>Jenkins</li>
</ul>
<p>工具选型考虑点：</p>
<ul>
<li>支持的代码库</li>
<li>易用性</li>
<li>插件生态</li>
</ul>
<h4 id="Drone"><a href="#Drone" class="headerlink" title="Drone"></a>Drone</h4><p>版本：1.0.0-rc2</p>
<p>GO语言开发的一个非常轻量级的开源CI工具，整个镜像大小只有60多M，原生支持docker并且完全基于docker的CI工具，甚至连插件都是容器形式，使用的CICD操作，编译，测试，构建全部是都在docker中进行。</p>
<p>部署方式：</p>
<ul>
<li>docker</li>
<li>Kubernetes</li>
</ul>
<p>项目地址<br><a href="https://github.com/drone/drone">https://github.com/drone/drone</a><br>插件地址<br><a href="http://plugins.drone.io/">http://plugins.drone.io/</a><br>文档地址<br><a href="https://docs.drone.io/user-guide/">https://docs.drone.io/user-guide/</a></p>
<p>支持的代码仓库</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_5.png"></p>
<p>易用性<br>Everything as docker，所有组件都是容器，包括你使用的插件</p>
<p>CI的执行是通过在代码仓库内定义个.drone.yml的文件,里面编写好对应的pipeline然后drone会去读取这个文件并执行。<br>让CI工作所需的步骤可归纳为</p>
<ul>
<li>部署drone-server</li>
<li>添加.drone.yml到代码仓库的根目录</li>
</ul>
<p>没有自己的用户系统及认证系统，通过OAuth2.0协议与对接的代码仓库进行用户登证及代码仓库同步。也就意味着你不需要在单独管理CI系统的用户。<br>UI整体简洁、美观。显示的是当前这个用户的项目信息，包括最新 commit信息及CICD</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_6.png"></p>
<p>能清晰看见当前项目所有执行过的pipeline，以及这个pipeline是谁触发的。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_7.png"></p>
<p>单个pipeline的完整执行流程，及每个流程完整日志及消耗时间<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_8.png"></p>
<p>Pipeline as code<br>pipeline编写，Pipeline as code 所有的操作都是通过代码实现，包括drone1.0版本pipeline语法更贴近Kubernetes yaml语法，使用起来更加高效。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kind: pipeline</span><br><span class="line">name: test</span><br><span class="line">steps:</span><br><span class="line">- name: build</span><br><span class="line">  image: golang</span><br><span class="line">  commands:</span><br><span class="line">    - CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .</span><br><span class="line">    - ./app</span><br><span class="line">- name: publish</span><br><span class="line">  image: plugins/docker</span><br><span class="line">  settings:</span><br><span class="line">     repo: 172.31.164.66/library/test</span><br><span class="line">     registry: 172.31.164.66</span><br><span class="line">     insecure: true</span><br><span class="line">     tags: $&#123;DRONE_COMMIT_MESSAGE&#125;</span><br><span class="line">trigger:</span><br><span class="line">  branch:</span><br><span class="line">  - dev</span><br><span class="line">  event:</span><br><span class="line">  - push</span><br></pre></td></tr></table></figure>


<p>插件生态<br><a href="http://plugins.drone.io/">http://plugins.drone.io/</a></p>
<p>插件比较丰富，函盖了目前主要的一些应用场景</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_9.png"></p>
<p>当需要自定义插件时也非常简单，因为drone Everything as docker，插件也是通过以容器方式运行，然后将pipeline中自定义的环境变量传到这个容器中去执行。</p>
<p>如设计个slack插件<br>确定好需要传输的参数<br>确定好需要传输哪些参数比如Slack插件将需要webhook网址，频道和消息文本</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kind: pipeline</span><br><span class="line">name: default</span><br><span class="line"></span><br><span class="line">steps:</span><br><span class="line">- name: webhook</span><br><span class="line">  image: janecitizen/slack</span><br><span class="line">  settings:</span><br><span class="line">    webhook: https://hooks.slack.com/services/...</span><br><span class="line">    channel: general</span><br><span class="line">    text: hello</span><br></pre></td></tr></table></figure>

<p>输入参数作为环境变量传递给插件，前缀为PLUGIN_。上面示例中的输入参数将传递给插件，如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PLUGIN_CHANNEL=general</span><br><span class="line">PLUGIN_WEBHOOK=https://hooks.slack.com/services/...</span><br><span class="line">PLUGIN_TEXT=hello</span><br></pre></td></tr></table></figure>
<p>创建脚本<br>使用上一节中定义的输入参数替换</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/sh</span><br><span class="line">curl -X POST \</span><br><span class="line">  -H &#x27;Content-type: application/json&#x27; \</span><br><span class="line">  -d &#x27;&#123;&quot;text&quot;:&quot;$&#123;PLUGIN_TEXT&#125;&quot;,&quot;channel&quot;:&quot;$&#123;PLUGIN_CHANNEL&quot;&#125;&#x27; \</span><br><span class="line">  $&#123;PLUGIN_WEBHOOK&#125;</span><br></pre></td></tr></table></figure>
<p>构建插件<br>插件作为Docker镜像打包。创建一个Dockerfile，将先前创建的shell脚本添加到镜像。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM alpine</span><br><span class="line">ADD script.sh /bin/</span><br><span class="line">RUN chmod +x /bin/script.sh</span><br><span class="line">RUN apk -Uuv add curl ca-certificates</span><br><span class="line">ENTRYPOINT /bin/script.sh</span><br></pre></td></tr></table></figure>

<p>构建你的插件docker镜像</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker build -t rancher/slack .</span><br></pre></td></tr></table></figure>
<p>总结：<br>优点:</p>
<ul>
<li>非常轻量级，部署简单。</li>
<li>高效率（Pipeline as code）</li>
<li>原生支持docker（ Everything as docker）</li>
<li>自定义插件和社区插件使用比较简单</li>
<li>难够针对pipeline的单个步骤多事件触发，或分支触发</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kind: pipeline</span><br><span class="line">name: default</span><br><span class="line">steps:</span><br><span class="line">- name: build</span><br><span class="line">  image: golang</span><br><span class="line">  commands:</span><br><span class="line">  - go build</span><br><span class="line">  - go test -short</span><br><span class="line"></span><br><span class="line">- name: integration</span><br><span class="line">  image: golang</span><br><span class="line">  commands:</span><br><span class="line">  - go test -v</span><br><span class="line">  when:</span><br><span class="line">    event:</span><br><span class="line">    - pull_request</span><br><span class="line">    branch:</span><br><span class="line">  - feature</span><br></pre></td></tr></table></figure>
<p>通过使用插件能够触发其他CI服务<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_10.png"></p>
<p>分布式<br>缺点：</p>
<ul>
<li>高度依赖社区，文档不完善</li>
<li>插件质量参差不齐</li>
<li>UI非常简陋</li>
<li>只支持对接一个代码仓库管理系统</li>
</ul>
<h4 id="GitlabCI"><a href="#GitlabCI" class="headerlink" title="GitlabCI"></a>GitlabCI</h4><p>GitLab CI &#x2F; CD是GitLab的一部分,gitlab 8.0版本开始新增的功能，是用Ruby和Go语言编写的。根我们通常的CI系统不一样通常的是一个master-slave架构，即使没有slave，master一样可以做CI，slave只是做为一个压力分担功能，gitlab是gitlab-server本身是不执行的，是通过api与GitLab Runner交互让gitlab-runner去执行CI。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_11.png"></p>
<p>GitLab Runner是一个go语言编写程序，它可以运行在任何可以运行go环境的平台上(二进制包、docker、k8s)<br>支持的代码仓库<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_12.png"><br>易用性<br>同样也是Everything as docker<br>Gitlab CI 整个流程和 Drone 以及流行的 Travis CI 都是比较类似的，通过在项目中添加一个 .gitlab-ci.yml 的配置文件，配置文件中描述构建流水线来执行任务，对不同编程语言的编译通过不同的docker image实现。<br>CI的执行是通过在代码仓库内定义个.drone.yml的文件,里面编写好对应的pipeline然后drone会去读取这个文件并执行。<br>让CI工作所需的步骤可归纳为</p>
<ul>
<li>添加.gitlab-ci.yml到存储库的根目录</li>
<li>配置一个Runner</li>
</ul>
<p>Runner分两种类型，<br>Specific Runners：单个项目独享<br>Shared Runners：整个gitlab共享</p>
<p>Runner也可以选择CI方式：docker, parallels, shell, kubernetes, docker-ssh, ssh, virtualbox, docker+machine, docker-ssh+machine。如果是容器，代码编译的容器镜像需要提前在runner配置文件里面定义好。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">concurrent = 1</span><br><span class="line">check_interval = 0</span><br><span class="line">[session_server]</span><br><span class="line">  session_timeout = 1800</span><br><span class="line">[[runners]]</span><br><span class="line">  name = &quot;49faa2f8e25b&quot;</span><br><span class="line">  url = &quot;http://47.104.182.1:1080/&quot;</span><br><span class="line">  token = &quot;6ca2241a2924bd8763137d3bebf1c4&quot;</span><br><span class="line">  executor = &quot;docker&quot;</span><br><span class="line">  [runners.docker]</span><br><span class="line">    tls_verify = false</span><br><span class="line">    image = &quot;golang:1.10&quot;</span><br><span class="line">    privileged = false</span><br><span class="line">    disable_entrypoint_overwrite = false</span><br><span class="line">    oom_kill_disable = false</span><br><span class="line">    disable_cache = false</span><br><span class="line">    volumes = [&quot;/var/run/docker.sock:/var/run/docker.sock&quot;, &quot;/cache&quot;]</span><br><span class="line">    shm_size = 0</span><br><span class="line">  [runners.cache]</span><br><span class="line">     Insecure = false</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>.gitlab-ci.yaml默认build， test和deploy三个stage<br>.gitlab-ci.yaml pipeline as code</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_13.png"></p>
<p>当前项目下CI的执行记录。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_14.png"></p>
<p>整个pipeline的流程。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_15.png"></p>
<p>pipeline中的单个job执行过程详细信息输出，同时可以针对但个job进行重复执行。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_16.png"></p>
<p>项目CICD信息统计，按年、月、周汇总，报表展示<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_17.png"></p>
<p>能直接对接现有Kubernetes集群，runner部署及pipeline CD流程。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_18.png"></p>
<p>插件生态<br>优点：</p>
<ul>
<li>根gitlab集成度非常高</li>
<li>不需要部署有gitlab&gt;&#x3D;8.0 就能直接使用</li>
<li>runner支持Autoscale</li>
<li>UI可视化，可操作性强，可针对但个流程进行重复执行及报表展示</li>
<li>CI完全属于你这个代码库</li>
</ul>
<p>缺点：</p>
<ul>
<li>没有插件，对接第三方系统需要自己实现</li>
<li>只能支持gitlab代码仓库</li>
</ul>
<h4 id="Jenkins"><a href="#Jenkins" class="headerlink" title="Jenkins"></a>Jenkins</h4><p><a href="https://jenkins.io/">https://jenkins.io/</a><br>Jenkins是一款java开发的功能强大的CI工具,其前身为oracle的Hudson (软件)项目，2011年正式独立出来，Jenkins也是目前非常老牌和主流的CI工具，最早只能支持java语言，后续通过各类语言插件实现多种编程语言支持，Jenkins也是目前插件种类最丰富的CI工具。</p>
<p>易用性：<br>万物基于插件<br>提供功能强大的UI，大部分操作都可以通过UI完成，但UI样式不符合当前审美于是Jenkins又开发一个Blueocean皮肤插件<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_19.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_20.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_21.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_22.png"></p>
<p>对不同编程语言、docker、k8s、代码仓库对接都是通过插件方式去实现支持，当然也可以直接在Jenkins宿主机上安装好，直接通过shell方式调用。<br>对代码仓库对Jenkins的对接，需要手动配置token，触发需要手动配置webhook和对应的触发事件。<br>支持master-slave的架构方式，同时也能通过对接Kubernetes实现一个slave的动态创建过程，当然此时你可以封装不同编程语言的编译环境镜像，或在一个镜像内实现全部的编程语言。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_23.png"></p>
<p>这样可以大节省资源和多语言环境下部署slave的难度。<br>Jenkins根drone和gitlab CI不一样，它拥有自己的用户认证系统，当然也可以去对接LDAP或AD。这样带来的问题是，权限管理很麻烦。<br>Jenkins可以通过UI去配置CI，也可以通过Jenkinsfile是Groovy语言语法<br>以下为例<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_24.png"></p>
<p>代码仓库支持<br>支持常见代码仓库</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_25.png"></p>
<p>插件生态<br>非常丰富的插件，覆盖面非常全<br><a href="https://plugins.jenkins.io/">https://plugins.jenkins.io/</a></p>
<p>自定义插件学习成本较大，需要掌握JAVA语言<br>总结：<br>优点：</p>
<ul>
<li>既有功能完善的UI，也支持pipeline as code</li>
<li>老牌CI工具文档很全面</li>
<li>插件生态丰富，基本上想要对接的工具都能找到对应插件</li>
<li>支持同时对接多个不同代码仓库</li>
</ul>
<p>缺点：</p>
<ul>
<li>对容器、k8s，代码仓库对接配置比较烦索</li>
<li>自定义插件难度大</li>
<li>独立的用户权限管理系统，多个开发团队共享一个master，会导致权限配置很困难，但若每个团队用各自Jenkins，又容易导致很多重复性工作</li>
</ul>
<h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_cicd_26.png"></p>
<p>没有最好的工具，只有最合适的应用场景。</p>
<p>小团队，用的代码管理软件是gitlab，容器编排工具是Kubernetes建议用Gitlab-CI或Drone，开箱即用，可以减少很多工作量。</p>
<p>对插件有强烈需求，并且喜欢UI操作流水线的建议用Jenkins。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://mp.weixin.qq.com/s?__biz=MzIzNjUxMzk2NQ==&amp;mid=2247489519&amp;idx=1&amp;sn=4b42ec3c14b7ba8e21c741413d7bd6e4&amp;chksm=e8d7e82ddfa0613b85cf828443311af52890571052c6b6dce911c1cfe4e814dde28f8f98e086&amp;scene=27#wechat_redirect</span><br><span class="line">https://www.itread01.com/content/1531239621.html</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>CI/CD</category>
      </categories>
      <tags>
        <tag>CI/CD</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes集群节点资源预留</title>
    <url>/2018/01/26/k8s_resource_resver/</url>
    <content><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>默认kubelet没配置资源预留应用没做应用资源限制情况下，那host上所有资源都是可以给pod调配使用的，这样很容易引起集群雪崩效应，比如集群内有一台上跑的pod没做resource limt导致占用资源过大导致将宿主机压死了，此时这个节点在kubernetes内就是一个no ready的状态了，kubernetes会将这台host上所有的pod在其他节点上重建，也就意味着那个有问题的pod重新跑在其他正常的节点上，将另外正常的节点压跨。循怀下去直到集群内所有主机都挂了，这就是集群雪崩效应。</p>
<p>如何避免？<br>在kubernetes中可以通过给kubelet配置参数预留资源给系统进程和kubernetes进程保证它们稳定运行。目前能实现到cpu、memory、ephemeral-storage层面的资源预留。<br>重点提两点<br>cpu：cpu是配置cpu shares实际上对应的是cpu的优先级，简单来说，这个在cpu繁忙时，它能有更高优先级获取更多cpu资源。</p>
<p>ephemeral-storage是kubernetes1.8开始引入的一个资源限制的对象，kubernetes 1.10版本中kubelet默认已经打开的了,到目前1.11还是beta阶段，主要是用于对本地临时存储使用空间大小的限制，如对pod的empty dir、&#x2F;var&#x2F;lib&#x2F;kubelet、日志、容器可读写层的使用大小的限制。</p>
<p>配置<br>在讲配置之前我们先了解几个概念</p>
<p>Node capacity：节点总共的资源<br>kube-reserved：给kubernetes进程预留的资源<br>system-reserved：给操作系统预留的资源<br>eviction-threshold：kubelet eviction的阀值<br>allocatable：留给pod使用的资源</p>
<p>node_allocatable&#x3D;Node_capacity-(kube-reserved+system-reserved+hard-eviction)<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_resource_reserved_1.png"></p>
<p>eviction-threshold<br>分两类：<br>1、kube-control-manager周期性的接收kubelet发送过来的心跳，检查所有节点的状态，当节点属于no ready时，驱逐重建上面的pod(默认超时5分钟)。<br>2、kubelet周期性的检查host上的资源，与配置项里面的配置进行比对，达到阀值后，按照优先级驱逐pod。</p>
<p>eviction-threshold实际上是对pod limit_resource的补充，因为limit_resource只能针对单个pod做资源限制，当这个pod达到限制的阀值后，kubelet便会oom_killer掉这个container，而eviction-threshold根据事先设定的Eviction Thresholds来触发Eviction，调用算法筛选出合适的几个pod，kill掉一个或多个pod回收资源，被eviction掉的pod会被kube-scheduler在其他节点重新调度起来。<br>eviction-threshold分两种类型<br>Soft Eviction Thresholds：达到触发值后，并不是马上去驱逐pod，而是等待一个缓冲时间，这个配置参考<br><a href="https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/">https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/</a></p>
<p>Hard Eviction Thresholds：达到触发值后，直接筛选出对应的pod kill掉</p>
<p>两种配置方式<br>绝对值：memory.available&lt;5Gi<br>百分比：memory.available&lt;5%</p>
<h3 id="配置kubelet资源预留"><a href="#配置kubelet资源预留" class="headerlink" title="配置kubelet资源预留"></a>配置kubelet资源预留</h3><p>环境<br>kubernetes：v1.11<br>kubeadm：v1.11<br>os：ubuntu16.04</p>
<p>一台master+2台worker节点</p>
<p>机器配置<br>内存：8175176KB<br>CPU：2vcpu<br>磁盘：40G</p>
<p>resource-reserved依赖cgroup，所以需要提前将kubelet和docker的cgroup-driver对应好<br>在修改配置前，先查看node节点目前可用资源</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl describe node rke-node3</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_resource_reserved_2.png"></p>
<p>这里以cpu、ephemeral-storage、memory为参考，capacity为host本身资源，Allocatable为host实际可分配资源，这里可以看见在没配置资源预留的情况下，Allocatable基本上等于capacity，然后我们配置rke-node3的kubelet</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim /var/lib/kubelet/kubeadm-flags.env</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">--enforce-node-allocatable=pods,kube-reserved,system-reserved --kube-reserved-cgroup=/system.slice/kubelet.service --system-reserved-cgroup=/system.slice --kube-reserved=cpu=1,memory=2Gi, --system-reserved=cpu=500m,memory=1Gi</span><br><span class="line"></span><br><span class="line">enforce-node-allocatable=pods,kube-reserved,system-reserved  #默认为pod设置，但我们这里要给kube进程和system预留所以要加上。</span><br><span class="line">kube-reserved-cgroup=/system.slice/kubelet.service  #kube组件对应cgroup目录</span><br><span class="line">system-reserved-cgroup=/system.slice #系统组件对应cgroup目录</span><br><span class="line">kube-reserved=cpu=1,memory=2Gi #kube组件资源预留大小</span><br><span class="line">system-reserved=cpu=500m,memory=1Gi #系统组件资源预留大小</span><br></pre></td></tr></table></figure>

<p>cpuset和hugetlb subsystem是默认没有初始化system.slice手动创建</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir -p /sys/fs/cgroup/cpuset/system.slice/kubelet.service/</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir -p /sys/fs/cgroup/hugetlb/system.slice/kubelet.service/</span><br></pre></td></tr></table></figure>


<p>配好后重启kubelet</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure>

<p>然后我们在通过kubectl describe node rke-node3查看信息。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_resource_reserved_3.png"></p>
<p>kube-reserved&#x3D;cpu&#x3D;1，system-reserved&#x3D;cpu&#x3D;500m 所以cpu就1.5个了，总共就2个，那么Allocatable就为2000-1500&#x3D;500m了<br>kube-reserved&#x3D;memory&#x3D;2Gi，system-reserved&#x3D;memory&#x3D;1Gi 所以你才为3Gi，8Gi-3GI&#x3D;5Gi&#x3D;5242990Ki</p>
<p>上面也论证了kubelet配置资源预留，实际上是kubelet上报资源时，把预留的值减去了，所以scheduler调度时得到的值就是减后的值。</p>
<p>配置eviction<br>生产环境尽量配置eviction-soft，少用eviction-hard，因为eviction-hard没有缓冲时间，马上去驱逐pod，而eviction-soft可以通过配置eviction-soft-grace-period(达到阀值后等待多久在进行eviction，默认是90s)<br>eviction-hard（默认就配置了eviction-hard&#x3D;memory.available&#x3D;100Mi，所以我们前面没做任何配置Allocatable看见的memory会少100M内存)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">--enforce-node-allocatable=pods,kube-reserved,system-reserved --kube-reserved-cgroup=/system.slice/kubelet.service --system-reserved-cgroup=/system.slice --kube-reserved=cpu=1,memory=2Gi, --system-reserved=cpu=500m,memory=1Gi, --eviction-hard=memory.available&lt;10%,nodefs.available&lt;10%,imagefs.available&lt;10%</span><br></pre></td></tr></table></figure>

<p> 配置了eviction-hard的话，</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Allocatable=Node_capacity-(kube-reserved+system-reserved+eviction-threshold)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">--enforce-node-allocatable=pods,kube-reserved,system-reserved --kube-reserved-cgroup=/system.slice/kubelet.service --system-reserved-cgroup=/system.slice --kube-reserved=cpu=1,memory=2Gi, --system-reserved=cpu=500m,memory=1Gi, --eviction-soft=memory.available&lt;10%,nodefs.available&lt;10%,imagefs.available&lt;10% --eviction-soft-grace-period=memory.available=2m,nodefs.available=2m,imagefs.available=2m</span><br></pre></td></tr></table></figure>

<p>配置了eviction-soft的话，</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Allocatable=Node_capacity-(kube-reserved+system-reserved)</span><br></pre></td></tr></table></figure>


<p>最佳实践<br>1、生产环境中，建议资源预留+应用资源限制结合使用，因为如果单纯只做资源预留的，没做应用资源限制或配置eviction，如果应用运行了一段时间的pod突然资源使用暴涨，就会触发host oomkill。<br>2、在kubernetes中资源分为可压缩资源和不可压缩资源 ，如我们常见的cpu是可压缩资源、内存、磁盘资源是不可压缩资源。建议只针对不可压缩资源进行资源预留。<br>3、建议给系统预留10%的内存，kubernetes组件预留3%~5%的内存，磁盘预留10%。     </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://www.kubernetes.org.cn/1150.html</span><br><span class="line">http://wsfdl.com/kubernetes/2017/12/18/k8s资源预留.html</span><br><span class="line">https://my.oschina.net/jxcdwangtao/blog/1629059</span><br><span class="line">http://www.sohu.com/a/231155920_268033</span><br><span class="line">https://www.kubernetes.org.cn/4022.html</span><br><span class="line">http://licyhust.com/容器技术/2017/10/24/eviction/</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>kdump</title>
    <url>/2017/09/21/kdump/</url>
    <content><![CDATA[<h3 id="什么是kdump"><a href="#什么是kdump" class="headerlink" title="什么是kdump?"></a>什么是kdump?</h3><p>kdump 是一种的基于 kexec 的内核崩溃转储机制，类似飞机的黑匣子，系统一但崩溃，内核无法正常记录信息了，这时kdump将转入带第二个捕获内核，将第二个内核加载的内存中，对第一个内核的信息进行捕获。由于 kdump 利用 kexec 启动捕获内核，绕过了 BIOS，所以第一个内核的内存得以保留。这是内核崩溃转储的本质。<br>kexec是一个快速启动机制，可以通过已经运行的内核，启动另外一个内核不需要经过bios</p>
<h3 id="kdump原理？"><a href="#kdump原理？" class="headerlink" title="kdump原理？"></a>kdump原理？</h3><p>kdump 需要两个不同目的的内核，生产内核和捕获内核。生产内核是捕获内核服务的对像。捕获内核会在生产内核崩溃时启动起来，与相应的 ramdisk 一起组建一个微环境，用以对生产内核下的内存进行收集和转存。</p>
<p>kdump配置和使用<br>操作系统:centos7.2  </p>
<p>安装配置分析工具crash<br>yum install crash</p>
<p>安装kernel-debuginfo(需要根内核版本一一对应)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget http://debuginfo.centos.org/7/x86_64/kernel-debuginfo-common-x86_64-3.10.0-327.el7.x86_64.rpm</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget http://debuginfo.centos.org/7/x86_64/kernel-debuginfo-3.10.0-327.el7.x86_64.rpm</span><br><span class="line">````</span><br><span class="line"></span><br><span class="line">下载完成后安装</span><br></pre></td></tr></table></figure>
<p>rpm -ivh kernel-debuginfo-common-x86_64-3.10.0-327.el7.x86_64.rpm kernel-debuginfo-3.10.0-327.el7.x86_64.rpm</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">安装完后确认是否有</span><br></pre></td></tr></table></figure>
<p>&#x2F;usr&#x2F;lib&#x2F;debug&#x2F;lib&#x2F;modules&#x2F;3.10.0-327.el7.x86_64&#x2F;vmlinux此文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">### 配置kdump</span><br><span class="line">可以配置内核崩溃后崩溃日志存到何地</span><br><span class="line"></span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kdmup_1.png)</span><br><span class="line"></span><br><span class="line">默认是放在本地/var/crash  </span><br><span class="line">下图为配置scp，表示将log文件放到192.168.2.100/log 目录，同时key文件目录需要指定</span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kdmup_2.png)</span><br><span class="line"></span><br><span class="line">### 测试kdump</span><br><span class="line">配置捕获占用的内存</span><br><span class="line">使用grubby修改grub2.cfg文件</span><br></pre></td></tr></table></figure>
<p>grubby –update-kernel&#x3D;DEFAULT –args&#x3D;crashkernel&#x3D;128M</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">重启服务器  </span><br><span class="line">reboot  </span><br><span class="line"></span><br><span class="line">启动kdump  </span><br></pre></td></tr></table></figure>
<p>systemctl start kdump  &amp;&amp; systemctl enable kdump</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ls /boot 会发现生成了一个kdump结尾的文件</span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kdmup_3.png)</span><br><span class="line"></span><br><span class="line">执行以下命令让内核crash  </span><br></pre></td></tr></table></figure>
<p>echo 1 &gt; &#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;sysrq<br>echo c &gt; &#x2F;proc&#x2F;sysrq-trigger</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">此时系统会立刻失去连接进入捕获内核</span><br><span class="line"></span><br><span class="line">开机后</span><br><span class="line"></span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kdmup_4.png)</span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kdmup_5.png)</span><br><span class="line"></span><br><span class="line">以下参考IBM文档链接见文尾  </span><br><span class="line"></span><br><span class="line">### crash解析崩溃日志</span><br><span class="line"></span><br><span class="line">vmcore-dmesg.txt可以查看错误信息  </span><br><span class="line">(1) 错误类型  </span><br><span class="line">首先可以在vmcore-dmesg.txt中先查看错误类型，如：  </span><br><span class="line">divide error: 0000 [#1] SMP，除数为0造成内核崩溃，由1号CPU触发。</span><br><span class="line"> BUG: unable to handle kernel NULL pointer dereference at 000000000000012c，引用空指针。</span><br><span class="line">这样一来就能知道引发内核崩溃的错误类型。  </span><br><span class="line"></span><br><span class="line">(2) 错误地点  </span><br><span class="line">RIP为造成内核崩溃的指令，Call Trace为函数调用栈，通过RIP和Call Trace可以确定函数的调用路径，以及在</span><br><span class="line">哪个函数中的哪条指令引发了错误。</span><br><span class="line"></span><br><span class="line">例如RIP为：[``&lt;ffffffff812cdb54&gt;``] ? tcp_enter_loss+0x1d3/0x23b</span><br><span class="line"></span><br><span class="line">[``&lt;ffffffff812cdb54&gt;``]是指令在内存中的虚拟地址。</span><br><span class="line">tcp_enter_loss是函数名(symbol)。</span><br><span class="line">0x1d3是这条指令相对于tcp_enter_loss入口的偏移，0x23b是函数编译成机器码后的长度。</span><br><span class="line">这样一来就能确定在哪个函数中引发了错误，以及错误的大概位置。</span><br><span class="line"></span><br><span class="line">Call Trace为函数的调用栈，是从下往上看的。可以用来分析函数的调用关系。</span><br><span class="line"></span><br><span class="line">### vmcore信息</span><br></pre></td></tr></table></figure>
<p>crash &#x2F;usr&#x2F;lib&#x2F;debug&#x2F;lib&#x2F;modules&#x2F;3.10.0-327.el7.x86_64&#x2F;vmlinux  &#x2F;var&#x2F;crash&#x2F;127.0.0.1-2017-09-20-13:09:30&#x2F;vmcore</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kdmup_6.png)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>KERNEL: 系统崩溃时运行的 kernel 文件<br>DUMPFILE: 内核转储文件<br>CPUS: 所在机器的 CPU 数量<br>DATE: 系统崩溃的时间<br>TASKS: 系统崩溃时内存中的任务数<br>NODENAME: 崩溃的系统主机名<br>RELEASE: 和 VERSION: 内核版本号<br>MACHINE: CPU 架构<br>MEMORY: 崩溃主机的物理内存<br>PID:3507表示当崩溃时3507这个bash的进程在操作<br>PANIC: 崩溃类型，常见的崩溃类型包括：<br>SysRq (System Request)：通常是测试使用。通过 echo c &gt; &#x2F;proc&#x2F;sysrq-trigger，就可以触发系统崩溃。<br>oops：可以看成是内核级的 Segmentation Fault。应用程序如果进行了非法内存访问或执行了非法指令，会得到 Segfault 信号，一般行为是 coredump，应用程序也可以自己截获 Segfault 信号，自行处理。如果内核自己犯了这样的错误，则会弹出 oops 信息。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">bt(backtrace)  </span><br><span class="line">显示内核堆栈信息</span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kdmup_7.png)</span><br><span class="line"></span><br><span class="line">如上输出中，以“# 数字”开头的行为调用堆栈，即系统崩溃前内核依次调用的一系列函数，通过这个可以迅速推断内核在何处崩溃。</span><br><span class="line"></span><br><span class="line">log - dump system message buffer  </span><br><span class="line">log 命令可以打印系统消息缓冲区，从而可能找到系统崩溃的线索。log 命令的截图如下（为节省篇幅，已将部分行省略）：</span><br><span class="line"></span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kdmup_8.png)</span><br><span class="line"></span><br><span class="line">ps 命令用于显示进程的状态，（如图）带 &gt; 标识代表是活跃的进程。ps 命令的截图如下（省略部分行）：</span><br><span class="line"></span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kdmup_9.png)</span><br><span class="line"></span><br><span class="line">dis - disassembling instruction</span><br><span class="line">dis 命令用于对给定地址的内容进行反汇编。dis 命令的截图如下：</span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kdmup_10.png)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>参考链接<br><a href="http://www.361way.com/centos-kdump/3751.html">http://www.361way.com/centos-kdump/3751.html</a><br><a href="http://blog.csdn.net/zhangskd/article/details/38084337">http://blog.csdn.net/zhangskd/article/details/38084337</a><br><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-kdump4/index.html">https://www.ibm.com/developerworks/cn/linux/l-cn-kdump4/index.html</a></p>
<pre><code>
</code></pre>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>kolla 3.0.3部署openstack newton</title>
    <url>/2017/02/09/killo_newton/</url>
    <content><![CDATA[<h3 id="kolla是什么？"><a href="#kolla是什么？" class="headerlink" title="kolla是什么？"></a>kolla是什么？</h3><p>容器化部署openstack，简而言之就是openstack里面每个组件都是用docker封装好了，对应docker的一个image。</p>
<h3 id="容器化好处是什么"><a href="#容器化好处是什么" class="headerlink" title="容器化好处是什么?"></a>容器化好处是什么?</h3><p>通过docker，降低openstack升级的难度，使升级对openstack影响最小，一旦升级失败，直接回滚旧的docker image，升级只需要三步：Pull新版本的容器镜像，停止老版本的容器服务，然后启动新版本容器。回滚也不需要重新安装包了，直接启动老版本容器服务就行，非常方便。对比过目前生产环境主流的部署方式，fuel、puppet、ROD，  我个人认为容器化部署将是未来的趋势。kolla底层通过ansoble去启动配置  已经封装好了的docker image。</p>
<h3 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h3><p>要熟悉kolla，不仅仅是要熟悉openstack那么简单了，还要熟悉docker、和ansiable。</p>
<p>环境<br>cpu：8核<br>内存：16G<br>根分区大小：100G<br>至少需要2个网络接口（一个管理口，一个外网口）<br>管理地址 192.168.122.77<br>haproxy vip：192.168.122.76  </p>
<p>安装时注意审查软件版本，我这里是安装newton版对应的是koll 3  ，2月24日发布的ocata版对应的是kolla 4<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/killo_newton_1.png"></p>
<p>搭建docker本地镜像库</p>
<p>安装epel源和python-pip</p>
<p>安装docker<br>curl -sSL <a href="https://get.docker.io/">https://get.docker.io</a> | bash</p>
<p>查看docker版本，确认docker是否安装成功。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/killo_newton_2.png">  </p>
<p>修改docker参数，如果没有修改的话，会造成部署 neutron-dhcp-agent container 和访问APIError&#x2F;HTTPError  </p>
<p>mkdir -p &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service.d</p>
<p>vi &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service.d&#x2F;koll.conf  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[Service]</span><br><span class="line">MountFlags=shared</span><br><span class="line">EnvironmentFile=/etc/sysconfig/docker</span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/docker daemon $other_args</span><br></pre></td></tr></table></figure>

<p>保存退出<br>修改&#x2F;etc&#x2F;sysconfig&#x2F;docker参数并添加下面一行，目的是为了配置本地镜像仓库。<br><code>other_args=&quot;--insecure-registry 192.168.122.77:4000&quot;  </code></p>
<p>重启docker进程<br>systemctl daemon-reload<br>systemctl enable docker<br>systemctl restart docker  </p>
<h3 id="制做docker本地镜像仓库"><a href="#制做docker本地镜像仓库" class="headerlink" title="制做docker本地镜像仓库"></a>制做docker本地镜像仓库</h3><p>链接：<a href="http://pan.baidu.com/s/1gf5LTV9">http://pan.baidu.com/s/1gf5LTV9</a> 密码：aysu<br>下载解压<br>将已经build好的openstack镜像解压到本地<br>tar -xvf kolla-image-newton-latest.tgz</p>
<p>加载下载好的docker registry，docker搭建私有镜像仓库使用registry这个软件<br>docker load &lt; .&#x2F;registry-server.tar<br>将镜像文件放到&#x2F;home&#x2F;下<br><code> docker run -d -p 4000:5000 --restart=always -e REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY=/tmp/registry -v /home/tmp/registry/:/tmp/registry  --name registry registry:2</code></p>
<p>测试是否搭建本地镜像仓库成功<br>curl -XGET <a href="http://127.0.0.1:4000/v2/_catalog">http://127.0.0.1:4000/v2/_catalog</a> #正常情况会输出很多image name。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/killo_newton_3.png"><br>仓库里面存在的镜像<br>查看该镜像的tag<br><code>curl -XGET http://127.0.0.1:4000/v2/kolla/centos-binary-nova-compute/tags/list</code></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/killo_newton_4.png"></p>
<p>3.0.3是版本号<br>查看仓库数据  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/killo_newton_5.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/killo_newton_6.png"></p>
<p>安装ansible<br>yum install ansible<br>clone newton分支<br><code>git clone -b stable/newton https://github.com/openstack/kolla</code><br>cd kolla<br>cp -r etc&#x2F;kolla &#x2F;etc&#x2F;  </p>
<p>安装kolla<br>pip install kolla  </p>
<p>vim &#x2F;etc&#x2F;kolla&#x2F;globals.yml  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">openstack_release: &quot;3.0.3&quot; #上面搭建的本地仓库里面curl出来的tag号，写错了的话会导致找不到image。</span><br><span class="line"></span><br><span class="line">network_interface: &quot;ens3&quot;</span><br><span class="line">kolla_internal_vip_address: &quot;192.168.123.76&quot;#配置给高可用的vip</span><br><span class="line">neutron_external_interface: &quot;eth0&quot;</span><br><span class="line"></span><br><span class="line">docker_registry: &quot;192.168.122.77:4000&quot; #本地仓库地址</span><br><span class="line">docker_namespace: &quot;kolla&quot;</span><br></pre></td></tr></table></figure>
<p>生成密码文件<br> kolla-genpwd<br>路径<br>&#x2F;etc&#x2F;kolla&#x2F;password.yml  </p>
<p>执行部署<br>cd &#x2F;root&#x2F;kolla&#x2F;tools&#x2F;<br>.&#x2F;kolla-ansible deploy -i &#x2F;root&#x2F;kolla&#x2F;ansible&#x2F;inventory&#x2F;all-in-one<br>这里需要注意的是，我是部署all-in-one，如果需要部署多节点的用multinode 修改一下里面的hostname<br>部署完成查看容器<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/killo_newton_7.png"><br>pip install  python-openstackclient<br>pip  install python-neutronclient  </p>
<p>输入以下命令生成一个环境变量文件<br>kolla-ansible post-deploy  </p>
<p>文件路径为<br>&#x2F;etc&#x2F;kolla&#x2F;admin-openrc.sh  </p>
<p>cp &#x2F;etc&#x2F;kolla&#x2F;admin-openrc.sh &#x2F;root&#x2F;  </p>
<p>source &#x2F;root&#x2F;admin-openrc.sh  </p>
<p>查看nova service<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/killo_newton_8.png"><br>查看neutron agent<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/killo_newton_9.png"></p>
<p>打开控制台访问</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/killo_newton_10.png"><br>帐户admin，密码通过刚刚生成的admin-openrc.sh获取  </p>
<p>执行<br>cd &#x2F;usr&#x2F;share&#x2F;kolla  </p>
<p>.&#x2F;init-runonce #一个测试脚本，自动下载镜像，上传，创建网络，创建路由器……  </p>
<p>最后创建虚拟机  </p>
<p>需要注意的是如果是在虚拟机中测试kolla需要在宿主机上修改nova-compute的配置文件 为virt_type&#x3D;qemu不然默认用的是kvm，会造成创建云主机失败。<br>vim &#x2F;etc&#x2F;kolla&#x2F;nova-compute&#x2F;nova.conf<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/killo_newton_11.png"></p>
<p>重启这个容器。<br>docker restart nova_compute<br>创建云主机测试<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/killo_newton_12.png"></p>
<p>最后</p>
<p>Docker使用heka来展现收集到的日志信息。这些openstack容器的log都在heka 容器内展现<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/killo_newton_13.png"><br>默认是没有安装cinder和其他一些软件的，如果需要安装在部署时可以修改&#x2F;etc&#x2F;kolla&#x2F;globals.yml<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/killo_newton_14.png">  </p>
<p>参考链接<br><a href="http://blog.csdn.net/u011211976/article/details/52085891">http://blog.csdn.net/u011211976/article/details/52085891</a><br><a href="http://docs.openstack.org/developer/kolla-ansible/quickstart.html">http://docs.openstack.org/developer/kolla-ansible/quickstart.html</a><br><a href="http://geek.csdn.net/news/detail/60805?utm_source=tuicool&utm_medium=referral">http://geek.csdn.net/news/detail/60805?utm_source=tuicool&amp;utm_medium=referral</a>  </p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes亲和性调度</title>
    <url>/2018/08/12/kubernetes_affinty/</url>
    <content><![CDATA[<h3 id="概述："><a href="#概述：" class="headerlink" title="概述："></a>概述：</h3><p>默认情况下创建pod调度是根据kubernetes scheduler默认的调度规则去进行调度的，但有些时候，有些应用有一些特殊的需求比如指定部署到对应的节点、多个pod之间需要部署在不同一个节点，需要互斥、pod和pod间相互交流比较频繁需要跑在同一个节点，需要亲和。这时就需要灵活配置scheduler来实现了。</p>
<h3 id="场景一：调度到一组具有相同特性的主机上（label-nodeSelector"><a href="#场景一：调度到一组具有相同特性的主机上（label-nodeSelector" class="headerlink" title="场景一：调度到一组具有相同特性的主机上（label+nodeSelector)"></a>场景一：调度到一组具有相同特性的主机上（label+nodeSelector)</h3><p>比如在整个kubernetes集群内，有一组配置比较好的服务器，然后有一些特殊的应用需要对硬件配置有需求，需要被调度到这些服务器上，这时可以通过kubernetes的label+nodeSelector去实现，实际上在kubernetes里面label是一个非常灵活的概念，kubernetes内不同资源对象之间的关联就是通过label的方式去进行的。<br>例<br>我们给那些配置高的节点打上个label</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_affinty_1.png"><br>给高配置的host打上一个label Configuration&#x3D;hight（这里以rke-node2为例)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl label node rke-node2 Configuration=hight</span><br></pre></td></tr></table></figure>
<p>在show labels可以看见我们刚刚打上去的label<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_affinty_2.png"></p>
<p>创建pod使用nodeSelector调度到指定label的节点上<br>mysql-deployment.yaml</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line"> name: mysql-2</span><br><span class="line">spec:</span><br><span class="line"> selector:</span><br><span class="line">   matchLabels:</span><br><span class="line">     app: mysql</span><br><span class="line"> template:</span><br><span class="line">   metadata:</span><br><span class="line">     labels:</span><br><span class="line">       app: mysql</span><br><span class="line">   spec:</span><br><span class="line">     containers:</span><br><span class="line">     - image: mysql:5.6</span><br><span class="line">       name: mysql</span><br><span class="line">       env:</span><br><span class="line">       - name: MYSQL_ROOT_PASSWORD</span><br><span class="line">         value: password</span><br><span class="line">       ports:</span><br><span class="line">       - containerPort: 3306</span><br><span class="line">         name: mysql</span><br><span class="line">     nodeSelector:</span><br><span class="line">       Configuration: hight</span><br></pre></td></tr></table></figure>
<p>配置nodeSelector调度到Configuration：hight的host上<br>创建deployment</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f mysql-deployment.yaml</span><br></pre></td></tr></table></figure>
<p>查看pod被调度到label为 Configuration: hight的host上了<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_affinty_3.png"></p>
<h3 id="场景二：部署的应用不想调度到某些节点上（nodeaffinity"><a href="#场景二：部署的应用不想调度到某些节点上（nodeaffinity" class="headerlink" title="场景二：部署的应用不想调度到某些节点上（nodeaffinity)"></a>场景二：部署的应用不想调度到某些节点上（nodeaffinity)</h3><p>比如在一个kubernetes集群内，需要将应用部署到指定标签的机器上，但如果没有的话这类机器的话，就按正常调度规则进行调度。这时通过nodeSelector就无法实现这个需求了，在kubernetes内可以通过nodeaffinity的方式的去实现。</p>
<p>nodeaffinity和nodeSelecotr非常类似，都是node层面的调度策略的配置，但区别是nodeSelect功能比较单一，只能根据label选择对应的主机，而nodeaffinity更加灵活，nodeaffinity有两种调度类型requiredDuringSchedulingIgnoredDuringExecution(硬要求)和preferredDuringSchedulingIgnoredDuringExecution(软要求)，两种类型区别在于，硬要求要求调度时必须满足设置的调度规则，否则抛异常调度失败，而软要求，只是调度时优先考虑设置的调度规则，当达不到设置的规则时，则按kube-scheduler默认的调度策略进行调度。nodeaffinity还支持多种规则匹配条件的配置如<br>In：label 的值在列表内<br>NotIn：label 的值不在列表内<br>Gt：label 的值大于设置的值<br>Lt：label 的值小于设置的值<br>Exists：设置的label 存在<br>DoesNotExist：设置的 label 不存在</p>
<p>例</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line"> name: mysql-2</span><br><span class="line">spec:</span><br><span class="line"> selector:</span><br><span class="line">   matchLabels:</span><br><span class="line">     app: mysql</span><br><span class="line"> template:</span><br><span class="line">   metadata:</span><br><span class="line">     labels:</span><br><span class="line">       app: mysql</span><br><span class="line">   spec:</span><br><span class="line">     containers:</span><br><span class="line">     - image: mysql:5.6</span><br><span class="line">       name: mysql</span><br><span class="line">       env:</span><br><span class="line">       - name: MYSQL_ROOT_PASSWORD</span><br><span class="line">         value: password</span><br><span class="line">       ports:</span><br><span class="line">       - containerPort: 3306</span><br><span class="line">         name: mysql</span><br><span class="line">     affinity:</span><br><span class="line">       nodeAffinity:</span><br><span class="line">         requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">           nodeSelectorTerms:</span><br><span class="line">           - matchExpressions:</span><br><span class="line">             - key: kubernetes.io/hostname</span><br><span class="line">               operator: NotIn</span><br><span class="line">               values:</span><br><span class="line">               - rke-node3</span><br><span class="line">               - rke-node4</span><br><span class="line">         preferredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">         - weight: 1</span><br><span class="line">           preference:</span><br><span class="line">             matchExpressions:</span><br><span class="line">             - key: disk_type</span><br><span class="line">               operator: In</span><br><span class="line">               values:</span><br><span class="line">               - ssd</span><br></pre></td></tr></table></figure>
<p>如上所例，要求pod不能被调度到rke-node3和rke-node4上，但如果有满足dis_type&#x3D;ssd的label的节点则优先选择。<br>如果在 nodeAffinity 类型中指定了多个 nodeSelectorTerms，那么 pod 将会被调度到只要满足其中一个 nodeSelectorTerms 的 node 上。<br>如果在 nodeSelectorTerms 中指定了多个 matchExpressions，那么 pod 将会被调度到 满足所有 matchExpressions 的 node 上。</p>
<h3 id="场景三：部署的应用关联性很强，需要尽量在一个节点上"><a href="#场景三：部署的应用关联性很强，需要尽量在一个节点上" class="headerlink" title="场景三：部署的应用关联性很强，需要尽量在一个节点上"></a>场景三：部署的应用关联性很强，需要尽量在一个节点上</h3><p>比如在kubernetes集群，有些pod和pod之间的交互很频繁，这时就需要将它们尽可能的调度到一台主机上，通过pod与pod之间的关系来选择调度，这时候我们就可以使用podaffinity。根nodeaffinity一样，podaffinity也有两种调度类型requiredDuringSchedulingIgnoredDuringExecution(硬要求)和preferredDuringSchedulingIgnoredDuringExecution(软要求)，多种规则匹配条件配置。<br>例 部署一个nginx和mysql因为nginx和mysql之间的交互很频繁，所以尽量将他们部署在一个host上。<br>先启动一个nginx。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: nginx</span><br></pre></td></tr></table></figure>

<p>启动mysql</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: mysql</span><br><span class="line">  labels:</span><br><span class="line">    app: mysql</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: mysql</span><br><span class="line">    image: nginx</span><br><span class="line">    env:</span><br><span class="line">     - name: &quot;MYSQL_ROOT_PASSWORD&quot;</span><br><span class="line">       value: &quot;123456&quot;</span><br><span class="line">  affinity:</span><br><span class="line">    podAffinity:</span><br><span class="line">      requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">      - labelSelector:</span><br><span class="line">          matchExpressions:</span><br><span class="line">          - key: app</span><br><span class="line">            operator: In</span><br><span class="line">            values:</span><br><span class="line">            - nginx</span><br><span class="line">        topologyKey: kubernetes.io/hostname</span><br></pre></td></tr></table></figure>

<p>topologyKey可以设置成如下几种类型<br>kubernetes.io&#x2F;hostname　　＃Node<br>failure-domain.beta.kubernetes.io&#x2F;zone　＃Zone<br>failure-domain.beta.kubernetes.io&#x2F;region #Region<br>可以设置node上的label的值来表示node的name,zone,region等信息，pod的规则中指定topologykey的值表示指定topology范围内的node上运行的pod满足指定规则</p>
<p>这里我们配置podaffinity，mysql会调度到有app:nginx这个label的pod的host上，所以你部署出来的mysql会调度到有nginx的pod上，因为默认部署出来的nginx就自带app:nginx这个label。</p>
<h3 id="场景四：部署应用需要互斥，不能同时运行在一台主机上，会冲突"><a href="#场景四：部署应用需要互斥，不能同时运行在一台主机上，会冲突" class="headerlink" title="场景四：部署应用需要互斥，不能同时运行在一台主机上，会冲突"></a>场景四：部署应用需要互斥，不能同时运行在一台主机上，会冲突</h3><p>我们继续以我们刚刚的nginx+mysql这个应用组合为例，现在我们修改mysql的podaffinity为podAntiAffinity</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: mysql</span><br><span class="line">  labels:</span><br><span class="line">    app: mysql</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: mysql</span><br><span class="line">    image: nginx</span><br><span class="line">    env:</span><br><span class="line">     - name: &quot;MYSQL_ROOT_PASSWORD&quot;</span><br><span class="line">       value: &quot;123456&quot;</span><br><span class="line">  affinity:</span><br><span class="line">    podAntiAffinity:</span><br><span class="line">      requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">      - labelSelector:</span><br><span class="line">          matchExpressions:</span><br><span class="line">          - key: app</span><br><span class="line">            operator: In</span><br><span class="line">            values:</span><br><span class="line">            - nginx</span><br><span class="line">        topologyKey: kubernetes.io/hostname</span><br></pre></td></tr></table></figure>
<p>将podaffinity改为podAntiAffinity这样调度mysql这个pod时会避开有label为app:nginx的pod的主机</p>
<h3 id="高级特性Taints-and-tolerations"><a href="#高级特性Taints-and-tolerations" class="headerlink" title="高级特性Taints and tolerations"></a>高级特性Taints and tolerations</h3><p>Taints其实根nodeaffinity正好相反，nodeaffinity是将设置策略的pod调度到期望的节点上，而taints正好相反，如果一个节点被标记为taints，除非pod配置了tolerations，否则是不会被允许调度过来在生产环境中我们一般会将master节点配置Taints，因为master只跑kubernetes 系统组件，如果跑了用户应用pod容易把资源耗尽，造成master节点崩溃，当然后期如果要添加额外的系统组件，这时就可以通过给对应的pod配置toleration。</p>
<p>设置rke-node2不能被调度</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl taint nodes rke-node2 key=value:NoSchedule</span><br></pre></td></tr></table></figure>
<p>value可以配置多个值如<br>NoSchedule：不能调度，当之前调度的不管。<br>PreferNoSchedule：尽量不调度上去，其实就是Noschedule软策略版。<br>NoExecute：不能调度，但之前已经调度上去的也会自动迁移走。</p>
<p>NoSchedule更像执行kubectl cordon xxx<br>NoExecute更想执行kubectl cordon xxx+kubectl drain <node></p>
<p>取消taints</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl taint nodes rke-node2 key-</span><br></pre></td></tr></table></figure>
<p>如何让pod调度到配置了taints主机上?<br>通过配置tolerations，可以让pod调度到配置了taints的机器上<br>如</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line"> name: mysql-2</span><br><span class="line">spec:</span><br><span class="line"> selector:</span><br><span class="line">   matchLabels:</span><br><span class="line">     app: mysql</span><br><span class="line"> template:</span><br><span class="line">   metadata:</span><br><span class="line">     labels:</span><br><span class="line">       app: mysql</span><br><span class="line">   spec:</span><br><span class="line">     containers:</span><br><span class="line">     - image: mysql:5.6</span><br><span class="line">       name: mysql</span><br><span class="line">       env:</span><br><span class="line">       - name: MYSQL_ROOT_PASSWORD</span><br><span class="line">         value: password</span><br><span class="line">       ports:</span><br><span class="line">       - containerPort: 3306</span><br><span class="line">         name: mysql</span><br><span class="line">     tolerations:</span><br><span class="line">     - key: &quot;key&quot;</span><br><span class="line">       operator: &quot;Equal&quot;</span><br><span class="line">       value: &quot;value&quot;</span><br><span class="line">       effect: &quot;NoSchedule&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><a href="https://blog.csdn.net/horsefoot/article/details/72827790">https://blog.csdn.net/horsefoot/article/details/72827790</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes pod的弹性伸缩(一)(基于CPU、memory的弹性伸缩)</title>
    <url>/2018/07/20/kubernetes_hpa/</url>
    <content><![CDATA[<h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>HPA是kubernetes里面pod弹性伸缩的实现,它能根据设置的监控阀值进行pod的弹性扩缩容，目前默认HPA只能支持cpu和内存的阀值检测扩缩容，但也可以通过custom metric api  调用prometheus实现自定义metric 来更加灵活的监控指标实现弹性伸缩。但hpa不能用于伸缩一些无法进行缩放的控制器如DaemonSet。这里我们用的是resource metric api.</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kube_hpa_1.png"></p>
<h3 id="实现hpa的两大关键"><a href="#实现hpa的两大关键" class="headerlink" title="实现hpa的两大关键"></a>实现hpa的两大关键</h3><p>1、监控指标的获取<br>早期kubernetes版本是使用hepster，在1.10后面版本更推荐使用metric-server</p>
<p>hepster简单来说是api-server获取节点信息，然后通过kubelet获取监控信息，因为kubelet内置了cadvisor。</p>
<p>metric-server，简单来说是通过metric-api来获取节点信息和监控信息。<a href="https://github.com/kubernetes-incubator/metrics-server">https://github.com/kubernetes-incubator/metrics-server</a></p>
<p>2、伸缩判定算法<br>HPA通过定期（定期轮询的时间通过–horizontal-pod-autoscaler-sync-period选项来设置，默认的时间为30秒）查询pod的状态，获得pod的监控数据。然后，通过现有pod的使用率的平均值跟目标使用率进行比较。<br>pod的使用率的平均值：<br>监控资源1分钟使用的平均值&#x2F;设定的每个Pod的request资源值</p>
<p>扩容的pod数计算公式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">TargetNumOfPods = ceil(sum(CurrentPodsCPUUtilization) / Target)</span><br></pre></td></tr></table></figure>
<p>celi函数作用：<br>返回大于或者等于指定表达式的最小整数</p>
<p>在每次扩容和缩容时都有一个窗口时间，在执行伸缩操作后，在这个窗口时间内，不会在进行伸缩操作，可以理解为类似等一下放技能的冷却时间。默认扩容为3分钟(–horizontal-pod-autoscaler-upscale-delay)，缩容为5分钟(–horizontal-pod-autoscaler-downscale-delay)。另外还需要以下情况下才会进行任何缩放avg（CurrentPodsConsumption）&#x2F; Target下降9%，进行缩容，增加至10%进行扩容。以上两条件需要都满足。</p>
<p>这样做好处是：<br>1、判断的精度高，不会频繁的扩缩pod，造成集群压力大。<br>2、避免频繁的扩缩pod，防止应用访问不稳定。  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kube_hpa_2.png"></p>
<p>实现hpa的条件：<br>1、hpa不能autoscale daemonset类型control<br>2、要实现autoscale，pod必须设置request  </p>
<h3 id="配置HPA"><a href="#配置HPA" class="headerlink" title="配置HPA"></a>配置HPA</h3><p>这里以kubeadm 部署和的kubernetes 1.11和Rancher2.0部署的kubernetes 1.10为例<br>环境信息  </p>
<p>操作系统：ubuntu16.04<br>kubernetes版本：1.11<br>rancher：2.0.6<br>metric-server：v0.3.1  </p>
<h4 id="kubeadm方式"><a href="#kubeadm方式" class="headerlink" title="kubeadm方式"></a>kubeadm方式</h4><p>将metric-server从github拉取下来  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/kubernetes-incubator/metrics-server.git -b v0.3.1</span><br></pre></td></tr></table></figure>

<p>早期kubelet的10255端口是开放，但后面由于10255是一个非安全的端口容易被入侵，所以被关闭了。metric-server默认是从kubelet的10255端口去拉取监控信息的，所以这里需要修改从10250去拉取  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">edit  metrics-server/deploy/1.8+/metrics-server-deployment.yaml</span><br></pre></td></tr></table></figure>

<p>添加command的一些配置参数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">containers:</span><br><span class="line">     - name: metrics-server</span><br><span class="line">       image: k8s.gcr.io/metrics-server-amd64:v0.3.1</span><br><span class="line">       command:</span><br><span class="line">        - /metrics-server</span><br><span class="line">        - --kubelet-insecure-tls</span><br><span class="line">        - --kubelet-preferred-address-types=InternalIP</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>apply yaml文件   </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f metrics-server/deploy/1.8+/</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kube_hpa_3.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kube_hpa_4.png"><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kube_hpa_5.png"></p>
<p>等待一分钟<br>执行  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubect top node  </span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl top pods</span><br></pre></td></tr></table></figure>
<p>查看pod和node监控信息  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kube_hpa_6.png"></p>
<p>创建个一个deployment，配置hpa测试  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: podinfo</span><br><span class="line">  labels:</span><br><span class="line">    app: podinfo</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports:</span><br><span class="line">    - port: 9898</span><br><span class="line">      targetPort: 9898</span><br><span class="line">      nodePort: 31198</span><br><span class="line">      protocol: TCP</span><br><span class="line">  selector:</span><br><span class="line">    app: podinfo</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: podinfo</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: podinfo</span><br><span class="line">      annotations:</span><br><span class="line">        prometheus.io/scrape: &#x27;true&#x27;</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: podinfod</span><br><span class="line">        image: stefanprodan/podinfo:0.0.1</span><br><span class="line">        imagePullPolicy: Always</span><br><span class="line">        command:</span><br><span class="line">          - ./podinfo</span><br><span class="line">          - -port=9898</span><br><span class="line">          - -logtostderr=true</span><br><span class="line">          - -v=2</span><br><span class="line">        volumeMounts:</span><br><span class="line">          - name: metadata</span><br><span class="line">            mountPath: /etc/podinfod/metadata</span><br><span class="line">            readOnly: true</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 9898</span><br><span class="line">          protocol: TCP</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            memory: &quot;32Mi&quot;</span><br><span class="line">            cpu: &quot;1m&quot;</span><br><span class="line">          limits:</span><br><span class="line">            memory: &quot;256Mi&quot;</span><br><span class="line">            cpu: &quot;100m&quot;</span><br><span class="line">      volumes:</span><br><span class="line">        - name: metadata</span><br><span class="line">          downwardAPI:</span><br><span class="line">            items:</span><br><span class="line">              - path: &quot;labels&quot;</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.labels</span><br><span class="line">              - path: &quot;annotations&quot;</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.annotations</span><br></pre></td></tr></table></figure>

<p>apply yaml文件<br>配置hpa  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: autoscaling/v2beta1</span><br><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">metadata:</span><br><span class="line">  name: podinfo</span><br><span class="line">spec:</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    apiVersion: extensions/v1beta1</span><br><span class="line">    kind: Deployment</span><br><span class="line">    name: podinfo</span><br><span class="line">  minReplicas: 2</span><br><span class="line">  maxReplicas: 10</span><br><span class="line">  metrics:</span><br><span class="line">  - type: Resource</span><br><span class="line">    resource:</span><br><span class="line">      name: cpu</span><br><span class="line">      targetAverageUtilization: 80</span><br><span class="line">  - type: Resource</span><br><span class="line">    resource:</span><br><span class="line">      name: memory</span><br><span class="line">      targetAverageValue: 200Mi</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>apply yaml文件</p>
<p>get hpa  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kube_hpa_7.png"></p>
<p>测试<br>使用webbench进行压力测试。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget http://home.tiscali.cz/~cz210552/distfiles/webbench-1.5.tar.gz</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tar -xvf webbench-1.5.tar.gz</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd webbench-1.5/</span><br><span class="line">make &amp;&amp; make install</span><br><span class="line"></span><br><span class="line">webbench -c 1000 -t 12360 http://172.31.164.104:31198/</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kube_hpa_8.png"></p>
<p>可以看见随着cpu压力的增加，已经自动scale了，需要注意的是，scale up是一个阶段性的过程，并不是一次性就直接scale到max了，而是一个阶段性的过程，判定算法就是上文介绍的内容。<br>隔断时间没操作压力下来后，自动缩减pod<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kube_hpa_9.png"></p>
<p><a href="https://zhuanlan.zhihu.com/p/34555654">https://zhuanlan.zhihu.com/p/34555654</a><br><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/autoscaling/horizontal-pod-autoscaler.md#autoscaling-algorithm">https://github.com/kubernetes/community/blob/master/contributors/design-proposals/autoscaling/horizontal-pod-autoscaler.md#autoscaling-algorithm</a><br><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-metrics-APIs">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-metrics-APIs</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes入门</title>
    <url>/2018/01/26/kubernetes_base/</url>
    <content><![CDATA[<p>操作系统：centos7.3<br>软件版本：kubernetesV1.9  </p>
<h3 id="初步认识kubernetes"><a href="#初步认识kubernetes" class="headerlink" title="初步认识kubernetes"></a>初步认识kubernetes</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_1.png"><br>What is kubernetes?<br>Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.  </p>
<h4 id="特点："><a href="#特点：" class="headerlink" title="特点："></a>特点：</h4><p>Automating deployment:自动部署<br>Scaling：伸缩<br>Management of containerized applications：管理容器应用  </p>
<p>1、google容器内部容器编排系统borg目前又称omega的开源版，该项目由google 2014年开始启动，kubernetes 1.0版本在2015年7月21日正式发布。<br>2、2017年7月google联合 linux基金会(Linux Foundation)创办了CNCF基金会(Cloud Native Computing Foundation)并将kubernetes种子项目捐献给了CNCF基金会。<br>3、2017年kubernetes总共发布了四个版本，主要是在多用户、多负载、安全性和易用性方面做了改进。分别是：<br>安全性和易用性方面做了改进。分别是：<br>Kubernetes 1.6：伸缩性SLO支持包含5000个节点（15万pod)的集群和动态存储。<br>Kubernetes 1.7：network policy API加强网络安全性。<br>Kubernetes 1.8：角色访问控制功能变为稳定版。<br>Kubernetes 1.9：支持window系统。  </p>
<h4 id="kubernetes能做什么？"><a href="#kubernetes能做什么？" class="headerlink" title="kubernetes能做什么？"></a>kubernetes能做什么？</h4><p>1、容器的自动化部署和升级<br>2、容器的自动伸缩<br>3、以集群的方式部署容器，并提供容器间的负载均衡<br>4、容器的自动修复和健康检查  </p>
<h4 id="为什么使用kubernetes"><a href="#为什么使用kubernetes" class="headerlink" title="为什么使用kubernetes?"></a>为什么使用kubernetes?</h4><p>两个维度<br>背景维度<br>1、大量巨头加入CNCF基金会，人多力量大，出现问题也不怕。<br>2、2017年下半年kubernetes已经完全赢得了容器大战。<br>技术维度<br>3、Kubernetes具备超强的横向扩展能力，只要架构设计的好，甚至可以在线的线性扩展。<br>4、kubernetes采用传统的master-slave架构在container上层又封装了层pod，这样设计非常简单灵活，能更好的适应和管理微服务。  </p>
<h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_2.png"></p>
<h3 id="kubernetes的基本概念"><a href="#kubernetes的基本概念" class="headerlink" title="kubernetes的基本概念"></a>kubernetes的基本概念</h3><p>cluster<br>cluster是计算、存储、网络的资源的集合，kubernetes利用这些资源运行各种基于容器的应用。  </p>
<p>POD<br>pod是kubernetes最小的调度单元，一个pod对应一个或多个container，通常会将紧密相连的一组应用放到一个pod中，同一个pod的container共享ip和namespace。  </p>
<p>为什么使用pod?<br>方便管理<br>在kubernetes中pod是container的载体，一个pod里面拥有一个或多个container，做为一个逻辑单元，方便管理。  </p>
<p>资源共享和通信<br>同一个pod中的container共享一个网络栈和存储，相互之间可以直接通过localhost进行通信，同时也共享同一块存储卷。    </p>
<p>灵活<br>kubernetes直接管理对象是pod，而不是底层的docker，对于docker的操作，被封装在pod中，不会直接操作，这也意味着，pod包含也可以是其他公司的容器产品，比如coreos的rkt或阿里巴巴的pouch。   </p>
<p>ontroller<br>kubernetes使用controller来创建和管理pod，controller中定义了pod的部署特性，比如部署几个副本，在什么样的node上运行。<br>为满足不同的业务需求，kubernetes提供了如下controller。  </p>
<p>Deployment<br>是最常用的 Controller，比如前面就是通过创建 Deployment 来部署应用的。Deployment 可以管理 Pod 的多个副本，并确保 Pod 按照期望的状态运行。  </p>
<p>Replicaset<br>实现了 Pod 的多副本管理。使用 Deployment 时会自动创建 ReplicaSet，也就是说 Deployment 是通过 ReplicaSet 来管理 Pod 的多个副本，我们通常不需要直接使用 ReplicaSet。  </p>
<p>Daemonset<br> 用于每个 Node 最多只运行一个 Pod 副本的场景。正如其名称所揭示的，DaemonSet 通常用于运行 daemon。  </p>
<p>StatefuleSet<br>能够保证 Pod 的每个副本在整个生命周期中名称是不变的。而其他 Controller 不提供这个功能，当某个 Pod 发生故障需要删除并重新启动时，Pod 的名称会发生变化。同时 StatefuleSet 会保证副本按照固定的顺序启动、更新或者删除。  </p>
<p>Job<br>用于运行结束就删除的应用。而其他 Controller 中的 Pod 通常是长期持续运行。  </p>
<p>service<br>直接接通过pod的ip加端口去访问应用是不靠谱的，因为pod的生命周期是不断变化的，每次重新生成的pod ip地址都是都是不一样的，并且当pod有多个副本时，这样的访问就更痛苦了，所以kubernetes通过service来解决这些问题，简单来说，你可以把service理解为一个负载均衡器，也可以说是service是为一组功能相同的pod提供统一入口。<br>Service默认有自己的ip和端口的叫cluster-ip和port，内部可以直接通过这个endpoint(clusterip+port)去访问应用。  不过有一点需要注意，这个cluster-ip是个virtual_IP，它是ping不通的，底层是通过node节点的kube-proxy调用iptables生成对应的转发规则，新版本的kube-proxy可以直接使用ipvs效率更高，不过目前还在测试阶段。<br>namespace<br>这里根linux的namespace不一样，这个更像是一个分组。<br>通过namespace将用户创建的controller和pod等资源分开。namespace可以将一个物理的cluster划成多个虚拟的cluster，每个cluster就是一个namespace，不同的namespace里面资源是完全隔离的。便于不同分组共享使用集群物理资源的同时，还能被管理。使用Namespace来组织Kubernetes的各种对象，可以实现对用户的分组，即“多租户”的管理。对不同的租户还可以进行单独的资源配额设置和管理，使得整个集群的资源配置非常灵活、方便。一个集群中的资源总是有限的，当这个集群被多个租户的应用同时使用时，为了更好地使用这种有限的共享资源，需要将资源配额的管理单元提升到租户级别，通过在不同租户对应的Namespace上设置对应的ResourceQuota即可达到目的。<br>kubernetes默认创建了三个namespace，default、kube-public、kube-system。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_3.png"></p>
<p>default：默认创建的资源将放到这个namespace里面。<br>kube-system：kubernetes自己创建的系统资源将放这里。<br>kube-public：kubernetes自己创建的命名空间，用于确保整个集群中公开的看到某些资源。<br>创建pod时指定namespace  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl --namespace=&lt;insert-namespace-name-here&gt; run nginx --image=nginx  </span><br></pre></td></tr></table></figure>
<p>label<br>Label以key&#x2F;value键值对的形式附加到各种对象上，如Pod、Node等。Service通过selector label建立service和pod的关联，简单来说，资源和资源之间的关联都是通过label，通过给一个资源绑定一个或多个不同的label，实现多维度的资源管理，selector label类似于sql语句的where。  </p>
<h4 id="总结：-1"><a href="#总结：-1" class="headerlink" title="总结："></a>总结：</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_4.png"></p>
<h3 id="kubernetes的架构"><a href="#kubernetes的架构" class="headerlink" title="kubernetes的架构"></a>kubernetes的架构</h3><p>角色上：<br>典型的master-slave架构<br>master节点<br>node节点或叫Minion节点<br>功能上：<br>master节点做为集群大脑<br>node节点用于承载部署的容器应用<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_5.png"></p>
<h4 id="master节点"><a href="#master节点" class="headerlink" title="master节点"></a>master节点</h4><p>master是cluster的大脑，类似openstack的控制节点，运行着apiserver、controller-manager、scheduler它的主要职责是对资源管理、调度，还有认证、弹性伸缩、安全认证。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_6.png"></p>
<p>master节点组件构成<br>etcd<br>kubernetes的后端数据存储系统，cluster中的所有资源对象数据都存储在这，用于配置共享和服务发现。    </p>
<p>kube-apiserver<br>1、整个集群管理的api接口，所有对集群的操作和查询都需要经过api-server。<br>2、集群内各个模块通信的枢钮，集群内其他模块，互相之间并不能直接通信，都是通过api-server打交道，来完成自己那部分工作。<br>3、集群安全控制，apiserver提供了集群的安全验证，和角色权限分配。<br>4、连接etcd，集群内所有组件都不能直接连接etcd只能通过api-server去连接etcd。  </p>
<p>kube-controller-manager<br>1、负责集群的故障检测和修复。<br>2、根据deployment的定义，维持正确的pod副本数。<br>3、根据Service与Pod的管理关系，完成服务的Endpoints对象的创建和更新。<br>4、为新的namespace创建帐户和api访问token。    </p>
<p>kube-scheduler<br>创建pod时，选择合适的node进行调度。<br>kube-dns<br>kubernetes中的域名解析服务器。<br>flannel<br>flannel的网络组件。<br>kube-dashboard<br>kubernetes的web控制端    </p>
<h4 id="node节点"><a href="#node节点" class="headerlink" title="node节点"></a>node节点</h4><p>运行容器应用，由mater管理，接收master节点的各类请求，进行容器的创建和管理，并将运行在上面的容器应用上报到master节点，类似openstack的计算节点。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_7.png"></p>
<h5 id="node节点组件介绍"><a href="#node节点组件介绍" class="headerlink" title="node节点组件介绍"></a>node节点组件介绍</h5><p>kubelet<br>负责node节点容器的创建、删除监控等生命周期管理，上报node信息如cpu、内存、pod的ip等信息导master节点的api-server<br>kube-proxy<br>用于实现端口映射和负载均衡。<br>kube-dns<br>kubernetes的域名解析服务器<br>flannel<br>网络flannel的组件  </p>
<h4 id="根openstack类比"><a href="#根openstack类比" class="headerlink" title="根openstack类比"></a>根openstack类比</h4><p>角色功能上<br>kubernetes分为master节点和node节点，其中master节点是大脑，node节点上承载master调度过来的资源，接收master节点的各类请求。这点根openstack的control节点和compute节点基本一样。<br>组件作用上<br>master节点<br>先从etcd说起，etcd做为kubernetes的后端数据存储系统用于配置共享和服务发现，和openstack中的控制节点的数据库和消息队列服务提供的服务相似。<br>kube-api-server在kubernetes提供集群管理的api接口，角色的权限分配和安全验证，和openstack中的keystone和各类组件的api-server相似。<br>kube-scheduler在kubernetes中为pod选择最合适的node节点。与openstack的nova-scheduler提供功能相似。<br>node节点<br>kubelet在node节点上负责pod的创建和生命周期的管理，同时对宿主机资源使用情况进行上报到master，与openstack的nova-compute所提供的功能非常一致。<br>docker运行容器的引擎根openstack的hypervisor提供的功能一致。kubelet调用docker创建容器，nova-compute调用kvm去创建虚拟机。</p>
<h4 id="总结：-2"><a href="#总结：-2" class="headerlink" title="总结："></a>总结：</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_8.png"></p>
<h3 id="kubernetes的部署"><a href="#kubernetes的部署" class="headerlink" title="kubernetes的部署"></a>kubernetes的部署</h3><h4 id="kubeadm"><a href="#kubeadm" class="headerlink" title="kubeadm"></a>kubeadm</h4><p>kubeadm为kubernetes官方推荐的自动化部署工具，它将kubernetes的组件以pod的形式部署在master节点和node节点上并自动完成证书认证等操作。因为kubeadm默认要从google的镜像仓库下载镜像，所以需要翻墙或提前下好导入。  </p>
<h4 id="源码安装"><a href="#源码安装" class="headerlink" title="源码安装"></a>源码安装</h4><p><a href="https://github.com/kubernetes/kubernetes/tree/release-1.9">https://github.com/kubernetes/kubernetes/tree/release-1.9</a><br>github上下载对应版本的源码包，编译，自己生成证书编写systemd启动文件。<br>推荐先用kubeadm装一遍，熟悉下各个组件，在用源码安装了解组件之间的调用关系。  </p>
<h4 id="总结：-3"><a href="#总结：-3" class="headerlink" title="总结："></a>总结：</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_9.png"></p>
<h3 id="kubernetes的基本操作"><a href="#kubernetes的基本操作" class="headerlink" title="kubernetes的基本操作"></a>kubernetes的基本操作</h3><h4 id="集群操作"><a href="#集群操作" class="headerlink" title="集群操作"></a>集群操作</h4><p>查看集群信息    </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl cluster-info</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_10.png"><br>查看组件健康状态  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get cs</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_11.png"></p>
<p>集群操作  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod --all-namespaces</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_12.png"></p>
<p>查看指定namespace的pod的状态以kube-system为例  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod --namespace=kube-system</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_13.png"></p>
<p>查看pod详细信息  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl describe pod kube-apiserver-master --namespace=kube-system</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_14.png"></p>
<p>查看pod详细run信息  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pods -o wide</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_15.png"></p>
<p>删除service  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl delete service service_name</span><br></pre></td></tr></table></figure>

<p>设置label  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl label node node-1 disktype=ssd  </span><br></pre></td></tr></table></figure>

<p>查看设置的label  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get node --show-labels</span><br></pre></td></tr></table></figure>
<h4 id="应用操作"><a href="#应用操作" class="headerlink" title="应用操作"></a>应用操作</h4><p>例子(参考cloudman<a href="https://steemit.com/kubernetes/@cloudman6/k8s-kubernetes-3">https://steemit.com/kubernetes/@cloudman6/k8s-kubernetes-3</a>)<br>部署应用（kubernetes-bootcamp)  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl run kubernetes-bootcamp --image=docker.io/jocatalin/kubernetes-bootcamp:v1 --port=8080</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_16.png"></p>
<p>查看部署的应用，这里deployment为kubernetes术语，理解为应用。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get deployment</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_17.png"></p>
<p>docker镜像通过–image指定<br>–port设置应用对外服务的端口。</p>
<p>流程:<br>1、kubectl发送应用部署请求到kube-apiserver，kube-apiserver将请求写入etcd中。<br>2、api-server通知kube-controller-manger，创建一个deployment对象。将结果通过api-server写入etcd。<br>3、Scheduler发现后，执行调度流程，为这个新Pod选定一个落户的Node，然后通过API Server将结果写入到etcd中。<br>4、目标节点kubelet进程通过api-server检测这个新生的pod，并按照定义创建pod。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_18.png"></p>
<p>查看当前pod  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pods</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_19.png"></p>
<p>查看 pod实际运行在哪个节点上  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod  -o wide</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_20.png"></p>
<p>访问<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_21.png"></p>
<p>默认情况下使用pod只能在集群内部进行访问，为了能在外部访问容器，需要将容器应用的端口映射到node节点端口。<br>修改kubernetes-bootcamp的端口类型为NodePort，容器内端口为8080，映射的端口会自动从30000~32767中挑选一个。<br>services可以认为是端口映射。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl expose deployment/kubernetes-bootcamp --type=&quot;NodePort&quot; --port=8080</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_22.png"></p>
<p>kubernetes-bootcamp分配到了node-2上，可以直接访问node-2的:31079<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_23.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_24.png"></p>
<p>scale应用（弹性伸缩)<br>当前kubernetes-bootcamp副本数还是为1<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_25.png"></p>
<p>扩展应用为3  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl scale deployment/kubernetes-bootcamp --replicas=3</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_26.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_27.png"></p>
<p>pod数也增长为3了。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_28.png"></p>
<p>每次访问nodeip:31079，都是不同的pod给回的返回结果。可以发现每次请求发到了不同的pod上从而实现了负载均衡。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_29.png"></p>
<p>缩减pod数为2  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl scale deployment/kubernetes-bootcamp --replicas=2</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_30.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_31.png"><br>滚动更新<br>当前应用image版本为v1，升级为v2  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl set image deployment/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_32.png"><br>可以看见kubernetes是创建出两个v2版本的镜像，然后在缓慢用v2将v1替换掉<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_33.png"><br>在次访问，版本是v2了<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_34.png"><br>回退版本  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl rollout undo  deployment/kubernetes-bootcamp</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_35.png">  </p>
<p>验证结果<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_36.png">  </p>
<h4 id="总结：-4"><a href="#总结：-4" class="headerlink" title="总结："></a>总结：</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_base_37.png">  </p>
<p>参考链接：<br><a href="http://blog.csdn.net/liukuan73/article/details/54971854">http://blog.csdn.net/liukuan73/article/details/54971854</a><br><a href="https://steemit.com/kubernetes/@cloudman6/k8s-kubernetes-3">https://steemit.com/kubernetes/@cloudman6/k8s-kubernetes-3</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes pod的弹性伸缩(二)(基于自定义监控指标的弹性伸缩)</title>
    <url>/2019/11/25/kubernetes_hpa2/</url>
    <content><![CDATA[<p>实际生产环境中，通过CPU和内存的监控指标弹性伸缩不能很好的反映应用真实的状态，所以我们需要根据应用本身自定义一些监控指标来进行弹性伸缩，如web应用，根据当前QPS来进行弹性，在这里Kubernetes HPA本身也支持自定义监控指标。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kube_hpa_23.png" alt="image"><br>自定义监控指标收集过程：</p>
<ul>
<li><p>pod 内置一个metrics 或者挂一个sidecar 当作exporter对外暴露。</p>
</li>
<li><p>Prometheus收集对应的监控指标。</p>
</li>
<li><p>Prometheus-adapter定期从prometheus收集指标对抓取的监控指标进行过滤和晒算，通过custom-metrics-apiserver将指标对外暴露。</p>
</li>
<li><p>HPA控制器从custom-metrics-apiserver获取数据。</p>
</li>
</ul>
<h3 id="部署prometheus"><a href="#部署prometheus" class="headerlink" title="部署prometheus"></a>部署prometheus</h3><p>clone代码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/stefanprodan/k8s-prom-hpa  </span><br></pre></td></tr></table></figure>
<p>切换到k8s-prom-hpa目录  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create namespace monitoring   </span><br><span class="line">kubectl create -f ./prometheus</span><br></pre></td></tr></table></figure>
<p>查看prometheus  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod -n monitoring</span><br><span class="line">NAME                                        READY   STATUS    RESTARTS   AGE</span><br><span class="line">prometheus-64bc56989f-qcs4p                 1/1     Running   1          22h</span><br><span class="line"></span><br><span class="line">kubectl  get svc -n monitoring</span><br><span class="line">NAME                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">prometheus                 NodePort    10.104.215.207   &lt;none&gt;        9090:31190/TCP   22h</span><br></pre></td></tr></table></figure>
<p>访问prometheus<br><a href="http://node_ip:31190/">http://node_ip:31190</a></p>
<p>prometheus内prometheus-cfg.yaml 配置了自动发生规则，会自动将组件注册。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/prometheus1_10.png">  </p>
<p>监控指标被prometheus收集 –&gt;Prometheus adapter变换指标格式–&gt;custom metrics apiserver–&gt;k8s hpa  </p>
<h3 id="部署kubernetes-prometheus-adapter"><a href="#部署kubernetes-prometheus-adapter" class="headerlink" title="部署kubernetes-prometheus-adapter"></a>部署kubernetes-prometheus-adapter</h3><p>进入k8s-prom-hpa目录<br>生成Prometheus-adapter所需的TLS证书：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">make certs</span><br></pre></td></tr></table></figure>
<p>custom-metrics-api&#x2F;custom-metrics-apiservice.yaml中配置了<br>insecureSkipTLSVerify: true选项，所以生成的证书是否受信任都无所谓。  </p>
<p>查看output文件夹有以下文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ls output/</span><br><span class="line">apiserver.csr  apiserver-key.pem  apiserver.pem</span><br></pre></td></tr></table></figure>
<p> 部署k8s-prometheus-adapter</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f ./custom-metrics-api</span><br></pre></td></tr></table></figure>
<p>prometheus-adapter配置文件查看<br>custom-metrics-apiserver-deployment.yaml文件  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">args:</span><br><span class="line">       - /adapter</span><br><span class="line">       - --secure-port=6443</span><br><span class="line">       - --tls-cert-file=/var/run/serving-cert/serving.crt //连接custom-metrics-apiserver的证书</span><br><span class="line">       - --tls-private-key-file=/var/run/serving-cert/serving.key</span><br><span class="line">       - --logtostderr=true //日志标准错误输出</span><br><span class="line">       - --prometheus-url=http://prometheus.monitoring.svc:9090/ //连接prometheus的地址，因为prometheus部署在集群内，所以可以直接用Service地址</span><br><span class="line">       - --metrics-relist-interval=30s</span><br><span class="line">       - --v=10 //日志debug等级，值越高日志越详细，可以适当调小</span><br><span class="line">       - --config=/etc/adapter/config.yaml //prometheus-adapter收集到prometheus监控指标后的过滤规则  </span><br></pre></td></tr></table></figure>
<p>custom-metrics-config-map.yaml文件</p>
<p>custom-metrics-config-map.yaml文件主要定义的是prometheus-adapter去prometheus获取指标的规则，因为prometheus的指标不能直接拿来用，需要通过prometheus-adapter进行一层中转和修改，然后将重新组装的指标通过自己接口暴露给custom-metrics-apiserver。  </p>
<p>所以这个文件内的值，实际上定义的就是从prometheus抓取规则的语句。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">- seriesQuery: &#x27;&#123;namespace!=&quot;&quot;,__name__!~&quot;^container_.*&quot;&#125;&#x27;</span><br><span class="line">     seriesFilters:</span><br><span class="line">     - isNot: .*_seconds_total</span><br><span class="line">     resources:</span><br><span class="line">       template: &lt;&lt;.Resource&gt;&gt;</span><br><span class="line">     name:</span><br><span class="line">       matches: ^(.*)_total$</span><br><span class="line">       as: &quot;&quot;</span><br><span class="line">     metricsQuery: sum(rate(&lt;&lt;.Series&gt;&gt;&#123;&lt;&lt;.LabelMatchers&gt;&gt;&#125;[1m])) by (&lt;&lt;.GroupBy&gt;&gt;)</span><br></pre></td></tr></table></figure>
<p>这个文件内容分为两部分第一个部分是 rules，用于 custom metrics；第二部分是 resourceRules，用于 metrics。<br> Prometheus adapter，可以将 Prometheus 中的任何一个指标都用于 HPA，但需要在prometheus-adapter内定义查询语句将它拿到。如果只需要使用一个指标做 HPA，可以只写一条查询，而不需要像这里面使用了多个查询。</p>
<p> 字段解释：    </p>
<ul>
<li>seriesQuery：prometheus的查询语句  </li>
<li>seriesFilters：指标过滤</li>
<li>is：需要筛选保留下来的。</li>
<li>isNot：需要过滤掉的。  </li>
<li>resource：将指标中的标签和k8s资源对应起来有两种方式一种是用overrides方式</li>
<li>name：用来给对应的指重命名的，有些指标是递增的http_request但采集的原始指标是http_request_total，需要进行一层计算然后过滤掉_total</li>
<li>matches：通过正则表达式来匹配指标名，可以进行分组；</li>
<li>as：默认值为 $1，也就是第一个分组。as 为空就是使用默认值的意思,也就是去<code>.*</code>对应的值。</li>
<li>metricsQuery： metricsQuery字段是一个Go模板，对调用出来的prometheus指标进行特定的处理。  </li>
<li>Series：指标名称</li>
<li>LabelMatchers：标签匹配列表。</li>
<li>1md：定义时间范围</li>
</ul>
<p>总体来说就是获取多次http_request_total指标，然后进行处理计算过去1分钟内每秒http_request，最后结果返回为http_request指标。  </p>
<p>查看k8s-prometheus-adapter部署情况</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl  get pod -n monitoring</span><br><span class="line">NAME                                        READY   STATUS    RESTARTS   AGE</span><br><span class="line">custom-metrics-apiserver-6c6c7f67d8-9vdkt   1/1     Running   0          4h58m</span><br><span class="line">prometheus-64bc56989f-qcs4p                 1/1     Running   0          4h59m</span><br></pre></td></tr></table></figure>



<p>查看创建的api组</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl api-versions  | grep metrics</span><br><span class="line">custom.metrics.k8s.io/v1beta1</span><br><span class="line">metrics.k8s.io/v1beta1</span><br></pre></td></tr></table></figure>
<p>获取自定义监控指标  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum install jq -y</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get --raw &quot;/apis/custom.metrics.k8s.io/v1beta1&quot; | jq .</span><br></pre></td></tr></table></figure>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;APIResourceList&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;groupVersion&quot;: &quot;custom.metrics.k8s.io/v1beta1&quot;,</span><br><span class="line">  &quot;resources&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;namespaces/go_gc_duration_seconds_count&quot;,</span><br><span class="line">      &quot;singularName&quot;: &quot;&quot;,</span><br><span class="line">      &quot;namespaced&quot;: false,</span><br><span class="line">      &quot;kind&quot;: &quot;MetricValueList&quot;,</span><br><span class="line">      &quot;verbs&quot;: [</span><br><span class="line">        &quot;get&quot;</span><br><span class="line">      ]</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>部署Podinfo应用测试custom-metric autoscale</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f ./podinfo/podinfo-svc.yaml,./podinfo/podinfo-dep.yaml</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>prometheus配置了自动发现规则，在podinfo-dep.yaml里面配置了对应的规则</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">annotations:</span><br><span class="line">        prometheus.io/scrape: &#x27;true&#x27;</span><br><span class="line">```        </span><br><span class="line">所以应用一启动就能直接在prometheus-target中发现。  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">获取自定义监控指标  </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>kubectl get –raw “&#x2F;apis&#x2F;custom.metrics.k8s.io&#x2F;v1beta1&#x2F;namespaces&#x2F;default&#x2F;pods&#x2F;*&#x2F;http_requests” | jq .<br>{<br>  “kind”: “MetricValueList”,<br>  “apiVersion”: “custom.metrics.k8s.io&#x2F;v1beta1”,<br>  “metadata”: {<br>    “selfLink”: “&#x2F;apis&#x2F;custom.metrics.k8s.io&#x2F;v1beta1&#x2F;namespaces&#x2F;default&#x2F;pods&#x2F;%2A&#x2F;http_requests”<br>  },<br>  “items”: [<br>    {<br>      “describedObject”: {<br>        “kind”: “Pod”,<br>        “namespace”: “default”,<br>        “name”: “podinfo-58b68656c9-b9cmg”,<br>        “apiVersion”: “&#x2F;v1”<br>      },<br>      “metricName”: “http_requests”,<br>      “timestamp”: “2019-11-09T09:00:09Z”,<br>      “value”: “888m”<br>    },<br>    {<br>      “describedObject”: {<br>        “kind”: “Pod”,<br>        “namespace”: “default”,<br>        “name”: “podinfo-58b68656c9-mr265”,<br>        “apiVersion”: “&#x2F;v1”<br>      },<br>      “metricName”: “http_requests”,<br>      “timestamp”: “2019-11-09T09:00:09Z”,<br>      “value”: “911m”<br>    }<br>  ]<br>}</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">配置podinfo custom hpa  </span><br></pre></td></tr></table></figure>
<p>kubectl create -f .&#x2F;podinfo&#x2F;podinfo-hpa-custom.yaml</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">```</span><br><span class="line">apiVersion: autoscaling/v2beta1</span><br><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">metadata:</span><br><span class="line">  name: podinfo</span><br><span class="line">spec:</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    apiVersion: extensions/v1beta1</span><br><span class="line">    kind: Deployment</span><br><span class="line">    name: podinfo</span><br><span class="line">  minReplicas: 2</span><br><span class="line">  maxReplicas: 10</span><br><span class="line">  metrics:</span><br><span class="line">  - type: Pods</span><br><span class="line">    pods:</span><br><span class="line">      metricName: http_requests</span><br><span class="line">      targetAverageValue: 10</span><br></pre></td></tr></table></figure>

<p>这里10指的是每秒10个请求，按照定义的规则metricsQuery中的时间范围1分钟，这就意味着过去1分钟内每秒如果达到10个请求则会进行扩容。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl  get hpa</span><br><span class="line">NAME      REFERENCE            TARGETS   MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">podinfo   Deployment/podinfo   713m/10   2         10        8          18h</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>713m是什么意思？<br>自定义API SERVER收到请求后会从Prometheus里面查询http_requests_total的值，然后把这个值换算成一个以时间为单位的请求率。713m的m就是milli-requests，按照定义的规则metricsQuery中的时间范围1分钟，大概每秒为0.71个请求</p>
<p>使用webbench进行压测  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">webbench -c 100  http://172.31.48.86:31198/</span><br><span class="line"></span><br><span class="line">-c表示发送100个请求</span><br></pre></td></tr></table></figure>

<p>查看hpa，可以看见请求数暴涨</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl  get hpa</span><br><span class="line">NAME      REFERENCE            TARGETS     MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">podinfo   Deployment/podinfo   57610m/10   2         10        8          12h</span><br></pre></td></tr></table></figure>
<p>查看HPA事件  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl describe hpa/podinfo</span><br><span class="line">Type     Reason                        Age                From                       Message</span><br><span class="line">  ----     ------                        ----               ----                       -------</span><br><span class="line">  Normal   SuccessfulRescale             33s                horizontal-pod-autoscaler  New size: 4; reason: pods metric http_requests above target</span><br><span class="line">  Normal   SuccessfulRescale             17s                horizontal-pod-autoscaler  New size: 8; reason: pods metric http_requests above target</span><br><span class="line">  Normal   SuccessfulRescale             2s                 horizontal-pod-autoscaler  New size: 10; reason: pods metric http_requests above target</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes ingress</title>
    <url>/2018/03/11/kubernetes_ingress/</url>
    <content><![CDATA[<p>操作系统：<br>centos7.3</p>
<p>软件版本:<br>kubernetes 1.9<br>nginx-ingress-controller:0.10.2<br>defaultbackend:1.4<br>haproxy-ingress:latest<br>defaultbackend:1.0<br>目前kubernetes内置暴露访问的方案  </p>
<h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>目前kubernetes内置暴露访问的方案<br>1、nodeport<br>指定Service的端口类型为nodeport，它将在的host节点上面暴露出一个端口，kube-proxy同时也会监听这个端口，目前是通过kube-proxy调用iptables实现将流量负载均衡到不同的pod上去，外部访问pod是先到host-ip:port—-&gt;pod_ip:port.为了提高可用性，可以在集群前面在加个负载均衡器这样就变成      loadbalance_ip:port—-&gt;host_ip:port—-&gt;port_ip:port。  </p>
<p>2、loadbalance<br>使用云平台本身提供的负载均衡器，分配访问pod的流量，这个目前只支持goole container engineand，目前也只支持tcp&#x2F;udp负载均衡。</p>
<p>3、ingress<br>ingress就是可以暴露内部访问工作在7层的负载均衡器，根据域名或服务名对后端Service进行端口转发，并且能根据后端Service的变化，动态刷新配置。  </p>
<h4 id="为什么需要ingress？"><a href="#为什么需要ingress？" class="headerlink" title="为什么需要ingress？"></a>为什么需要ingress？</h4><p>因为如果Service使用nodeport暴露host的端口方式去访问应用的话，当Service有多个时，会造成端口管理上的混乱，在企业内部中还涉及到开防火墙问题。  </p>
<p>这里我们介绍3种ingress类型，用户可以自由选择使用哪种类型：  </p>
<ul>
<li>1、nginx-ingress</li>
<li>2、haproxy-ingress</li>
<li>3、Traefik-ingress</li>
</ul>
<p>ingress包含两大组件<br>ingress-controller<br>default-backend  </p>
<p>ingress-controller本身是一个是一个pod，这个pod里面的容器安装了反向代理软件，通过读取添加的Service，动态生成负载均衡器的反向代理配置，你添加对应的ingress服务后,里面规则包含了对应的规则，里面有域名和对应的Service-backend</p>
<p>ingress-controller怎么去读取我们添加的ingress服务的配置然后动态刷新呢？  </p>
<p>ingress-controller通过与kube-apiserver交互，去读取ingress服务规则的变化，通过读取ingress服务的规则，生成一段新的负载均衡器的配置然后重新reload，生配置生效。</p>
<p>default-backend<br>用来将未知请求全部负载到这个默认后端上，这个默认后端会返回 404 页面。</p>
<p>ingress-control有nginx类型的还有haproxy类型的和traefik类型。  </p>
<h3 id="部署一个nginx的ingress"><a href="#部署一个nginx的ingress" class="headerlink" title="部署一个nginx的ingress"></a>部署一个nginx的ingress</h3><p>需要这些yaml文件<br>namespace.yaml—————创建名为ingress-nginx命名空间<br>default-backend.yaml———–这是官方要求必须要给的默认后端，提供404页面的。它还提供了nginx-ingress-controll健康状态检查，通过每隔一定时间访问nginx-ingress-controll的&#x2F;healthz页面，如是没有响应就返回404之类的错误码。<br>configmap.yaml ——- 创建名为nginx-configuration的configmap<br>tcp-services-configmap.yaml——–创建名为 tcp-services的configmap<br>udp-services-configmap.yaml——-创建名为udp-services的configmap<br>rbac.yaml——创建对应的Serviceaccount并绑定对应的角色<br>with-rbac.yaml—–创建带rbac的ingress-control   </p>
<p>创建文件夹<br>mkdir &#x2F;root&#x2F;ingress  </p>
<p>下载</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/namespace.yaml -P /root/ingress</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/default-backend.yaml -P /root/ingress</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/configmap.yaml -P /root/ingress</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/tcp-services-configmap.yaml -P /root/ingress</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/udp-services-configmap.yaml -P /root/ingress</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/rbac.yaml -P /root/ingress</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/with-rbac.yaml -P /root/ingress</span><br></pre></td></tr></table></figure>
<p>安装  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f /root/ingress/</span><br></pre></td></tr></table></figure>
<p>检查对应的pod是否创建好  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod --namespace=ingress-nginx</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_1.png"></p>
<p>因为新版本取消host模式的网络模式，所以需要我们创建server将ingress的端口映射出来<br>下载  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/baremetal/service-nodeport.yaml  -P /root/ingress</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f /root/ingress/service-nodeport.yaml</span><br></pre></td></tr></table></figure>
<p>查看对应的enpoint和service  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get endpoints,service --namespace=ingress-nginx</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_2.png"></p>
<p>80被映射到宿主机的31159端口了443被映射到31030端口了。<br>部署应用测试<br>部署两个httpd  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat httpd-app.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1beta2 # for versions before 1.8.0 use apps/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: http-app</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: http</span><br><span class="line">  replicas: 1 # tells deployment to run 2 pods matching the template</span><br><span class="line">  template: # create pods using pod definition in this template</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: http</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: httpd</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">        volumeMounts:</span><br><span class="line">          - name: httpd</span><br><span class="line">            mountPath: /usr/local/apache2/htdocs</span><br><span class="line">      volumes:</span><br><span class="line">       - name: httpd</span><br><span class="line">         hostPath:</span><br><span class="line">          path: /root/httpd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1beta2 # for versions before 1.8.0 use apps/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: http-app2</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: http2</span><br><span class="line">  replicas: 1 # tells deployment to run 2 pods matching the template</span><br><span class="line">  template: # create pods using pod definition in this template</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: http2</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx2</span><br><span class="line">        image: httpd</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">        volumeMounts:</span><br><span class="line">          - name: httpd</span><br><span class="line">            mountPath: /usr/local/apache2/htdocs</span><br><span class="line">      volumes:</span><br><span class="line">       - name: httpd</span><br><span class="line">         hostPath:</span><br><span class="line">          path: /root/httpd2</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>创建两个目录替换httpd的网页目录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir /root/httpd</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir /root/httpd2</span><br></pre></td></tr></table></figure>
<p>简单写个测试页  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">echo This is test1!! &gt; /root/httpd/index.html</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">echo This is test2!! &gt; /root/httpd2/index.html</span><br></pre></td></tr></table></figure>
<p>创建应用  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f httpd-app.yaml</span><br></pre></td></tr></table></figure>
<p>检查  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get deployment</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_3.png"></p>
<p>创建对应的servic  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat httpd_service.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line"> name: httpd-deployment</span><br><span class="line">spec:</span><br><span class="line"> ports:</span><br><span class="line"> - protocol: TCP</span><br><span class="line">   port: 80</span><br><span class="line">   targetPort: 80</span><br><span class="line"> selector:</span><br><span class="line">   app: http</span><br><span class="line">---</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line"> name: httpd2-deployment</span><br><span class="line">spec:</span><br><span class="line"> ports:</span><br><span class="line"> - protocol: TCP</span><br><span class="line">   port: 80</span><br><span class="line">   targetPort: 80</span><br><span class="line"> selector:</span><br><span class="line">   app: http2</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>创建service  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f httpd_service.yaml</span><br></pre></td></tr></table></figure>
<p>检查  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get service</span><br></pre></td></tr></table></figure>

<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_4.png"></p>
<p>创建ingress  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat httpd-ingress.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: httpd-ingress</span><br><span class="line">  namespace: default #服务在哪个空间内就写哪个空间</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: wanshaoyuan1.com  #此service的访问域名</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - backend:</span><br><span class="line">          serviceName: httpd-deployment  #servicname</span><br><span class="line">          servicePort: 80          #service暴露出来的port</span><br><span class="line">  - host: wanshaoyuan2.com  #此service的访问域名</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - backend:</span><br><span class="line">          serviceName: httpd2-deployment #servicname</span><br><span class="line">          servicePort: 80  #service暴露出来的port</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>应用  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f httpd-ingress.yaml</span><br></pre></td></tr></table></figure>
<p>检查  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get ingress --all-namespaces</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_5.png"></p>
<p>测试服务<br>修改hosts<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_6.png"></p>
<p>打开浏览器访问<br><a href="http://wanshaoyuan1.com:31159/">http://wanshaoyuan1.com:31159</a>  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_7.png"></p>
<p><a href="http://wanshaoyuan2.com:31159/">http://wanshaoyuan2.com:31159</a><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_8.png"></p>
<p>查看ingress-control生成的nginx.conf配置文件  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl --namespace=ingress-nginx exec -it nginx-ingress-controller-756ffbdb79-5tjs5 cat /etc/nginx/nginx.conf</span><br></pre></td></tr></table></figure>
<p> 可以看见ingress-control添加了这两个upstream<br> <img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_9.png"></p>
<p>如果需要ingress-control不使用service暴露端口，网络模式使用主机模式的话需要修改with-rbac.yaml<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_10.png"></p>
<h3 id="部署一个haproxy的ingress"><a href="#部署一个haproxy的ingress" class="headerlink" title="部署一个haproxy的ingress"></a>部署一个haproxy的ingress</h3><p>default-backend.yaml ——-这是官方要求必须要给的默认后端，提供404页面的。它还提供了haproxy-ingress-controll健康状态检查，通过每隔一定时间访问haproxy-ingress-controll的&#x2F;healthz页面，如是没有响应就返回404之类的错误码。   </p>
<p>haproxy-ingress.yaml—–部署haproxy-ingress-controller。    </p>
<p>rbac.yaml—–创建rbac权限和Serviceaccount</p>
<p>github<br><a href="https://github.com/jcmoraisjr/haproxy-ingress/tree/master/examples">https://github.com/jcmoraisjr/haproxy-ingress/tree/master/examples</a></p>
<p>如果你kubernetes配置了RBAC的验证模式，你需要创建个servicaccount，然后绑定相对应的role和clusterrole，否则你的ingress-control将无法从kube-apiserver获取对应的数据。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat rbam.yaml  </span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-controller</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-controller</span><br><span class="line">  namespace: ingress-controller</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-controller</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">      - endpoints</span><br><span class="line">      - nodes</span><br><span class="line">      - pods</span><br><span class="line">      - secrets</span><br><span class="line">    verbs:</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - nodes</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - services</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;extensions&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - ingresses</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - events</span><br><span class="line">    verbs:</span><br><span class="line">      - create</span><br><span class="line">      - patch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;extensions&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - ingresses/status</span><br><span class="line">    verbs:</span><br><span class="line">      - update</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: Role</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-controller</span><br><span class="line">  namespace: ingress-controller</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">      - pods</span><br><span class="line">      - secrets</span><br><span class="line">      - namespaces</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - update</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">    verbs:</span><br><span class="line">      - create</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - endpoints</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - create</span><br><span class="line">      - update</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-controller</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: ingress-controller</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: ingress-controller</span><br><span class="line">    namespace: ingress-controller</span><br><span class="line">  - apiGroup: rbac.authorization.k8s.io</span><br><span class="line">    kind: User</span><br><span class="line">    name: ingress-controller</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: RoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-controller</span><br><span class="line">  namespace: ingress-controller</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Role</span><br><span class="line">  name: ingress-controller</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: ingress-controller</span><br><span class="line">    namespace: ingress-controller</span><br><span class="line">  - apiGroup: rbac.authorization.k8s.io</span><br><span class="line">    kind: User</span><br><span class="line">    name: ingress-controller</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat default-backend.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-default-backend</span><br><span class="line">  labels:</span><br><span class="line">    app: ingress-default-backend</span><br><span class="line">  namespace: ingress-controller</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: ingress-default-backend</span><br><span class="line">    spec:</span><br><span class="line">      terminationGracePeriodSeconds: 60</span><br><span class="line">      containers:</span><br><span class="line">      - name: ingress-default-backend</span><br><span class="line">        # Any image is permissable as long as:</span><br><span class="line">        # 1. It serves a 404 page at /</span><br><span class="line">        # 2. It serves 200 on a /healthz endpoint</span><br><span class="line">        image: gcr.io/google_containers/defaultbackend:1.0</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 10m</span><br><span class="line">            memory: 20Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 10m</span><br><span class="line">            memory: 20Mi</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-default-backend</span><br><span class="line">  namespace: ingress-controller</span><br><span class="line">  labels:</span><br><span class="line">    app: ingress-default-backend</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 8080</span><br><span class="line">    targetPort: 8080</span><br><span class="line">  selector:</span><br><span class="line">    app: ingress-default-backend</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat haproxy-ingress.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    run: haproxy-ingress</span><br><span class="line">  name: haproxy-ingress</span><br><span class="line">  namespace: ingress-controller</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      run: haproxy-ingress</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        run: haproxy-ingress</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: ingress-controller</span><br><span class="line">      hostNetwork: true  </span><br><span class="line">      containers:</span><br><span class="line">      - name: haproxy-ingress</span><br><span class="line">        image: quay.io/jcmoraisjr/haproxy-ingress</span><br><span class="line">        args:</span><br><span class="line">        - --default-backend-service=$(POD_NAMESPACE)/ingress-default-backend</span><br><span class="line">        - --default-ssl-certificate=$(POD_NAMESPACE)/tls-secret</span><br><span class="line">        - --configmap=$(POD_NAMESPACE)/haproxy-ingress</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 80</span><br><span class="line">        - name: https</span><br><span class="line">          containerPort: 443</span><br><span class="line">        - name: stat</span><br><span class="line">          containerPort: 1936</span><br><span class="line">        env:</span><br><span class="line">        - name: POD_NAME</span><br><span class="line">          valueFrom:</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: metadata.name</span><br><span class="line">        - name: POD_NAMESPACE</span><br><span class="line">          valueFrom:</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: metadata.namespace</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>应用  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f rbac.yaml</span><br><span class="line">kubectl apply -f default-backend.yaml</span><br><span class="line">kubectl apply -f haproxy-ingress.yaml</span><br></pre></td></tr></table></figure>
<p>查看对应的pod  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod --namespace=ingress-controller</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_11.png"></p>
<p>创建应用测试，这里还是用上面的http-app  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f httpd-app.yaml</span><br><span class="line">kubectl apply -f httpd_service.yaml</span><br><span class="line">kubectl apply -f httpd-ingress.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get ing</span><br></pre></td></tr></table></figure>

<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_12.png"></p>
<p>配置hosts访问<br>测试  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_13.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_14.png"></p>
<p>查看ingress-control的haproxy配置文件   </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl -n ingress-controller exec -it haproxy-ingress-785cc87f6b-h7kxn cat /etc/haproxy/haproxy.cfg</span><br></pre></td></tr></table></figure>

<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_15.png"></p>
<p>我ingress-control-haproxy使用的是hostnetwork模式，当我不使用hostnetwork模式时，我将ingress-control-haproxy使用service暴露nodeport端口去连接时会出现下面情况。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_16.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_17.png"></p>
<p>但使用curl指定http报文host字段头部<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_18.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_19.png"></p>
<p>通过判断haproxy配置文件判断应该是haproxy.cfg里面的acl的配置<br>限制了报文头部host字段。  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_20.png"></p>
<p>但这样使用的话还有个问题，因为ingress-control也是以pod的方式部署的，在实际应用中，pod会发生漂移，这样的话ip就会发生变化，但在企业中，允许通过的ip都需要在防火墙上放行。所以这里我们需要一个vip，使用haproxy+keepalive实现高可用，这里就不介绍具体操作方法了。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_21.png"></p>
<h3 id="部署一个Traefik的ingress"><a href="#部署一个Traefik的ingress" class="headerlink" title="部署一个Traefik的ingress"></a>部署一个Traefik的ingress</h3><p>tracfik是一款开源的反向代理和负载均衡工具,traefik是为微服务而生的，它可以随时感知后端服务的变化，自动更改配置并热重新加载，在这期间服务是不会暂停和停止的。  </p>
<p>为什么选择traefik(<a href="http://blog.csdn.net/aixiaoyang168/article/details/78557739">http://blog.csdn.net/aixiaoyang168/article/details/78557739</a>)</p>
<ul>
<li>速度快</li>
<li>不需要安装其他依赖，使用 GO 语言编译可执行文件</li>
<li>支持最小化官方 Docker 镜像<br>支持多种后台，如 Docker, Swarm mode, Kubernetes, Marathon, Consul, Etcd, Rancher, Amazon ECS 等等</li>
<li>支持 REST API</li>
<li>配置文件热重载，不需要重启进程</li>
<li>支持自动熔断功能</li>
<li>支持轮训、负载均衡</li>
<li>提供简洁的 UI 界面</li>
<li>支持 Websocket, HTTP&#x2F;2, GRPC</li>
<li>自动更新 HTTPS 证书</li>
<li>支持高可用集群模式</li>
</ul>
<p>根上面的nginx-ingress和haproxy-ingress相比有什么区别呢 ？<br>1、使用nginx-ingress和haproxy-ingress需要一个ingress-controller去根kube-apiserver进行交互去读取后端的service、pod等的变化，然后在动态刷新nginx的配置，来达到服务自动发现的目的，而traefik本身设计就能直接和kube-apiserver进行交互，感知后端service、pod的变化，自动更新并热重载。</p>
<p>2、traefik快速和方便自带web页面和监控状态检查。  </p>
<p>部署traefik<br>git clone <a href="https://github.com/containous/traefik.git">https://github.com/containous/traefik.git</a><br>traefik&#x2F;examples&#x2F;k8s&#x2F;目录存放的就是kubernetes集群所需要的yml文件<br>traefik-rbac.yaml对创建traefik对应的ClusterRole和绑定对应的servicaccount。<br>执行<br>kubectl apply -f traefik-rbac.yaml</p>
<p>traefik-deployment.yaml 创建对应的serviceaccount，创建对应的pod和service<br>执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f traefik-deployment.yaml</span><br></pre></td></tr></table></figure>
<p>查看</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_22.png"></p>
<p>它同时启动了80和8080端口，80端口对应服务端口，8080对应ui端口  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_23.png"></p>
<p>访问<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_24.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_25.png"></p>
<p>部署ui<br>主要采用ingress的方式暴露服务<br>traefix很快发现我们刚刚添加的ingress<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_26.png"></p>
<p>使用上面ingress-nginx和ingress-haproxy的测试用例进行测试<br>创建对应的deployment</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f httpd-app.yaml</span><br></pre></td></tr></table></figure>
<p>创建对应的service</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f httpd_service.yaml</span><br></pre></td></tr></table></figure>
<p>创建对应的ingress</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f httpd-ingress.yaml</span><br></pre></td></tr></table></figure>
<p>测试访问<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_27.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_28.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_ingress_29.png"></p>
<p>参考链接：<br><a href="http://dockone.io/article/1418">http://dockone.io/article/1418</a><br><a href="http://blog.csdn.net/aixiaoyang168/article/details/78557739">http://blog.csdn.net/aixiaoyang168/article/details/78557739</a><br><a href="http://www.mamicode.com/info-detail-2109270.html">http://www.mamicode.com/info-detail-2109270.html</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes V1.9安装(附离线安装包和离线镜像）</title>
    <url>/2018/01/02/kubernetes_install/</url>
    <content><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>环境信息（采用一个master节点+两个node节点)<br>master 192.168.2.110<br>node-1 192.168.2.112<br>node-2 192.168.2.113</p>
<p>软件版本<br>kubernetes v1.9<br>docker：17.03<br>kubeadm:v1.9.0<br>kube-apiserver:v1.9.0<br>kube-controller-manager:v1.9.0<br>kube-scheduler:v1.9.0<br>k8s-dns-sidecar:1.14.7<br>k8s-dns-kube-dns:1.14.7<br>k8s-dns-dnsmasq-nanny:1.14.7<br>kube-proxy:v1.9.0<br>etcd:3.1.10<br>pause :3.0<br>flannel:v0.9.1<br>kubernetes-dashboard:v1.8.1</p>
<p>采用kubeadm安装</p>
<p>kubeadm为kubernetes官方推荐的自动化部署工具，他将kubernetes的组件以pod的形式部署在master和node节点上，并自动完成证书认证等操作。<br>因为kubeadm默认要从google的镜像仓库下载镜像，但目前国内无法访问google镜像仓库，所以这里我提交将镜像下好了，只需要将离线包的镜像导入到节点中就可以了。</p>
<h3 id="开始安装"><a href="#开始安装" class="headerlink" title="开始安装"></a>开始安装</h3><h4 id="所有节点操作"><a href="#所有节点操作" class="headerlink" title="所有节点操作"></a>所有节点操作</h4><p>下载<br>链接: <a href="https://pan.baidu.com/s/1c2O1gIW">https://pan.baidu.com/s/1c2O1gIW</a> 密码: 9s92<br>比对md5解压离线包</p>
<p>MD5 (k8s_images.tar.bz2) &#x3D; b60ad6a638eda472b8ddcfa9006315ee</p>
<p>解压下载下来的离线包</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tar -xjvf k8s_images.tar.bz2</span><br></pre></td></tr></table></figure>
<p>安装docker-ce17.03(kubeadmv1.9最大支持docker-ce17.03)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rpm -ihv docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm</span><br><span class="line">rpm -ivh docker-ce-17.03.2.ce-1.el7.centos.x86_64.rpm</span><br></pre></td></tr></table></figure>
<p>修改docker的镜像源为国内的daocloud的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://a58c8480.m.daocloud.io</span><br></pre></td></tr></table></figure>
<p>启动docker-ce</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl start docker &amp;&amp; systemctl enable docker</span><br></pre></td></tr></table></figure>


<p>绑定hosts<br>将master和node-1、node-2绑定hosts</p>
<p>master节点与node节点做互信</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# ssh-keygen</span><br><span class="line">[root@master ~]# ssh-copy-id node-1</span><br><span class="line">[root@master ~]# ssh-copy-id node-2</span><br></pre></td></tr></table></figure>
<p>关闭防火墙和selinux</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl stop firewalld  &amp;&amp; systemctl disable firewalld</span><br></pre></td></tr></table></figure>
<p>vim &#x2F;etc&#x2F;selinux&#x2F;config<br>SELINUX&#x3D;disabled</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">setenforce 0</span><br></pre></td></tr></table></figure>
<p>配置系统路由参数,防止kubeadm报路由警告</p>
<pre><code>echo &quot;
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
&quot; &gt;&gt; /etc/sysctl.conf
sysctl -p
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">导入镜像</span><br><span class="line"></span><br></pre></td></tr></table></figure>
docker load &lt; /root/k8s_images/docker_images/etcd-amd64_v3.1.10.tar
docker load &lt;/root/k8s_images/docker_images/flannel\:v0.9.1-amd64.tar
docker load &lt;/root/k8s_images/docker_images/k8s-dns-dnsmasq-nanny-amd64_v1.14.7.tar
docker load &lt;/root/k8s_images/docker_images/k8s-dns-kube-dns-amd64_1.14.7.tar
docker load &lt;/root/k8s_images/docker_images/k8s-dns-sidecar-amd64_1.14.7.tar
docker load &lt;/root/k8s_images/docker_images/kube-apiserver-amd64_v1.9.0.tar
docker load &lt;/root/k8s_images/docker_images/kube-controller-manager-amd64_v1.9.0.tar
docker load &lt;/root/k8s_images/docker_images/kube-scheduler-amd64_v1.9.0.tar
docker load &lt; /root/k8s_images/docker_images/kube-proxy-amd64_v1.9.0.tar
docker load &lt;/root/k8s_images/docker_images/pause-amd64_3.0.tar
docker load &lt; /root/k8s_images/docker_images/kubernetes-dashboard_v1.8.1.tar

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">安装安装kubelet kubeadm kubectl包</span><br></pre></td></tr></table></figure>
rpm -ivh socat-1.7.3.2-2.el7.x86_64.rpm
rpm -ivh kubernetes-cni-0.6.0-0.x86_64.rpm  kubelet-1.9.9-9.x86_64.rpm  kubectl-1.9.0-0.x86_64.rpm
rpm -ivh kubectl-1.9.0-0.x86_64.rpm
rpm -ivh kubeadm-1.9.0-0.x86_64.rpm
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">### master节点操作</span><br><span class="line">启动kubelete</span><br></pre></td></tr></table></figure>
systemctl enable kubelet &amp;&amp; sudo systemctl start kubelet
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">开始初始化master</span><br></pre></td></tr></table></figure>
kubeadm init --kubernetes-version=v1.9.0 --pod-network-cidr=10.244.0.0/16
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubernetes默认支持多重网络插件如flannel、weave、calico，这里使用flanne，就必须要设置--pod-network-cidr参数，10.244.0.0/16是kube-flannel.yml里面配置的默认网段，如果需要修改的话，需要把kubeadm init的--pod-network-cidr参数和后面的kube-flannel.yml里面修改成一样的网段就可以了。</span><br><span class="line"></span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_1.png)</span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_2.png)</span><br><span class="line"></span><br><span class="line">发现kubelet启动不了</span><br><span class="line">查看日志/var/log/message</span><br><span class="line"></span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_4.png)</span><br><span class="line"></span><br><span class="line">发现原来是kubelet默认的cgroup的driver和docker的不一样，docker默认的cgroupfs，kubelet默认为systemd</span><br><span class="line">修改</span><br></pre></td></tr></table></figure>
vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_5.png)</span><br><span class="line"></span><br><span class="line">重启reload</span><br></pre></td></tr></table></figure>
 systemctl daemon-reload &amp;&amp; systemctl restart kubelet
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">查看状态</span><br><span class="line"></span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_6.png)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">此时记得将环境reset一下</span><br><span class="line">执行</span><br></pre></td></tr></table></figure>
kubeadm reset
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">在重新执行</span><br></pre></td></tr></table></figure>
kubeadm init --kubernetes-version=v1.9.0 --pod-network-cidr=10.244.0.0/16
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_7.png)</span><br><span class="line"></span><br><span class="line">将kubeadm join xxx保存下来，等下node节点需要使用</span><br><span class="line">如果忘记了，可以在master上通过kubeadmin token list得到</span><br><span class="line"></span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_8.png)</span><br><span class="line"></span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_9.png)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">默认token 24小时就会过期，后续的机器要加入集群需要重新生成token   </span><br></pre></td></tr></table></figure>
kubeadm token create   
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">然后在执行   </span><br></pre></td></tr></table></figure>
kubeadm join --token xxx master_ip:6443    
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">按照上面提示，此时root用户还不能使用kubelet控制集群需要，配置下环境变量     </span><br><span class="line">对于非root用户</span><br></pre></td></tr></table></figure>
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">对于root用户</span><br></pre></td></tr></table></figure>
export KUBECONFIG=/etc/kubernetes/admin.conf
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">也可以直接放到~/.bash_profile</span><br></pre></td></tr></table></figure>
echo &quot;export KUBECONFIG=/etc/kubernetes/admin.conf&quot; &gt;&gt; ~/.bash_profile
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">source一下环境变量</span><br></pre></td></tr></table></figure>
source ~/.bash_profile
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl version测试</span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_10.png)</span><br><span class="line"></span><br><span class="line">安装网络，可以使用flannel、calico、weave、macvlan这里我们用flannel。</span><br><span class="line"></span><br><span class="line">下载此文件</span><br></pre></td></tr></table></figure>
wget https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">或直接使用离线包里面的</span><br><span class="line">若要修改网段，需要kubeadm --pod-network-cidr=和这里同步</span><br><span class="line">vim kube-flannel.yml</span><br><span class="line">修改network项</span><br></pre></td></tr></table></figure>
&quot;Network&quot;: &quot;10.244.0.0/16&quot;,
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">执行</span><br><span class="line"></span><br></pre></td></tr></table></figure>
kubectl create  -f kube-flannel.yml
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">### node节点操作</span><br><span class="line">修改kubelet配置文件根上面有一将cgroup的driver由systemd改为cgroupfs  </span><br><span class="line">vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf  </span><br><span class="line">Environment=&quot;KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs&quot;  </span><br></pre></td></tr></table></figure>
systemctl daemon-reload  
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">```</span><br><span class="line">systemctl enable kubelet&amp;&amp;systemctl restart kubelet  </span><br></pre></td></tr></table></figure>
使用刚刚执行kubeadm后的kubeadm join --xxx  
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubeadm join --token 361c68.fbafaa96a5381651 192.168.2.110:6443 --discovery-token-ca-cert-hash sha256:e5e392f4ce66117635431f76512d96824b88816dfdf0178dc497972cf8631a98</span><br></pre></td></tr></table></figure>



多次加入报错查看/var/log/message日志.
这个错是因为没有配置前面sysctl的router的环境变量
![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_26.png)


发现node节点启动不了flannel容器，查看容器log发现是host上没有默认路由，在网卡配置文件里面设置好默认路由。

![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_11.png)

加入成功
在master节点上check一下
![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_27.png)

kubernetes会在每个node节点创建flannel和kube-proxy的pod
![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_14.png)


### 测试集群
在master节点上发起个创建应用请求
这里我们创建个名为httpd-app的应用，镜像为httpd，有两个副本pod
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl run httpd-app --image=httpd --replicas=2</span><br></pre></td></tr></table></figure>

![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_15.png)


检查pod
可以看见pod分布在node-1和node-2上
![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_16.png)

因为创建的资源不是service所以不会调用kube-proxy
直接访问测试
![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_17.png)

至此kubernetes基本集群安装完成。

### 部署kubernetes-dashboard
kubernetes-dashboard是可选组件，因为，实在不好用，功能太弱了。
建议在部署master时一起把kubernetes-dashboard一起部署了,不然在node节点加入集群后，kubernetes-dashboard会被kube-scheduler调度node节点上，这样根kube-apiserver通信需要额外配置。
直接使用离线包里面的kubernetes-dashboard.yaml

修改kubernetes-dashboard.yaml

如果需要让外面访问需要修改这个yaml文件端口类型为NodePort默认为clusterport外部访问不了，
![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_18.png)

nodeport默认端口范围30000-32767
32666就是我的映射端口，根docker run -d xxx:xxx差不多
创建kubernetes-dashboard
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f kubernetes-dashboard.yaml</span><br></pre></td></tr></table></figure>

访问
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://master_ip:NodePort</span><br></pre></td></tr></table></figure>

![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_19.png)

默认验证方式有kubeconfig和token，这里我们都不用。
这里我们使用basicauth的方式进行apiserver的验证
创建/etc/kubernetes/pki/basic_auth_file 用于存放用户名和密码
#user,password,userid
admin,admin,2

给kube-apiserver添加basic_auth验证
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim /etc/kubernetes/manifests/kube-apiserver.yaml</span><br></pre></td></tr></table></figure>
![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_20.png)

加上这行

更新kube-apiserver容器
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl  apply -f kube-apiserver.yaml</span><br></pre></td></tr></table></figure>
授权
k8s1.6后版本都采用RBAC授权模型
给admin授权
默认cluster-admin是拥有全部权限的，将admin和cluster-admin bind这样admin就有cluster-admin的权限。


![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_21.png)

那我们将admin和cluster-admin bind在一起这样admin也拥用cluster-admin的权限了
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create clusterrolebinding login-on-dashboard-with-cluster-admin --clusterrole=cluster-admin --user=admin</span><br></pre></td></tr></table></figure>
查看
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get clusterrolebinding/login-on-dashboard-with-cluster-admin -o yaml</span><br></pre></td></tr></table></figure>
![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_22.png)


在此访问https://master:32666

![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_28.png)
选基本，就可以通过用户名和密码访问了

![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_23.png)


创建个应用测试
![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_24.png)

部署成功
![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_install_25.png)




参考链接:
https://kubernetes.io/docs/setup/independent/install-kubeadm/
http://tonybai.com/2017/07/20/fix-cannot-access-dashboard-in-k8s-1-6-4/
</code></pre>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes pod的弹性伸缩(三)(基于事件驱动伸缩)</title>
    <url>/2021/06/18/kubernetes_keda/</url>
    <content><![CDATA[<table>
<thead>
<tr>
<th>软件</th>
<th>版本</th>
</tr>
</thead>
<tbody><tr>
<td>keda</td>
<td>2.3.0</td>
</tr>
<tr>
<td>Kubernetes</td>
<td>1.20.7</td>
</tr>
</tbody></table>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>目前在Kubernetes中做POD弹性伸缩HPA有基于CPU、memory这种资源指标实现和通过custom-metrics-apiserver和Prometheus-adapter实现的自定义业务监控指标的弹性伸缩，但这些都不是很灵活有效，基于cpu、memory实际对业务带来价值并不大，并不能很准确代表业务实际是否需要扩缩容。基于自定义业务监控指标配置起来相当繁琐。所以社区这些问题背景下开发了KEDA，基于事件的伸缩</p>
<h4 id="KEDA是什么？"><a href="#KEDA是什么？" class="headerlink" title="KEDA是什么？"></a>KEDA是什么？</h4><p>KEDA是红帽和微软联合开发的一个开源项目，后面贡献给了CNCF基金会，目前处于sandbox阶段，实现KEDA的目的主要为了更好和更加灵活使用HPA。</p>
<p>项目地址<br><a href="https://github.com/kedacore/keda">https://github.com/kedacore/keda</a></p>
<p>文档地址<br><a href="https://keda.sh/">https://keda.sh/</a></p>
<h4 id="什么是事件驱动？"><a href="#什么是事件驱动？" class="headerlink" title="什么是事件驱动？"></a>什么是事件驱动？</h4><p>举个简单例子，常见的消息队列系统，有生产者和消费者，生产者往消息队列中吐数据，消费者从消息队列中消费数据，当出现消息堆积时，此时是消费者处理不过来了，应该及时扩容消费者。这就是keda可以实现的，通过对应的接口去查看消息队列系统的情况，进行判断，调用HPA进行扩缩容。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/keda-2.png"></p>
<p>keda能做什么？<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/keda-3.png"></p>
<p>图片来源：<a href="https://developer.ibm.com/technologies/messaging/articles/introduction-to-keda/">https://developer.ibm.com/technologies/messaging/articles/introduction-to-keda/</a></p>
<p><img src="https://cloudblogs.microsoft.com/uploads/prod/sites/37/2019/05/Build-KEDA-1-1024x879.png"><br>图片来源：<a href="https://keda.sh/docs/1.4/concepts/">https://keda.sh/docs/1.4/concepts/</a></p>
<p>KEDA 监控定义的事件源，并定期检查是否有任何事件。达到设置的阈值后，KEDA 会根据部署的复制件计数设置为 1 或 0，根据配置最小副本计数，激活或停用POD，具体取决于是否有任何事件。<br>通过上面两幅架构图可以看见keda和HPA是相辅相成的，keda实现的是事件收集和pod副本数0-&gt;1，1-&gt;0的调整，HPA实现1-&gt;n,n-&gt;1的POD副本调整.</p>
<h4 id="keda组成"><a href="#keda组成" class="headerlink" title="keda组成"></a>keda组成</h4><p>keda由两个组件组成：<br>keda-operator：创建维护hpa资源对象，同时激活hpa伸缩(0-&gt;1,1-&gt;0)。  </p>
<p>keda-metrics-apiserver: 实现了hpa中external metrics，根据事件源配置返回计算结果,推动hpa进行。</p>
<p>keda定义的CRD对象主要有三个</p>
<ul>
<li>ScaledObjects：定义事件源和资源映射以及控制的范围。</li>
<li>ScaledJobs：定义事件源和job资源映射以及控制的范围。</li>
<li>TriggerAuthentication：监控事件源的认证配置</li>
</ul>
<h3 id="测试使用"><a href="#测试使用" class="headerlink" title="测试使用"></a>测试使用</h3><h4 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h4><p>keda部署非常简单，有三种方式</p>
<ul>
<li>Helm charts</li>
<li>Operator Hub</li>
<li>YAML declarations</li>
</ul>
<p>这里，我们直接用yaml进行，一个yaml即可搞定</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f https://github.com/kedacore/keda/releases/download/v2.3.0/keda-2.3.0.yaml</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>部署完后检查</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod -n keda</span><br><span class="line">NAME                                      READY   STATUS    RESTARTS   AGE</span><br><span class="line">keda-metrics-apiserver-84d8fc689d-62kc2   1/1     Running   0          5h23m</span><br><span class="line">keda-operator-54f95d8598-l5vjg            1/1     Running   0          5h23m</span><br></pre></td></tr></table></figure>

<p>看状态是否正常，也建议看看这两个组件的log是否有报错。</p>
<p>接下来进行测试使用，直接使用官方github里面的例子。</p>
<h4 id="RabbitMQ弹性"><a href="#RabbitMQ弹性" class="headerlink" title="RabbitMQ弹性"></a>RabbitMQ弹性</h4><p>实现效果<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/keda-4.png"><br>图片来源:(<a href="https://jstobigdata.com/rabbitmq/direct-exchange-in-amqp-rabbitmq/">https://jstobigdata.com/rabbitmq/direct-exchange-in-amqp-rabbitmq/</a>)<br>当rabbimq队列达到设定的长度后扩容consumer副本数，当队列长度降下去后将副本数缩小为对应的配置值。</p>
<p>部署Rabbimq-server</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">helm repo add bitnami https://charts.bitnami.com/bitnami</span><br></pre></td></tr></table></figure>

<p>部署rabbitmq</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">helm install rabbitmq --set auth.username=user --set auth.password=PASSWORD bitnami/rabbitmq</span><br></pre></td></tr></table></figure>

<p>查看是否部署成功</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl  get pod </span><br><span class="line">NAME                                     READY   STATUS      RESTARTS   AGE</span><br><span class="line">rabbitmq-0                               1/1     Running     0          7h37m</span><br></pre></td></tr></table></figure>
<p>部署 RabbitMQ consumer</p>
<p>repo里面是配置rabbimq-consumer和keda规则</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/kedacore/sample-go-rabbitmq.git</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd sample-go-rabbitmq</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>部署</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f deploy/deploy-consumer.yaml</span><br><span class="line"></span><br><span class="line">secret/rabbitmq-consumer-secret created</span><br><span class="line">deployment.apps/rabbitmq-consumer created</span><br><span class="line">scaledobject.keda.sh/rabbitmq-consumer created</span><br><span class="line">triggerauthentication.keda.sh/rabbitmq-consumer-trigger created</span><br></pre></td></tr></table></figure>

<p>主要看看ScaledObject</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: keda.sh/v1alpha1</span><br><span class="line">kind: ScaledObject</span><br><span class="line">metadata:</span><br><span class="line">  name: rabbitmq-consumer</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    name: rabbitmq-consumer</span><br><span class="line">  pollingInterval: 5 # Optional. Default: 30 seconds</span><br><span class="line">  cooldownPeriod: 30 # Optional. Default: 300 seconds</span><br><span class="line">  maxReplicaCount: 30 # Optional. Default: 100</span><br><span class="line">  triggers:</span><br><span class="line">    - type: rabbitmq</span><br><span class="line">      metadata:</span><br><span class="line">        queueName: hello</span><br><span class="line">        queueLength: &quot;5&quot;</span><br><span class="line">      authenticationRef:</span><br><span class="line">        name: rabbitmq-consumer-trigger</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>参数</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>pollingInterval</td>
<td>定时获取scaler的时间间隔，默认30s</td>
</tr>
<tr>
<td>cooldownPeriod</td>
<td>上次active至缩容为0需要等待的时间，默认300s</td>
</tr>
<tr>
<td>maxReplicaCount</td>
<td>最大POD副本数</td>
</tr>
<tr>
<td>minReplicaCount</td>
<td>最小POD副本数，默认为0</td>
</tr>
<tr>
<td>triggers.type</td>
<td>事件类型是keda支持的哪种，如是redis就写redis，是kafka是写kafka</td>
</tr>
<tr>
<td>trigger.metadata.queueName</td>
<td>这个根实际事件类型有关的触发器指标了，这里定义的是rabbitmq的队列名</td>
</tr>
<tr>
<td>trigger.metadata.queueLength</td>
<td>触发器定义的队列长度</td>
</tr>
<tr>
<td>trigger.authenticationRef</td>
<td>认证关联，关联TriggerAuthentication</td>
</tr>
</tbody></table>
<p>部署后会发现rabbitmq-consumer副本数会自动变为0，因为此时队列是空的</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl  get deployment</span><br><span class="line">NAME                     READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">rabbitmq-consumer        0/0     0            0           1h53m</span><br></pre></td></tr></table></figure>

<p>此时get hpa也创建出来了</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get hpa</span><br><span class="line">NAME                         REFERENCE                      TARGETS             MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">keda-hpa-rabbitmq-consumer   Deployment/rabbitmq-consumer   &lt;unknown&gt;/5 (avg)   1         30        0          1h</span><br></pre></td></tr></table></figure>
<p>部署produce往Rabbitmq里面推数据制造拥堵</p>
<p>这个job将推送 300信息到 “hello” 这个队列 keda将在2分种扩容 deployment到30个</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f deploy/deploy-publisher-job.yaml</span><br></pre></td></tr></table></figure>

<p>对应的我们也可以观察hpa和pod的扩缩容</p>
<p>已经在迅速扩容了</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ubectl get pod </span><br><span class="line">NAME                                     READY   STATUS              RESTARTS   AGE</span><br><span class="line">nfs-client-provisioner-68df844cc-dclwb   1/1     Running             0          8h</span><br><span class="line">rabbitmq-0                               1/1     Running             0          8h</span><br><span class="line">rabbitmq-consumer-5c486c869c-4pwxj       0/1     ContainerCreating   0          40s</span><br><span class="line">rabbitmq-consumer-5c486c869c-5wd9k       0/1     ContainerCreating   0          40s</span><br><span class="line">rabbitmq-consumer-5c486c869c-6zgs9       1/1     Running             0          56s</span><br><span class="line">rabbitmq-consumer-5c486c869c-78lx9       0/1     ContainerCreating   0          40s</span><br><span class="line">rabbitmq-consumer-5c486c869c-7g4wf       1/1     Running             0          71s</span><br><span class="line">rabbitmq-consumer-5c486c869c-7qhqn       0/1     ContainerCreating   0          40s</span><br><span class="line">rabbitmq-consumer-5c486c869c-8rj6l       1/1     Running             0          56s</span><br><span class="line">rabbitmq-consumer-5c486c869c-bmjqx       0/1     ContainerCreating   0          40s</span><br><span class="line">rabbitmq-consumer-5c486c869c-db2ts       1/1     Running             0          71s</span><br><span class="line">rabbitmq-consumer-5c486c869c-ddzhp       1/1     Running             0          56s</span><br><span class="line">rabbitmq-consumer-5c486c869c-dgwn4       0/1     ContainerCreating   0          40s</span><br><span class="line">rabbitmq-consumer-5c486c869c-f4k8l       1/1     Running             0          87s</span><br><span class="line">rabbitmq-consumer-5c486c869c-jqbhl       1/1     Running             0          87s</span><br><span class="line">rabbitmq-consumer-5c486c869c-k2mdw       0/1     ContainerCreating   0          40s</span><br><span class="line">rabbitmq-consumer-5c486c869c-kpgc2       0/1     ContainerCreating   0          56s</span><br><span class="line">rabbitmq-consumer-5c486c869c-kzlwl       0/1     ContainerCreating   0          56s</span><br><span class="line">rabbitmq-consumer-5c486c869c-ls4dm       1/1     Running             0          56s</span><br><span class="line">rabbitmq-consumer-5c486c869c-lvtx8       0/1     ContainerCreating   0          40s</span><br><span class="line">rabbitmq-consumer-5c486c869c-ngmnj       1/1     Running             0          71s</span><br><span class="line">rabbitmq-consumer-5c486c869c-nxp4n       0/1     ContainerCreating   0          40s</span><br><span class="line">rabbitmq-consumer-5c486c869c-p27lk       1/1     Running             0          71s</span><br><span class="line">rabbitmq-consumer-5c486c869c-p4d59       0/1     ContainerCreating   0          40s</span><br><span class="line">rabbitmq-consumer-5c486c869c-q9wdl       0/1     ContainerCreating   0          40s</span><br><span class="line">rabbitmq-consumer-5c486c869c-qqgdw       1/1     Running             0          56s</span><br><span class="line">rabbitmq-consumer-5c486c869c-rpghb       1/1     Running             0          89s</span><br><span class="line">rabbitmq-consumer-5c486c869c-sr7hd       0/1     ContainerCreating   0          40s</span><br><span class="line">rabbitmq-consumer-5c486c869c-sxl8t       1/1     Running             0          87s</span><br><span class="line">rabbitmq-consumer-5c486c869c-vk9wp       0/1     ContainerCreating   0          56s</span><br><span class="line">rabbitmq-consumer-5c486c869c-xgnbf       0/1     ContainerCreating   0          40s</span><br><span class="line">rabbitmq-consumer-5c486c869c-xn76k       0/1     ContainerCreating   0          40s</span><br><span class="line">rabbitmq-publish-rxjpl                   0/1     Completed           0          109s</span><br></pre></td></tr></table></figure>

<p>等待队列数据消费完了，等待冷却期，pod会自行销毁。</p>
<h4 id="基于Prometheus自定义监控指标弹性"><a href="#基于Prometheus自定义监控指标弹性" class="headerlink" title="基于Prometheus自定义监控指标弹性"></a>基于Prometheus自定义监控指标弹性</h4><p>使用自定义HPA监控的例子测试</p>
<p>clone代码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/stefanprodan/k8s-prom-hpa  </span><br></pre></td></tr></table></figure>
<p>切换到k8s-prom-hpa目录  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create namespace monitoring   </span><br><span class="line">kubectl create -f ./prometheus</span><br></pre></td></tr></table></figure>
<p>查看prometheus  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod -n monitoring</span><br><span class="line">NAME                                        READY   STATUS    RESTARTS   AGE</span><br><span class="line">prometheus-64bc56989f-qcs4p                 1/1     Running   1          22h</span><br><span class="line"></span><br><span class="line">kubectl  get svc -n monitoring</span><br><span class="line">NAME                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">prometheus                 NodePort    10.104.215.207   &lt;none&gt;        9090:31190/TCP   22h</span><br></pre></td></tr></table></figure>
<p>访问prometheus<br><a href="http://node_ip:31190/">http://node_ip:31190</a></p>
<p>prometheus内prometheus-cfg.yaml 配置了自动发生规则，会自动将组件注册。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/prometheus1_10.png">  </p>
<p>部署Podinfo应用测试custom-metric autoscale</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f ./podinfo/podinfo-svc.yaml,./podinfo/podinfo-dep.yaml</span><br></pre></td></tr></table></figure>
<p>prometheus配置了自动发现规则，在podinfo-dep.yaml里面配置了对应的规则</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">annotations:</span><br><span class="line">        prometheus.io/scrape: &#x27;true&#x27;</span><br></pre></td></tr></table></figure>
<p>所以应用一启动就能直接在prometheus-target中发现。  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/keda-1.png"></p>
<p>配置keda</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: keda.sh/v1alpha1</span><br><span class="line">kind: ScaledObject</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus-scaledobject</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    name: podinfo</span><br><span class="line">  pollingInterval: 5# Optional. Default: 30 seconds</span><br><span class="line">  cooldownPeriod: 30 # Optional. Default: 300 seconds</span><br><span class="line">  maxReplicaCount: 30 # Optional. Default: 100</span><br><span class="line"></span><br><span class="line">  triggers:</span><br><span class="line">  - type: prometheus</span><br><span class="line">    metadata:</span><br><span class="line">      serverAddress: http://192.168.0.32:31190/</span><br><span class="line">      metricName: http_requests_total</span><br><span class="line">      threshold: &#x27;10&#x27;</span><br><span class="line">      query: sum(rate(http_requests_total[1m]))</span><br></pre></td></tr></table></figure>



<p>这里10指的是每秒10个请求，按照定义的规则metricsQuery中的时间范围1分钟，这就意味着过去1分钟内每秒如果达到10个请求则会进行扩容。</p>
<p>配置完后使用ab进行压力测试</p>
<p>使用ab进行压力测试，模拟1000个并发量</p>
<p>安装ab</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apt install apache2-utils -y</span><br></pre></td></tr></table></figure>

<p>ab命令最基本的参数是<code>-n</code>和<code>-c</code>：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-n 执行的请求数量</span><br><span class="line">-c 并发请求个数</span><br><span class="line"></span><br><span class="line">其他参数：</span><br><span class="line">-t 测试所进行的最大秒数</span><br><span class="line">-p 包含了需要POST的数据的文件</span><br><span class="line">-T POST数据所使用的Content-<span class="built_in">type</span>头信息</span><br><span class="line">-k 启用HTTP KeepAlive功能，即在一个HTTP会话中执行多个请求，默认时，不启用KeepAlive功能</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">ab -n 10000 -c 1000  http://192.168.0.32:31198/</span><br></pre></td></tr></table></figure>


<p>查看是否进行弹性伸缩。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod </span><br><span class="line">NAME                                     READY   STATUS              RESTARTS   AGE</span><br><span class="line">nfs-client-provisioner-68df844cc-dclwb   1/1     Running             3          3d8h</span><br><span class="line">podinfo-56874dc7f8-4n9m4                 0/1     Running             4          74m</span><br><span class="line">podinfo-56874dc7f8-g89vr                 0/1     Running             0          43s</span><br><span class="line">podinfo-56874dc7f8-v2pn8                 1/1     Running             4          74m</span><br><span class="line">podinfo-56874dc7f8-xgrz6                 0/1     ContainerCreating   0          12s</span><br><span class="line">rabbitmq-0                               1/1     Running             0          3d8h</span><br><span class="line">rabbitmq-publish-rxjpl                   0/1     Completed           0          2d23h</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>keda会根据实际的值计算需要弹性的副本数，保证业务可用性。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">root@cka01:~# kubectl get hpa</span><br><span class="line">NAME                               REFERENCE                      TARGETS             MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">keda-hpa-prometheus-scaledobject   Deployment/podinfo             13/10 (avg)         1         30        3          3m3s</span><br></pre></td></tr></table></figure>




<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>总的感受下来，还是非常好用的，像之前配一些业务自定义指标的HPA监控还需要配置Prometheus-adapter<br>配置规则，但通过keda直接可以对接Prometheus，配置对应表达式尽快。其他一些集成的事件触发器也直接使用就可以了。</p>
<p>keda支持的业务事件触发器</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/keda-5.png"></p>
<p>参考链接：</p>
<p><a href="https://keda.sh/">https://keda.sh</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes namespace quota</title>
    <url>/2018/09/06/kubernetes_namespace_quota/</url>
    <content><![CDATA[<h3 id="概述："><a href="#概述：" class="headerlink" title="概述："></a>概述：</h3><p>当多个用户共享一个集群时，会担心其中某个用户将资源全部抢占，影响其他用户，namespace quota就是帮助我们解决这个问题的。namespace quota，就像我们在Iaas中针对每个project设置quota一样，来达到租户使用资源控制。</p>
<h3 id="环境："><a href="#环境：" class="headerlink" title="环境："></a>环境：</h3><p>os：ubuntu 16.04<br>docker：17.03<br>kubernetes：11.02</p>
<h3 id="使用："><a href="#使用：" class="headerlink" title="使用："></a>使用：</h3><p>目前kubernetes提供两种资源配额控制策略<br>LimitRange：用来给namespace内的pod设置一个默认的request和limit值<br>ResourceQuota：用来限制namespace中资源的占用</p>
<p>ResourceQuota：<br>注意点：<br>1、当创建的资源超过配额所限制的数量时，请求将失败，会抛出403错误。<br>2、在配置namespace quota之前创建的资源对象还会被算到used里面。<br>3、使用ResourceQuota后创建pod时必须配置Resource的request和limit不然会创建失败，当然你可以直接结合LimitRange使用，因为配置了limirange后会自动给pod配置limit和request。<br>4、在集群资源总容量小于namespace的配额的情况下，可能会存在资源占用情况，以先到先得方式处理。</p>
<p>ResourceQuota能控制三种类型的资源<br>计算资源配额<br>主要控制一些物理资源如CPU、内存这些<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/namespace_quota_1.png"></p>
<p>存储资源配额<br>主要配置请求存储的总量，pvc的数量<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/namespace_quota_2.png"></p>
<p>对象数量配额<br>主要配置kubernetes内的资源的数量，比如控制namespace内能启动多少pod<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/namespace_quota_3.png"></p>
<p>例<br>先创建一个namespace</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create namespace quota-object-example</span><br></pre></td></tr></table></figure>
<p>使用以下yaml文件创建一个namespace quota，应用到quota-object-example这个namespace</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ResourceQuota</span><br><span class="line">metadata:</span><br><span class="line">  name: object-quota-demo</span><br><span class="line">spec:</span><br><span class="line">  hard:</span><br><span class="line">    persistentvolumeclaims: &quot;1&quot;</span><br><span class="line">    services.loadbalancers: &quot;2&quot;</span><br><span class="line">    services.nodeports: &quot;0&quot;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f 1.yaml --namespace=quota-object-example</span><br></pre></td></tr></table></figure>
<p>简单分析一下，这里限制只能创建一个pvc、一个loadblance。不能创建nodeport</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get resourcequota object-quota-demo --namespace=quota-object-example --output=yaml</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">status:</span><br><span class="line">  hard:</span><br><span class="line">    persistentvolumeclaims: &quot;1&quot;</span><br><span class="line">    services.loadbalancers: &quot;2&quot;</span><br><span class="line">    services.nodeports: &quot;0&quot;</span><br><span class="line">  used:</span><br><span class="line">    persistentvolumeclaims: &quot;0&quot;</span><br><span class="line">    services.loadbalancers: &quot;0&quot;</span><br><span class="line">    services.nodeports: &quot;0&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>测试<br>创建应用，并用nodeport方式暴露  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl  run test --image nginx</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl expose deployment test --port=80 --type=NodePort</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/namespace_quota_4.png"></p>
<p>因为我们设置的namespace的quota services.nodeports:0所以这里创建失败直接抛403错误了。<br>创建pvc<br>使用下面文件创建一个pvc</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: pvc-quota-demo</span><br><span class="line">spec:</span><br><span class="line">  storageClassName: manual</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 3Gi</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f 1.yaml  --namespace=quota-object-example</span><br></pre></td></tr></table></figure>
<p>查看资源情况</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get resourcequota object-quota-demo --namespace=quota-object-example --output=yaml</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hard:</span><br><span class="line">   persistentvolumeclaims: &quot;1&quot;</span><br><span class="line">   services.loadbalancers: &quot;2&quot;</span><br><span class="line">   services.nodeports: &quot;0&quot;</span><br><span class="line">status:</span><br><span class="line"> hard:</span><br><span class="line">   persistentvolumeclaims: &quot;1&quot;</span><br><span class="line">   services.loadbalancers: &quot;2&quot;</span><br><span class="line">   services.nodeports: &quot;0&quot;</span><br><span class="line"> used:</span><br><span class="line">   persistentvolumeclaims: &quot;1&quot;</span><br><span class="line">   services.loadbalancers: &quot;0&quot;</span><br><span class="line">   services.nodeports: &quot;0&quot;</span><br></pre></td></tr></table></figure>
<p>可以看见的是在used这栏persistentvolumeclaims使用已经是1了，我们设置的配额是1如果我们再在这个namespace里面创建个pvc就会抛403了。</p>
<p>LimitRange：<br>前面已经说过了，LimitRange是用来给namespace内的pod设置一个默认的request和limit值，所以这里我们简单介绍几个场景  </p>
<p>先创建namespace</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create namespace default-cpu-example</span><br></pre></td></tr></table></figure>
<p>使用以下yaml文件创建limitRange，指定一个默认的cpu request和cpu limit</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: LimitRange</span><br><span class="line">metadata:</span><br><span class="line">  name: cpu-limit-range</span><br><span class="line">spec:</span><br><span class="line">  limits:</span><br><span class="line">  - default:</span><br><span class="line">      cpu: 1</span><br><span class="line">    defaultRequest:</span><br><span class="line">      cpu: 0.5</span><br><span class="line">    type: Container</span><br></pre></td></tr></table></figure>
<p>应用到default-cpu-example这个namespace</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f 1.yaml  --namespace=default-cpu-example</span><br></pre></td></tr></table></figure>

<p>场景一：在此命名空间创建pod，不设置Resource的limit和request<br>使用以下yaml文件创建pod</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: default-cpu-demo</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: default-cpu-demo-ctr</span><br><span class="line">    image: nginx</span><br></pre></td></tr></table></figure>
<p>部署pod</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f 1.yaml  --namespace=default-cpu-example</span><br></pre></td></tr></table></figure>
<p>查看pod yaml配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod default-cpu-demo --output=yaml --namespace=default-cpu-example</span><br></pre></td></tr></table></figure>
<p>可以看见Resource这里自动配置了limit和request，这个就是继承LimitRange</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">resources:</span><br><span class="line">      limits:</span><br><span class="line">        cpu: &quot;1&quot;</span><br><span class="line">      requests:</span><br><span class="line">        cpu: 500m</span><br></pre></td></tr></table></figure>
<p>场景二：只配置pod Resource的limit不配置request<br>使用以下yaml文件创建pod</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: default-cpu-demo-2</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: default-cpu-demo-2-ctr</span><br><span class="line">    image: nginx</span><br><span class="line">    resources:</span><br><span class="line">      limits:</span><br><span class="line">        cpu: &quot;1&quot;</span><br></pre></td></tr></table></figure>
<p>部署pod</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f 1.yaml  --namespace=default-cpu-example</span><br></pre></td></tr></table></figure>
<p>查看pod yaml配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod default-cpu-demo-2 --output=yaml --namespace=default-cpu-example</span><br></pre></td></tr></table></figure>

<p>需要注意的是这里request值没有继承LimitRange配置的值0.5而是直接根limit相等。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">resources:</span><br><span class="line">      limits:</span><br><span class="line">        cpu: &quot;1&quot;</span><br><span class="line">      requests:</span><br><span class="line">        cpu: &quot;1&quot;</span><br></pre></td></tr></table></figure>
<p>场景三：指定容器请求值，不指定容器限额值<br>使用以下yaml文件创建pod</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: default-cpu-demo-3</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: default-cpu-demo-3-ctr</span><br><span class="line">    image: nginx</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        cpu: &quot;0.75&quot;</span><br></pre></td></tr></table></figure>
<p>部署pod</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f 1.yaml  --namespace=default-cpu-example</span><br></pre></td></tr></table></figure>

<p>查看pod yaml配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod default-cpu-demo-2 --output=yaml --namespace=default-cpu-example</span><br></pre></td></tr></table></figure>
<p>需要注意的是这里request值没有继承LimitRange配置的值，而是直接是我们在pod中配置的值，limit继承的是LimitRange的值</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">resources:</span><br><span class="line">      limits:</span><br><span class="line">        cpu: &quot;1&quot;</span><br><span class="line">      requests:</span><br><span class="line">        cpu: 750m</span><br></pre></td></tr></table></figure>

<p>总结：</p>
<ul>
<li>如果没有在pod内设置request和limit默认就继承在namespace中配置的LimitRange。</li>
<li>如果在pod只配置了Resource的limit没配置request，这时request值不会继承LimitRange配置的值而是直接根pod中配置limit相等。</li>
<li>如果在pod中配置了request没有配置limit，这时request值以pod中配置的为准，limit值以namespace中的LimitRange为主。</li>
</ul>
<p><a href="https://kubernetes.io/docs/tasks/administer-cluster/quota-api-object/">https://kubernetes.io/docs/tasks/administer-cluster/quota-api-object/</a><br><a href="https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes资源模型概述</title>
    <url>/2020/08/24/kubernetes_request_limit/</url>
    <content><![CDATA[<h3 id="为业务配置Request和Limit"><a href="#为业务配置Request和Limit" class="headerlink" title="为业务配置Request和Limit"></a>为业务配置Request和Limit</h3><p>容器本身共享宿主机的资源，通过配置资源限制，能够更大程度利用宿主机的硬件资源，必免因为部分应用的问题导致影响其他运行的业务，在Kubernetes中资源限制方式主要通过Request和Limit。其中Request表示应用使用资源的最小值，Limit表示应用使用资源的最大值。资源管理的对象主要分为两种类型，可压缩资源和不可压缩资源，其中可压缩资源一般都以CPU资源为例，不可压缩资源主要对应的是内存。</p>
<p>配置Request  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line"> </span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment-request</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.14.2</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">        resources:</span><br><span class="line">         requests:</span><br><span class="line">           memory: &quot;500m&quot;</span><br><span class="line">           cpu: &quot;100m&quot;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>





<p>配置Limit</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line"></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment-limit</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.14.2</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">        resources:</span><br><span class="line">         limits:</span><br><span class="line">           memory: &quot;1Gi&quot;</span><br><span class="line">           cpu: &quot;500m&quot;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>



<p>CPU和RAM单位<br>CPU资源以cpus为单位。允许小数值。单位为millicores ，你可以用后缀m来表示mili。例如100m， 意思是使用单个核心的1&#x2F;100。1core&#x3D;1000m，若节点是4cpu对应的为4*1000&#x3D;4000m。  </p>
<p>1000m (milicores) &#x3D; 1 core<br>100m (milicores) &#x3D; 0.1 core</p>
<p>RAM资源以bytes为单位。你可以将RAM表示为纯整数或具有这些后缀之一的定点整数：<br>E, P, T, G, M, K, Ei, Pi, Ti, Gi, Mi, Ki。例如，以下代表大约相同的数值：</p>
<p>k8s中内存M和Mi的区别<br>M&#x3D;1000<em>1000<br>Mi&#x3D;1024</em>1024</p>
<h3 id="CPU-Request的实现原理"><a href="#CPU-Request的实现原理" class="headerlink" title="CPU Request的实现原理"></a>CPU Request的实现原理</h3><p>request则主要用以声明最小的CPU核数。一方面则体现在设置cpushare上。比如request.cpu&#x3D;3，则cpushare&#x3D;1024*3&#x3D;3072。</p>
<p>以刚刚部署的nginx-deployment-request和nginx-deployment-limit为例查看对应的操作系统上的cgroup</p>
<p>查看对应POD所在节点</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod  -o wide</span><br><span class="line">NAME                                        READY   STATUS    RESTARTS   AGE   IP           NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">nginx-deployment-limit-6f555f68df-pt45x     1/1     Running   0          18s   10.42.1.41   rke-node3   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-deployment-request-76d9f4d69d-7nh6j   1/1     Running   0          46s   10.42.1.40   rke-node3   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">test-7fd4b897b8-592fl                       1/1     Running   0          64m   10.42.0.30   rke-node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p>ssh连接到节点，通过docker查看对应的cgroup目录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker ps | grep request|grep -v pause | cut -d&#x27; &#x27; -f1</span><br><span class="line"></span><br><span class="line">fa82dcca40b4</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker inspect fa82dcca40b4 --format &#x27;&#123;&#123;.HostConfig.CgroupParent&#125;&#125;&#x27;</span><br><span class="line"></span><br><span class="line">/kubepods/burstable/pod51932f96-0e7f-4528-92bd-c01023e97abd</span><br></pre></td></tr></table></figure>

<p>查看对应cgroup目录  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pod51932f96-0e7f-4528-92bd-c01023e97abd/cpu.shares </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">102</span><br></pre></td></tr></table></figure>


<p>比如nginx-deployment-request设置request.cpu&#x3D;100m，则102 &#x3D; (request.cpu * 1024) &#x2F;1000</p>
<p>通过cpu.shares实现<br>在cpu繁忙情况下仍然能给优先级高的应用分配到对应的CPU计算量。</p>
<h3 id="Memory、cpu-Limit的实现原理"><a href="#Memory、cpu-Limit的实现原理" class="headerlink" title="Memory、cpu Limit的实现原理"></a>Memory、cpu Limit的实现原理</h3><p>cpu limit主要用以声明使用的最大的CPU核数。通过设置cfs_quota_us和cfs_period_us。比如limits.cpu&#x3D;3，则cfs_quota_us&#x3D;300000。<br>cfs_period_us值一般都使用默认的100000<br>在cgroup，cpu子系统中通过cfs_quota_us&#x2F;cfs_period_us严格限制cpu的的使用量</p>
<p>继续以部署的nginx-deployment-limit为例讲解，找到对应的cgroup目录</p>
<p>cpu.cfs_quota_us参数为<br>50000</p>
<p>cfs_period_us参数为 100000</p>
<p>cfs_quota_us&#x2F;cfs_period_us&#x3D;0.5</p>
<p>对应的500m</p>
<p>memory limit通过对应的cgroup子目录的memory.limit_in_bytes字段进行限制。  </p>
<h3 id="到达资源限制后的响应措施"><a href="#到达资源限制后的响应措施" class="headerlink" title="到达资源限制后的响应措施"></a>到达资源限制后的响应措施</h3><p>当pod 内存超过limit时，会被oom。<br>当cpu超过limit时，不会被kill，但是会限制不超过limit值。  </p>
<p>内存资源限制   </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line"></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: deployment-oom</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: polinux/stress</span><br><span class="line">        command:</span><br><span class="line">        - stress</span><br><span class="line">        - --vm</span><br><span class="line">        - &quot;1&quot;</span><br><span class="line">        - --vm-bytes</span><br><span class="line">        - 250M</span><br><span class="line">        - --vm-hang</span><br><span class="line">        - &quot;1&quot;</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">        resources:</span><br><span class="line">         limits:</span><br><span class="line">           memory: &quot;100Mi&quot;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p>部署一个压测容器，压测时会分配250M内存，但实际pod的内存limit为100Mi。</p>
<p>一达到设置的阈值，便会被OOMKILL掉。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod </span><br><span class="line">NAME                                        READY   STATUS      RESTARTS   AGE</span><br><span class="line">deployment-oom-75d47b57c4-sst77             0/1     OOMKilled   0          33s</span><br></pre></td></tr></table></figure>




<p>将limits值调整到1000Mi查看<br>可以看见内存使用值稳定在250MB左右，因将limit值调大了，所以它不会被杀死。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl top pod</span><br><span class="line">NAME                                        CPU(cores)   MEMORY(bytes)   </span><br><span class="line">deployment-oom-76cd9448cc-gxprn             230m         251Mi    </span><br></pre></td></tr></table></figure>


<p>cpu使用率超过limit时，不会被kill，但是会限制不超过limit值，影响实际业务性能。</p>
<p>可以看见cpu使用在快速增长<br><img src="https://ae01.alicdn.com/kf/H254e4cdd48e740928b1a0dd3bec80f036.jpg">  </p>
<p>配置cpu limit限制cpu使用为1000 查看资源监控</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line"></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: test-cpu</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        args:</span><br><span class="line">        - -cpus</span><br><span class="line">        - &quot;2&quot;</span><br><span class="line">        image: vish/stress</span><br><span class="line">        resources:</span><br><span class="line">         limits:</span><br><span class="line">           cpu: &quot;1&quot;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<p>查看监控，cpu usage为cpu使用率，当使用率达到限制值1000后，便不在增长，保持平稳。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl top pod </span><br><span class="line">NAME                                        CPU(cores)   MEMORY(bytes)   </span><br><span class="line">test-cpu-6c945c9469-bnprz                   999m         1Mi   </span><br></pre></td></tr></table></figure>



<h3 id="配置的最佳实践"><a href="#配置的最佳实践" class="headerlink" title="配置的最佳实践"></a>配置的最佳实践</h3><p>POD对应多个不同的QOS级别，不同的QOS级别在资源抢占时对应不同的策略</p>
<h4 id="配置方式最佳实践："><a href="#配置方式最佳实践：" class="headerlink" title="配置方式最佳实践："></a>配置方式最佳实践：</h4><p>Guaranteed 级别<br>1、 pod 中的每一个 container 都必须包含cpu和内存资源的 limit、request 信息，并且这两个值必须相等  </p>
<p>Burstable 级别<br>1、资源申请信息不满足 Guaranteed 级别的要求• 2、pod 中至少有一个 container 指定了 cpu 或者 memory 的 request。</p>
<p>BestEffort级别<br>1、 pod 中任何一个container都没有配置cpu或者 memory的request，limit信息</p>
<p>所以建议对于一些关键型业务为避免因为资源不够被杀，建议配置好cpu和内存的Request和limit并使他们相等。  </p>
<h4 id="配置值的最佳实际"><a href="#配置值的最佳实际" class="headerlink" title="配置值的最佳实际"></a>配置值的最佳实际</h4><p>实际上应用容器化改造上线时，切不可以当时申请虚拟机的cpu和内存值做为参考值，因为这部分值是偏大的，容易造成资源浪费，实际上我们可以用两种方式获取业务的实际资源值。</p>
<p>方式一：通过是旧的业务，我们可以通过历史的监控信息看见对应的cpu和内存在不同业务量下小的消耗值，这个值可以做为我们实际的配置值。</p>
<p>方式二：新业务可以通过性能压测，模拟业务请求量得到对应业务量下的资源消耗，这样往往得到的值是准确的。  </p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes RBAC</title>
    <url>/2018/02/26/kubernetes_rbac/</url>
    <content><![CDATA[<p>操作系统：<br>centos7.3<br>软件版本：   </p>
<ul>
<li>kubernetes 1.8</li>
<li>docker-ce 17.03.02</li>
</ul>
<h3 id="概念："><a href="#概念：" class="headerlink" title="概念："></a>概念：</h3><p>权限管理<br>是指控制用户对某些特定资源的访问权限。<br>设计理念<br>ACL(访问控制列表):对资源进行权限控制，每个资源都有一个权限列表记录着哪些用户对这个资源能进行哪些操作，优点：实现非常简单，只需要将用户和资源连接起来就可以。缺点是：当用户和资源增多时会产生大量访问控制列表，管理起来非常麻烦。<br>ABAC(基于属性的访问控制)：ABAC控制权限的粒度非常细，非常灵活；属性通常来说分为四类：用户属性、环境属性、操作属性、对象属性；ABAC通过动态的控制一组属性是否满足条件来进行授权判断。优点：权限控制的粒度非常细，非常灵活。缺点：太过于复杂。<br>例如规则：“禁止所有学生在上课时间进出校门”这条规则，其中，“学生”是用户的角色属性，“上课时间”是环境属性，“进出”是操作属性，而“校门”就是对象属性了。<br>RBAC(基于角色的访问控制)：引入角色概念，通过将权限和角色关联，来实现用户根权限的解耦，角色可以看做是权限的一个集合。通过权限与角色关联，角色与用户的关联可以实现用户和角色多对多的对应关系。优点 ；将权限与角色关联，用户关联角色，这样权限控制更加灵活 。缺点：相比ABAC控制的粒度没有那么细。<br>RBAC的模型<br>Who是否可以对What进行How的访问操作（Operator）    </p>
<p>kubernetes之前的版本都是使用ABAC做为权限管理控制，在kubernetes1.3发布alpha版本RBAC,在kubernetes1.6版本RBAC提升为Beta版，kubernetes1.8版本正式提升为GA版，为什么用RBAC取代ABAC是因为在kubernetes中ABAC过于复杂，不好管理，难以理解，RBAC相对功能和可管理性来说更加合适kubernetes。所以说没有更好，只有更合适。  </p>
<p>在kubernetes中的RBAC定义了四类API资源分别为：<br>Role：role是一组权限的集合，namespace范围的Role，role权限的有效范围为指定的单一namespace。<br>ClusterRole：clusterrole根role一样也是一组权限的集合，cluster范围的Role，但clusterrole的有效范围是整个集群的权限和一些集群级别的资源如：集群的node节点、非资源类型endpoint、跨所有命名空间的命名空间范围资源（例如pod，需要运行命令kubectl get pods –all-namespaces来查询集群中所有的pod）。<br>RoleBinding：将角色根用户或组或ServiceAccount绑定，授予它们对该命名空间资源的权限。<br>ClusterRoleBinding：将ClusterRole与用户绑定，授予它们对cluster的资源访问权限。  </p>
<p>例子<br>首先我们创建个wanshaoyuan的普通用户<br>确认kubernetes证书目录是否有以下文件<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_rbac_1.png"><br>若没有ca-config.json<br>创建一个  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &gt; ca-config.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;signing&quot;: &#123;</span><br><span class="line">    &quot;default&quot;: &#123;</span><br><span class="line">      &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;profiles&quot;: &#123;</span><br><span class="line">      &quot;kubernetes&quot;: &#123;</span><br><span class="line">        &quot;usages&quot;: [</span><br><span class="line">            &quot;signing&quot;,</span><br><span class="line">            &quot;key encipherment&quot;,</span><br><span class="line">            &quot;server auth&quot;,</span><br><span class="line">            &quot;client auth&quot;</span><br><span class="line">        ],</span><br><span class="line">        &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>ca-config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile；<br>1、创建证书签名请求  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &gt; wanshaoyuan-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;wanshaoyuan&quot;,</span><br><span class="line">  &quot;hosts&quot;: [],</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>生成证书和公钥  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cfssl gencert -ca=ca.crt -ca-key=ca.key -config=ca-config.json -profile=kubernetes wanshaoyuan-csr.json | cfssljson -bare wanshaoyuan</span><br></pre></td></tr></table></figure>

<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_rbac_2.png"></p>
<p>创建kubeconfig文件  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export KUBE_APISERVER=&quot;https://192.168.1.10:6443&quot;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true --server=$&#123;KUBE_APISERVER&#125; --kubeconfig=wanshaoyuan.kubeconfig</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl config set-credentials wanshaoyuan --client-certificate=/etc/kubernetes/pki/wanshaoyuan.pem --client-key=/etc/kubernetes/pki/wanshaoyuan-key.pem --embed-certs=true --kubeconfig=wanshaoyuan.kubeconfig</span><br></pre></td></tr></table></figure>
<p>设置context  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl config set-context kubernetes --cluster=kubernetes --user=wanshaoyuan  --kubeconfig=wanshaoyuan.kubeconfig</span><br></pre></td></tr></table></figure>
<p>设置默认context,将集群参数和用户参数关联起来，如果配置了多个集群，可以通过集群名来切换不同的环境  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl config use-context kubernetes --kubeconfig=wanshaoyuan.kubeconfig</span><br></pre></td></tr></table></figure>
<p>查看kubectl的context<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_rbac_3.png"></p>
<p>用户目前还是kubernetes-admin，切换到wanshaoyuan</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">KUBECONFIG=/etc/kubernetes/pki/wanshaoyuan.kubeconfig</span><br></pre></td></tr></table></figure>
<p>或</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp /etc/kubernetes/pki/wanshaoyuan.kubeconfig  /root/.kube/config</span><br></pre></td></tr></table></figure>
<p>再次查看<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_rbac_4.png"></p>
<p>用户目前还是kubernetes-admin，切换到wanshaoyuan  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">KUBECONFIG=/etc/kubernetes/pki/wanshaoyuan.kubeconfig</span><br></pre></td></tr></table></figure>
<p>或  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp /etc/kubernetes/pki/wanshaoyuan.kubeconfig  /root/.kube/config</span><br></pre></td></tr></table></figure>
<p>再次查看<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_rbac_6.png"></p>
<p>创建角色<br>定义这个角色只能对default这个namespace 执行get、watch、list权限<br>定义角色  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat role.yaml</span><br><span class="line"></span><br><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  namespace: default</span><br><span class="line">  name: pod-reader</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;] # 空字符串&quot;&quot;表明使用core API group</span><br><span class="line">  resources: [&quot;pods&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>角色绑定  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">cat role_bind.yaml</span><br><span class="line"></span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: read-pods</span><br><span class="line">  namespace: default</span><br><span class="line">subjects:</span><br><span class="line">- kind: User</span><br><span class="line">  name: wanshaoyuan</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">roleRef:</span><br><span class="line">  kind: Role</span><br><span class="line">  name: pod-reader</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>应用     </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f role.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f role_bind.yaml</span><br></pre></td></tr></table></figure>

<p>check一下<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_rbac_5.png"></p>
<p>切到wanshaoyuan这个用户<br>root用户将KUBECONFIG这个变量指向刚刚创建用户出来的kubeconfig</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">KUBECONFIG=/root/RBAC/wanshaoyuan.kubeconfig</span><br></pre></td></tr></table></figure>
<p>非root用户替换默认的config文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp wanshaoyuan.kubeconfig  /root/.kube/config</span><br></pre></td></tr></table></figure>
<p>检查是否切换成功<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_rbac_6.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_rbac_7.png"></p>
<p>验证<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_rbac_8.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_rbac_9.png">  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_rbac_10.png">  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_rbac_11.png">  </p>
<p>可以看见wanshaoyuan这个用户只能访问default这个namespace的pod资源，其他的如namespace都访问不了，同样namespace的其他资源也访问不了<br>clusterrole定义的是集群级别的权限，也可以把它授予一些集群级别的资源  </p>
<p>例如<br>集群的node节点<br>非资源类型endpoint<br>跨所有命名空间的命名空间范围资源（例如pod，需要运行命令kubectl get pods –all-namespaces来查询集群中所有的pod）  </p>
<p>例如<br>定义一个clusterrole限制只能访问集群中的node节点，不能修改，其他资源都不能访问。<br>cat cluster_role.yaml   </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line"> name: secret-reader</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;nodes&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>再定义一个ClusterRoleBinding，将上面的clusterrole和用户wanshaoyuan绑定起来  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat cluster_role_bind.yaml</span><br><span class="line"></span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: read-secrets-global</span><br><span class="line">subjects:</span><br><span class="line">- kind: User</span><br><span class="line">  name: wanshaoyuan</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: secret-reader</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>应用  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f cluster_role.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f cluster_role_bind.yaml</span><br></pre></td></tr></table></figure>

<p>切换用户   </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">KUBECONFIG=/root/RBAC/wanshaoyuan.kubeconfig</span><br></pre></td></tr></table></figure>
<p>测试<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_rbac_12.png">  </p>
<p>只能访问node，不能做其他编辑操作，集群内其他资源也访问不了。<br>例如<br>定义一个clusterrole限制只能访问集群所有namespace的cesecret，其他资源都不能访问<br>先定义一个clusterRole</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat /root/RBAC/cluster_role.yaml  </span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line"> name: secret-reader</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;secrets&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>再定义一个ClusterRoleBinding，将上面的clusterrole和用户wanshaoyuan绑定起来  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat /root/RBAC/cluster_role_bind.yaml</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: read-secrets-global</span><br><span class="line">subjects:</span><br><span class="line">- kind: User</span><br><span class="line">  name: wanshaoyuan</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: secret-reader</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>应用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f cluster_role.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f cluster_role_bind.yaml</span><br></pre></td></tr></table></figure>
<p>切换用户  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">KUBECONFIG=/root/RBAC/wanshaoyuan.kubeconfig</span><br></pre></td></tr></table></figure>

<p>验证<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_rbac_13.png">  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_rbac_14.png">  </p>
<p>可以看到，wanshaoyuan这个用户，可以get集群中任意namespace的secret，但其他资源，如pod，之类的都无法查看<br>修改下，让wanshaoyuan这个帐户可以同时查看pod、secret、deployment  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">KUBECONFIG=/root/RBAC/wanshaoyuan.kubeconfig</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">KUBECONFIG=/etc/kubernetes/admin.conf</span><br></pre></td></tr></table></figure>
<p>修改  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/root/RBAC/cluster_role.yaml</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line"> name: secret-reader</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;secrets&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</span><br><span class="line"></span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;pod&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</span><br><span class="line"></span><br><span class="line">- apiGroups: [&quot;extensions&quot;, &quot;apps&quot;]</span><br><span class="line">  resources: [&quot;deployments&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</span><br></pre></td></tr></table></figure>
<p>用kubectl  apply去实时更新  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f cluster_role.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f cluster_role.yaml</span><br></pre></td></tr></table></figure>


<p>切换用户测试<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_rbac_15.png">  </p>
<p>pod、secret、deployment都能正常获取    </p>
<p>角色绑定包含一组相关主体即subject，subject可以是User(用户)也可以是Group(组)还可以是Service Account。<br>这里要强调下<br>Kubernetes 集群中包含两类用户：一类是由 Kubernetes 管理的 service account，另一类是普通用户。  </p>
<p>普通用户被假定为由外部独立服务管理。管理员分发私钥，用户存储（如 Keystone 或 Google 帐户），甚至包含用户名和密码列表的文件。在这方面，Kubernetes 没有代表普通用户帐户的对象。无法通过 API 调用的方式向集群中添加普通用户。  </p>
<p>相对的，service account 是由 Kubernetes API 管理的帐户。它们都绑定到了特定的 namespace，并由 API server 自动创建，或者通过 API 调用手动创建。Service account 关联了一套凭证，存储在 Secret，这些凭证同时被挂载到 pod 中，从而允许 pod 与 kubernetes API 之间的调用。  </p>
<p>API 请求被绑定到普通用户或 serivce account 上，或者作为匿名请求对待。这意味着集群内部或外部的每个进程，输入 kubectl ，都必须在向 API Server 发出请求时进行身份验证，否则被视为匿名用户。<br>来源：<a href="https://jimmysong.io/posts/user-authentication-in-kubernetes/">https://jimmysong.io/posts/user-authentication-in-kubernetes/</a>  </p>
<p>kubernetes在每个namespace内都有一个默认的service account,当在创建pod或其他资源时，没有手工指定Service account时会默认使用namespace里面这个default这个Service account  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_rbac_16.png">  </p>
<p>一般来说为了安全起见，建议一个应用使用一个独立的Service account运行，这样RoleBindings就不会无意中授予其他应用程序的权限，如果需要这样使用的话在部署yaml文件中添加  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_rbac_17.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_rbac_18.png"><br>可以通过以下命令对namespace中的Service account进行默认权限配置<br>授予my-namespace命名空间内的default这个Service account帐户只读权限(view是集群中的一组通用角色，它的默认权限的是只读，它归属于clusterrole  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create rolebinding default-view \</span><br><span class="line">  --clusterrole=view \</span><br><span class="line">  --serviceaccount=my-namespace:default \</span><br><span class="line">  --namespace=my-namespace</span><br></pre></td></tr></table></figure>
<p>授予my-namespace命名空间内的my-sa这个Service account帐户只读权限  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create rolebinding my-sa-view \</span><br><span class="line">  --clusterrole=view \</span><br><span class="line">  --serviceaccount=my-namespace:my-sa \</span><br><span class="line">  --namespace=my-namespace</span><br></pre></td></tr></table></figure>

<p>授予my-namespace命名空间内所有的Service account帐户只读权限  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create rolebinding serviceaccounts-view \</span><br><span class="line">  --clusterrole=view \</span><br><span class="line">  --group=system:serviceaccounts:my-namespace \</span><br><span class="line">  --namespace=my-namespace</span><br></pre></td></tr></table></figure>
<p>默认的Role和ClusteRrole<br>api-service会默认创建一些role和rolebinding 和clusterrole和clusterrolebind，他们默认都是以system开头的，以system开头的都是为kubernetes系统使用而保留的  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_rbac_19.png">  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_rbac_21.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_rbac_22.png">  </p>
<p>这里不多介绍主要讲讲clusterrole里面cluster-admin、admin、edit、view这几个clusterrole，直接参考<a href="https://v1-8.docs.kubernetes.io/docs/admin/authorization/rbac/">https://v1-8.docs.kubernetes.io/docs/admin/authorization/rbac/</a><br>cluster-admin、admin、edit、view这几个角色是不包含system前缀的，他们是面向用户的角色，可以直接使用rolebind直接使用<br>具体权限参考下图  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/kubernetes_rbac_23.png">  </p>
<p>比如我们需要访问kubernetes-dashboard时，需要一个能够访问获取所有集群资源的用户此时就可以创建一个帐户关联cluster-admin这个角色。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line"> name: admin</span><br><span class="line"> annotations:</span><br><span class="line">   rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot;</span><br><span class="line">roleRef:</span><br><span class="line"> kind: ClusterRole</span><br><span class="line"> name: cluster-admin</span><br><span class="line"> apiGroup: rbac.authorization.k8s.io</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line"> name: admin</span><br><span class="line"> namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line"> name: admin</span><br><span class="line"> namespace: kube-system</span><br><span class="line"> labels:</span><br><span class="line">   kubernetes.io/cluster-service: &quot;true&quot;</span><br><span class="line">   addonmanager.kubernetes.io/mode: Reconcile</span><br></pre></td></tr></table></figure>

<p>上面创建一个ServiceAccount帐户admin并将它与cluster-admin角色绑定，然后我们可以通过获取admin的token去访问dashboard。<br>kubectl -n kube-system describe secret admin-token-nwphb</p>
<p>apiGroup解释<br>上面各类角色转换中的apiGroup的解释<br>来源：<a href="https://segmentfault.com/a/1190000008700595">https://segmentfault.com/a/1190000008700595</a>  </p>
<p>k8s里面有各种资源，如Pod、Service、RC、namespaces等资源，用户操作的其实也就是这一大堆资源。但这些资源并不是杂乱无章的，使用了GroupVersion的方式组织在一起。每一种资源都属于一个Group，而资源还有版本之分，如v1、v1beta1等。<br>k8s目前正在使用的API groups：  </p>
<ul>
<li>“core” group：它的REST path是api&#x2F;v1  </li>
<li>“extensions” group：它的REST path是&#x2F;apis&#x2F;extensions&#x2F;v1beta1  </li>
<li>“autoscaling”, “abac” …</li>
</ul>
<p>k8s现阶段，API一共分为13个Group：Core、apps、authentication、authorization、autoscaling、batch、certificates、componentconfig、extensions、imagepolicy、policy、rbac、storage。其中Core的Group Name为空，它包含的API是最核心的API,如Pod、Service等。  </p>
<p>参考链接：<br><a href="http://blog.csdn.net/wangjingna/article/details/49226727">http://blog.csdn.net/wangjingna/article/details/49226727</a><br><a href="http://blog.csdn.net/jettery/article/details/70138003">http://blog.csdn.net/jettery/article/details/70138003</a><br><a href="http://blog.csdn.net/yan234280533/article/details/75808048">http://blog.csdn.net/yan234280533/article/details/75808048</a><br><a href="http://blog.csdn.net/yan234280533/article/details/76359199">http://blog.csdn.net/yan234280533/article/details/76359199</a><br><a href="http://blog.csdn.net/qqhappy8/article/details/78891200">http://blog.csdn.net/qqhappy8/article/details/78891200</a><br><a href="http://www.bijishequ.com/detail/570501">http://www.bijishequ.com/detail/570501</a><br><a href="http://www.sohu.com/a/205750642_610730">http://www.sohu.com/a/205750642_610730</a><br><a href="https://v1-8.docs.kubernetes.io/docs/admin/authorization/rbac/">https://v1-8.docs.kubernetes.io/docs/admin/authorization/rbac/</a><br><a href="https://jimmysong.io/kubernetes-handbook/">https://jimmysong.io/kubernetes-handbook/</a><br><a href="https://www.cnblogs.com/charlieroro/p/8489515.html">https://www.cnblogs.com/charlieroro/p/8489515.html</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes安全机制(一)</title>
    <url>/2020/07/09/kubernetes_security_1/</url>
    <content><![CDATA[<h3 id="常见安全攻击面描述"><a href="#常见安全攻击面描述" class="headerlink" title="常见安全攻击面描述"></a>常见安全攻击面描述</h3><h4 id="安全攻击面"><a href="#安全攻击面" class="headerlink" title="安全攻击面"></a>安全攻击面</h4><p>图片来源网络</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_1.png" alt="image"></p>
<p>外部访问攻击：核心组件非安全端口对外暴露，以及连接集群关键凭证泄漏。</p>
<p>平台组件攻击：  利用平台组件docker、kube-apiserver、etcd、kubelet 非安全端口对外暴露，以及集群组件间连接安全，以及组件的安全漏洞进行攻击。</p>
<p>POD层面攻击：利用运行在集群内的恶意POD进行攻击。</p>
<h4 id="安全矩阵"><a href="#安全矩阵" class="headerlink" title="安全矩阵"></a>安全矩阵</h4><p>图片来源网络<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_2.png" alt="image"></p>
<h4 id="安全问题案例"><a href="#安全问题案例" class="headerlink" title="安全问题案例"></a>安全问题案例</h4><p>图片来源网络<img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_3.png" alt="image"></p>
<p>2018年黑客入侵了特斯拉在亚马逊上的Kubernetes容器集群。由于该集群控制台未设置密码保护，黑客便得以在一个Kubernetes pod中获取到访问凭证，然后据此访问其网络存储桶S3，通过S3获取到了一些敏感数据，比如遥测技术，并且还在特斯拉的Kubernetes pod中进行挖矿。</p>
<h4 id="安全攻击点—DockerDaemon公网暴露"><a href="#安全攻击点—DockerDaemon公网暴露" class="headerlink" title="安全攻击点—DockerDaemon公网暴露"></a>安全攻击点—DockerDaemon公网暴露</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_4.png" alt="image"></p>
<p>Docker本身是C&#x2F;S架构，Docker-client连接Docker-Daemon对docker进行操作。</p>
<p>Client连接Daemon有以下常见方式：<br>1、tcp:&#x2F;&#x2F;host:port<br>2、unix:&#x2F;&#x2F;path_to_socket<br>3、fd:&#x2F;&#x2F;socketfd。</p>
<p>若通过TCP方式将dockerDaemon对外暴露，攻击者可以通过Api接口接管本地Docker服务。</p>
<h4 id="安全攻击点—包含恶意程序的镜像"><a href="#安全攻击点—包含恶意程序的镜像" class="headerlink" title="安全攻击点—包含恶意程序的镜像"></a>安全攻击点—包含恶意程序的镜像</h4><p>图片来源网络</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_5.png" alt="image"></p>
<p>图片来源网络</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_6.png" alt="image"></p>
<h4 id="安全攻击点—组件安全漏洞利用风险"><a href="#安全攻击点—组件安全漏洞利用风险" class="headerlink" title="安全攻击点—组件安全漏洞利用风险"></a>安全攻击点—组件安全漏洞利用风险</h4><p>图片来源网络</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_7.png" alt="image"></p>
<p>利用平台docker、k8s组件漏洞进行攻击如：<br>Docker Runc逃逸漏洞<br>此漏洞允许以root身份运行的容器以特权用户身份在主机上执行任意代码。这意味着容器可能会破坏Docker主机（覆盖Runc CLI），而所需要的只是能够使用root来运行容器。攻击者可以使用受感染的Docker镜像或对未受感染的正在运行的容器运行exec命令。</p>
<p>漏洞影响:<br>运行那些从不受信任的来源处获取到的镜像，那么其主机很有可能被攻击者所全面接收。</p>
<h4 id="安全攻击点—容器特权模式逃逸"><a href="#安全攻击点—容器特权模式逃逸" class="headerlink" title="安全攻击点—容器特权模式逃逸"></a>安全攻击点—容器特权模式逃逸</h4><p>图片来源网络</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_8.png" alt="image"></p>
<p>特权模式容器允许允许容器内的root拥有外部物理机的root权限， ，可以直接获取宿主机设备文件访问权限。</p>
<p>可以通过mount命令将宿主机磁盘设备挂载进容器内部，获取对整个宿主机的文件读写权限。</p>
<h4 id="安全攻击点—利用挂载目录逃逸"><a href="#安全攻击点—利用挂载目录逃逸" class="headerlink" title="安全攻击点—利用挂载目录逃逸"></a>安全攻击点—利用挂载目录逃逸</h4><p>图片来源网络</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_9.png" alt="image"></p>
<p>将宿主机部分核心目录以写权限挂载进容器内，攻击者进入容器内可以对这些目录进行修改，如利用crontab定义一些执行脚本，来达到破坏目的。</p>
<h4 id="安全攻击点—利用Linux-Capabilities逃逸"><a href="#安全攻击点—利用Linux-Capabilities逃逸" class="headerlink" title="安全攻击点—利用Linux Capabilities逃逸"></a>安全攻击点—利用Linux Capabilities逃逸</h4><p>图片来源网络</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_10.png" alt="image"></p>
<p>Linux Capabilities将root用户关联的特权拆分为一个个单元，每个单元可以独立启用和禁用，来达到细粒化权限控制。</p>
<p>容器通过cgroup和Namespace实现资源隔离同时也支持通过Capabilities进行细粒化权限管理，docker和k8s都是通过白名单方式添加Capabilities。如添加CAP_SYS_PTRACE特性在配合pid&#x3D;host参数可以实现在容器内使用strance进行宿主机的进程追踪。</p>
<h4 id="安全攻击点—利用Linux-Capabilities逃逸-1"><a href="#安全攻击点—利用Linux-Capabilities逃逸-1" class="headerlink" title="安全攻击点—利用Linux Capabilities逃逸"></a>安全攻击点—利用Linux Capabilities逃逸</h4><p>图片来源网络</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_11.png" alt="image"><br>图片来源网络</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_12.png" alt="image"></p>
<h4 id="安全攻击点—凭证泄漏"><a href="#安全攻击点—凭证泄漏" class="headerlink" title="安全攻击点—凭证泄漏"></a>安全攻击点—凭证泄漏</h4><p>图片来源网络</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_13.png" alt="image"></p>
<p>公有云上平台凭证泄漏，导致可以直接获取访问Kubernetes平台的连接信息，对Kubernetes进行控制和操作。</p>
<h3 id="防范机制"><a href="#防范机制" class="headerlink" title="防范机制"></a>防范机制</h3><h4 id="平台安全框架"><a href="#平台安全框架" class="headerlink" title="平台安全框架"></a>平台安全框架</h4><p>认证(Authenticating)是服务端对客户端请求的合法性鉴别。</p>
<p>授权(Authorization)是对访问资源的授权，当一个请求经过认证后，需要访问某一个资源（比如创建一个pod），授权检查都会通过访问策略比较该请求上下文的属性，（比如用户，资源和Namespace），根据授权规则判定该资源（比如某namespace下的pod）是否是该客户可访问的。</p>
<p>准入(Admission Control)机制是一种请求拦截机制，用于对请求的资源对象进行校验，修改。如Istio中的自动在对应namespace的POD中自动注入sidecar。</p>
<h4 id="Kubernetes组件安全机制"><a href="#Kubernetes组件安全机制" class="headerlink" title="Kubernetes组件安全机制"></a>Kubernetes组件安全机制</h4><p>图片来源网络</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_14.png" alt="image"></p>
<p>1、全部组件都需要连接Api-server，默认情况下使用 TLS 对集群中其他组件到 Api-server 通信进行加密。</p>
<p>2、组件通过对应的ServiceAccount运行，通过RBAC进行授权限制。</p>
<p>3、限制etcd访问，集群中只有api-server能够访问ETCD，并且对于etcd的访问采用TLS客户端证书相互认证。</p>
<p>4、kubelet做为apiserver获取节点pod信息的入口，启用身份验证和授权。</p>
<h4 id="认证机制"><a href="#认证机制" class="headerlink" title="认证机制"></a>认证机制</h4><ul>
<li><p>X509 Client Certs</p>
</li>
<li><p>Static Token File</p>
</li>
<li><p>Bootstrap Tokens</p>
</li>
<li><p>Static Password File</p>
</li>
<li><p>Service Account Tokens</p>
</li>
<li><p>OpenID Connect Tokens</p>
</li>
</ul>
<h4 id="X509-Client-Certs"><a href="#X509-Client-Certs" class="headerlink" title="X509 Client Certs"></a>X509 Client Certs</h4><p>图片来源网络</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_15.png" alt="image"></p>
<p>双向数字证书认证，HTTPS证书认证，是基于CA根证书签名的双向数字证书认证方式，是所有认证方式中最严格的认证。也是Kubernetes默认认证方式，组件之间以及外部客户端认证方式。</p>
<p>Api-Server相关的三个启动参数：</p>
<ul>
<li>client-ca-file: 指定CA根证书文件为&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.pem</li>
<li>tls-private-key-file: 指定ApiServer私钥文件为&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;apiserver-key.pem</li>
<li>tls-cert-file：指定ApiServer证书文件为&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;apiserver.pem</li>
</ul>
<h4 id="Service-Account-Tokens"><a href="#Service-Account-Tokens" class="headerlink" title="Service Account Tokens"></a>Service Account Tokens</h4><p>图片来源网络</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_16.png" alt="image"></p>
<p>用于控制集群内部POD对象访问API-server.</p>
<p>Kubernetes会为每个namespace自动创建一个默认的service account资源，并命名为”default”。</p>
<p>创建POD时没有明确指定ServiceAccount，将会以default的ServiceAccount挂载到POD中。
</p>
<h4 id="授权机制"><a href="#授权机制" class="headerlink" title="授权机制"></a>授权机制</h4><p>ABAC: ABAC(基于属性的访问控制)：ABAC控制权限的粒度非常细，非常灵活；属性通常来说分为四类：用户属性、环境属性、操作属性、对象属性；ABAC通过动态的控制一组属性是否满足条件来进行授权判断。优点：权限控制的粒度非常细，非常灵活。缺点：太过于复杂。例如规则：“禁止所有学生在上课时间进出校门”这条规则，其中，“学生”是用户的角色属性，“上课时间”是环境属性，“进出”是操作属性，而“校门”就是对象属性了。</p>
<p>RBAC:引入角色概念，通过将权限和角色关联，来实现用户根权限的解耦，角色可以看做是权限的一个集合。通过权限与角色关联，角色与用户的关联可以实现用户和角色多对多的对应关系。</p>
<p>RBAC的模型<br>Who是否可以对What进行How的访问操作<br>kubernetes之前的版本都是使用ABAC做为权限管理控制，在kubernetes1.3发布alpha版本RBAC,在kubernetes1.6版本RBAC提升为Beta版，kubernetes1.8版本正式提升为GA版，为什么用RBAC取代ABAC是因为在kubernetes中ABAC过于复杂，不好管理，难以理解，RBAC相对功能和可管理性来说更加合适kubernetes。所以说没有更好，只有更合适。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_17.png" alt="image"></p>
<p>RBAC（Role base access control)：k8s上发展过程：<br>1.3 Alpha，1.6 Beta 1.8 GA</p>
<p>概念：<br>引入角色概念，通过将权限和角色关联，来实现用户根权限的解耦，角色可以看做是权限的一个集合。通过权限与角色关联，角色与用户的关联可以实现用户和角色多对多的对应关系。</p>
<p>优点 :<br>将权限与角色关联，用户关联角色，根传统用户关联权限相比这样权限控制更加灵活 。</p>
<p>控制粒度更细，可以对单个资源对象的做权限的分配。<br>缺点：<br>复杂度较高，有一定学习成本。</p>
<p>角色：<br>Kubernetes中的RBAC定义了两类角色</p>
<p>Role：<br>role是一组权限的集合， 例如一个角色可以包含读取 Pod 的权限和列出 Pod 的权限。Role 只能用来给某个特定 namespace 中的资源作鉴权 。</p>
<p>ClusterRole：<br>clusterrole根role一样也是一组权限的集合，cluster范围的Role，但clusterrole的有效范围是整个集群的权限和一些集群级别的资源如：集群的node节点、非资源类型endpoint、跨所有命名空间的命名空间范围资源（例如pod，需要运行命令kubectl get pods –all-namespaces来查询集群中所有的pod）。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_18.png" alt="image"></p>
<p>Role：<br>Resource：表示控制的资源对象，可以是pod，当然也可以是namespace级别的其他资源如：Service，secrets，pvc，configmap等。</p>
<p>Verbs：表示控制的权限操作，因为安全认证是通过api-server来完成的，api-server支持的10种权限操作为”get”, “ list “, “watch “,”create”,”update”, “patch”, “delete”,”deletecollection”,”redirect”,”proxy”]</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_19.png" alt="image"></p>
<p>ClusterRole：</p>
<p>Resource：ClusterRole定义的资源对象是集群层面的或需要进行跨多个namespace的资源控制</p>
<p>Verbs：根role一样</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_20.png" alt="image"></p>
<p>有了角色后，需要将这些角色根对应对象绑定起来，这样才能完成一次权限的分配，根Role和ClusterRole是对应的角色的棒定也分两种RoleBinding和ClusterRoleBinding 。</p>
<p>RoleBinding &amp; ClusterRoleBinding ：</p>
<p>Subjects：关联的对象类型，在k8s主要分为三种user，group，ServiceAccount</p>
<p>roleRef：为bind的对象，如果你是role，kind就写role是Clusterrolekind就写ClusterRole，name为对象的名字。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_21.png" alt="image"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_22.png" alt="image"></p>
<p>默认的Role和ClusterRole</p>
<p>api-service会默认创建一些role和rolebinding 和clusterrole和clusterrolebind，他们默认都是以system开头的，以system开头的都是为kubernetes系统使用而保留。</p>
<p>clusterrole里面cluster-admin、admin、edit、view这几个clusterrole，直接参考<a href="https://v1-8.docs.kubernetes.io/docs/admin/authorization/rbac/%0Bcluster-admin%E3%80%81admin%E3%80%81edit%E3%80%81view%E8%BF%99%E5%87%A0%E4%B8%AA%E8%A7%92%E8%89%B2%E6%98%AF%E4%B8%8D%E5%8C%85%E5%90%ABsystem%E5%89%8D%E7%BC%80%E7%9A%84%EF%BC%8C%E4%BB%96%E4%BB%AC%E6%98%AF%E9%9D%A2%E5%90%91%E7%94%A8%E6%88%B7%E7%9A%84%E8%A7%92%E8%89%B2%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%9B%B4%E6%8E%A5%E4%BD%BF%E7%94%A8rolebind%E7%9B%B4%E6%8E%A5%E4%BD%BF%E7%94%A8%E3%80%82">https://v1-8.docs.kubernetes.io/docs/admin/authorization/rbac/cluster-admin、admin、edit、view这几个角色是不包含system前缀的，他们是面向用户的角色，可以直接使用rolebind直接使用。</a></p>
<p>User和ServiceAccount</p>
<p>Kubernetes 集群中包含两类用户：一类是由 Kubernetes 管理的 service account，另一类是普通用户。<br>普通用户<br>假定为由外部独立服务管理。管理员分发私钥，用户存储（如 Keystone 或 Google 帐户），甚至包含用户名和密码列表的文件。在这方面，Kubernetes 没有代表普通用户帐户的对象。无法通过 API 调用的方式向集群中添加普通用户。</p>
<p>Service Account<br>是由 Kubernetes API 管理的帐户。是用于POD中的进程访问API-Service的，为POD中的进程提供一个身份标识。</p>
<p>为什么用ServiceAccount？<br>通过ServiceAccount可以更精确控制POD内应用访问集群权限。</p>
<p>User和ServiceAccount</p>
<p>kubernetes在每个namespace内都有一个默认的service account,当在创建pod或其他资源时，没有手工指定Service account时会默认使用namespace里面这个default这个Service account<br><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_23.png" alt="image"></p>
<p>一般来说为了安全起见，建议一个应用使用一个独立的Service account运行，这样RoleBindings就不会无意中授予其他应用程序的权限，如果需要这样使用的话在部署yaml文件中添加
</p>
<h4 id="准入控制机制—Admission-Control"><a href="#准入控制机制—Admission-Control" class="headerlink" title="准入控制机制—Admission Control"></a>准入控制机制—Admission Control</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_24.png" alt="image"></p>
<p>准入(Admission Control)机制是一种请求拦截机制，用于对请求的资源对象进行校验。</p>
<p>包含两个控制器：<br>变更（Mutating）准入控制：修改请求的对象<br>验证（Validating）准入控制：验证请求的对象</p>
<p>当请求到达 API Server 的时候首先执行变更准入控制，然后再执行验证准入控制。</p>
<p>默认的控制策略</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_25.png" alt="image"></p>
<p>OPA介绍<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_26.png" alt="image"></p>
<p>开源的通用策略引擎，可以统一进行请求策略控制。OPA通过一种高级声明性语言（Rego）来定义策略配置规则，实现对请求的控制。</p>
<p>在Kubernetes中，admission在创建，更新和删除操作期间强制执行对象的语义验证，需要重新编译或重新配置Kubernetes API服务器。但使用OPA，直接可以在Kubernetes对象上实施自定义策略</p>
<p>配合强大的声明式策略语言Rego，直接通过K8S对象配置规则。</p>
<p>Service可以是以下任意：</p>
<ul>
<li>Kubernetes API server</li>
<li>API gateway</li>
<li>Custom service</li>
<li>CI&#x2F;CD pipeline</li>
</ul>
<p><br>OPA介绍—Gatekeeper<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_27.png" alt="image"></p>
<p>Gatekeeper的创建是为了让用户能够通过配置（而不是代码）定制许可控制。</p>
<ul>
<li>通过CRD和模板定义，</li>
<li>可分享，可重复使用，</li>
<li>参数化配置</li>
</ul>
<p>集群当前状态的可知和反馈，并非仅仅操作或下发策略的人员知晓<br>更方便与其他系统集成，如CI&#x2F;CD</p>
<h4 id="POD安全策略"><a href="#POD安全策略" class="headerlink" title="POD安全策略"></a>POD安全策略</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_28.png" alt="image"></p>
<p>POD直接运行在宿主机上可以映射宿主机的端口和使用宿主机网络，共享宿主机的内核，这样情况下会一定的安全风险，可以通过配置POD安全策略限制并约束POD的一些行为。PodSecurityPolicy对象定义了一组条件，指示 Pod 必须按系统所能接受的设定运行。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_29.png" alt="image"></p>
<h4 id="网络安全策略"><a href="#网络安全策略" class="headerlink" title="网络安全策略"></a>网络安全策略</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_30.png" alt="image"></p>
<p>概述：</p>
<p>Kubernetes的一个重要特性就是要把不同node节点的pod（container）连接起来，无视物理节点的限制。但是在某些应用环境中，比如公有云，不同租户的pod不应该互通，这个时候就需要网络隔离。Kubernetes提供了NetworkPolicy，支持按Namespace级别的网络隔离。Network Policy提供了基于策略的网络控制，用于隔离应用并减少攻击面。它使用标签选择器模拟传统的分段网络，并通过策略控制它们之间的流量以及来自外部的流量</p>
<p>先决条件<br>kubernetes NetworkPolicy是通过调用对应的网络组件的policy功能来实现的，像weave和calico、cilium自带policy组件来实现功能，但flannel本身是没有这个的，但flannel可以通过直接使用calico的calico-controler组件来实现这个功能</p>
<p>网络策略是用来约束pod组与彼此和其他网络端点通信的规范<br>通过标签来筛选pods<br>定义Ingress与Egress流量与规则<br>应用场景：<br>1、多租户网络隔离、不同级别业务系统策略控制<br>2、内外部业务系统请求细粒化控制限制</p>
<h4 id="镜像安全扫描"><a href="#镜像安全扫描" class="headerlink" title="镜像安全扫描"></a>镜像安全扫描</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_31.png" alt="image"></p>
<ul>
<li><p>镜像仓库集成镜像漏洞扫描，镜像仓库内镜像进行CVE漏洞扫描</p>
</li>
<li><p>配置扫描策略，上传后立刻扫描，超过定义的高风险漏洞就禁止pull镜像。</p>
</li>
<li><p>定期同步外网漏洞仓库</p>
</li>
</ul>
<h4 id="镜像签名"><a href="#镜像签名" class="headerlink" title="镜像签名"></a>镜像签名</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_32.png" alt="image"></p>
<p>通过在容器镜像中可以对镜像进行加密签名用来保证镜像件来源和镜像内容防篡改。</p>
<p>ca证书进行镜像的签发密钥和验证。</p>
<h4 id="最小镜像原则，降低攻击面"><a href="#最小镜像原则，降低攻击面" class="headerlink" title="最小镜像原则，降低攻击面"></a>最小镜像原则，降低攻击面</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_33.png" alt="image"></p>
<p>最小镜像原则，使用体积小的base镜像降低攻击面<br>Google发布的Distroless镜像仅包含应用程序及其运行时依赖项。不包含程序包管理器，shell和在标准Linux发行版中找到的任何其他程序。</p>
<p>Distroless提供java、python、Nodejs、dotnet等开发语言的基础镜像</p>
<p>优点：<br>1、降低镜像体积、减少磁盘空间占用。<br>2、只安装应用所依赖的组件，无其他组件降低攻击面。</p>
<p>缺点：<br>1、没有shell和一些调试工具，出故障时无法方便调试，但可以通过临时容器进行调试</p>
<h4 id="集群CIS安全扫描"><a href="#集群CIS安全扫描" class="headerlink" title="集群CIS安全扫描"></a>集群CIS安全扫描</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_34.png" alt="image"></p>
<p>互联网安全中心（CIS）发布的全面的Kubernetes Benchmark 建立Kubernetes 的安全配置规范</p>
<p>1、集群组件配置检查（Api-server、controller-manager、scheduler、etcd）是否符合安全配置参数。</p>
<p>2、集群组件配置文件权限检查。</p>
<p>3、策略检查（pod安全策略、网络安全策略检查）。</p>
<h4 id="使用Rootless-Container提升容器的安全性"><a href="#使用Rootless-Container提升容器的安全性" class="headerlink" title="使用Rootless Container提升容器的安全性"></a>使用Rootless Container提升容器的安全性</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_35.png" alt="image"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_36.png" alt="image"></p>
<p>Docker Daemon因为需要创建Namespace和挂载分层文件系统等所以一直以来是以root用户来运行的。这也导致了有Docker访问权限的用户可以通过连接Docker Engine获取root权限，而且可以绕开系统的审计能力对系统进行攻击。</p>
<p>Rootless Container实现使用非特权用户运行docker engine。</p>
<p>Docker 19.03中发布了“Rootless Container”特性</p>
<p>实现原理：<br>1、通过user Namespace实现docker demon运行的用户重映射</p>
<p>2、利用用户态的网络SLiRP通过一个TAP设备连接到非特权用户名空间，为容器提供外网连接能力</p>
<h4 id="集群及运行安全-–-实践"><a href="#集群及运行安全-–-实践" class="headerlink" title="集群及运行安全 – 实践"></a>集群及运行安全 – 实践</h4><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/">https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/</a><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_security_37.png" alt="image"></p>
<h4 id="容器和应用安全-–-实践"><a href="#容器和应用安全-–-实践" class="headerlink" title="容器和应用安全 – 实践"></a>容器和应用安全 – 实践</h4><p>限制对Kubernetes API的访问</p>
<ul>
<li>对所有API通讯使用TLS加密</li>
<li>API认证</li>
<li>API授权<br>限制对Kubelet的访问</li>
<li>关闭匿名访问</li>
<li>限制运行时Workload和用户具备的能力</li>
<li>限制资源使用</li>
<li>限制特权容器</li>
<li>限制容器加载特定内核模块</li>
<li>限制容器网络访问</li>
<li>限制云元数据API访问</li>
<li>限制Pod能够使用的node<br>保护集群组件不受侵害</li>
<li>限制对etcd的访问</li>
<li>启用审计日志</li>
<li>限制对无关特性或alpha&#x2F;beta特性的使用</li>
<li>定期更新基础设施credential</li>
<li>评估第三方集成安全性</li>
<li>加密Secrets资源</li>
<li>接收安全更新和漏洞报告</li>
</ul>
<p>限制资源使用</p>
<ul>
<li>配置Namespace配额</li>
<li>配置容器默认资源限制<br>镜像安全</li>
<li>最小镜像原则，减少攻击面</li>
<li>基础镜像规划化避免乱使用</li>
<li>镜像内使用非特权用户</li>
<li>镜像漏洞扫描，定义风险阈值</li>
<li>代码安全</li>
<li>通过TLS访问，对传输内容加密</li>
<li>限制通信端口范围</li>
<li>确保第三方依赖安全性</li>
<li>静态代码分析</li>
<li>动态探测攻击</li>
</ul>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes services</title>
    <url>/2018/05/05/kubernetes_service/</url>
    <content><![CDATA[<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_service_1.png"><br>pod是随时变化的，动态的伸缩，所以应用程序直接连接pod是不可靠的，所以kubernetes引入service，service是为一组功能相同的pod提供统一入口。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_service_2.png"><br>例</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line"> name: httpd-deployment</span><br><span class="line">spec:</span><br><span class="line"> ports:</span><br><span class="line"> - protocol: TCP</span><br><span class="line">   port: 80</span><br><span class="line">   targetPort: 80</span><br><span class="line"> selector:</span><br><span class="line">   app: http</span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1beta2</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: http-app</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: http</span><br><span class="line">  replicas: 2</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: http</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: httpd</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br></pre></td></tr></table></figure>
<p>上面例子，我们创建一个名为http-app的deployment，然后创建了个Service，selector labels方式将Service与deployment绑定，service类型为 cluster-ip，也就是内部访问类型<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_service_3.png"><br>在kubernetes集群任意一个节点curl访问<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_service_4.png"></p>
<h3 id="端口类型"><a href="#端口类型" class="headerlink" title="端口类型"></a>端口类型</h3><p>port<br>Service暴露出来的内部访问端口<br>targetPort<br>pod暴露出来的端口<br>NodePort<br>NodePort是kubernetes提供外部访问service的暴露端口，它暴露在集群所有宿主机上，默认端口范围为30000~32767</p>
<h3 id="服务类型"><a href="#服务类型" class="headerlink" title="服务类型"></a>服务类型</h3><p>ClusterIP（默认)<br>使用clusterip类型的Service默认会使用一个集群内部ip，通过kube-proxy调用iptables创建规则，将流量转发到pod中，需要注意的是clusterip是一个virtual_ip没有真正的网络设备绑定，所以是ping不通它的，直接在集群内部的访问就好。</p>
<p>NodePort<br>使用NodePort类型的Service时会在集群内部所有host上暴露一个端口用于外部访问</p>
<p>创建方法</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: httpd-deployment</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports:</span><br><span class="line">    - port: 80</span><br><span class="line">      targetPort: 80</span><br><span class="line">      nodePort: 30080</span><br><span class="line">   selector:</span><br><span class="line">     app: http</span><br></pre></td></tr></table></figure>
<p>不指定nodePort端口将从30000~32767里面随机选一个<br>LoadBlancer</p>
<p>使用loadblance类型时，会向cloud provider申请映射到service本身的负载均衡，比如AWS的ELB、google cloud 的GCP、Azure的azure-load-balancer。</p>
<h3 id="外部访问pod"><a href="#外部访问pod" class="headerlink" title="外部访问pod"></a>外部访问pod</h3><p>NodePort<br>loadblance<br>ingress</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_service_5.png"></p>
<h3 id="Service负载均衡"><a href="#Service负载均衡" class="headerlink" title="Service负载均衡"></a>Service负载均衡</h3><p>kube-proxy 会检测 API Server 上对于 service 和 endpoint 的新增或者移除。对于每个新的 service，在每个 node 上，kube-proxy 都会设置相应的 iptables 的规则来记录应该转发的地址。当一个 service 被删除的时候，kube-proxy 会在所有的 pod 上移除这些 iptables 的规则。默认转发规则是round robin 或session affinity<br>kube-proxy 模式：userspace、iptables、ipvs(beta)</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_service_6.png"></p>
<p>当创建一个service后，service会后iptables添加如下规则，以上面创建的httpd-deployment service为例，它的cluster-ip为10.43.129.201</p>
<h4 id="cluster-ip转发流程"><a href="#cluster-ip转发流程" class="headerlink" title="cluster-ip转发流程"></a>cluster-ip转发流程</h4><p>在集群任意一台节点上查看iptables规则</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">iptables-save &gt;/tmp/1</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-A KUBE-SERVICES -d 10.43.129.201/32 -p tcp -m comment --comment &quot;default/httpd-deployment: cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-KO6WMUDK3F2YFERC</span><br><span class="line">-A KUBE-SVC-KO6WMUDK3F2YFERC -m comment --comment &quot;default/httpd-deployment:&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-37DWITJLRCDPHWZD</span><br><span class="line">-A KUBE-SEP-37DWITJLRCDPHWZD -s 10.42.0.87/32 -m comment --comment &quot;default/httpd-deployment:&quot; -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-SEP-37DWITJLRCDPHWZD -p tcp -m comment --comment &quot;default/httpd-deployment:&quot; -m tcp -j DNAT --to-destination 10.42.0.87:80</span><br><span class="line">-A KUBE-SEP-BWU5RQOROOHNX3YY -s 10.42.1.82/32 -m comment --comment &quot;default/httpd-deployment:&quot; -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-SEP-BWU5RQOROOHNX3YY -p tcp -m comment --comment &quot;default/httpd-deployment:&quot; -m tcp -j DNAT --to-destination 10.42.1.82:80</span><br></pre></td></tr></table></figure>
<p>第一条规则目的ip是cluster-ip，命中规则，然后将请求丢给KUBE-SVC-KO6WMUDK3F2YFERC处理<br>第二条规则链KUBE-SVC-KO6WMUDK3F2YFERC 实现了将报文按50%的统计概率随机匹配，转给KUBE-SEP-37DWITJLRCDPHWZD<br>第三条和第五条是指对pod的SNAT操作<br>第四条和第六条KUBE-SEP-37DWITJLRCDPHWZD链直接进行DNAT操作将cluster-ip的80端口映射到pod的80</p>
<h4 id="NodePort转发流程"><a href="#NodePort转发流程" class="headerlink" title="NodePort转发流程"></a>NodePort转发流程</h4><p>修改Service类型为NodePort</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s_service_7.png"></p>
<p>查看iptables</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/httpd-deployment:&quot; -m tcp --dport 30080 -j KUBE-SVC-KO6WMUDK3F2YFERC</span><br><span class="line">-A KUBE-SERVICES -d 10.43.129.201/32 -p tcp -m comment --comment &quot;default/httpd-deployment: cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-KO6WMUDK3F2YFERC</span><br></pre></td></tr></table></figure>
<p>第一条规则目的端口味30080的请求转发给KUBE-SVC-KO6WMUDK3F2YFERC处理<br>第二条规则将请求给cluster-ip10.43.129.201然后转发给KUBE-SVC-KO6WMUDK3F2YFERC<br>后面就根上面的cluster-ip转发流程是一样的了，所以NodePort就是在cluster-ip转发基础上加了层DNAT到cluster-ip</p>
<h3 id="服务发现"><a href="#服务发现" class="headerlink" title="服务发现"></a>服务发现</h3><p>两种方式<br>环境变量<br>DNS:1.10版之前是通过kube-dns实现服务发现，1.10版后可以用CoreDns替代kube-dns做服务发现<br>创建一个Service后，kubernetes会自动将servername做为服务名，添加一条dns记录</p>
<p>比如上面那个例子Service名为httpd-deployment,那么其他pod要访问直接访问httpd-deployment就会解析到对应cluster-ip,跨namespace的只需要在域名后接上.namespace_name 如httpd-deployment.default</p>
<p><a href="http://www.dbsnake.net/how-kubernetes-use-iptables.html">http://www.dbsnake.net/how-kubernetes-use-iptables.html</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes安全机制(二)</title>
    <url>/2020/10/11/kubernetes_security_2/</url>
    <content><![CDATA[<h2 id="1-网络安全策略"><a href="#1-网络安全策略" class="headerlink" title="1. 网络安全策略"></a>1. 网络安全策略</h2><h3 id="1-1-相同namespace的NetworkPolicy的隔离性"><a href="#1-1-相同namespace的NetworkPolicy的隔离性" class="headerlink" title="1.1 相同namespace的NetworkPolicy的隔离性"></a>1.1 相同namespace的NetworkPolicy的隔离性</h3><p>创建一个namespace  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create ns policy-demo</span><br></pre></td></tr></table></figure>
<p>创建pod  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create deployment --namespace=policy-demo nginx  --image=nginx</span><br></pre></td></tr></table></figure>
<p>创建service</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl expose --namespace=policy-demo deployment nginx --port=80</span><br></pre></td></tr></table></figure>
<p>测试访问(可以正常访问)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/sh</span><br><span class="line">wget -q nginx -O -</span><br></pre></td></tr></table></figure>
<p>创建NetworkPolicy规则  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f - &lt;&lt;EOF</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: default-deny</span><br><span class="line">  namespace: policy-demo</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels: &#123;&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p>此规则表示拒绝pod连接policy-demo namespace下的pod</p>
<p>在次测试</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/sh</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">wget -q nginx -O -</span><br><span class="line"> </span><br><span class="line">wget: Download time out</span><br></pre></td></tr></table></figure>
<p>可以看见被拒绝访问了</p>
<p>添加允许规则  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f - &lt;&lt;EOF</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: access-nginx</span><br><span class="line">  namespace: policy-demo</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  ingress:</span><br><span class="line">    - from:</span><br><span class="line">      - podSelector:</span><br><span class="line">          matchLabels:</span><br><span class="line">            run: access</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<p>这条规则意思，允许，label为   run:access的pod访问policy-demo   namespace下label为run：nginx的pod  </p>
<p>刚刚我们执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/sh  pod名称为access会自动会这个deployment创建run:access这个label</span><br></pre></td></tr></table></figure>




<p>在次测试,可以访问成功</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/sh </span><br><span class="line"></span><br><span class="line">wget -q nginx -O -</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;For online documentation and support please refer to</span><br><span class="line">&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;</span><br><span class="line">Commercial support is available at</span><br><span class="line">&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>创建一个不是run:access的pod去测试访问  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl run --namespace=policy-demo cannot-access --rm -ti --image busybox  /bin/sh</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">测试  </span><br><span class="line">wget -q nginx -O -</span><br><span class="line"></span><br><span class="line">wget：Download time out</span><br></pre></td></tr></table></figure>


<p>结论：同namespace下可以使用Policy在限制pod与pod之间的访问</p>
<p>清空环境</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl delete ns policy-demo</span><br></pre></td></tr></table></figure>



<h3 id="1-2-不同namespace-pod的隔离性"><a href="#1-2-不同namespace-pod的隔离性" class="headerlink" title="1.2 不同namespace pod的隔离性"></a>1.2 不同namespace pod的隔离性</h3><p>创建两个namespace policy-demo、policy-demo2，然后在policy-demo里面创建nginx-pod和对应的service和busybox，在policy-demo2里面创建busybox，两个namespace的busybox去访问policy-demo里面的nginx</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create ns policy-demo</span><br><span class="line">kubectl create ns policy-demo2</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create deployment  --namespace=policy-demo nginx  --image=nginx</span><br><span class="line">kubectl run --namespace=policy-demo access --rm -ti --image busybox /bin/sh</span><br><span class="line">kubectl run --namespace=policy-demo2 access --rm -ti --image busybox /bin/sh</span><br><span class="line">kubectl expose --namespace=policy-demo deployment nginx --port=80</span><br></pre></td></tr></table></figure>
<p>还没设置NetworkPolicy时分别从policy-demo和policy-demo2两个namespace去busybox去访问nginx,访问成功。</p>
<p>需要注意的是<br>policy-demo2去访问要接上namespace名</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget -q nginx.policy-demo -O -</span><br></pre></td></tr></table></figure>

<p>配置NetworkPolicy</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f - &lt;&lt;EOF</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: default-deny</span><br><span class="line">  namespace: policy-demo</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels: &#123;&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<p>配置拒绝所有Policy，此时两个namespace的busybox都不能访问了</p>
<p>在添加允许run:access label的pod访问Policy  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f - &lt;&lt;EOF</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: access-nginx</span><br><span class="line">  namespace: policy-demo</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  ingress:</span><br><span class="line">    - from:</span><br><span class="line">      - podSelector:</span><br><span class="line">          matchLabels:</span><br><span class="line">            run: access</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>



<p>此时<br>policy-demo这个namespace下的busybox可以访问本namespace下的这个nginx<br>policy-demo2这个namespace下的busybox访问不了policy-demo这个namespace下的nginx</p>
<p>配置允许policy-demo2下的run:access标签的POD访问policy-demo namespace下的app：nginx服务</p>
<p>给policy-demo2命名空间打上label</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl label ns/policy-demo2 project=policy-demo2</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f - &lt;&lt;EOF</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: access-nginx2</span><br><span class="line">  namespace: policy-demo</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  ingress:</span><br><span class="line">    - from:</span><br><span class="line">      - namespaceSelector:</span><br><span class="line">          matchLabels:</span><br><span class="line">            project: policy-demo2</span><br><span class="line">        podSelector:</span><br><span class="line">          matchLabels:</span><br><span class="line">            run: access</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p>此时policy-demo2下的run:access标签的POD访问policy-demo namespace下的app：nginx服务，但其他标签不可以。</p>
<p>运行个run：access2标签的busybox去访问policy-demo namespace下的app：nginx服务</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl run --namespace=policy-demo2 access2 --rm -ti --image busybox /bin/sh  </span><br><span class="line"></span><br><span class="line">wget -q nginx.policy-demo -O -  </span><br><span class="line"></span><br><span class="line">wget：Download time out</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">ingress:</span><br><span class="line">- from:</span><br><span class="line">  - namespaceSelector:</span><br><span class="line">      matchLabels:</span><br><span class="line">        user: alice</span><br><span class="line">  - podSelector:</span><br><span class="line">      matchLabels:</span><br><span class="line">        role: client</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>像上面这样定义的 namespaceSelector 和 podSelector，是“或”（OR）的关系。所以说，这个 from 字段定义了两种情况，无论是 Namespace 满足条件，还是 Pod 满足条件，这个 NetworkPolicy 都会生效。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">  ingress:</span><br><span class="line">  - from:</span><br><span class="line">    - namespaceSelector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          user: alice</span><br><span class="line">      podSelector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          role: client</span><br><span class="line">  ...</span><br><span class="line">   </span><br></pre></td></tr></table></figure>
<p>这样定义的 namespaceSelector 和 podSelector，其实是“与”（AND）的关系。所以说，这个 from 字段只定义了一种情况，只有 Namespace 和 Pod 同时满足条件，这个 NetworkPolicy 才会生效。  </p>
<p>清空环境</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl delete ns policy-demo</span><br><span class="line"></span><br><span class="line">kubectl delete ns policy-demo2</span><br></pre></td></tr></table></figure>


<h3 id="1-3-南北向流量隔离实战"><a href="#1-3-南北向流量隔离实战" class="headerlink" title="1.3 南北向流量隔离实战"></a>1.3 南北向流量隔离实战</h3><p>创建namespace</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create ns policy-demo</span><br><span class="line">kubectl create ns policy-demo2</span><br></pre></td></tr></table></figure>

<p>在policy-demo命名空间内创建两个测试POD</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl run  --namespace=policy-demo test-network1 --command sleep 1000000 --image=busybox</span><br><span class="line"></span><br><span class="line">kubectl run  --namespace=policy-demo test-network2 --command sleep 1000000 --image=busybox</span><br></pre></td></tr></table></figure>

<p>在policy-demo2命名空间内创建一个测试pod</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl run  --namespace=policy-demo2 test-network3 --command sleep 1000000 --image=busybox</span><br></pre></td></tr></table></figure>


<p>创建全局禁止外访规则</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f - &lt;&lt;EOF</span><br><span class="line"></span><br><span class="line">apiVersion: crd.projectcalico.org/v1</span><br><span class="line">kind: GlobalNetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: global-deny-all-egress</span><br><span class="line">spec:</span><br><span class="line">  selector: all()</span><br><span class="line">  types:</span><br><span class="line">  - Egress</span><br><span class="line">  egress:</span><br><span class="line">  - action: Deny</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p>单个POD外访白名单<br>以允许policy-demo命名空间中的test-network pod为例</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f - &lt;&lt;EOF</span><br><span class="line"></span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: allow-testnetwork-egress-ssh</span><br><span class="line">  namespace: policy-demo</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      run: test-network1  #通过Label Selector匹配到具体某一类Pod</span><br><span class="line">  policyTypes:</span><br><span class="line">  - Egress</span><br><span class="line">  egress:</span><br><span class="line">  - to:</span><br><span class="line">    - ipBlock:</span><br><span class="line">        cidr: 172.16.0.5/32  #白名单IP</span><br><span class="line">    ports:</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 22  #白名单端口</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p>查看NetworkPolicy</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl  get networkpolicy -n policy-demo</span><br><span class="line">NAME                           POD-SELECTOR        AGE</span><br><span class="line">allow-testnetwork-egress-ssh   run=test-network1   16s</span><br></pre></td></tr></table></figure>

<p>测试访问<br>此时test-network1可以访问其他pod无法访问</p>
<p>Namespace外访白名单</p>
<p>允许policy-demo命名空间下全部POD都访问172.16.0.5的22端口</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f - &lt;&lt;EOF</span><br><span class="line"></span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: allow-egress-to-ssh-policy-demo</span><br><span class="line">  namespace: policy-demo</span><br><span class="line">spec:</span><br><span class="line">  podSelector: &#123;&#125;</span><br><span class="line">  policyTypes:</span><br><span class="line">  - Egress</span><br><span class="line">  egress:</span><br><span class="line">  - to:</span><br><span class="line">    - ipBlock:</span><br><span class="line">        cidr: 172.16.0.5/32   #白名单IP</span><br><span class="line">    ports:</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 22  #白名单端口</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p>此时test-network1、test-networ2点可以访问其它命名空间的pod无法访问</p>
<p>全局外访白名单  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f - &lt;&lt;EOF</span><br><span class="line"></span><br><span class="line">apiVersion: crd.projectcalico.org/v1</span><br><span class="line">kind: GlobalNetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: global-allow-all-egress-to-ssh</span><br><span class="line">spec:</span><br><span class="line">  selector: all()</span><br><span class="line">  types:</span><br><span class="line">  - Egress</span><br><span class="line">  egress:</span><br><span class="line">  - action: Allow</span><br><span class="line">    source: &#123;&#125;</span><br><span class="line">    destination:</span><br><span class="line">      nets:</span><br><span class="line">      - 172.16.0.5  #白名单IP</span><br><span class="line">      ports:</span><br><span class="line">      - 22  #白名单端口</span><br><span class="line">    protocol: TCP</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>


<p>配置此规则后，集群内全部pod都可以访问172.16.0.5的22端口</p>
<h2 id="2-RBAC"><a href="#2-RBAC" class="headerlink" title="2. RBAC"></a>2. RBAC</h2><p>安装cfssl</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -s -L -o /bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64</span><br><span class="line">curl -s -L -o /bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64</span><br><span class="line">curl -s -L -o /bin/cfssl-certinfo https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64</span><br><span class="line">chmod +x /bin/cfssl*</span><br></pre></td></tr></table></figure>


<h3 id="2-1-user创建"><a href="#2-1-user创建" class="headerlink" title="2.1. user创建"></a>2.1. user创建</h3><p>创建名为test-cka的平台用户</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /etc/kubernetes/pki</span><br></pre></td></tr></table></figure>
<p>确认kubernetes证书目录是否有以下文件</p>
<p><img src="https://training-1251900790.cos.ap-guangzhou.myqcloud.com/image/kubernetes_rbac_1.png"></p>
<p>若没有ca-config.json</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &gt; ca-config.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;signing&quot;: &#123;</span><br><span class="line">    &quot;default&quot;: &#123;</span><br><span class="line">      &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;profiles&quot;: &#123;</span><br><span class="line">      &quot;kubernetes&quot;: &#123;</span><br><span class="line">        &quot;usages&quot;: [</span><br><span class="line">            &quot;signing&quot;,</span><br><span class="line">            &quot;key encipherment&quot;,</span><br><span class="line">            &quot;server auth&quot;,</span><br><span class="line">            &quot;client auth&quot;</span><br><span class="line">        ],</span><br><span class="line">        &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<p>ca-config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile；</p>
<p>创建一个创建证书签名请求</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &gt; test-cka-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;test-cka&quot;,</span><br><span class="line">  &quot;hosts&quot;: [],</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;System&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cfssl gencert -ca=ca.crt -ca-key=ca.key -config=ca-config.json -profile=kubernetes test-cka-csr.json | cfssljson -bare test-cka</span><br></pre></td></tr></table></figure>

<p>创建kubeconfig文件  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export KUBE_APISERVER=&quot;https://192.168.1.10:6443&quot;</span><br></pre></td></tr></table></figure>
<p>KUBE_APISERVER写你master节点IP地址</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true --server=$&#123;KUBE_APISERVER&#125; --kubeconfig=test-cka.kubeconfig</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl config set-credentials test-cka --client-certificate=/etc/kubernetes/pki/test-cka.pem --client-key=/etc/kubernetes/pki/test-cka-key.pem --embed-certs=true --kubeconfig=test-cka.kubeconfig</span><br></pre></td></tr></table></figure>

<p>设置context</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl config set-context kubernetes --cluster=kubernetes --user=test-cka  --kubeconfig=test-cka.kubeconfig</span><br></pre></td></tr></table></figure>
<p>设置默认context,将集群参数和用户参数关联起来，如果配置了多个集群，可以通过集群名来切换不同的环境</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl config use-context kubernetes --kubeconfig=test-cka.kubeconfig </span><br></pre></td></tr></table></figure>
<p>查看kubectl的context</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl config get-contexts</span><br><span class="line">CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE</span><br><span class="line">*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>用户目前还是kubernetes-admin，切换到test-cka</p>
<p>查看用户切换</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl config get-contexts  --kubeconfig=/etc/kubernetes/pki/test-cka.kubeconfig</span><br><span class="line">CURRENT   NAME         CLUSTER      AUTHINFO   NAMESPACE</span><br><span class="line">*         kubernetes   kubernetes   test-cka    </span><br></pre></td></tr></table></figure>


<p>此时去get pod，get node</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod  --kubeconfig=test-cka.kubeconfig </span><br><span class="line">No resources found.</span><br><span class="line">Error from server (Forbidden): pods is forbidden: User &quot;test-cka&quot; cannot list pods in the namespace &quot;default&quot;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get node --kubeconfig=test-cka.kubeconfig </span><br><span class="line">No resources found.</span><br><span class="line">Error from server (Forbidden): nodes is forbidden: User &quot;test-cka&quot; cannot list nodes at the cluster scope4</span><br></pre></td></tr></table></figure>

<h3 id="2-2-Role和RoleBinding创建"><a href="#2-2-Role和RoleBinding创建" class="headerlink" title="2.2. Role和RoleBinding创建"></a>2.2. Role和RoleBinding创建</h3><p>创建角色<br>定义这个角色只能对default这个namespace 执行get、watch、list权限<br>定义角色<br>role.yaml  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f - &lt;&lt;EOF</span><br><span class="line"></span><br><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  namespace: default</span><br><span class="line">  name: pod-reader</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;] # 空字符串&quot;&quot;表明使用core API group</span><br><span class="line">  resources: [&quot;pods&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<p>role_bind.yaml</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f - &lt;&lt;EOF</span><br><span class="line"></span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: read-pods</span><br><span class="line">  namespace: default</span><br><span class="line">subjects:</span><br><span class="line">- kind: User</span><br><span class="line">  name: test-cka</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">roleRef:</span><br><span class="line">  kind: Role</span><br><span class="line">  name: pod-reader</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  </span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p>使用admin创建</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f role.yaml --kubeconfig=/root/.kube/config </span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f role_bind.yaml --kubeconfig=/root/.kube/config </span><br></pre></td></tr></table></figure>

<p>check一下<br><img src="https://training-1251900790.cos.ap-guangzhou.myqcloud.com/image/kubernetes_rbac_5.png"></p>
<p>此时用test-cka这个用户去get pod</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod  --kubeconfig /etc/kubernetes/pki/test-cka.kubeconfig </span><br><span class="line">NAME                         READY     STATUS    RESTARTS   AGE</span><br><span class="line">http-app-844765cb6c-nfp7l    1/1       Running   0          10h</span><br><span class="line">http-app2-58d4c447c5-qzg99   1/1       Running   0          10h</span><br><span class="line">test-679b667858-pzdn2        1/1       Running   0          1h</span><br><span class="line">[root@master pki]# </span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get node --kubeconfig=test-cka.kubeconfig </span><br><span class="line">No resources found.</span><br><span class="line">Error from server (Forbidden): nodes is forbidden: User &quot;test-cka&quot; cannot list nodes at the cluster scope</span><br></pre></td></tr></table></figure>
<p>get pod可以但get node不行，因为我们刚刚配置role只有pod权限</p>
<p>删除pod看看</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl delete pod/http-app-844765cb6c-nfp7l</span><br><span class="line">Error from server (Forbidden): pods &quot;http-app-844765cb6c-nfp7l&quot; is forbidden: User &quot;test-cka&quot; cannot delete pods in the namespace &quot;default&quot;</span><br></pre></td></tr></table></figure>
<p>你会发现也删不掉，因为我们role里面配置的权限是watch和list和get</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> kubectl get pod -n kube-system --kubeconfig=test-cka.kubeconfig </span><br><span class="line">No resources found.</span><br><span class="line">Error from server (Forbidden): pods is forbidden: User &quot;test-cka&quot;&quot; cannot list pods in the namespace &quot;kube-system&quot;</span><br><span class="line">[root@master pki]# </span><br></pre></td></tr></table></figure>

<p>可以看见test-cka这个用户只能访问default这个namespace的pod资源，其他的namespace都访问不了，同样namespace的其他资源也访问不了</p>
<h3 id="2-3-ClusterRole和ClusterRoleBinding创建"><a href="#2-3-ClusterRole和ClusterRoleBinding创建" class="headerlink" title="2.3. ClusterRole和ClusterRoleBinding创建"></a>2.3. ClusterRole和ClusterRoleBinding创建</h3><p>cluster_role.yaml</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f - &lt;&lt;EOF</span><br><span class="line"></span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line"> name: secret-reader</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;nodes&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p>再定义一个ClusterRoleBinding，将上面的clusterrole和用户rancher绑定起来<br>cluster_role_bind.yaml</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f - &lt;&lt;EOF</span><br><span class="line"></span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: read-secrets-global</span><br><span class="line">subjects:</span><br><span class="line">- kind: User</span><br><span class="line">  name: test-cka</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: secret-reader</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>


<p>应用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f cluster_role.yaml  --kubeconfig=/root/.kube/config </span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f cluster_role_bind.yaml  --kubeconfig=/root/.kube/config </span><br></pre></td></tr></table></figure>


<p>此时去get 节点</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get node --kubeconfig /etc/kubernetes/pki/test-cka.kubeconfig   </span><br><span class="line">NAME      STATUS    ROLES     AGE       VERSION</span><br><span class="line">master    Ready     master    9d        v1.15.5</span><br></pre></td></tr></table></figure>

<h2 id="3-Security-Context"><a href="#3-Security-Context" class="headerlink" title="3. Security Context"></a>3. Security Context</h2><p>Security Context   的目的是限制容器的行为，保护操作系统和其他容器不受其影响。</p>
<p>Kubernetes 提供了三种配置 Security Context 的方法：</p>
<ul>
<li>Container-level Security Context：仅应用到指定的容器</li>
<li>Pod-level Security Context：应用到 Pod 内所有容器以及 Volume</li>
<li>Pod Security Policies（PSP）：应用到集群内部所有 Pod 以及 Volume</li>
</ul>
<h3 id="3-1-Container-level-Security-Context"><a href="#3-1-Container-level-Security-Context" class="headerlink" title="3.1 Container-level Security Context"></a>3.1 Container-level Security Context</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: test-container</span><br><span class="line">  labels:</span><br><span class="line">    app: web</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: test1</span><br><span class="line">      image: busybox</span><br><span class="line">      command: [ &quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1h&quot; ]</span><br><span class="line">      securityContext:</span><br><span class="line">        runAsUser: 1000</span><br><span class="line">    - name: test2</span><br><span class="line">      image: busybox</span><br><span class="line">      command: [ &quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1h&quot; ]</span><br><span class="line"></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl exec -it test-container -c test1 id</span><br><span class="line"> </span><br><span class="line">uid=1000 gid=0(root)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">kubectl exec -it test-container -c test2 id</span><br><span class="line">uid=0(root) gid=0(root) groups=10(wheel)</span><br></pre></td></tr></table></figure>
<p>通过securityContext将test1 container的运行user自动修改为1000了，test2仍然保持不变为root user。  </p>
<h3 id="3-2-Pod-level-Security-Context"><a href="#3-2-Pod-level-Security-Context" class="headerlink" title="3.2 Pod-level Security Context"></a>3.2 Pod-level Security Context</h3><p>创建POD层面securityContext</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: test-container2</span><br><span class="line">  labels:</span><br><span class="line">    app: web2</span><br><span class="line">spec:</span><br><span class="line">  securityContext:</span><br><span class="line">    runAsUser: 1000</span><br><span class="line">  containers:</span><br><span class="line">    - name: test1</span><br><span class="line">      image: busybox</span><br><span class="line">      command: [ &quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1h&quot; ]</span><br><span class="line">    - name: test2</span><br><span class="line">      image: busybox</span><br><span class="line">      command: [ &quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1h&quot; ]</span><br><span class="line"></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>


<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl exec -it test-container2 -c test1 id</span><br><span class="line">uid=1000 gid=0(root)</span><br><span class="line">kubectl exec -it test-container2 -c test2 id</span><br><span class="line">uid=1000 gid=0(root)</span><br></pre></td></tr></table></figure>

<p>通过securityContext将POD内 container的运行user都自动修改为1000了</p>
<h3 id="3-3-Pod-Security-Policies（PSP）"><a href="#3-3-Pod-Security-Policies（PSP）" class="headerlink" title="3.3 Pod Security Policies（PSP）"></a>3.3 Pod Security Policies（PSP）</h3><p>当一个pod安全策略资源被创建,它本身是一种kubernetes资源,但此时它什么都不会做.为了使用它,需要通过RBAC将user或ServiceAccount与它进行绑定</p>
<p>PSP 的用法和 RBAC 是紧密相关的，换句话说，应用 PSP 的基础要求是：</p>
<ul>
<li>不同运维人员的操作账号需要互相隔离并进行单独授权。</li>
<li>不同命名空间，不同 ServiceAccount 也同样要纳入管理流程。</li>
</ul>
<p>创建POD安全策略，这个策略主要限制POD使用特权模式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line"></span><br><span class="line">apiVersion: policy/v1beta1</span><br><span class="line">kind: PodSecurityPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: privileged</span><br><span class="line">  annotations:</span><br><span class="line">    seccomp.security.alpha.kubernetes.io/allowedProfileNames: &#x27;*&#x27;</span><br><span class="line">spec:</span><br><span class="line">  seLinux:</span><br><span class="line">    rule: RunAsAny</span><br><span class="line">  supplementalGroups:</span><br><span class="line">    rule: RunAsAny</span><br><span class="line">  runAsUser:</span><br><span class="line">    rule: RunAsAny</span><br><span class="line">  fsGroup:</span><br><span class="line">    rule: RunAsAny</span><br><span class="line">  privileged: false</span><br><span class="line">  allowPrivilegeEscalation: true</span><br><span class="line">  allowedCapabilities:</span><br><span class="line">  - &#x27;*&#x27;</span><br><span class="line">  volumes:</span><br><span class="line">  - &#x27;*&#x27;</span><br><span class="line"></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p>创建一个名称空间和一个serviceaccount.使用这个serviceaccount来模拟一个非管理员用户</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create namespace psp-demo</span><br></pre></td></tr></table></figure>

<p>授权psp-demo Namespace中默认ServiceAccount的使用privileged这个PodSecurityPolicy</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create rolebinding default:psp:privileged \</span><br><span class="line">    --role=psp:privileged \</span><br><span class="line">    --serviceaccount=psp-demo:default</span><br></pre></td></tr></table></figure>

<p>创建特权模式的workload测试</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl --as=system:serviceaccount:psp-demo:default applyapply -f -</span><br><span class="line"></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.14.2</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">      securityContext:</span><br><span class="line">        privileged: true</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p>from server for: “2.yaml”: pods “privileged” is forbidden: User “system:serviceaccount:psp-demo:default” cannot get resource “pods” in API group “” in the namespace “development”&#96;&#96;&#96;</p>
<p>提示无法创建，因为被Pod Security Policy给限制住了</p>
<p>去除<code>securityContext：privileged: true</code>字段，在次测试，可以正常创建，符合预期。</p>
<h2 id="4-手动注册节点到Kubernetes集群中"><a href="#4-手动注册节点到Kubernetes集群中" class="headerlink" title="4. 手动注册节点到Kubernetes集群中"></a>4. 手动注册节点到Kubernetes集群中</h2><p>先决条件<br>1、集群中安装好docker和kubelet</p>
<p>使用TLS bootstrap的自动注册节点到k8s集群中</p>
<p>kubelet需要申请那些证书</p>
<p>集群启用RBAC后各组件之间的通信是基于TLS加密的，client和server需要通过证书中的CN，O来确定用户的user和group，因此client和server都需要持有有效证书</p>
<ul>
<li>node节点的kubelet需要和master节点的apiserver通信，此时kubelet是一个client需要申请证书</li>
<li>node节点的kubelet启动为守住进程通过10250端口暴露自己的状态，此时kubelet是一个server需要申请证书</li>
<li></li>
</ul>
<p>kubelet申请证书的步骤  </p>
<p>1、集群产生一个低权账号用户组，并通过TOKEN进行认证<br>2、创建ClusterRole使其具有创建证书申请CSR的权限<br>3、给这个组添加ClusterRoleBinding，使得具有这个组的账号的kubelet具有上述权限<br>4、给添加ClusterRoleBinding，使得controller-manager自动同意上述两个证书的下发<br>5、调整 Controller Manager确保启动tokencleaner和bootstrapsigner（4中自动证书下发的功能）<br>6、基于上述TOKEN生成bootstrap.kubeconfig文件，并下发给node节点<br>7、node节点的kubelet拿着这个bootstrap.kubeconfig向master的apiserver发起CSR<br>7、master自动同意并下发第一个证书<br>node记得点的kubelet自动拿着第一个证书与master的apiserver通信申请第二个证书<br>8、master自动同意并下发第二个证书<br>node节点加入集群</p>
<h3 id="创建token"><a href="#创建token" class="headerlink" title="创建token"></a>创建token</h3><p>创建类型为”bootstrap.kubernetes.io&#x2F;token”的secret</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">echo &quot;$(head -c 6 /dev/urandom | md5sum | head -c 6)&quot;.&quot;$(head -c 16 /dev/urandom | md5sum | head -c 16)&quot;</span><br><span class="line">485bd8.711b717a196f47f4</span><br></pre></td></tr></table></figure>

<p>执行上述命令得到一个TOKEN值”485bd8.711b717a196f47f4”</p>
<p>这个 485bd8.711b717a196f47f4<br> 就是生成的 Bootstrap Token，保存好 token，因为后续要用；关于这个 token 解释如下:</p>
<p>Token 必须满足 [a-z0-9]{6}.[a-z0-9]{16} 格式；以 . 分割，前面的部分被称作 Token ID ， Token ID 并不是 “机密信息”，它可以暴露出去；相对的后面的部分称为 Token Secret ，它应该是保密的。</p>
<p>基于token创建secret<br>将下列secret对应的字段修改为刚刚申请的token值</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  # Name MUST be of form &quot;bootstrap-token-&lt;token id&gt;&quot;</span><br><span class="line">  name: bootstrap-token-xxx</span><br><span class="line">  namespace: kube-system</span><br><span class="line"></span><br><span class="line"># Type MUST be &#x27;bootstrap.kubernetes.io/token&#x27;</span><br><span class="line">type: bootstrap.kubernetes.io/token</span><br><span class="line">stringData:</span><br><span class="line">  # Human readable description. Optional.</span><br><span class="line">  description: &quot;The default bootstrap token generated by &#x27;kubeadm init&#x27;.&quot;</span><br><span class="line"></span><br><span class="line">  # Token ID and secret. Required.</span><br><span class="line">  token-id: </span><br><span class="line">  token-secret: </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  # Allowed usages.</span><br><span class="line">  usage-bootstrap-authentication: &quot;true&quot;</span><br><span class="line">  usage-bootstrap-signing: &quot;true&quot;</span><br><span class="line"></span><br><span class="line">  # Extra groups to authenticate the token as. Must start with &quot;system:bootstrappers:&quot;</span><br><span class="line">  auth-extra-groups: system:bootstrappers:cka:default-node-token</span><br><span class="line"></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>


<h3 id="配置RBAC"><a href="#配置RBAC" class="headerlink" title="配置RBAC"></a>配置RBAC</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line"></span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: system:certificates.k8s.io:certificatesigningrequests:nodeclient</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;certificates.k8s.io&quot;]</span><br><span class="line">  resources: [&quot;certificatesigningrequests/nodeclient&quot;]</span><br><span class="line">  verbs: [&quot;create&quot;]</span><br><span class="line">---</span><br><span class="line"># A ClusterRole which instructs the CSR approver to approve a node renewing its</span><br><span class="line"># own client credentials.</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;certificates.k8s.io&quot;]</span><br><span class="line">  resources: [&quot;certificatesigningrequests/selfnodeclient&quot;]</span><br><span class="line">  verbs: [&quot;create&quot;]</span><br><span class="line">---</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: system:node-bootstrapper</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - certificates.k8s.io</span><br><span class="line">  resources:</span><br><span class="line">  - certificatesigningrequests</span><br><span class="line">  verbs:</span><br><span class="line">  - create</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<p>配置ClusterRoleBinding</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line"></span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: cka:kubelet-bootstrap</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:node-bootstrapper</span><br><span class="line">subjects:</span><br><span class="line">- apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Group</span><br><span class="line">  name: system:bootstrappers:cka:default-node-token</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: cka:node-autoapprove-bootstrap</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:certificates.k8s.io:certificatesigningrequests:nodeclient</span><br><span class="line">subjects:</span><br><span class="line">- apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Group</span><br><span class="line">  name: system:bootstrappers:cka:default-node-token</span><br><span class="line"> </span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p>确认controller-manager是否开启bootstrapsigner</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat /etc/kubernetes/manifests/kube-controller-manager.yaml|grep bootstrapsigner</span><br><span class="line">    - --controllers=*,bootstrapsigner,tokencleaner</span><br></pre></td></tr></table></figure>

<h3 id="生成bootstrap-kubeconfig文件"><a href="#生成bootstrap-kubeconfig文件" class="headerlink" title="生成bootstrap.kubeconfig文件"></a>生成bootstrap.kubeconfig文件</h3><p>我这里的apiserver的地址为”<a href="https://172.16.0.7:6443/">https://172.16.0.7:6443</a>“</p>
<p>设置集群参数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl config set-cluster cka \</span><br><span class="line">  --certificate-authority=/etc/kubernetes/pki/ca.crt \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=https://172.16.0.7:6443 \</span><br><span class="line">  --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf</span><br></pre></td></tr></table></figure>

<p>设置客户端认证参数，替换token</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl config set-credentials system:bootstrap:485bd8 \</span><br><span class="line">  --token=485bd8.711b717a196f47f4 \</span><br><span class="line">  --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf</span><br></pre></td></tr></table></figure>

<p>设置上下文，替换token id</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl config set-context default \</span><br><span class="line">  --cluster=cka \</span><br><span class="line">  --user=system:bootstrap:485bd8 \</span><br><span class="line">  --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf</span><br></pre></td></tr></table></figure>
<p>设置默认上下文  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl config use-context default --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf</span><br></pre></td></tr></table></figure>

<p>将其拷贝到node节点</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scp /etc/kubernetes/bootstrap-kubelet.conf rke-node2:/etc/kubernetes/bootstrap-kubelet.conf</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>拷贝config.yaml到node节点</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scp /var/lib/kubelet/config.yaml rke-node2:/var/lib/kubelet/</span><br></pre></td></tr></table></figure>

<h3 id="配置node节点的kubelet"><a href="#配置node节点的kubelet" class="headerlink" title="配置node节点的kubelet"></a>配置node节点的kubelet</h3><p>创建证书目录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir /etc/kubernetes/pki/</span><br></pre></td></tr></table></figure>

<p>将master节点ca证书拷贝过来</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scp /etc/kubernetes/pki/ca.crt rke-node2:/etc/kubernetes/pki/</span><br><span class="line"></span><br><span class="line">scp /etc/kubernetes/pki/ca.key rke-node2:/etc/kubernetes/pki/</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>修改Address为实际上节点ip</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim /etc/kubernetes/kubelet.config</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kind: KubeletConfiguration</span><br><span class="line">apiVersion: kubelet.config.k8s.io/v1beta1</span><br><span class="line">address: 172.16.0.5</span><br><span class="line">port: 10250</span><br><span class="line">readOnlyPort: 10255</span><br><span class="line">cgroupDriver: cgroupfs</span><br><span class="line">clusterDNS: [&quot;10.96.0.10&quot;]</span><br><span class="line">clusterDomain: cluster.local.</span><br><span class="line">failSwapOn: false</span><br><span class="line">authentication:</span><br><span class="line">  anonymous:</span><br><span class="line">    enabled: true</span><br></pre></td></tr></table></figure>

<p>编辑kubelet.service<br>修改hostname-override为实际节点ip,pause镜像地址按实际镜像仓库地址修改。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim /etc/systemd/system/kubelet.service</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Kubelet</span><br><span class="line">After=docker.service</span><br><span class="line">Requires=docker.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">#EnvironmentFile=/k8s/kubernetes/cfg/kubelet</span><br><span class="line">ExecStart=/usr/bin/kubelet \</span><br><span class="line">--logtostderr=true \</span><br><span class="line">--v=4 \</span><br><span class="line">--hostname-override=172.16.0.5 \</span><br><span class="line">--kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><br><span class="line">--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf \</span><br><span class="line">--config=/etc/kubernetes/kubelet.config \</span><br><span class="line">--cert-dir=/etc/kubernetes/pki \</span><br><span class="line">--pod-infra-container-image=k8s.gcr.io/pause:3.1</span><br><span class="line">Restart=on-failure</span><br><span class="line">KillMode=process</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure>

<p>启动服务  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload </span><br><span class="line">systemctl restart kubelet.service</span><br><span class="line">systemctl status kubelet.service</span><br></pre></td></tr></table></figure>

<p>检查</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl  get node</span><br><span class="line">NAME        STATUS   ROLES    AGE    VERSION</span><br><span class="line">rke-node1   Ready    master   37m    v1.19.1</span><br><span class="line">rke-node2   Ready    &lt;none&gt;   115s   v1.19.1</span><br></pre></td></tr></table></figure>


<h2 id="5-使用临时容器调试现有POD"><a href="#5-使用临时容器调试现有POD" class="headerlink" title="5. 使用临时容器调试现有POD"></a>5. 使用临时容器调试现有POD</h2><p>截止到目前k8s1.18版本，k8s已经支持四种类型的container：标准容器，sidecar容器，init容器，ephemeral容器。</p>
<h3 id="什么是ephemeral容器"><a href="#什么是ephemeral容器" class="headerlink" title="什么是ephemeral容器"></a>什么是ephemeral容器</h3><p>临时容器与其他容器的不同之处在于，它们缺少对资源或执行的保证，并且永远不会自动重启，因此不适用于构建应用程序。临时容器使用与常规容器相同的 ContainerSpec 段进行描述，但许多字段是不相容且不允许的。</p>
<p>临时容器没有端口配置，因此像 ports，livenessProbe，readinessProbe 这样的字段是不允许的。<br>Pod 资源分配是不可变的，因此 resources 配置是不允许的。<br>有关允许字段的完整列表，</p>
<h3 id="ephemeral容器的用途"><a href="#ephemeral容器的用途" class="headerlink" title="ephemeral容器的用途"></a>ephemeral容器的用途</h3><p>　　当由于容器崩溃或容器镜像不包含调试实用程序而导致 kubectl exec 无用时，临时容器对于交互式故障排查很有用。</p>
<p>　　尤其是，distroless 镜像能够使得部署最小的容器镜像，从而减少攻击面并减少故障和漏洞的暴露。由于 distroless 镜像不包含 shell 或任何的调试工具，因此很难单独使用 kubectl exec 命令进行故障排查。</p>
<p>使用临时容器时，启用进程命名空间共享很有帮助，可以查看其他容器中的进程。</p>
<h3 id="使用ephemeral容器"><a href="#使用ephemeral容器" class="headerlink" title="使用ephemeral容器"></a>使用ephemeral容器</h3><p>ephemeral容器目前还是个alpha的功能所以需要在Kubernetes的api-server、scheduler、controller-manager和节点的kubelet开启对应的参数  </p>
<p>修改<code>/etc/kubernetes/manifests/kube-apiserver.yaml、kube-controller-manager.yaml、kube-scheduler.yaml</code><br>添加组件的参数<code>--feature-gates=EphemeralContainers=true</code></p>
<p>修改kubelet参数  </p>
<p>&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;config.yaml</p>
<p>底部的以下几行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">featureGates:</span><br><span class="line">  EphemeralContainers: true</span><br></pre></td></tr></table></figure>
<p>保存文件并运行以下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure>

<p>创建个nginx用于模拟正常pod，打开shareProcessNamespace，让同个pod内的不同container可以查看共同进程空间  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  shareProcessNamespace: true</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: nginx</span><br><span class="line">    stdin: true</span><br><span class="line">    tty: true</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>


<p>目前使用ephemeral容器可以直接通过kubectl进行，在Kubernetes 1.18版本加入了kubectl debug的特性，方便用户进行trouble shooting,目前还是alpha特性</p>
<p>使用kubectl创建ephemeral容器  ，附加个busybox到刚刚创建的nginx容器中</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl alpha debug nginx  -it --image=busybox</span><br></pre></td></tr></table></figure>
<p>因为打开了shareProcessNamespace所以在同一个pod中的不同container可以看见对应互相的进程。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Defaulting debug container name to debugger-mbzbp.</span><br><span class="line"></span><br><span class="line">If you don&#x27;t see a command prompt, try pressing enter.</span><br><span class="line"></span><br><span class="line">/ # ps aux</span><br><span class="line">PID   USER     TIME  COMMAND</span><br><span class="line">    1 root      0:00 /pause</span><br><span class="line">    6 root      0:00 nginx: master process nginx -g daemon off;</span><br><span class="line">   33 101       0:00 nginx: worker process</span><br><span class="line">   46 root      0:00 sh</span><br><span class="line">   51 root      0:00 ps aux</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>安全</category>
      </categories>
      <tags>
        <tag>安全</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes 持久化存储</title>
    <url>/2018/05/19/kubernetes_storage/</url>
    <content><![CDATA[<p>操作系统：ubuntu 16.04<br>kubernetes版本：1.10.0<br>ceph：10.2.10<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s-storage-1.png"></p>
<p>kubernetes常见的三种应用<br>1、无状态应用<br>容器内的应用状态无需保持，<br>kubernetes通过replicaset保证pod的数量，一旦pod挂掉或崩溃，会基于image重建pod，此时pod内数据丢失。<br>2、有状态应用<br>和无状态应用相比，有状态应用多了一个应用状态保存的需求.<br>3、有状态集群应用<br>和有状态应用相比，多了集群管理的需求，那么需要解决的问题有两个，一个是应用状态的保存，一个是集群的管理。</p>
<p>有状态应用和有状态集群应用需要持久化存储里面的数据，比如数据库我们需要存储里面的数据库文件，直接用docker，我们可以使用docker的bind-mont、docker-managed-volume 、volume-container的方式，在kubernetes中解决方案是kubernetes volume和kubernetes persistent volume 。volume独立于pod，pod被销毁了数据没有了，但volume还是存在的，可以给其他pod使用。本质上，volume就是一个目录根目录的映射，它后端可以对应各种各样的存储，但pod在使用时，并不需要关心这些，对于pod来说，它看见的只是一个目录，这点根docker volume基本一样，当一个volume mount到pod中时，这个pod中所有容器都可以访问这个volume，kuberne volume支持多种类型的后端存如awsElasticBlockStore、azureDisk、ceph-rbd</p>
<p>完整参考列表<br><a href="https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes">https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes</a></p>
<p>kubernetes中volume分为两种类型静态供给的和动态供给的<br>静态供给volume：emptyDir、hostpath，Persistent volume<br>动态供给volume：storage-class</p>
<h3 id="静态供给volume"><a href="#静态供给volume" class="headerlink" title="静态供给volume"></a>静态供给volume</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s-storage-2.png"></p>
<p>缺点：<br>pod直接访问volume可移值性和可扩展性、安全性较差</p>
<p>emptyDir<br>emptydir：临时空目录，与pod紧密联接在创建pod是创建，在删除这个pod时，也会自动删除，pod迁移到其他节点数据会丢失。<br>用途：用于存储一些临时数据，如cookie等。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">例：</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line"> name: producer-consumer</span><br><span class="line">spec:</span><br><span class="line"> containers:</span><br><span class="line"> - image: busybox</span><br><span class="line">   name: producer</span><br><span class="line">   volumeMounts:</span><br><span class="line">   - mountPath: /producer_dir</span><br><span class="line">     name: shared-volume</span><br><span class="line">   args:</span><br><span class="line">   - /bin/sh</span><br><span class="line">   - -c</span><br><span class="line">   - echo &quot;hello world&quot; &gt;/producer_dir/hello; sleep 30000</span><br><span class="line"> volumes:</span><br><span class="line"> - name: shared-volume</span><br><span class="line">   emptyDir: &#123;&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>创建一个pod里面有两个container，生产者、消费者共享一个emptyDir，生产者向&#x2F;producer_dir&#x2F;hello写入hello world</p>
<p>通过docker inspect查看映射到宿主机哪个目录  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker inspect 5681556dfd95|grep Source</span><br><span class="line">                &quot;Source&quot;: &quot;/var/lib/kubelet/pods/a22e4253-8290-11e8-85ec-00163e04ede2/volumes/kubernetes.io~empty-dir/shared-volume&quot;,</span><br></pre></td></tr></table></figure>

<p>hostpath：bind-mount与宿主机目录1:1映射，这类存储卷，当数据迁移到其他节点后，就会造成数据丢失。<br>用途：根DaemonSet配合使用如EFK，中fluentd 根容器日志目录映射，来达到收集日志效果。</p>
<p>例</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line"> name: mysql</span><br><span class="line">spec:</span><br><span class="line"> selector:</span><br><span class="line">   matchLabels:</span><br><span class="line">     app: mysql</span><br><span class="line"> template:</span><br><span class="line">   metadata:</span><br><span class="line">     labels:</span><br><span class="line">       app: mysql</span><br><span class="line">   spec:</span><br><span class="line">     containers:</span><br><span class="line">     - image: mysql:5.6</span><br><span class="line">       name: mysql</span><br><span class="line">       env:</span><br><span class="line">       - name: MYSQL_ROOT_PASSWORD</span><br><span class="line">         value: password</span><br><span class="line">       ports:</span><br><span class="line">       - containerPort: 3306</span><br><span class="line">         name: mysql</span><br><span class="line">       volumeMounts:</span><br><span class="line">       - name: vol1</span><br><span class="line">         mountPath: /var/lib/mysql</span><br><span class="line">     volumes:</span><br><span class="line">     - hostPath:</span><br><span class="line">            path: /tmp/mysql</span><br><span class="line">       name: vol1</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>将pod内的&#x2F;var&#x2F;lib&#x2F;mysql与宿主机的&#x2F;tmp&#x2F;mysql映射。</p>
<h3 id="storage-private"><a href="#storage-private" class="headerlink" title="storage-private"></a>storage-private</h3><p>不使用host存储空间，使用公有云厂商对象存储或分布式</p>
<h3 id="persisten-volume"><a href="#persisten-volume" class="headerlink" title="persisten volume"></a>persisten volume</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s-storage-3.png"></p>
<p>volume虽然能提供很好的数据持久化，但在可管理性上，还是不足的，要使用volume，用户必须，知道当前的volume信息和提前创建好对应的volume，kubernetes推荐使用pv、pvc来解决存储持久化的问题。因此kubernetes给出的解决方案是pv(persistent volume)、pvc(persistent volume Claim)<br>persisten volume(pv)是k8s里面的一个资源对象，它是直接和底层存储关联的，pv具有持久性，生命周期独立于pod。<br>persisten volume claim(pvc)是对pv的具体实现，pod使用存储是直接使用pvc，然后pvc会根据pod需要的存储空间大小和访问模式在去寻找合适的pv然后绑定。  </p>
<p>kubernetes支持的pv类型<br><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes">https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes</a><br>例子<br>这里以nfs做为pv后端存储为例讲解pv和pvc的使用<br>搭建nfs</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apt install nfs-kernel-server nfs-common rpcbind</span><br></pre></td></tr></table></figure>
<p>配置共享目录<br>修改&#x2F;etc&#x2F;exports文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/nfsdata *(rw,sync,no_root_squash)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">*：表示所有用户都可以访问</span><br><span class="line">rw：挂接此目录的客户端对该共享目录具有读写权限</span><br><span class="line">sync：资料同步写入内存和硬盘</span><br><span class="line">no_root_squash：root用户具有对根目录的完全管理访问权限</span><br></pre></td></tr></table></figure>

<p>启动rpcbind</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl status rpcbind</span><br></pre></td></tr></table></figure>
<p>启动nfs</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl start nfs-kernel-server</span><br></pre></td></tr></table></figure>
<p>验证</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">showmount -e  nfs_server_ip</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s-storage-4.png"></p>
<p>挂载</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mount -t nfs 172.31.164.57:/nfsdata /mnt/</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s-storage-5.png"></p>
<p>配置pv</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line"> name: mysql-pv</span><br><span class="line">spec:</span><br><span class="line"> accessModes:</span><br><span class="line">     - ReadWriteOnce  #可读写模式支持单节点挂载</span><br><span class="line"> capacity:</span><br><span class="line">   storage: 1Gi</span><br><span class="line"> persistentVolumeReclaimPolicy: Retain</span><br><span class="line"> storageClassName: nfs</span><br><span class="line"> nfs:</span><br><span class="line">   path: /nfsdata/mysql-pv</span><br><span class="line">   server: 172.31.164.57</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s-storage-6.png"></p>
<p>pv的回收模式</p>
<ul>
<li>persistentVolumeReclaimPolicy 为当pvc删除后pv的回收策略。</li>
<li>Retain – pvc删除后pv和数据仍然保留但此时不可以在创建pvc了，需要管理员手工回收。</li>
<li>Recycle – pvc删除后回自动起一个pod将pv内的数据全部清空，可以创建新的pvc。</li>
<li>Delete – 删除 Storage Provider 上的对应存储资源，例如 AWS EBS、GCE PD、Azure Disk、OpenStack Cinder Volume 等。</li>
</ul>
<p>pv的访问模式<br>在pvc绑定pv时通常根据两个条件绑定，一个是存储的大小，另外一个是存储的模式</p>
<ul>
<li>ReadWriteOnce(RWO)：可读写模式支持单节点挂载。</li>
<li>ReadOnlyMany(ROX)： 只读模式支持多节点挂载。</li>
<li>ReadWriteMany(RWX)：可读写模式支持多节点挂载，目前只有少数存储支持这种方式，像ceph-rbd目前只能当个节点挂载。</li>
</ul>
<p>创建pvc</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: mysql-pvc</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br><span class="line">  storageClassName: nfs</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s-storage-7.png"></p>
<p>pvc只需要指定大小，访问模式和className就会根pv关联。</p>
<p>创建mysql使用pvc</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line"> name: mysql</span><br><span class="line">spec:</span><br><span class="line"> selector:</span><br><span class="line">   matchLabels:</span><br><span class="line">     app: mysql</span><br><span class="line"> template:</span><br><span class="line">   metadata:</span><br><span class="line">     labels:</span><br><span class="line">       app: mysql</span><br><span class="line">   spec:</span><br><span class="line">     containers:</span><br><span class="line">     - image: mysql:5.6</span><br><span class="line">       name: mysql</span><br><span class="line">       env:</span><br><span class="line">       - name: MYSQL_ROOT_PASSWORD</span><br><span class="line">         value: password</span><br><span class="line">       ports:</span><br><span class="line">       - containerPort: 3306</span><br><span class="line">         name: mysql</span><br><span class="line">       volumeMounts:</span><br><span class="line">       - name: mysql-persistent-storage</span><br><span class="line">         mountPath: /var/lib/mysql</span><br><span class="line">     volumes:</span><br><span class="line">     - name: mysql-persistent-storage</span><br><span class="line">       persistentVolumeClaim:</span><br><span class="line">        claimName: mysql-pvc</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s-storage-8.png"><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s-storage-9.png"></p>
<p>进入container创建些数据<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s-storage-10.png"><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s-storage-11.png"></p>
<p>将pod所在宿主机关闭了，过几分钟kubernetes会在另外宿主机上启动,并且还是使用这个pvc，因为nfs支持多个host同时挂载.</p>
<h3 id="动态供给volume-storage-class"><a href="#动态供给volume-storage-class" class="headerlink" title="动态供给volume(storage_class)"></a>动态供给volume(storage_class)</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s-storage-12.png"></p>
<p>storage-class：<br>直接使用pv方式都是静态供给，需要管理员提前将pv创建好，然后再与pvc绑定，在kubernetes中动态卷是通过storage-class去实现的。配好storage-class与backend对接，当没有满足pvc条件的pv时，storage-class会动态的去创建一个pv<br>动态卷的优势</p>
<p>1、不需要提前创建好pv，提高效率和资源利用率</p>
<p>2、封装不同的存储类型给pvc使用，在StorageClass出现以前，PVC绑定一个PV只能根据两个条件，一个是存储的大小，另一个是访问模式。在StorageClass出现后，等于增加了一个绑定维度。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s-storage-13.png"><br>在PVC里除了常规的大小、访问模式的要求外，还通过annotation指定了Storage Class的名字为fast，这样这个PVC就会绑定一个SSD，而不会绑定一个普通的磁盘。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s-storage-14.png"></p>
<p>例这里我们以ceph-rbd为例配置一个storage-class<br>因为<br>操作系统加载rbd module<br>modprobe rbd<br>记得加入开机启动脚本，不然重启module又没加载<br>创建一个ceph-rbd的storage-class<br>获取admin的secret并用base64加密</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ceph auth get-key client.admin |base64</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s-storage-15.png"></p>
<p>创建个secret对象</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: ceph-secret-admin</span><br><span class="line">type: &quot;kubernetes.io/rbd&quot;</span><br><span class="line">data:</span><br><span class="line">  key: QVFEMDJ1VmE0L1pxSEJBQUtTUnFwS3JFVjErRjFNM1kwQ2lyWmc9PQ==</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s-storage-16.png"></p>
<p>创建storage-class</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kind: StorageClass</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: rbd</span><br><span class="line">provisioner: kubernetes.io/rbd</span><br><span class="line">parameters:</span><br><span class="line">  monitors: 172.31.164.57:6789</span><br><span class="line">  adminId: admin</span><br><span class="line">  adminSecretName: ceph-secret-admin</span><br><span class="line">  adminSecretNamespace: default</span><br><span class="line">  pool: rbd</span><br><span class="line">  userId: admin</span><br><span class="line">  userSecretName: ceph-secret-admin</span><br></pre></td></tr></table></figure>
<p>monitors:  ceph-monitor地址+端口<br>adminId:  secret的用户名<br>adminSecretName: 上面创建secret的名字<br>pool：rbd 在ceph哪个pool<br>userSecretName：上面创建secret的名字</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: mysql-pvc</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br><span class="line">  storageClassName: rbd</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>storageClassName指向我们刚刚创建storage-class<br>可以看见已经Bound了<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s-storage-22.png"></p>
<p>查看ceph的pool，发现块设备已经创建好了<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s-storage-17.png"><br>继续使用上面的yml创建mysql，会直接用这个pvc<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s-storage-18.png"></p>
<p>到rke-node3上去<br>可以看见kernel rbd module将pool里面的块映射出来了，挂载到pod里面了<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s-storage-19.png"></p>
<p>如果非hyperkube部署的kubernetes集群，如kubeadm部署的，在创建pvc时会报错如下</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">persistentvolume-controller     Warning   ProvisioningFailed  Failed to provision volume with StorageClass &quot;rbd&quot;: failed to create rbd image: executable file not found in $PATH, command output:</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>因为我们的k8s集群是使用kubeadm创建的，k8s的几个服务也是跑在集群静态pod中，而kube-controller-manager组件会调用rbd的api，但是因为它的pod中没有安装rbd，所以会报错。解决办法如下<br>使用kubernetes上的存储扩展卷来解决<br><a href="https://github.com/kubernetes-incubator/external-storage">https://github.com/kubernetes-incubator/external-storage</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone  https://github.com/kubernetes-incubator/external-storage</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s-storage-20.png"></p>
<p>执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f ./</span><br><span class="line">kubectl get pods</span><br><span class="line">NAME                              READY     STATUS    RESTARTS   AGE</span><br><span class="line">rbd-provisioner-23232323-i6223   1/1       Running   0          13s</span><br></pre></td></tr></table></figure>
<p>然后在重新创建storage-class</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat ceph-storageclass.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kind: StorageClass</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: rbd</span><br><span class="line">#provisionen: kubernetes.io/rbd</span><br><span class="line">provisioner: ceph.com/rbd</span><br><span class="line">parameters:</span><br><span class="line">  monitors: 172.31.164.57:6789</span><br><span class="line">  adminId: admin</span><br><span class="line">  adminSecretName: ceph-secret-admin</span><br><span class="line">  adminSecretNamespace: default</span><br><span class="line">  pool: rbd</span><br><span class="line">  userId: admin</span><br><span class="line">  userSecretName: ceph-secret-admin</span><br></pre></td></tr></table></figure>
<p>注意provisioner由provisionen: kubernetes.io&#x2F;rbd改成provisioner: ceph.com&#x2F;rbd<br>然后在创建pvc就没有问题了。</p>
<p>使用rbd做为pv的backend或storagclass的backend整体原理如下：<br>创建pvc—-对应在ceph pool内创建一个rbd对象。<br>pod挂载这个pvc—-将对应的rbd块通过内核rbd模块map到宿主机上。然后在mount到宿主机 &#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;pods&#x2F;xxxx目录，在将这个目录根pod对bind mount。<br>删除pod—-在对应的host上umount目录然后将rbd块与宿主机unmap。<br>在kubernetes1.11之前的版本有个问题就是删除pod后，不会自动将rbd块从host上unmap。<br>PR如下 1.11版本已经修复这个问题<br><a href="https://github.com/kubernetes/kubernetes/pull/63579">https://github.com/kubernetes/kubernetes/pull/63579</a></p>
<p>就是在使用pod如果挂载后端为rbd的pvc后，在底层实际上是会从ceph的pool里面将一个rbd块设备map的host上然后在mount到pod内，<br>需要注意的是：</p>
<ul>
<li>因为在k8s里面是直接用的是内核的rbd块，openstack用的是librbd接口，所以这里需要在每个host上内核要加载rbd模块，不然挂载不上去的，modprobe rbd。  </li>
<li>ceph-rbd 在AccessMode为ReadWriteOnce情况下只支持单节点挂载，不能被多个pod同时使用这个pvc。也就是说一个host挂了，如果上面pod使用了ceph-rbd，哪里在另外一个host上重建这个pod时会报这个pvc已经被使用了，但在AccessMode为ReadWriteOnce+ReadOnlyMany的情况下支持一个pvc被多个pod挂载。</li>
</ul>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/k8s-storage-21.png"></p>
<h4 id="local-pv"><a href="#local-pv" class="headerlink" title="local-pv"></a>local-pv</h4><p>Local-pv是让用户使用标准的k8s pv和pvc的接口使用node节点的本地存储，kubernetes1.7为alpha版本，1.14正式GA，这和我们上面说的host-path和empty-Volume有个共同点都是使用node节点的本地存储，但区别在于hostpath是单节点的本地存储，也就是说只有当pod调度到哪台节点便使用那台节点对应的存储，无法提供node亲和性和POD调度支持，但通过localpv可以在pv的定义中配置节点亲和性，这样对应的使用这个pvc的POD也会根据亲和性配置调度到对应的节点上，比如在local-pv的定义配置了主机亲和性为node-2，那么使用这个pvc的pod也会调度到node-2上。  </p>
<p>使用场景：<br>1、分布式数据库和分布式文件系统（redis、ElasticSearch、memcache、ceph)，这类应用在应用本身能实现高可用，只是依靠localpv实现高性能存储。<br>2、需要临时缓存存储的应用，当应用删除后对应的数据也能随之清理，因为使用localpv可以灵活设置pv的回收策略。  </p>
<p>测试版本：kubernetesv1.14.3<br>部署storageclass  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: local-storage</span><br><span class="line">provisioner: kubernetes.io/no-provisioner</span><br><span class="line">volumeBindingMode: WaitForFirstConsumer</span><br></pre></td></tr></table></figure>

<p>volumeBindingMode: WaitForFirstConsumer ：配置延迟绑定，也就是pv control并不会立刻将pvc与pv关联，而是等待pod调度后才去bond，还有种默认的Immediate模式则是当pvc创建时立刻绑定。  </p>
<p>创建pv  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: example-pv</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 100Gi</span><br><span class="line">  volumeMode: Filesystem</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteOnce</span><br><span class="line">  persistentVolumeReclaimPolicy: delete</span><br><span class="line">  storageClassName: local-storage</span><br><span class="line">  local:</span><br><span class="line">    path: /mnt/</span><br><span class="line">  nodeAffinity:</span><br><span class="line">    required:</span><br><span class="line">      nodeSelectorTerms:</span><br><span class="line">      - matchExpressions:</span><br><span class="line">        - key: kubernetes.io/hostname</span><br><span class="line">          operator: In</span><br><span class="line">          values:</span><br><span class="line">          - rke-node2</span><br></pre></td></tr></table></figure>
<p>注意点 ： volumeMode: Filesystem为文件系统模式，也就是对应的目录，这里也可以直接设置为BlockVolume模式直接对应主机上的块设备，当需要在kube-apiserver、kube-controlmanager、kubelet开启–feature-gates&#x3D;BlockVolume&#x3D;true参数，但这个参数在1.13版本已经默认是true了，参考：<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/</a>  </p>
<p>local path设置的就是对应宿主机的供挂载的目录，nodeselectorTerms设置的节点的亲和性。<br>创建pvc  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    cattle.io/creator: norman</span><br><span class="line">  name: example-local-claim</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteOnce</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 100Gi</span><br><span class="line">  storageClassName: local-storage</span><br><span class="line">  volumeMode: Filesystem</span><br><span class="line">  volumeName: example-pv</span><br></pre></td></tr></table></figure>
<p>创建POD使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kind: Pod</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: example-pv-pod</span><br><span class="line">spec:</span><br><span class="line">  volumes:</span><br><span class="line">    - name: example-pv-storage</span><br><span class="line">      persistentVolumeClaim:</span><br><span class="line">       claimName: example-local-claim</span><br><span class="line">  containers:</span><br><span class="line">    - name: example-pv-container</span><br><span class="line">      image: nginx</span><br><span class="line">      ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          name: &quot;http-server&quot;</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - mountPath: &quot;/usr/share/nginx/html&quot;</span><br><span class="line">          name: example-pv-storage</span><br></pre></td></tr></table></figure>
<p>局限性<br>1、目前local-pv不支持对空间申请管理，需要手动对空间进行配置和管理.<br>2、默认local pv的StorageClass的provisioner是kubernetes.io&#x2F;no-provisioner , 这是因为local pv不支持Dynamic Provisioning, 所以它没有办法在创建出pvc的时候, 自动创建对应pv.  </p>
<p>static-provisioner配置   </p>
<p>可以自己实现一个static-provisioner来创建和管理PV，可以直接使用社区已经写的static-provisioner：<a href="https://github.com/kubernetes-incubator/external-storage/tree/master/local-volume#option-1-using-the-local-volume-static-provisioner">https://github.com/kubernetes-incubator/external-storage/tree/master/local-volume#option-1-using-the-local-volume-static-provisioner</a><br>当然也可以自己实现，这里使用Rancher实现的static-provisioner<br><a href="https://github.com/rancher/local-path-provisioner">https://github.com/rancher/local-path-provisioner</a><br>部署  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml</span><br></pre></td></tr></table></figure>

<p>默认pv的回收模式为delete模式，需要修改为recycle模式直接修改yaml即可。<br>执行后会部署  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">namespace/local-path-storage created</span><br><span class="line">serviceaccount/local-path-provisioner-service-account created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/local-path-provisioner-role created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/local-path-provisioner-bind created</span><br><span class="line">deployment.apps/local-path-provisioner created</span><br><span class="line">storageclass.storage.k8s.io/local-path created</span><br></pre></td></tr></table></figure>

<p>查看POD  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod -n local-path-storage</span><br><span class="line">NAME                                    READY   STATUS    RESTARTS   AGE</span><br><span class="line">local-path-provisioner-848fdcff-tqwc2   1/1     Running   0          35s</span><br></pre></td></tr></table></figure>
<p>查看storage-class  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get storageclass</span><br><span class="line">NAME         PROVISIONER             AGE</span><br><span class="line">local-path   rancher.io/local-path   4m17s</span><br></pre></td></tr></table></figure>

<p>创建PVC  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/examples/pvc.yaml</span><br></pre></td></tr></table></figure>

<p>在没创建pod使用此PVC前，默认模式为 WaitForFirstConsumer，所以pvc的状态会是pending状态。  </p>
<p>创建POD  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/examples/pod.yaml</span><br></pre></td></tr></table></figure>

<p>默认目录为&#x2F;opt&#x2F;local-path-provisioner目录下，可以在POD所在节点上查看<br>当我们需要进行细粒化配置时，如在node1节点上使用&#x2F;data目录，在node2节点使用&#x2F;data2目录，其他节点使用&#x2F;data3目录时可以修改 local-path-storage命名空间内的local-path-config这个configmap  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: local-path-config</span><br><span class="line">  namespace: local-path-storage</span><br><span class="line">data:</span><br><span class="line">  config.json: |-</span><br><span class="line">        &#123;</span><br><span class="line">                &quot;nodePathMap&quot;:[</span><br><span class="line">                &#123;</span><br><span class="line">                        &quot;node&quot;:&quot;DEFAULT_PATH_FOR_NON_LISTED_NODES&quot;,</span><br><span class="line">                        &quot;paths&quot;:[&quot;/opt/local-path-provisioner&quot;]</span><br><span class="line">                &#125;,</span><br><span class="line">                &#123;</span><br><span class="line">                        &quot;node&quot;:&quot;rke-node2&quot;,</span><br><span class="line">                        &quot;paths&quot;:[&quot;/data2&quot;]</span><br><span class="line">                &#125;</span><br><span class="line">                ]</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p>更新成功后，查看  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl logs  pod/local-path-provisioner-848fdcff-lr2pj -n local-path-storage</span><br></pre></td></tr></table></figure>
<p>可以看见其加载配置的信息。</p>
<p>参考链接<br><a href="https://www.kubernetes.org.cn/3462.html">https://www.kubernetes.org.cn/3462.html</a><br><a href="https://blog.csdn.net/liukuan73/article/details/60089305">https://blog.csdn.net/liukuan73/article/details/60089305</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux下单网卡配置属于不同vlan的ip(vlan子接口)</title>
    <url>/2017/05/23/linux_vconfig/</url>
    <content><![CDATA[<p>一台服务器上一块网卡想同时划到两个不同vlan，并且让他们之间可以互相通信。</p>
<p>1、首先确认Linux系统内核是否已经支持VLAN功能，加载了8021q模块，lsmod |grep 8021q。<br>2、关于网卡的解释，好多人不知道网卡接口上的冒号和点号的区别，以下是一些解释（我也是从网上查的，仅供参考）<br>    a、物理网卡：物理网卡指的是服务器上实际的网络接口设备，如在系统中看到的2个物理网卡分别对应是eth0和eth1这两个网络接口。<br>    b、子网卡：子网卡并不是实际上的网络接口设备，但是可以作为网络接口在系统中出现，如eth0:1、eth1:2这种网络接口。它们必须要依赖 于物理网卡，虽然可以与物理网卡的网络接口同时在系统中存在并使用不同的IP地址，而且也拥有它们自己的网络接口配置文件。但是当所依赖的物理网卡不启用 时（Down状态）这些子网卡也将一同不能工作。<br>    c、虚拟VLAN网卡：这些虚拟VLAN网卡也不是实际上的网络接口设备，也可以作为网络接口在系统中出现，但是与子网卡不同的是，他们没有自己 的配置文件。他们只是通过将物理网加入不同的VLAN而生成的VLAN虚拟网卡。如果将一个物理网卡添加到多个VLAN当中去的话，就会有多个VLAN虚 拟网卡出现，他们的信息以及相关的VLAN信息都是保存在&#x2F;proc&#x2F;net &#x2F;vlan&#x2F;config这个临时文件中的，而没有独自的配置文件。它们的网络接口名是eth0.1、eth1.2这种名字。</p>
<p>下面为实际操作，我的环境是一台H3C S5120三层交换机，服务器有4个网卡。</p>
<h3 id="交换机配置"><a href="#交换机配置" class="headerlink" title="交换机配置"></a>交换机配置</h3><p>配置trunk，并且配置允许vlan id，我这里端口是g1&#x2F;0&#x2F;11 到g1&#x2F;0&#x2F;15</p>
<p>设置该端口为trunk模式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[H3C-Git1/0/11]port link-type trunk //vlan端口默认为access模式</span><br><span class="line">```    </span><br><span class="line">设置允许vlan 10和20的tag通过该端口</span><br></pre></td></tr></table></figure>
<p>[H3c-Git1&#x2F;0&#x2F;11]port trunk permit vlan 40 50 &#x2F;&#x2F;替换后面的数字为all则是允许所有vlan tag通过该端口</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">其他12，13，14，15口一样配置</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">接下来给vlan配置ip(配置网关)</span><br></pre></td></tr></table></figure>
<p>[H3C]interface Vlan-interface 40<br>[H3C-Vlan-interface1]ip address 192.168.4.254 255.255.255.0</p>
<p>[H3C]interface Vlan-interface 50<br>[H3C-Vlan-interface1]ip address 192.168.5.254 255.255.255.0</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">交换机配置到此结束，接下来是Linux网卡配置</span><br><span class="line"></span><br><span class="line">Linux要配置多vlan口。注：当需要启用VLAN虚拟网卡工作的时候，关联的物理网卡网络接口上必须没有IP地址的配置信息。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">首先确认是否安装vconfig</span><br><span class="line"></span><br><span class="line">然后查看核心是否提供VLAN 功能，执行</span><br></pre></td></tr></table></figure>
<p>dmesg | grep -i 802</p>
<p>[root@test]# dmesg | grep -i 802</p>
<p>802.1Q VLAN Support v1.8 Ben Greear<a href="mailto:&#103;&#114;&#101;&#x65;&#97;&#x72;&#98;&#64;&#x63;&#97;&#x6e;&#100;&#101;&#108;&#97;&#116;&#x65;&#99;&#x68;&#x2e;&#x63;&#x6f;&#109;">&#103;&#114;&#101;&#x65;&#97;&#x72;&#98;&#64;&#x63;&#97;&#x6e;&#100;&#101;&#108;&#97;&#116;&#x65;&#99;&#x68;&#x2e;&#x63;&#x6f;&#109;</a></p>
<p>[root@test]# modprobe 8021q</p>
<p>[root@test~]#lsmod |grep 8021q &#x2F;&#x2F;查看系统内核是否支持802.1q协议</p>
<p>8021q 18633 0</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>vconfig add eno3 300 &#x2F;&#x2F; eno3为物理网络接口名称，300为 802.1q tag id 也即 vlan ID<br>vconfig add eno3 500</p>
<p>ifconfig eno3.300  192.168.4.5&#x2F;24 up<br>ifconfig eno3.500  172.31.5.5&#x2F;24  up </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">最终可以在可以创建成功的vlan接口  </span><br><span class="line">root@node-78:~# ls /proc/net/vlan/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">然后把这些写进配置文件不然重启就没有了</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vim /etc/network/interfaces </span><br></pre></td></tr></table></figure>
<p>auto lo eno3<br>iface lo inet loopback<br>iface eno3.300 inet static<br>    address 192.168.4.5<br>    netmask 255.255.255.0<br>iface eno3.500 inet static<br>    address 172.31.5.5<br>    netmask 255.255.255.0</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">测试在服务器上分别ping 192.168.4.254和172.31.5.254可以ping通就行。</span><br><span class="line">不能通的话， router -n 查看路由，是不是还是以前的老路由，是的话要删除</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ip route del xxxx via xxx.xxx.xxx.xxx</span><br><span class="line">                            dev  接口</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">5,删除VLAN命令</span><br></pre></td></tr></table></figure>
<p>[root@test0001<del>]#vconfig rem eth0.100<br>Removed VLAN -:eth0.100:-<br>[root@test0001</del>]#vconfig rem eth0.200<br>Removed VLAN -:eth0.200:-</p>
<pre><code>在openstack虚拟机里不能做操作，不过newton版开发了这个plugin叫VLAN-aware-VMs  
http://blog.csdn.net/bc_vnetwork/article/details/53927687
</code></pre>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>使用ECK部署ElasticSearch到Kubernetes中</title>
    <url>/2020/09/25/logging_1/</url>
    <content><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>通过operator对有状态应用可以快速进行部署，将ElasticSearch集群快速部署在Kubernetes集群上，并且可以通过operator快速对集群进行管理和扩容。</p>
<h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><table>
<thead>
<tr>
<th>软件</th>
<th>版本</th>
</tr>
</thead>
<tbody><tr>
<td>Rancher</td>
<td>2.3.8-ent</td>
</tr>
<tr>
<td>Kubernetes</td>
<td>1.17.6</td>
</tr>
<tr>
<td>OS</td>
<td>centos7.6</td>
</tr>
<tr>
<td>ElasticSearch</td>
<td>7.9</td>
</tr>
<tr>
<td>ECK-Opera</td>
<td>1.2</td>
</tr>
</tbody></table>
<h3 id="架构规划"><a href="#架构规划" class="headerlink" title="架构规划"></a>架构规划</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/efk_operator_2.png"></p>
<h3 id="详细部署"><a href="#详细部署" class="headerlink" title="详细部署"></a>详细部署</h3><h4 id="调整宿主机操作系统内核参数"><a href="#调整宿主机操作系统内核参数" class="headerlink" title="调整宿主机操作系统内核参数"></a>调整宿主机操作系统内核参数</h4><p>限制一个进程可以拥有的VMA(虚拟内存区域)的数量要超过262144，不然elasticsearch会报max virtual memory areas vm.max_map_count [65535] is too low, increase to at least [262144]</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">echo &quot;vm.max_map_count=655360&quot;&gt;&gt;/etc/sysctl.conf </span><br></pre></td></tr></table></figure>

<p>执行生效</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sysctl -p</span><br></pre></td></tr></table></figure>


<h4 id="部署Local-pv"><a href="#部署Local-pv" class="headerlink" title="部署Local-pv"></a>部署Local-pv</h4><p>使用Rancher开源的local-path-provisioner驱动将节点本地存储用于存储ElasticSearch的数据  </p>
<p><a href="https://github.com/rancher/local-path-provisioner">https://github.com/rancher/local-path-provisioner</a></p>
<p><a href="https://www.bladewan.com/2018/05/19/kubernetes_storage/">https://www.bladewan.com/2018/05/19/kubernetes_storage/</a></p>
<h4 id="部署es-operator"><a href="#部署es-operator" class="headerlink" title="部署es-operator"></a>部署es-operator</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f https://download.elastic.co/downloads/eck/1.2.1/all-in-one.yaml</span><br></pre></td></tr></table></figure>

<p>检查es-operator状态</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl -n elastic-system logs -f statefulset.apps/elastic-operator</span><br></pre></td></tr></table></figure>


<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod/elastic-operator-0 -n elastic-system</span><br><span class="line">NAME                 READY   STATUS    RESTARTS   AGE</span><br><span class="line">elastic-operator-0   1/1     Running   0          2d23h</span><br></pre></td></tr></table></figure>


<h4 id="部署ElasticSearch"><a href="#部署ElasticSearch" class="headerlink" title="部署ElasticSearch"></a>部署ElasticSearch</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line"></span><br><span class="line">apiVersion: elasticsearch.k8s.elastic.co/v1</span><br><span class="line">kind: Elasticsearch</span><br><span class="line">metadata:</span><br><span class="line">  name: quickstart</span><br><span class="line">spec:</span><br><span class="line">  version: 7.9.0</span><br><span class="line">  nodeSets:</span><br><span class="line">  - name: master-nodes</span><br><span class="line">    count: 3</span><br><span class="line">    podTemplate:</span><br><span class="line">      spec:</span><br><span class="line">        containers:</span><br><span class="line">        - name: elasticsearch</span><br><span class="line">          env:</span><br><span class="line">          - name: ES_JAVA_OPTS</span><br><span class="line">            value: -Xms2g -Xmx2g</span><br><span class="line">          resources:</span><br><span class="line">            requests:</span><br><span class="line">              memory: 4Gi</span><br><span class="line">              cpu: 0.5</span><br><span class="line">            limits:</span><br><span class="line">              memory: 4Gi</span><br><span class="line">              cpu: 2</span><br><span class="line">          image: docker.elastic.co/elasticsearch/elasticsearch:7.9.0 </span><br><span class="line">    config:</span><br><span class="line">      node.master: true</span><br><span class="line">      node.data: false</span><br><span class="line">      node.ingest: false</span><br><span class="line">    volumeClaimTemplates:</span><br><span class="line">    - metadata:</span><br><span class="line">        name: elasticsearch-data</span><br><span class="line">      spec:</span><br><span class="line">        accessModes:</span><br><span class="line">        - ReadWriteOnce</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            storage: 10Gi</span><br><span class="line">        storageClassName: local-path</span><br><span class="line">  - name: data-nodes</span><br><span class="line">    count: 3</span><br><span class="line">    podTemplate:</span><br><span class="line">      spec:</span><br><span class="line">        containers:</span><br><span class="line">        - name: elasticsearch</span><br><span class="line">          env:</span><br><span class="line">          - name: ES_JAVA_OPTS</span><br><span class="line">            value: -Xms2g -Xmx2g</span><br><span class="line">          resources:</span><br><span class="line">            requests:</span><br><span class="line">              memory: 4Gi</span><br><span class="line">              cpu: 0.5</span><br><span class="line">            limits:</span><br><span class="line">              memory: 4Gi</span><br><span class="line">              cpu: 2</span><br><span class="line">          image: docker.elastic.co/elasticsearch/elasticsearch:7.9.0             </span><br><span class="line">    config:</span><br><span class="line">      node.master: false</span><br><span class="line">      node.data: true</span><br><span class="line">      node.ingest: false</span><br><span class="line">    volumeClaimTemplates:</span><br><span class="line"></span><br><span class="line">    - metadata:</span><br><span class="line">        name: elasticsearch-data</span><br><span class="line">      spec:</span><br><span class="line">        accessModes:</span><br><span class="line">        - ReadWriteOnce</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            storage: 10Gi</span><br><span class="line">        storageClassName: local-path</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p>注意点：<br>1、修改实际的Request和limit,Xms和Xmx设置为实际limit的一半。<br>2、storageClassName修改为实际的存储名称。<br>3、镜像地址修改为实际镜像地址。</p>
<p>查看部署状态 </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get elasticsearch</span><br><span class="line">NAME         HEALTH   NODES   VERSION   PHASE   AGE</span><br><span class="line">quickstart   green    6       7.9.0     Ready   41m</span><br></pre></td></tr></table></figure>

<p>查看部署的POD状态信息，部署了3个Master+3个Data节点  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod </span><br><span class="line">NAME                           READY   STATUS    RESTARTS   AGE</span><br><span class="line">quickstart-es-data-nodes-0     1/1     Running   0          10m</span><br><span class="line">quickstart-es-data-nodes-1     1/1     Running   0          12m</span><br><span class="line">quickstart-es-data-nodes-2     1/1     Running   0          14m</span><br><span class="line">quickstart-es-master-nodes-0   1/1     Running   0          5m57s</span><br><span class="line">quickstart-es-master-nodes-1   1/1     Running   0          7m23s</span><br><span class="line">quickstart-es-master-nodes-2   1/1     Running   0          8m36s</span><br></pre></td></tr></table></figure>

<p>默认开启了x-pack插件有认证系统，帐户默认为elastic，密码保存在secret密钥中</p>
<p>获取访问密钥  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PASSWORD=$(kubectl get secret quickstart-es-elastic-user -o go-template=&#x27;&#123;&#123;.data.elastic | base64decode&#125;&#125;&#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>使用NodePort方式对外暴露</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl patch svc quickstart-es-http -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&#x27;</span><br></pre></td></tr></table></figure>
<p>访问ElasticSearch</p>
<p>查看集群状态</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl --insecure  -u &quot;elastic:$PASSWORD&quot; -k  https://node_ip:NodePort/_cat/health?v </span><br><span class="line">epoch      timestamp cluster    status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent</span><br><span class="line">1599476283 10:58:03  quickstart green           6         3      0   0    0    0        0             0                  -                100.0%</span><br><span class="line"> </span><br></pre></td></tr></table></figure>

<p>查看集群节点信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl --insecure  -u &quot;elastic:$PASSWORD&quot; -k  https://172.16.1.6:30620/_cat/nodes?v</span><br><span class="line">ip          heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name</span><br><span class="line">10.42.1.92            27          58  16    2.97    2.15     1.66 dlrt      -      quickstart-es-data-nodes-1</span><br><span class="line">10.42.4.15             8          58  15    2.00    1.03     0.95 lmr       -      quickstart-es-master-nodes-0</span><br><span class="line">10.42.0.41             5          58  15    1.88    2.82     2.92 dlrt      -      quickstart-es-data-nodes-0</span><br><span class="line">10.42.2.183           35          58   6    0.12    0.09     0.09 lmr       *      quickstart-es-master-nodes-2</span><br><span class="line">10.42.5.8              8          58  12    0.85    0.45     0.34 dlrt      -      quickstart-es-data-nodes-2</span><br><span class="line">10.42.3.20            56          58   8    0.05    0.18     0.38 lmr       -      quickstart-es-master-nodes-1</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>角色说明：</p>
<p>d (data node)<br>i (ingest node)<br>m (master-eligible node)<br>l (machine learning node)<br>v (voting-only node)<br>t (transform node)<br>r (remote cluster client node)<br>‘-‘ (coordinating node only).</p>
<p>master处带*表示为3个master节点中选举出来的leader节点。  </p>
<h4 id="部署Kibana"><a href="#部署Kibana" class="headerlink" title="部署Kibana"></a>部署Kibana</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line">apiVersion: kibana.k8s.elastic.co/v1</span><br><span class="line">kind: Kibana</span><br><span class="line">metadata:</span><br><span class="line">  name: quickstart</span><br><span class="line">spec:</span><br><span class="line">  version: 7.9.1</span><br><span class="line">  count: 1</span><br><span class="line">  elasticsearchRef:</span><br><span class="line">    name: quickstart</span><br><span class="line">  podTemplate:</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: kibana</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            memory: 1Gi</span><br><span class="line">            cpu: 0.5</span><br><span class="line">          limits:</span><br><span class="line">            memory: 2Gi</span><br><span class="line">            cpu: 2</span><br><span class="line">        image: docker.elastic.co/kibana/kibana:7.9.1</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>



<p>检查kibana部署状态</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get kibana</span><br><span class="line"></span><br><span class="line">NAME         HEALTH   NODES   VERSION   AGE</span><br><span class="line">quickstart   green            7.9.1     6m21s</span><br></pre></td></tr></table></figure>


<p>使用NodePort对外暴露</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl patch svc quickstart-kb-http -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&#x27;</span><br></pre></td></tr></table></figure>


<p>获取密码，默认情况下kibana的密码也是存储在secret中  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get secret quickstart-es-elastic-user -o=jsonpath=&#x27;&#123;.data.elastic&#125;&#x27; | base64 --decode; echo</span><br></pre></td></tr></table></figure>


<p>通过<a href="https://host_ip:nodeport访问kibana">https://host_ip:nodeport访问kibana</a></p>
<p>帐号为elastic，密码为刚刚通过secret获取的密码</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/efk_operator_1.png"></p>
<h4 id="部署Fluentd"><a href="#部署Fluentd" class="headerlink" title="部署Fluentd"></a>部署Fluentd</h4><p>部署Fluentd<br>Fluentd是使用c+Ruby语言开发的一个日志收集端，目前也是CNCF官方项目，拥有非常丰富的插件生态。</p>
<p><img src="https://pic4.zhimg.com/v2-2fcadfa9e850bdc75e27859f2edcfe2f_b.jpg"></p>
<p>优点：</p>
<ul>
<li>1、插件生态丰富，可以使用目前社区比较丰富的插件避免重复造轮子。</li>
<li>2、插件使用Ruby语言自定义编写方便</li>
</ul>
<p>缺点：</p>
<ul>
<li>1、受限于Ruby的GIL无法实现真正高性能</li>
</ul>
<p>部署Fluentd</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: fluentd</span><br><span class="line">  namespace: kube-system</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: fluentd</span><br><span class="line">  namespace: kube-system</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - &quot;&quot;</span><br><span class="line">  resources:</span><br><span class="line">  - pods</span><br><span class="line">  - namespaces</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: fluentd</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: fluentd</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: fluentd</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: fluentd</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: fluentd-logging</span><br><span class="line">    version: v1</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: fluentd-logging</span><br><span class="line">      version: v1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: fluentd-logging</span><br><span class="line">        version: v1</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccount: fluentd</span><br><span class="line">      serviceAccountName: fluentd</span><br><span class="line">      tolerations:</span><br><span class="line">      - key: node-role.kubernetes.io/master</span><br><span class="line">        effect: NoSchedule</span><br><span class="line">      containers:</span><br><span class="line">      - name: fluentd</span><br><span class="line">        image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch</span><br><span class="line">        env:</span><br><span class="line">          - name:  FLUENT_ELASTICSEARCH_HOST</span><br><span class="line">            value: &quot;10.43.7.26&quot;</span><br><span class="line">          - name:  FLUENT_ELASTICSEARCH_PORT</span><br><span class="line">            value: &quot;9200&quot;</span><br><span class="line">          - name: FLUENT_ELASTICSEARCH_SCHEME</span><br><span class="line">            value: &quot;https&quot;</span><br><span class="line">          # Option to configure elasticsearch plugin with self signed certs</span><br><span class="line">          # ================================================================</span><br><span class="line">          - name: FLUENT_ELASTICSEARCH_SSL_VERIFY</span><br><span class="line">            value: &quot;false&quot;</span><br><span class="line">          # Option to configure elasticsearch plugin with tls</span><br><span class="line">          # ================================================================</span><br><span class="line">          - name: FLUENT_ELASTICSEARCH_SSL_VERSION</span><br><span class="line">            value: &quot;TLSv1_2&quot;</span><br><span class="line">          # X-Pack Authentication</span><br><span class="line">          # =====================</span><br><span class="line">          - name: FLUENT_ELASTICSEARCH_USER</span><br><span class="line">            value: &quot;elastic&quot;</span><br><span class="line">          - name: FLUENT_ELASTICSEARCH_PASSWORD</span><br><span class="line">            value: &quot;g38SlQ408i5SI6E24SoqdE3N&quot;</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            memory: 200Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 100m</span><br><span class="line">            memory: 200Mi</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: varlog</span><br><span class="line">          mountPath: /var/log</span><br><span class="line">        - name: varlibdockercontainers</span><br><span class="line">          mountPath: /var/lib/docker/containers</span><br><span class="line">          readOnly: true</span><br><span class="line">      terminationGracePeriodSeconds: 30</span><br><span class="line">      volumes:</span><br><span class="line">      - name: varlog</span><br><span class="line">        hostPath:</span><br><span class="line">          path: /var/log</span><br><span class="line">      - name: varlibdockercontainers</span><br><span class="line">        hostPath:</span><br><span class="line">          path: /var/lib/docker/containers</span><br></pre></td></tr></table></figure>
<p>注意将以下几个环境变量改为实际得ElasticSearch地址和配置帐号</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FLUENT_ELASTICSEARCH_HOST #修改为实际的ElasticSearch地址</span><br><span class="line">FLUENT_ELASTICSEARCH_PORT</span><br><span class="line">#修改为实际的ElasticSearch的端口</span><br><span class="line">FLUENT_ELASTICSEARCH_SSL_VERIFY</span><br><span class="line">#因为采用的是自签名证书，所以这里关闭SSL认证</span><br><span class="line">FLUENT_ELASTICSEARCH_USER</span><br><span class="line">#连接的x-pack的帐号</span><br><span class="line">FLUENT_ELASTICSEARCH_PASSWORD</span><br><span class="line">#连接的x-pack的密码</span><br></pre></td></tr></table></figure>
<p>采用Daemonset方式进行部署，部署后会将主机的&#x2F;var&#x2F;log目录映射到了容器内，在fluentd内配置文件对应的<code>/fluentd/etc/kubernetes.conf</code>文件中，tail插件对&#x2F;var&#x2F;log&#x2F;container&#x2F;目录的*.log文件进行收集，此目录存储的是对应的容器标准输出的日志文件的软链接。</p>
<p>配置kibana，选择对应的Index，查看</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/efk_operator_4.png"></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Longhorn部署安装和管理</title>
    <url>/2020/06/16/longhorn_install/</url>
    <content><![CDATA[<h2 id="Longhorn部署"><a href="#Longhorn部署" class="headerlink" title="Longhorn部署"></a>Longhorn部署</h2><h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><table>
<thead>
<tr>
<th>软件</th>
<th>版本</th>
</tr>
</thead>
<tbody><tr>
<td>操作系统</td>
<td>centos7.8</td>
</tr>
<tr>
<td>rancher</td>
<td>v2.3.8-ent2</td>
</tr>
<tr>
<td>Kubernetes</td>
<td>1.17.4</td>
</tr>
<tr>
<td>Longhorn</td>
<td></td>
</tr>
</tbody></table>
<h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>节点上安装iscsi插件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum install iscsi-initiator-utils</span><br></pre></td></tr></table></figure>

<p>将需要添加到Longhorn的磁盘进行格式化和挂载到&#x2F;var&#x2F;lib&#x2F;longhorn目录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkfs.xfs /dev/vdb</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mount /dev/vdb /var/lib/longhorn</span><br></pre></td></tr></table></figure>


<h3 id="部署Longhorn"><a href="#部署Longhorn" class="headerlink" title="部署Longhorn"></a>部署Longhorn</h3><p>通过Rancher应用商店启动Longhorn<br><img src="https://pic.downk.cc/item/5ed1d77fc2a9a83be50cb29d.jpg"></p>
<p>根据实际情况配置参数<br><img src="https://pic.downk.cc/item/5ed1d7a6c2a9a83be50ce522.jpg"></p>
<p>启动后访问Longhorn</p>
<p><img src="https://pic.downk.cc/item/5ed1d855c2a9a83be50dc5ad.jpg"></p>
<p>在rancher中创建workload绑定PVC进行测试</p>
<p><img src="https://pic.downk.cc/item/5ed1d892c2a9a83be50e105f.jpg"></p>
<p><img src="https://pic.downk.cc/item/5ed1d8eec2a9a83be50e815c.jpg"></p>
<h3 id="扩容磁盘"><a href="#扩容磁盘" class="headerlink" title="扩容磁盘"></a>扩容磁盘</h3><p>在Longhorn界面上选择对应的节点<br><img src="https://pic.downk.cc/item/5ed1f81ac2a9a83be535b93f.jpg"></p>
<p>添加磁盘挂载路径即可  </p>
<p><img src="https://pic.downk.cc/item/5ed1f866c2a9a83be535fce0.jpg"></p>
<h3 id="存储卷扩容"><a href="#存储卷扩容" class="headerlink" title="存储卷扩容"></a>存储卷扩容</h3><p>有两种扩展Longhorn体积的方法：使用PersistentVolumeClaim（PVC）和Longhorn UI。</p>
<p>如果使用的是Kubernetes v1.14或v1.15，则只能使用Longhorn UI扩展卷。</p>
<p>通过PVC<br>仅在以下情况下应用此方法：</p>
<ul>
<li>Kubernetes版本v1.16或更高版本。</li>
<li>Kubernetes使用Longhorn StorageClass动态设置PVC。</li>
<li>StorageClass中的allowVolumeExpansion字段设置为true。</li>
</ul>
<p>用法：将workload副本设置为0，在Longhorn卷找到相应的PVC，然后通过kubectl修改对应的pvc中的spec.resources.requests.字段，在Longhorn中对应的卷大小会自动更新。</p>
<h3 id="存储卷快照"><a href="#存储卷快照" class="headerlink" title="存储卷快照"></a>存储卷快照</h3><p>选择对应的存储卷，创建快照<br><img src="https://pic.downk.cc/item/5ed2200bc2a9a83be5658a4d.jpg"></p>
<p>还原快照<br>1、将挂载此卷的workload副本数设置为0<br>2、重新将此卷使用维护模式挂载到对应的节点。</p>
<p><img src="https://pic.downk.cc/item/5ed2216ec2a9a83be567a3e8.jpg"></p>
<p>3、选择卷快照还原，还原后将磁盘从对应节点卸载。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Longhorn-12.png"></p>
<p>4、应用启动查看数据。    </p>
<h2 id="Longhorn备份和灾难恢复"><a href="#Longhorn备份和灾难恢复" class="headerlink" title="Longhorn备份和灾难恢复"></a>Longhorn备份和灾难恢复</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>Longhorn做为分布式块存储，本身能支持针对存储卷的备份和数据卷的灾难恢复。用以下示例讲解Longhorn存储卷备份和灾难恢复。</p>
<h3 id="数据卷备份"><a href="#数据卷备份" class="headerlink" title="数据卷备份"></a>数据卷备份</h3><p>Longhorn支持存储卷的备份，将数据卷备份到AWS S3、NFSv4，采用快照方式增量备份到后端的backend存储上。以NFSv4做为后端backend为例配置后端存储。</p>
<p>节点安装NFS套件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum install nfs-utils</span><br></pre></td></tr></table></figure>

<p>创建共享目录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir /nfsdata</span><br></pre></td></tr></table></figure>
<p>配置exports</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim /etc/exports</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/nfsdata  *(rw,sync,no_root_squash)</span><br></pre></td></tr></table></figure>

<p>启动服务并设置开机启动  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl enable rpcbind</span><br><span class="line">systemctl enable nfs-server</span><br><span class="line">systemctl enable nfs-lock</span><br><span class="line">systemctl enable nfs-idmap</span><br><span class="line">systemctl start rpcbind</span><br><span class="line">systemctl start nfs-server</span><br><span class="line">systemctl start nfs-lock</span><br><span class="line">systemctl start nfs-idmap</span><br></pre></td></tr></table></figure>
<p>NFSV4挂载测试  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mount -t nfs host_ip:/nfsdata /mnt/</span><br></pre></td></tr></table></figure>

<p>在Longhorn界面上配置存储对接  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Longhorn-1.png"></p>
<p>创建备份  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Longhorn-2.png"></p>
<p>在对应的backup选项内可以看见创建出来的备份</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Longhorn-3.png"></p>
<p>对应nfs服务器查看数据，可以看见数据被分成了一个个块，以分块的方式存储。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@iZ2zegq8vbaetetmgkyq4pZ /]# tree /nfsdata/</span><br><span class="line">/nfsdata/</span><br><span class="line">└── backupstore</span><br><span class="line">    └── volumes</span><br><span class="line">        └── 5d</span><br><span class="line">            └── d6</span><br><span class="line">                └── pvc-0853fa8c-512a-11ea-98d5-00163e122063</span><br><span class="line">                    ├── backups</span><br><span class="line">                    │   └── backup_backup-b39c1feeeef94bf5.cfg</span><br><span class="line">                    ├── blocks</span><br><span class="line">                    │   ├── 2e</span><br><span class="line">                    │   │   └── 96</span><br><span class="line">                    │   │       └── 2e96c43b271147b95c36f12d09a284062e7ca746185a99f203d6e857cb62721a.blk</span><br><span class="line">                    │   ├── 4b</span><br><span class="line">                    │   │   └── f3</span><br><span class="line">                    │   │       └── 4bf324201d1805d2c15fe5f97158ac376c7e01ffb21eaca00094eb70ecf673b5.blk</span><br><span class="line">                    │   ├── 73</span><br><span class="line">                    │   │   └── 18</span><br><span class="line">                    │   │       └── 731859029215873fdac1c9f2f8bd25a334abf0f3a9e1b057cf2cacc2826d86b0.blk</span><br><span class="line">                    │   ├── a6</span><br><span class="line">                    │   │   └── b3</span><br><span class="line">                    │   │       └── a6b3b084f3a14e2557a4185d34fd07707ece662570298a02cf3fd745c6521725.blk</span><br><span class="line">                    │   ├── b2</span><br><span class="line">                    │   │   └── 2c</span><br><span class="line">                    │   │       └── b22c71a142d7c1f8b8ba287e27bf62752087cec2bc0c8b58db5a96047d138a73.blk</span><br><span class="line">                    │   ├── bd</span><br><span class="line">                    │   │   └── 90</span><br><span class="line">                    │   │       └── bd90eab7f660b8ae3c97a3a4cd4808d361518730282befc8e2113ae31b0a45ee.blk</span><br><span class="line">                    │   ├── d9</span><br><span class="line">                    │   │   └── 46</span><br><span class="line">                    │   │       └── d9462886143128037ae7c2e07bebd980c87ae4f1590c2dc4307962cc6d9f9f9f.blk</span><br><span class="line">                    │   └── ea</span><br><span class="line">                    │       └── 2c</span><br><span class="line">                    │           └── ea2c43cad3a82e7cc76695b0fa04bacced0c5bb4d041785eef85580d120fb383.blk</span><br><span class="line">                    └── volume.cfg</span><br><span class="line"></span><br><span class="line">23 directories, 10 files</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="数据卷还原"><a href="#数据卷还原" class="headerlink" title="数据卷还原"></a>数据卷还原</h3><p>数据卷还原步骤：<br>1、workload暂停使用,副本设置为0。<br>2、基于备份卷创建一个还原卷。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Longhorn-4.png"></p>
<p>3、基于还原卷可创建新的PV和PVC或覆盖原有pvc。</p>
<p><img src="https://pic.downk.cc/item/5ed20017c2a9a83be53ebac6.jpg"></p>
<h4 id="基于备份卷创建新的PVC"><a href="#基于备份卷创建新的PVC" class="headerlink" title="基于备份卷创建新的PVC"></a>基于备份卷创建新的PVC</h4><p>填写卷名</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Longhorn-13.png"></p>
<p>基于卷创建对应的PV和PVC  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Longhorn-14.png"></p>
<h4 id="若覆盖现用的PVC"><a href="#若覆盖现用的PVC" class="headerlink" title="若覆盖现用的PVC"></a>若覆盖现用的PVC</h4><p>1、覆盖原有pvc需要在Longhorn界面将原有volume删除或删除对应的PVC。</p>
<p>2、若覆盖现用的PVC,需要勾选 Use Previous Name，就创建与之前对应的卷名。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Longhorn-13.png"></p>
<p>2、基于volume创建对应PV&#x2F;PVC<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Longhorn-14.png"></p>
<p>3、将POD副本重新设置为正常,启动后查看卷内数据。  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Longhorn-7.png"></p>
<h3 id="数据卷的灾难恢复"><a href="#数据卷的灾难恢复" class="headerlink" title="数据卷的灾难恢复"></a>数据卷的灾难恢复</h3><p><img src="https://pic.downk.cc/item/5ed20429c2a9a83be5433ddd.jpg"><br>Longhorn数据卷的灾难恢复步骤：</p>
<p>1、A集群做为主集群对数据卷进行备份到远端backend。<br>2、B集群做为备集群指向与A集群一样的远端backend。</p>
<p>3、在B集群基于备份卷创建Create Disaster Recovery Volume。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Longhorn-8.png"></p>
<p>4、基于创建的Create Disaster Recovery Volume创建PVC到对应的namespace。</p>
<p>激活Disaster Recovery</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Longhorn-9.png">  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Longhorn-10.png"></p>
<p>5、创建POD挂载新建的PVC。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/Longhorn-11.png"></p>
]]></content>
      <categories>
        <category>分布式存储</category>
      </categories>
      <tags>
        <tag>分布式存储</tag>
      </tags>
  </entry>
  <entry>
    <title>Mariadb galera</title>
    <url>/2016/12/25/mariadb-base/</url>
    <content><![CDATA[<h2 id="传统的数据同步方式"><a href="#传统的数据同步方式" class="headerlink" title="传统的数据同步方式"></a>传统的数据同步方式</h2><p>传统的mysql数据库同步是使用二进制日志进行主从同步也就是semi-sync，这种同步也只是一种半同步，并不是完全的实时同步，在mysql 5.7.17中推出了MySQL Group Replication，这种实现方式与Galera cluster基本完全一样，不过MGR比Galera的优势在于  </p>
<ul>
<li>Mysql官方出品，能得到后续技术支持，Galera是芬兰的一家叫codership的公司开发的。</li>
<li>MGR使用Paxos协议，性能比Galera好，并且性能稳定，Galera实际只能使用三个节点，有性能和稳定性问题。  </li>
<li>Galera目前只支持linux，MGR支持多个平台。</li>
</ul>
<h3 id="mysql-5-6下semi-sync数据同步方式"><a href="#mysql-5-6下semi-sync数据同步方式" class="headerlink" title="mysql 5.6下semi-sync数据同步方式"></a>mysql 5.6下semi-sync数据同步方式</h3><p>master的dump进程需要发送binlog日志到slave，master要等待至少一个slave通知，slave将已经接收到传过来的events并写入relay log，salve发送ack信息到master，这个事务才能提交。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/mariadb-8.png"></p>
<p>5.6版semi-sync的缺陷，dump thread要承但两份任务，传送binlog给slave还要等slave的返回，并且这两个任务是串行的，也就是说，dump thread要先传送binlog给slave，并还要slave返回才能传送下一个events事务，这样的dump thread成为性能瓶径。</p>
<h3 id="mysql-5-7下semi-sync数据同步方式"><a href="#mysql-5-7下semi-sync数据同步方式" class="headerlink" title="mysql 5.7下semi-sync数据同步方式"></a>mysql 5.7下semi-sync数据同步方式</h3><p>5.7.4为解决上述的问题，在master端增加了ack进程。这样事务写发送binlog与接收ack可以并行进行，提高semi-sync的效率。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/mariadb-9.png"></p>
<h2 id="Galera"><a href="#Galera" class="headerlink" title="Galera"></a>Galera</h2><p>Galera cluster是可以实现mariadb多主集群的软件，它目前只能用于linux和只支持XtraDB和Innodb存储引擎 ，mariadb和perconna提供了原生的Galera cluster的支持，所以可以直接使用Galera cluster，mysql要使用Galera cluster需要使用Galera cluster提供的插件。传统主从只能有一个数据库进行写服务，Galera集群，每个节点都可读可写，在Galera上层部署负载均衡软件如lvs和haproxy进行流量分担是常用做法。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/mariadb-10.png"></p>
<p>原理<br>各个节点的数据同步由wsrep接口实现。<br>client发起一个commit命令时，所有本事务内对数据库的操作和primary_key都会打包写入到write-set，write-set随后会复制到其他节点，各个节点接收后，会根据write-set传送的primary key进行检验，检查是否与本地事务种read-write或write-write锁冲突，冲突则回滚，没有冲突就执行，完成后返回success。如果其他节点没有执行成功则存放在队列种，稍后会重新尝式。  </p>
<p>Galera集群的优点  </p>
<ul>
<li>支持多个节点的数据写入，能保证数据的强一致性。</li>
<li>同步复制，各个节点无延迟，不会因为一台宕机导致数据丢失。</li>
<li>故障节点将自动从集群中移除</li>
<li>基于行级的并行复制</li>
</ul>
<p>缺点  </p>
<ul>
<li>只支持Innodb存储引擎</li>
<li>只支持linux平台，</li>
<li>集群写入的tps由最弱节点限制，如果一个节点变慢，整的集群就是缓慢的，所以一般情况下部署，要求统一的硬件配置。</li>
<li>会因为网络抖动造成性能和稳定性问题。</li>
</ul>
<p>参考链接:<br><a href="http://www.oschina.net/news/79983/galera-will-die-mysql-group-replication-realease">http://www.oschina.net/news/79983/galera-will-die-mysql-group-replication-realease</a><br><a href="http://www.itbaofeng.com/?p=236">http://www.itbaofeng.com/?p=236</a></p>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>手动安装ceph</title>
    <url>/2017/01/01/manual_ceph/</url>
    <content><![CDATA[<h2 id="环镜"><a href="#环镜" class="headerlink" title="环镜"></a>环镜</h2><p>操作系统centos6.5<br>网络规划<br>Cluster_net：192.168.20.0&#x2F;24<br>Public_net：192.168.2.0&#x2F;24  </p>
<table>
<thead>
<tr>
<th>角色</th>
<th align="center">Public_net</th>
<th align="right">Cluster_net</th>
</tr>
</thead>
<tbody><tr>
<td>Ceph-mon</td>
<td align="center">192.168.2.4</td>
<td align="right">192.168.20.2</td>
</tr>
<tr>
<td>Ceph-mon2</td>
<td align="center">192.168.2.5</td>
<td align="right">192.168.20.5</td>
</tr>
<tr>
<td>Ceph-mon3</td>
<td align="center">192.168.2.6</td>
<td align="right">192.168.20.6</td>
</tr>
<tr>
<td>Ceph-osd1</td>
<td align="center">192.168.2.7</td>
<td align="right">192.168.20.7</td>
</tr>
<tr>
<td>Ceph-osd2</td>
<td align="center">192.168.2.8</td>
<td align="right">192.168.20.8</td>
</tr>
<tr>
<td>Ceph-osd3</td>
<td align="center">192.168.2.9</td>
<td align="right">192.168.20.9</td>
</tr>
</tbody></table>
<p>绑定hosts</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">192.168.2.4 ceph-mon.novalocal mon</span><br><span class="line">192.168.2.5 ceph-mon2.novalocal mon2</span><br><span class="line">192.168.2.6 ceph-mon3.novalocal mon3</span><br><span class="line">192.168.2.7 ceph-osd1.novalocal osd1</span><br><span class="line">192.168.2.8 ceph-osd2.novalocal osd2</span><br><span class="line">192.168.2.9 ceph-osd3.novalocal osd3</span><br></pre></td></tr></table></figure>


<p>osd数规划<br>每台osd节点挂载3块SSD硬盘  </p>
<h2 id="配置yum源"><a href="#配置yum源" class="headerlink" title="配置yum源"></a>配置yum源</h2><p>根据操作系统版本任意调整(所有节点)<br>vim &#x2F;etc&#x2F;yum.repos.d&#x2F;ceph.repo  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[ceph]</span><br><span class="line">name=ceph</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-hammer/el6/x86_64/</span><br><span class="line">gpgcheck=0</span><br><span class="line">enablde=1</span><br><span class="line">[ceph-noarch]</span><br><span class="line">name=cephnoarch</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-hammer/el6/x86_64/</span><br><span class="line">gpgcheck=0</span><br><span class="line">enablde=1</span><br></pre></td></tr></table></figure>
<p>yum clean<br>yum makecache  </p>
<p>在第一台mon节点安装pssh批量执行操作<br>yum install pssh<br>创建hosts（会被批量执行的主机）<br>vim &#x2F;root&#x2F;hosts.txt  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">192.168.2.5</span><br><span class="line">192.168.2.6</span><br><span class="line">192.168.2.7</span><br><span class="line">192.168.2.8</span><br><span class="line">192.168.2.9</span><br></pre></td></tr></table></figure>
<p>先进行测试<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/maual-ceph-1.png">  </p>
<p>安装软件（所有节点）<br>pssh  -P -h hosts.txt  yum install ceph –y<br>同步hosts<br>pscp.pssh -h &#x2F;root&#x2F;hosts.txt  &#x2F;etc&#x2F;hosts &#x2F;etc&#x2F;  </p>
<p>都success进行下一步<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/maual-ceph-2.png">  </p>
<h2 id="配置ceph-mon"><a href="#配置ceph-mon" class="headerlink" title="配置ceph-mon"></a>配置ceph-mon</h2><p>创建集群fsid<br>uuidgen<br>fdc3d06b-7e05-44a8-b982-e8e04e4156db<br>创建&#x2F;etc&#x2F;ceph&#x2F;ceph.conf将fsid写入配置文件<br>[global]<br>fsid &#x3D; fdc3d06b-7e05-44a8-b982-e8e04e4156db  </p>
<p>将ceph-mon写入配置文件(有多个mon，用逗号隔开)<br>mon_initial_members &#x3D; mon,mon2,mon3<br>将mon节点ip写入ceph配置文件<br>mon_host &#x3D; 192.168.2.4,192.168.2.5,192.168.2.6<br>为集群创建mon密钥<br><code>ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon &#39;allow *&#39;</code></p>
<p>生成管理员密钥<br><code>ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --set-uid=0 --cap mon &#39;allow *&#39; --cap osd &#39;allow *&#39; --cap mds &#39;allow&#39;</code></p>
<p>将client.admin密钥加入到ceph.mon.keyring<br><code>ceph-authtool/tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring</code></p>
<p>生成mon map<br><code>monmaptool --create --add mon 192.168.2.4 --fsid fdc3d06b-7e05-44a8-b982-e8e04e4156db /tmp/monmap</code><br>在每个mon节点数据目录<br>分别在mon、mon2、mon3上执行<br>格式为(默认cluster-name为ceph)<br><code>mkdir /var/lib/ceph/mon/&#123;cluster-name&#125;-&#123;hostname&#125;</code><br>如mon为<br><code>mkdir /var/lib/ceph/mon/ceph-mon</code><br>mon初始化(-i后接hostname)<br><code>ceph-mon --mkfs -i mon --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring</code>  </p>
<p>ceph.conf增加如下配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public network = 192.168.2.0/24</span><br><span class="line">auth cluster required = cephx</span><br><span class="line">auth service required = cephx</span><br><span class="line">auth client required = cephx</span><br></pre></td></tr></table></figure>
<p>创建两个空文件<br><code>touch /var/lib/ceph/mon/ceph-mon/done</code><br><code>touch /var/lib/ceph/mon/ceph-mon/sysvinit</code><br>启动第一个ceph-mon<br><code>/etc/init.d/ceph status mon.mon  </code><br>部署第二个mon<br>将keyring复制到mon2<br>scp  &#x2F;tmp&#x2F;ceph.mon.keyring  mon2:&#x2F;tmp&#x2F;<br>在mon2节点上建立一个&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-mon2目录<br>mkdir –p &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-mon2&#x2F;<br>在mon2节点上初始化mon节点<br><code>ceph-mon --mkfs -i mon2  --keyring /tmp/ceph.mon.keyring</code><br>为了防止重新被安装，初始化一个空的done文件<br><code>touch /var/lib/ceph/mon/ceph-mon2/done</code><br><code>touch /var/lib/ceph/mon/ceph-mon2/sysvinit</code><br>&#x2F;etc&#x2F;init.d&#x2F;ceph start mon.mon2<br>检查进程<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/maual-ceph-3.png">  </p>
<p>第三个mon同上<br>完后检查  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/maual-ceph-4.png"><br>发现有时种偏移问题，默认ceph是0.05s，为了方便同步直接把时钟偏移设置成0.5s<br>修改ceph配置文件增加两条配置  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[global]</span><br><span class="line">mon osd down out interval = 900 #设置osd节点down后900s，把此osd节点逐出ceph集群，把之前映射到此节点的数据映射到其他节点。  </span><br><span class="line">[mon]  </span><br><span class="line">mon clock drift allowed = .50  </span><br></pre></td></tr></table></figure>
<p>同步配置  </p>
<p>pscp.pssh -h hosts.txt  &#x2F;etc&#x2F;ceph&#x2F;ceph.conf  &#x2F;etc&#x2F;ceph&#x2F;<br>重启进程<br>pssh -h &#x2F;root&#x2F;hosts.txt  &#x2F;etc&#x2F;init.d&#x2F;ceph restart  </p>
<h2 id="配置osd节点"><a href="#配置osd节点" class="headerlink" title="配置osd节点"></a>配置osd节点</h2><p>将keyring同步到osd节点<br>pscp.pssh -h &#x2F;root&#x2F;hosts.txt  &#x2F;etc&#x2F;ceph&#x2F;ceph.client.admin.keyring  &#x2F;etc&#x2F;ceph&#x2F;  </p>
<p>为osd分配uuid(我每台osd节点有3个osd所以创建3个uuid)<br>uuidgen<br>19ebc47d-9b29-4cf3-9720-b62896ce6f33<br>uuidgen<br>1721ce0b-7f65-43ef-9dfc-c49e6210d375<br>uuidgen<br>f5e0f54b-2ee3-41df-bf25-ad37371ab6ce  </p>
<p>创建3个osd</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ceph osd create 19ebc47d-9b29-4cf3-9720-b62896ce6f33</span><br><span class="line">ceph osd create 1721ce0b-7f65-43ef-9dfc-c49e6210d375</span><br><span class="line">ceph osd create f5e0f54b-2ee3-41df-bf25-ad37371ab6ce</span><br></pre></td></tr></table></figure>
<p>创建数据存储目录  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir /var/lib/ceph/osd/&#123;cluster-name&#125;-&#123;osd-number&#125;</span><br><span class="line">mkdir /var/lib/ceph/osd/ceph-0</span><br><span class="line">mkdir /var/lib/ceph/osd/ceph-1</span><br><span class="line">mkdir /var/lib/ceph/osd/ceph-2</span><br><span class="line">mkfs.xfs /dev/vdb</span><br><span class="line">mkfs.xfs /dev/vdc</span><br><span class="line">mkfs.xfs /dev/vdd</span><br></pre></td></tr></table></figure>

<p>挂载</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mount -o defaults,_netdev /dev/vdb /var/lib/ceph/osd/ceph-0</span><br><span class="line">mount -o defaults,_netdev /dev/vdc /var/lib/ceph/osd/ceph-1</span><br><span class="line">mount -o defaults,_netdev /dev/vdd /var/lib/ceph/osd/ceph-2</span><br></pre></td></tr></table></figure>
<p>修改fstable<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/maual-ceph-5.png">  </p>
<p>初始化osd目录  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ceph-osd -i 0 --mkfs --mkkey --osd-uuid 19ebc47d-9b29-4cf3-9720-b62896ce6f33</span><br><span class="line">ceph-osd -i 1 --mkfs --mkkey --osd-uuid 1721ce0b-7f65-43ef-9dfc-c49e6210d375</span><br><span class="line">ceph-osd -i 2 --mkfs --mkkey --osd-uuid f5e0f54b-2ee3-41df-bf25-ad37371ab6ce</span><br></pre></td></tr></table></figure>
<p>注册此osd密钥  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ceph auth add osd.0 osd &#x27;allow *&#x27; mon &#x27;allow profile osd&#x27; -i /var/lib/ceph/osd/ceph-0/keyring</span><br><span class="line">ceph auth add osd.1 osd &#x27;allow *&#x27; mon &#x27;allow profile osd&#x27; -i /var/lib/ceph/osd/ceph-1/keyring</span><br><span class="line">ceph auth add osd.2 osd &#x27;allow *&#x27; mon &#x27;allow profile osd&#x27; -i /var/lib/ceph/osd/ceph-2/keyring</span><br></pre></td></tr></table></figure>

<p>加入crush map  </p>
<p>ceph osd crush add-bucket ceph-osd1 host<br>ceph osd crush move ceph-osd1 root&#x3D;default  </p>
<p>设置权重  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ceph osd crush add osd.0 1.0 host=ceph-osd1  </span><br><span class="line">ceph osd crush add osd.1 1.0 host=ceph-osd1</span><br><span class="line">ceph osd crush add osd.2 1.0 host=ceph-osd1</span><br></pre></td></tr></table></figure>
<p>要已守护进程开机启动，须创建一个空文件  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">touch /var/lib/ceph/osd/ceph-0/sysvinit</span><br><span class="line">touch /var/lib/ceph/osd/ceph-1/sysvinit</span><br><span class="line">touch /var/lib/ceph/osd/ceph-2/sysvinit</span><br></pre></td></tr></table></figure>

<p>启动osd进程</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/etc/init.d/ceph start osd.0</span><br><span class="line">/etc/init.d/ceph start osd.1</span><br><span class="line">/etc/init.d/ceph start osd.2</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/maual-ceph-6.png"><br>查看osd 树<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/maual-ceph-7.png"><br>按上述方法配置osd2、osd3<br>3台节点添加完毕<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/maual-ceph-8.png">  </p>
<p>这里有个warn，是pool的pg问题，我们重新计算，修改。<br>查看我们现在拥有的pool<br>ceph osd lspools<br>0 rbd,  </p>
<p>查看默认的rbd pool的pg  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ceph osd pool get rbd pg_num</span><br><span class="line">pg_num: 64</span><br><span class="line">ceph osd pool get rbd pg_num</span><br><span class="line">pg_num: 64</span><br><span class="line">修改为128</span><br><span class="line">ceph osd pool set rbd pg_num 128</span><br><span class="line">ceph osd pool set rbd pgp_num 128</span><br></pre></td></tr></table></figure>
<p>再查看<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/maual-ceph-9.png">  </p>
<p>修改ceph.conf默认数据存两份，默认pg为128<br>同步配置<br>重启ceph</p>
<h2 id="配置ceph-radosgw"><a href="#配置ceph-radosgw" class="headerlink" title="配置ceph-radosgw"></a>配置ceph-radosgw</h2><p>直接将第一个mon节点当radosgw，ceph在0.80及以上版本可以直接使用civeweb来构建对象网关，可以不需要使用apache或nginx+fastcgi了，所以我这里用civetweb。</p>
<p>安装软件<br>yum install ceph-radosgw<br>没有www-data用户创建<br>useradd –r –s &#x2F;sbin&#x2F;nologin www-data<br>创建gateway keyring并授权  </p>
<p><code>ceph auth get-or-create client.radosgw.gateway osd &#39;allow rwx&#39; mon &#39;allow rwx&#39; -o /etc/ceph/keyring.radosgw.gateway</code></p>
<p>编辑ceph.conf文件<br>增加下面内容  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[client.radosgw.gateway]</span><br><span class="line">keyring = /etc/ceph/keyring.radosgw.gateway</span><br><span class="line">rgw_socket_path = /tmp/radosgw.sock</span><br><span class="line">rgw_frontends= &quot;civetweb port=7480&quot;</span><br><span class="line">host = ceph-mon</span><br><span class="line">rgw_dns_name = *.domain.tld</span><br><span class="line">rgw_print_continue = True</span><br><span class="line">rgw_data = /var/lib/ceph/radosgw</span><br><span class="line">user = www-data</span><br><span class="line">rgw s3 auth use keystone = true</span><br><span class="line">log file =/var/log/radosgw/client.radosgw.gateway.log</span><br></pre></td></tr></table></figure>
<p>重启进程<br>&#x2F;etc&#x2F;init.d&#x2F;ceph-radosgw restart<br>查看是否启动成功<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/maual-ceph-10.png">  </p>
<p>新建用户<br>radosgw-admin user create –uid&#x3D;”testuser” –display-name&#x3D;”First User”<br>得到如下结果<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/maual-ceph-11.png">  </p>
]]></content>
      <categories>
        <category>分布式存储</category>
      </categories>
      <tags>
        <tag>分布式存储</tag>
      </tags>
  </entry>
  <entry>
    <title>Mariadb galera集群搭建</title>
    <url>/2016/12/22/mariadb-galera-2/</url>
    <content><![CDATA[<h1 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h1><p>准备3台服务器  </p>
<p>192.168.1.16 mariadb-1.novalocal mariadb-1<br>192.168.1.19 mariadb-2.novalocal mariadb-2<br>192.168.1.18 mariadb-3.novalocal mariadb-3  </p>
<p>配置repo文件</p>
<p>vim &#x2F;etc&#x2F;yum.repos.d&#x2F;MariaDB.repo  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[mariadb]</span><br><span class="line">name = MariaDB</span><br><span class="line">baseurl = http://yum.mariadb.org/10.1/centos7-amd64</span><br><span class="line">gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDB</span><br><span class="line">gpgcheck=1</span><br></pre></td></tr></table></figure>
<p>yum makecache</p>
<p>禁用防火墙和selinux  </p>
<p>如果要使用防火墙添加允许3306、和4567 端口规则。</p>
<p>安装Mariadb和galera cluster（三个节点都执行）  </p>
<p>yum install MariaDB-server MariaDB-client galera</p>
<p>启动mariadb</p>
<p>systemctl start mariadb  </p>
<p>一些初始化安全配置</p>
<p>&#x2F;usr&#x2F;bin&#x2F;mysql_secure_installation</p>
<p>关闭数据库<br>systemctl stop mariadb  </p>
<p>修改mariadb-1上的&#x2F;etc&#x2F;my.cnf.d&#x2F;server.cnf文件如下  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[galera]</span><br><span class="line">wsrep_provider = /usr/lib64/galera/libgalera_smm.so</span><br><span class="line">wsrep_cluster_address = &quot;gcomm://192.168.1.16,192.168.1.18,192.168.1.19&quot;</span><br><span class="line">wsrep_node_name = mariadb-1</span><br><span class="line">wsrep_node_address=192.168.1.16</span><br><span class="line">wsrep_on=ON</span><br><span class="line">binlog_format=ROW</span><br><span class="line">default_storage_engine=InnoDB</span><br><span class="line">innodb_autoinc_lock_mode=2</span><br><span class="line">#bind-address=0.0.0.0</span><br><span class="line">wsrep_slave_threads=1</span><br><span class="line">innodb_flush_log_at_trx_commit=0</span><br><span class="line">innodb_buffer_pool_size=122M</span><br><span class="line">wsrep_sst_method=rsync</span><br></pre></td></tr></table></figure>

<p>将此文件复制到mariadb-2、mariadb-3，注意要把 wsrep_node_name 和 wsrep_node_address 改成相应节点的 hostname 和 ip。</p>
<p>启动 MariaDB Galera Cluster 服务  </p>
<p>&#x2F;usr&#x2F;sbin&#x2F;mysqld –wsrep-new-cluster –user&#x3D;root &amp;<br>–wsrep-new-cluster 这个参数只能在初始化集群使用，且只能在一个节点使用。  </p>
<p>观察日志：</p>
<p>[root@node4 ~]# tail -f &#x2F;var&#x2F;log&#x2F;message</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">150701 19:54:17 [Note] WSREP: wsrep_load(): loading provider library &#x27;none&#x27;</span><br><span class="line">150701 19:54:17 [Note] /usr/libexec/mysqld: ready for connections.</span><br><span class="line">Version: &#x27;5.5.40-MariaDB-wsrep&#x27;  socket: &#x27;/var/lib/mysql/mysql.sock&#x27;  port: 3306  MariaDB Server, wsrep_25.11.r4026</span><br></pre></td></tr></table></figure>

<p>出现 ready for connections ,证明我们启动成功  </p>
<p>查看是否启用galera插件<br>连接mariadb,查看是否启用galera插件</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/mariadb-1.png"></p>
<p>目前集群机器数<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/mariadb-2.png"></p>
<p>查看集群状态<br>show status like ‘wsrep%’;<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/mariadb-3.png"></p>
<p>查看连接的主机  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/mariadb-4.png"></p>
<p>另外两个节点mariadb会自动加入集群<br>systemctl start mariadb<br>这时查看galera集群机器数量<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/mariadb-5.png"></p>
<p>已经连接机器的ip<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/mariadb-6.png"></p>
<p>测试<br>在mariadb-1上创建数据库，创建表，插入数据  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; create database test1；</span><br><span class="line">MariaDB [test1]&gt; insert into test values(1);</span><br><span class="line">Query OK, 1 row affected (0.01 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test1]&gt; insert into test values(2);</span><br><span class="line">Query OK, 1 row affected (0.01 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test1]&gt; insert into test values(3);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test1]&gt; insert into test values(4);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test1]&gt; insert into test values(5);</span><br><span class="line">Query OK, 1 row affected (0.03 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test1]&gt; insert into test values(6);</span><br><span class="line">Query OK, 1 row affected (0.01 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test1]&gt; insert into test values(7);</span><br><span class="line">Query OK, 1 row affected (0.01 sec)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在另外两台mariadb-2、mariadb-3上可以看见刚刚插入的数据，说明数据同步了。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/mariadb-7.png"></p>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>使用MNIST数据集训练数字识别</title>
    <url>/2024/06/09/mnist_train/</url>
    <content><![CDATA[<p>环境情况<br>OS：ubuntu-22.04<br>Kernel：5.15.0-101-generic<br>GPU：NVIDIA-T4<br>Python版本：3.10.12<br>Docker：24.0.5</p>
<p>使用MNIST数据集训练手写数字识别<br>下载数据集，使用以下脚本</p>
<h3 id="环境初始化配置"><a href="#环境初始化配置" class="headerlink" title="环境初始化配置"></a>环境初始化配置</h3><p>先安装torch和torchvision</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install torch torchvision</span><br></pre></td></tr></table></figure>

<p>安装cuda和GPU驱动，直接按照官网手册进行，这里安装cuda-12.1，默认会自动安装对应的GPU驱动<br><a href="https://developer.nvidia.com/cuda-12-1-0-download-archive">https://developer.nvidia.com/cuda-12-1-0-download-archive</a><br>也可以用cuda12.4。同样按此目录下载即可<br>安装完成后能执行nvidia-smi看见gpu即可</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nvidia-smi </span><br><span class="line">Sun May 26 13:53:22 2024       </span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |</span><br><span class="line">|-----------------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                                         |                      |               MIG M. |</span><br><span class="line">|=========================================+======================+======================|</span><br><span class="line">|   0  Tesla T4                        On | 00000000:00:08.0 Off |                    0 |</span><br><span class="line">| N/A   29C    P8               11W /  70W|      2MiB / 15360MiB |      0%      Default |</span><br><span class="line">|                                         |                      |                  N/A |</span><br><span class="line">+-----------------------------------------+----------------------+----------------------+</span><br><span class="line">                                                                                         </span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                            |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |</span><br><span class="line">|        ID   ID                                                             Usage      |</span><br><span class="line">|=======================================================================================|</span><br><span class="line">|  No running processes found                                                           |</span><br><span class="line">+---------------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>

<p>安装docker-ce</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">参考：https://docs.docker.com/engine/install/ubuntu/</span><br><span class="line"></span><br><span class="line">安装后版本为docker-ce:v24.0.5</span><br></pre></td></tr></table></figure>

<p>为了能够让容器内使用GPU安装nvidia-container-toolkit</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | apt-key add -</span><br><span class="line"></span><br><span class="line">curl -s -L https://nvidia.github.io/nvidia-docker/ubuntu22.04/nvidia-docker.list &gt; /etc/apt/sources.list.d/nvidia-docker.list</span><br><span class="line"></span><br><span class="line">apt update</span><br><span class="line"></span><br><span class="line">apt -y install nvidia-container-toolkit</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>


<p>验证<br>执行docker命令启动nvidia&#x2F;cuda:12.1.0-base-ubuntu20.04容器通过–gpus命令将宿主机gpu透传进去，执行nvidia-smi命令查看能否看见gpu</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --gpus all nvidia/cuda:12.1.0-base-ubuntu20.04 nvidia-smi</span><br><span class="line"></span><br><span class="line">Sun May 26 06:03:56 2024       </span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |</span><br><span class="line">|-----------------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                                         |                      |               MIG M. |</span><br><span class="line">|=========================================+======================+======================|</span><br><span class="line">|   0  Tesla T4                        On | 00000000:00:08.0 Off |                    0 |</span><br><span class="line">| N/A   29C    P8               11W /  70W|      2MiB / 15360MiB |      0%      Default |</span><br><span class="line">|                                         |                      |                  N/A |</span><br><span class="line">+-----------------------------------------+----------------------+----------------------+</span><br><span class="line">                                                                                         </span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                            |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |</span><br><span class="line">|        ID   ID                                                             Usage      |</span><br><span class="line">|=======================================================================================|</span><br><span class="line">|  No running processes found                                                           |</span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h3 id="下载MNIST训练数据"><a href="#下载MNIST训练数据" class="headerlink" title="下载MNIST训练数据"></a>下载MNIST训练数据</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import os</span><br><span class="line">from torchvision import datasets</span><br><span class="line"></span><br><span class="line">rootdir = &quot;/home/mnist-data/&quot;</span><br><span class="line">traindir = rootdir + &quot;/train&quot;</span><br><span class="line">testdir = rootdir + &quot;/test&quot;</span><br><span class="line"></span><br><span class="line">train_dataset = datasets.MNIST(root=rootdir, train=True, download=True)</span><br><span class="line">test_dataset = datasets.MNIST(root=rootdir, train=False, download=True)</span><br><span class="line"></span><br><span class="line">number = 0</span><br><span class="line">for img, label in train_dataset:</span><br><span class="line">    savedir = traindir + &quot;/&quot; + str(label)</span><br><span class="line">    os.makedirs(savedir, exist_ok=True)</span><br><span class="line">    savepath = savedir + &quot;/&quot; + str(number).zfill(5) + &quot;.png&quot;</span><br><span class="line">    img.save(savepath)</span><br><span class="line">    number = number + 1</span><br><span class="line">    print(savepath)</span><br><span class="line"></span><br><span class="line">number = 0</span><br><span class="line">for img, label in test_dataset:</span><br><span class="line">    savedir = testdir + &quot;/&quot; + str(label)</span><br><span class="line">    os.makedirs(savedir, exist_ok=True)</span><br><span class="line">    savepath = savedir + &quot;/&quot; + str(number).zfill(5) + &quot;.png&quot;</span><br><span class="line">    img.save(savepath)</span><br><span class="line">    number = number + 1</span><br><span class="line">    print(savepath)</span><br></pre></td></tr></table></figure>


<p>保存为文件，执行下载。</p>
<p>下载后的目录会包含3个文件夹</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ls /home/image/</span><br><span class="line">MNIST  test  train</span><br></pre></td></tr></table></figure>
<p>MNIST文件夹:存放MNIST训练和测试数据集，包括</p>
<ul>
<li><p>t10k-images-idx3-ubyte：包含训练集的图像数据。</p>
</li>
<li><p>train-labels-idx1-ubyte：包含训练集标签数据。</p>
</li>
<li><p>t10k-images-idx3-ubyte.gz：测试图像数据集。</p>
</li>
<li><p>t10k-labels-idx1-ubyte：测试集标签数据。</p>
</li>
</ul>
<p>train文件夹：训练集图像,这个文件夹包含训练数据集，通常包括60,000张28x28像素的手写数字图像以及相应的标签。这些图像用于训练机器学习模型。</p>
<p>test文件夹： 这个文件夹包含测试数据集，通常包括10,000张28x28像素的手写数字图像以及相应的标签。这些图像用于评估训练好的模型的性能。</p>
<p>特点：</p>
<ul>
<li>标签：每张图片都有一个对应的标签，表示该图片上的数字是多少（0到9）。</li>
<li>标准化：所有图片都被标准化到28x28像素，并且中心对齐，保证数字位于图像的中心位置。</li>
</ul>
<p>配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --gpus all -itd --rm -v /home/mnist-data:/workspace/data nvcr.io/nvidia/pytorch:24.05-py3</span><br></pre></td></tr></table></figure>
<h3 id="在容器中进行训练"><a href="#在容器中进行训练" class="headerlink" title="在容器中进行训练"></a>在容器中进行训练</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.optim as optim</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">from torchvision import datasets, transforms</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line"></span><br><span class="line"># 定义网络架构</span><br><span class="line">class Net(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(1, 32, 3, 1)</span><br><span class="line">        self.conv2 = nn.Conv2d(32, 64, 3, 1)</span><br><span class="line">        self.dropout1 = nn.Dropout2d(0.25)</span><br><span class="line">        self.dropout2 = nn.Dropout2d(0.5)</span><br><span class="line">        self.fc1 = nn.Linear(9216, 128)</span><br><span class="line">        self.fc2 = nn.Linear(128, 10)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = F.max_pool2d(x, 2)</span><br><span class="line">        x = self.dropout1(x)</span><br><span class="line">        x = torch.flatten(x, 1)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.dropout2(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        output = F.log_softmax(x, dim=1)</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"># 定义数据预处理</span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((0.1307,), (0.3081,))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"># 加载训练集和测试集</span><br><span class="line">train_dataset = datasets.MNIST(root=&#x27;/workspace/data&#x27;, train=True, download=False, transform=transform)</span><br><span class="line">test_dataset = datasets.MNIST(root=&#x27;/workspace/data&#x27;, train=False, download=False, transform=transform)</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)</span><br><span class="line"></span><br><span class="line"># 检查是否有GPU可用，并选择设备</span><br><span class="line">device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br><span class="line">model = Net().to(device)</span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line"></span><br><span class="line"># 训练模型</span><br><span class="line">def train(model, device, train_loader, optimizer, epoch):</span><br><span class="line">    model.train()</span><br><span class="line">    for batch_idx, (data, target) in enumerate(train_loader):</span><br><span class="line">        data, target = data.to(device), target.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(data)</span><br><span class="line">        loss = F.nll_loss(output, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        if batch_idx % 100 == 0:</span><br><span class="line">            print(f&#x27;Train Epoch: &#123;epoch&#125; [&#123;batch_idx * len(data)&#125;/&#123;len(train_loader.dataset)&#125; &#x27;</span><br><span class="line">                  f&#x27;(&#123;100. * batch_idx / len(train_loader):.0f&#125;%)]\tLoss: &#123;loss.item():.6f&#125;&#x27;)</span><br><span class="line"></span><br><span class="line"># 测试模型</span><br><span class="line">def test(model, device, test_loader):</span><br><span class="line">    model.eval()</span><br><span class="line">    test_loss = 0</span><br><span class="line">    correct = 0</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        for data, target in test_loader:</span><br><span class="line">            data, target = data.to(device), target.to(device)</span><br><span class="line">            output = model(data)</span><br><span class="line">            test_loss += F.nll_loss(output, target, reduction=&#x27;sum&#x27;).item()</span><br><span class="line">            pred = output.argmax(dim=1, keepdim=True)</span><br><span class="line">            correct += pred.eq(target.view_as(pred)).sum().item()</span><br><span class="line"></span><br><span class="line">    test_loss /= len(test_loader.dataset)</span><br><span class="line">    print(f&#x27;\nTest set: Average loss: &#123;test_loss:.4f&#125;, Accuracy: &#123;correct&#125;/&#123;len(test_loader.dataset)&#125; &#x27;</span><br><span class="line">          f&#x27;(&#123;100. * correct / len(test_loader.dataset):.0f&#125;%)\n&#x27;)</span><br><span class="line"></span><br><span class="line"># 运行训练和测试，并保存模型</span><br><span class="line">for epoch in range(1, 11):</span><br><span class="line">    train(model, device, train_loader, optimizer, epoch)</span><br><span class="line">    test(model, device, test_loader)</span><br><span class="line"></span><br><span class="line"># 保存模型</span><br><span class="line">torch.save(model.state_dict(), &quot;/workspace/mnist_cnn.pt&quot;)</span><br><span class="line">print(&quot;Model saved to /workspace/mnist_cnn.pt&quot;)</span><br></pre></td></tr></table></figure>

<p>保存为mnist_train.py文件，执行<code>python mnist_train.py</code><br>会加载我们下载映射到容器内的MNIST数据集，进行训练，训练后的文件mnist_cnn.pt会存储到workspace目录</p>
<h3 id="加载模型进行测试验证"><a href="#加载模型进行测试验证" class="headerlink" title="加载模型进行测试验证"></a>加载模型进行测试验证</h3><p>保存为test.py文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">from torchvision import transforms</span><br><span class="line">from PIL import Image</span><br><span class="line">import argparse</span><br><span class="line"></span><br><span class="line"># 定义相同的网络架构</span><br><span class="line">class Net(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(1, 32, 3, 1)</span><br><span class="line">        self.conv2 = nn.Conv2d(32, 64, 3, 1)</span><br><span class="line">        self.dropout1 = nn.Dropout2d(0.25)</span><br><span class="line">        self.dropout2 = nn.Dropout2d(0.5)</span><br><span class="line">        self.fc1 = nn.Linear(9216, 128)</span><br><span class="line">        self.fc2 = nn.Linear(128, 10)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = F.max_pool2d(x, 2)</span><br><span class="line">        x = self.dropout1(x)</span><br><span class="line">        x = torch.flatten(x, 1)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.dropout2(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        output = F.log_softmax(x, dim=1)</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"># 检查是否有GPU可用，并选择设备</span><br><span class="line">device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br><span class="line"></span><br><span class="line"># 加载模型</span><br><span class="line">model = Net().to(device)</span><br><span class="line">model.load_state_dict(torch.load(&quot;/workspace/mnist_cnn.pt&quot;))</span><br><span class="line">model.eval()</span><br><span class="line"></span><br><span class="line"># 定义数据预处理</span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Grayscale(num_output_channels=1),</span><br><span class="line">    transforms.Resize((28, 28)),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((0.1307,), (0.3081,))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">def predict_image(image_path):</span><br><span class="line">    image = Image.open(image_path)</span><br><span class="line">    image = transform(image).unsqueeze(0).to(device)</span><br><span class="line">    </span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        output = model(image)</span><br><span class="line">        pred = output.argmax(dim=1, keepdim=True)</span><br><span class="line">    </span><br><span class="line">    return pred.item()</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    parser = argparse.ArgumentParser(description=&#x27;MNIST Image Prediction&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;image_path&#x27;, type=str, help=&#x27;Path to the image to be predicted&#x27;)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    # 预测图片</span><br><span class="line">    prediction = predict_image(args.image_path)</span><br><span class="line">    print(f&#x27;The predicted digit is: &#123;prediction&#125;&#x27;)</span><br></pre></td></tr></table></figure>



<p>执行验证，指定图片路径</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python test.py data/test/8/00527.png </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">结果如下：</span><br><span class="line">The predicted digit is: 8</span><br><span class="line"></span><br><span class="line">python test.py data/test/1/00239.png </span><br><span class="line"></span><br><span class="line">The predicted digit is: 1</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>可以用test目录下数据进行快速验证。</p>
<p>也可以使用DIGITS进行图形化加载验证。</p>
<p><a href="https://licensecounter.jp/engineer-voice/blog/articles/20240408_ngc_nvidia_gpu_cloud.html">https://licensecounter.jp/engineer-voice/blog/articles/20240408_ngc_nvidia_gpu_cloud.html</a></p>
]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>从零开始构建云平台监控(写在前面)</title>
    <url>/2017/10/03/monitor/</url>
    <content><![CDATA[<h3 id="监控的重要性"><a href="#监控的重要性" class="headerlink" title="监控的重要性"></a>监控的重要性</h3><p>在整个it系统中，监控是最为重要的一个环节，监控是发现集群问题的关键，没有监控的话，只能等到出了问题才能发现集群的问题，那时为时以晚。</p>
<h3 id="监控的范围"><a href="#监控的范围" class="headerlink" title="监控的范围"></a>监控的范围</h3><p>在一套完整的openstack私有云中，组件非常复杂因为它不仅仅有openstack的东西还有分布式存储ceph的东西。<br>计算组件：nova-api、nova-scheduler、nova-condutor、nova-novncproxy、nova-compute……<br>网络组件：neutron-server、neutron-dhcp-agent、neutron-metadata-agent、neutron-openvswitch-agent、openvswitch……<br>存储组件：cinder-api、cinder-scheduler 、cinder-volume<br> 其他：等还有认证组件keystone、消息队列、镜像组件、还有底层分布式存储ceph、一个这么庞大的系统可想而知如果没有一个比较完善的监控很多并且在不同的角色上所监控的组件也是不一样的，比如控制节点没有nova-compute服务，计算节点没有控制节点服务，所以监控项还得针对不同的角色做区分。</p>
<p>除了组件上的监控外，其实少不了还有硬件上的监控，比如说，在私有云生产环境中，网卡都是会做bond的，做了bond的好处是当其中一个网卡出现故障时并并不是影响到整个集群，但你也得通过监控发现这个挂了的网卡，然后准备更换，ceph的osd down了，有可能是osd进程被kill掉了，也有可能是硬盘坏了导致osd进程挂了，这也都通过监控去发现。</p>
<h3 id="组件构成"><a href="#组件构成" class="headerlink" title="组件构成"></a>组件构成</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_0_1.jpg"></p>
<p>监控节点：nginx、php、mysql、influxdb、grafana、zabbix-server、zabbix-agent<br>控制节点：telegraf、zabbix-agent<br>融合节点：zabbix-agent  </p>
<h3 id="组件作用"><a href="#组件作用" class="headerlink" title="组件作用"></a>组件作用</h3><p>nginx+php+mysql提供基础的lnmp平台让zabbix可以运行，mysql存储zabbix监控数据<br>zabbix-server：用于接收各个机器agent端发过来的数据<br>zabbix-agent：采集数据发给zabbix-server<br>telegraf：用于采集ceph的监控数据<br>influxdb：存储telegraf的数据<br>grafana：用于对采集数据图形化展示  </p>
<p>其中需要注意的是因为教程所以监控节点组件都放一起了，并且还是单节点，在生产环境中监控节点要做高可用，根据监控的机器数决定是否将数据库能独立出来。</p>
<p>以下为最终效果。</p>
<h3 id="效果展示"><a href="#效果展示" class="headerlink" title="效果展示"></a>效果展示</h3><p>ceph监控界面<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_0_2.png"></p>
<p>openstack节点性能状态  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_0_3.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_0_4.png"></p>
<p>控制节点服务状态展示<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_0_5.png"></p>
<p>计算节点服务状态展示</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_0_6.png"></p>
<p>存储节点osd监控</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_0_7.png"></p>
<p>zabbix-server监控</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_0_8.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_0_9.png"></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>从零开始构建云平台监控(一)搭建LNMP</title>
    <url>/2017/10/04/monitor_1/</url>
    <content><![CDATA[<h3 id="搭建LNMP"><a href="#搭建LNMP" class="headerlink" title="搭建LNMP"></a>搭建LNMP</h3><p>先安装mysql—-&gt;安装php—–&gt;安装nginx<br>版本:<br>mysql 5.7.19<br>php 7.1.7<br>nginx 1.12.1  </p>
<p>本次搭建除mysql外全部采用源码方式安装，因为采用源码的方式安装可以更好的对功能模块是否需要做定制。</p>
<p>安装依赖</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum -y install gcc gcc-devel gcc-c++ gcc-c++-devel libaio-devel boost boost-devel autoconf* automake* zlib* libxml* ncurses-devel ncurses libgcrypt* libtool* cmake openssl openssl-devel bison bison-devel unzip numactl-devel</span><br></pre></td></tr></table></figure>

<p>创建mysql软件源<br>vim &#x2F;etc&#x2F;yum.repo.d&#x2F;mysql.repo  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[mysql]</span><br><span class="line">name = mysql</span><br><span class="line">baseurl = https://mirrors.ustc.edu.cn/mysql-repo/yum/mysql-5.7-community/el/7/x86_64/</span><br><span class="line">gpgcheck=0</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum install mysql-community-libs mysql-community-common  mysql-community-client mysql-community-devel mysql-community-server</span><br></pre></td></tr></table></figure>
<p>启动mysql<br>systemctl start mysqld  </p>
<p>mysql初始化<br>mysqld –initialize –user&#x3D;mysql  </p>
<p>查找默认密码<br>grep “temporary password” &#x2F;var&#x2F;log&#x2F;mysqld.log  </p>
<p>默认密码策略设置密码需要大写、小写、特殊符号。关闭默认密码策略  </p>
<p>编辑&#x2F;etc&#x2F;my.cnf<br>validate_password&#x3D;off  </p>
<p>重启mysql<br>systemctl restart mysqld  </p>
<p>用初始密码登录mysql重启密码  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">alter user root@localhost identified by &#x27;passw0rd&#x27;;  </span><br></pre></td></tr></table></figure>
<p>修改默认编码<br>由下图可见database和server的字符集使用了latin1编码方式，不支持中文，即存储中文时会出现乱码。以下是命令行修改为utf-8编码的过程，以支持中文  </p>
<p>修改为utf-8打开，编辑&#x2F;etc&#x2F;my.cnf.d&#x2F;server.cnf<br>在mysqld里面添加<br>character_set_server &#x3D; utf8</p>
<p>重启mysql<br>systemctl restart mysqld</p>
<p>再次查看<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_1_1.png"></p>
<h3 id="安装php7"><a href="#安装php7" class="headerlink" title="安装php7"></a>安装php7</h3><p>php7在性能上比php5.x有2倍多的提升，同时兼容性也特别好。同时php5.x很多openssl和openssh的漏洞  </p>
<p>安装依赖包  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum -y install libmcrypt-devel mcrypt mhash gd-devel ncurses-devel libxml2-devel bzip2-devel libcurl-devel curl-devel libjpeg-devel libpng-devel freetype-devel net-snmp-devel openssl-devel</span><br></pre></td></tr></table></figure>

<p>安装libconv<br>创建目录<br>mkdir &#x2F;lnmp<br>进入目录<br>cd &#x2F;lnmp<br>wget <a href="http://ftp.gnu.org/pub/gnu/libiconv/libiconv-1.14.tar.gz">http://ftp.gnu.org/pub/gnu/libiconv/libiconv-1.14.tar.gz</a><br>解压包  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tar xvf libiconv-1.14.tar.gz</span><br><span class="line"></span><br><span class="line">cd libiconv-1.14</span><br><span class="line"></span><br><span class="line">./configure --prefix=/usr/local/libiconv1.14</span><br></pre></td></tr></table></figure>

<p>make时包此错<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_1_3.png"></p>
<p>解决方法</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi libiconv-1.14/srclib/stdio.in.h</span><br><span class="line">将698行的代码：_GL_WARN_ON_USE (gets, &quot;gets is a security hole - use fgets instead&quot;);替换为：</span><br><span class="line"></span><br><span class="line">#if defined(__GLIBC__) &amp;&amp; !defined(__UCLIBC__) &amp;&amp; !__GLIBC_PREREQ(2, 16)</span><br><span class="line"> _GL_WARN_ON_USE (gets, &quot;gets is a security hole - use fgets instead&quot;);</span><br><span class="line">#endif</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>重新make &amp;&amp; make install<br>保存动态链接库</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">echo &quot;/usr/local/lib64/&quot; &gt; /etc/ld.so.conf.d/lnmp.conf</span><br><span class="line">echo &quot;/usr/local/lib/&quot; &gt;&gt; /etc/ld.so.conf.d/lnmp.conf</span><br></pre></td></tr></table></figure>
<p>刷新<br>ldconfig</p>
<p>下载php7.1.7源码包<br>cd &#x2F;lnmp<br>wget <a href="http://cn2.php.net/distributions/php-7.1.7.tar.gz">http://cn2.php.net/distributions/php-7.1.7.tar.gz</a>  </p>
<p>解压<br>tar -xvf php-7.1.7.tar.gz</p>
<p>配置和检查依赖php7.1.7  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./configure --prefix=/usr/local/php7.1.7 --with-config-file-path=/usr/local/php7.1.7/etc --enable-mysqlnd --with-mysqli=mysqlnd --with-pdo-mysql=mysqlnd --with-iconv-dir=/usr/local/libiconv1.14 --with-pcre-regex --with-zlib --with-bz2 --enable-calendar --with-curl --enable-dba --with-libxml-dir --enable-ftp --with-gd --with-jpeg-dir --with-png-dir --with-zlib-dir --with-freetype-dir --enable-gd-native-ttf --with-mhash --enable-mbstring --with-mcrypt --enable-pcntl --enable-xml --disable-rpath --enable-shmop --enable-sockets --enable-zip --enable-bcmath --with-snmp --disable-ipv6 --with-gettext  --enable-fpm --with-fpm-user=www  --with-fpm-group=www --with-openssl</span><br></pre></td></tr></table></figure>

<p>make &amp;&amp; make install<br>复制php配置文件  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp /lnmp/php-7.1.7/php.ini-production /usr/local/php7.1.7/etc/php.ini  </span><br></pre></td></tr></table></figure>
<p>配置php<br>修改时区  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sed -i &#x27;s#;date.timezone =#date.timezone = Asia/Shanghai#g&#x27; /usr/local/php7.1.7/etc/php.ini</span><br></pre></td></tr></table></figure>
<p>隐藏php版本号  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sed -i &#x27;s#expose_php = On#expose_php = Off#g&#x27; /usr/local/php7.1.7/etc/php.ini</span><br></pre></td></tr></table></figure>

<p>配置启动脚本  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@localhost php-7.1.7]# cp /lnmp/php-7.1.7/sapi/fpm/init.d.php-fpm /etc/init.d/php-fpm</span><br><span class="line">[root@localhost php-7.1.7]# chmod +x /etc/init.d/php-fpm</span><br></pre></td></tr></table></figure>

<p>配置fast-cgi  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp /usr/local/php7.1.7/etc/php-fpm.d/www.conf /usr/local/php7.1.7/etc/php-fpm.conf</span><br></pre></td></tr></table></figure>
<p>修改fast-cgi<br>vim  &#x2F;usr&#x2F;local&#x2F;php7.1.7&#x2F;etc&#x2F;php-fpm.conf<br>修改已下参数<br>rlimit_files &#x3D; 65535</p>
<p>创建www用户和组    </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">useradd www -s /sbin/nologin</span><br></pre></td></tr></table></figure>


<p>启动php-fpm<br> &#x2F;etc&#x2F;init.d&#x2F;php-fpm start<br>设置开机自启<br>&#x2F;sbin&#x2F;chkconfig –add php-fpm<br>修改php.ini以满足zabbix要求<br>vim &#x2F;usr&#x2F;local&#x2F;php7.1.7&#x2F;etc&#x2F;php.ini<br>限制执行目录,nginx网页放在&#x2F;var&#x2F;www&#x2F;html下  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">open_basedir = &quot;/var/www/html/:/tmp&quot;  </span><br><span class="line">max_execution_time = 300  </span><br><span class="line">max_input_time = 300  </span><br><span class="line">post_max_size = 24M  </span><br><span class="line">upload_max_filesize = 4M  </span><br></pre></td></tr></table></figure>
<p>修改php连接mysql<br>pdo_mysql.default_socket&#x3D; &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql.sock<br>mysqli.default_socket &#x3D; &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql.sock</p>
<h3 id="安装nginx"><a href="#安装nginx" class="headerlink" title="安装nginx"></a>安装nginx</h3><p>wget <a href="http://nginx.org/download/nginx-1.12.1.tar.gz">http://nginx.org/download/nginx-1.12.1.tar.gz</a><br>解压  </p>
<p>[root@localhost lnmp]# tar -xvf nginx-1.12.1.tar.gz</p>
<p>[root@localhost lnmp]# cd nginx-1.12.1</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@localhost lnmp]#./configure --prefix=/usr/local/nginx1.12.1 --user=www --group=www --with-http_stub_status_module --with-http_gzip_static_module --with-http_ssl_module</span><br></pre></td></tr></table></figure>
<p>[root@localhost lnmp]#make &amp;&amp; make install  </p>
<p>配置nginx  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp /usr/local/nginx1.12.1/conf/nginx.conf /usr/local/nginx1.12.1/conf/nginx.conf.bak</span><br></pre></td></tr></table></figure>
<p>vim &#x2F;usr&#x2F;local&#x2F;nginx1.12.1&#x2F;conf&#x2F;nginx.conf</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">user  www www;</span><br><span class="line">worker_processes  4;</span><br><span class="line"></span><br><span class="line">error_log  logs/error.log  info;</span><br><span class="line">pid        logs/nginx.pid;</span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections  65535;</span><br><span class="line">    use epoll;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    include       mime.types;</span><br><span class="line">    default_type  application/octet-stream;</span><br><span class="line">    sendfile        on;</span><br><span class="line">    tcp_nopush     on;</span><br><span class="line">    keepalive_timeout  65;</span><br><span class="line">    gzip  on;</span><br><span class="line">    log_format  main  &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27;</span><br><span class="line">                      &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27;</span><br><span class="line">                      &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;;</span><br><span class="line">    access_log  logs/access.log  main;</span><br><span class="line"></span><br><span class="line">    server &#123;</span><br><span class="line">        listen       80;</span><br><span class="line">        server_name  localhost;</span><br><span class="line">        charset utf8;</span><br><span class="line">        location / &#123;</span><br><span class="line">            root   /var/www/html;</span><br><span class="line">            index index.php index.html index.htm;</span><br><span class="line">        &#125;</span><br><span class="line">        error_page   500 502 503 504  /50x.html;</span><br><span class="line">        location = /50x.html &#123;</span><br><span class="line">            root   /var/www/html;</span><br><span class="line">        &#125;</span><br><span class="line">        location ~ \.php$ &#123;</span><br><span class="line">            root           /var/www/html;</span><br><span class="line">            fastcgi_pass   127.0.0.1:9000;</span><br><span class="line">            fastcgi_index  index.php;</span><br><span class="line">            fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;</span><br><span class="line">            include        fastcgi_params;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>修改ulimit<br>ulimit -n 65535<br>编辑<br>&#x2F;etc&#x2F;systemd&#x2F;system.conf<br>设置DefaultLimitNOFILE&#x3D;65535</p>
<p>重启系统  </p>
<p>启动nginx<br>&#x2F;usr&#x2F;local&#x2F;nginx1.12.1&#x2F;sbin&#x2F;nginx<br>重载nginx<br>&#x2F;usr&#x2F;local&#x2F;nginx1.12.1&#x2F;sbin&#x2F;nginx -s reload<br>关闭nginx<br>&#x2F;usr&#x2F;local&#x2F;nginx1.12.1&#x2F;sbin&#x2F;nginx -s stop<br>重启nginx<br>&#x2F;usr&#x2F;local&#x2F;nginx1.12.1&#x2F;sbin&#x2F;nginx -s reopen  </p>
<p>设置nginx启动脚本<br>编辑&#x2F;etc&#x2F;init.d&#x2F;nginx<br>注意PATH和NAME变量  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"># chkconfig: - 85 15</span><br><span class="line">PATH=/usr/local/nginx1.12.1</span><br><span class="line">DESC=&quot;nginx daemon&quot;</span><br><span class="line">NAME=nginx</span><br><span class="line">DAEMON=$PATH/sbin/$NAME</span><br><span class="line">CONFIGFILE=$PATH/conf/$NAME.conf</span><br><span class="line">PIDFILE=$PATH/logs/$NAME.pid</span><br><span class="line">SCRIPTNAME=/etc/init.d/$NAME</span><br><span class="line">set -e</span><br><span class="line">[ -x &quot;$DAEMON&quot; ] || exit 0</span><br><span class="line">do_start() &#123;</span><br><span class="line">$DAEMON -c $CONFIGFILE || echo -n &quot;nginx already running&quot;</span><br><span class="line">&#125;</span><br><span class="line">do_stop() &#123;</span><br><span class="line">$DAEMON -s stop || echo -n &quot;nginx not running&quot;</span><br><span class="line">&#125;</span><br><span class="line">do_reload() &#123;</span><br><span class="line">$DAEMON -s reload || echo -n &quot;nginx can&#x27;t reload&quot;</span><br><span class="line">&#125;</span><br><span class="line">case &quot;$1&quot; in</span><br><span class="line">start)</span><br><span class="line">echo -n &quot;Starting $DESC: $NAME&quot;</span><br><span class="line">do_start</span><br><span class="line">echo &quot;.&quot;</span><br><span class="line">;;</span><br><span class="line">stop)</span><br><span class="line">echo -n &quot;Stopping $DESC: $NAME&quot;</span><br><span class="line">do_stop</span><br><span class="line">echo &quot;.&quot;</span><br><span class="line">;;</span><br><span class="line">reload|graceful)</span><br><span class="line">echo -n &quot;Reloading $DESC configuration...&quot;</span><br><span class="line">do_reload</span><br><span class="line">echo &quot;.&quot;</span><br><span class="line">;;</span><br><span class="line">restart)</span><br><span class="line">echo -n &quot;Restarting $DESC: $NAME&quot;</span><br><span class="line">do_stop</span><br><span class="line">do_start</span><br><span class="line">echo &quot;.&quot;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">echo &quot;Usage: $SCRIPTNAME &#123;start|stop|reload|restart&#125;&quot; &gt;&amp;2</span><br><span class="line">exit 3</span><br><span class="line">;;</span><br><span class="line">esac</span><br><span class="line">exit 0</span><br></pre></td></tr></table></figure>

<p>添加执行权限<br>chmod a+x &#x2F;etc&#x2F;init.d&#x2F;nginx<br>注册成服务<br>&#x2F;sbin&#x2F;chkconfig –add nginx  </p>
<p>添加开机自启<br>&#x2F;sbin&#x2F;chkconfig nginx on  </p>
<p>查看<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_1_4.png"><br>创建目录<br>mkdir -p &#x2F;var&#x2F;www&#x2F;html<br>修改用户和属组<br>chown www:www &#x2F;var&#x2F;www&#x2F;html<br>重启测试<br>启动nginx<br>systemctl start nginx<br>重启<br>systemctl restart nginx<br>关闭<br>systemctl stop nginx  </p>
<p>测试php<br>在&#x2F;var&#x2F;www&#x2F;html下编写index.php文件  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;?php  </span><br><span class="line">phpinfo();</span><br><span class="line">?&gt;</span><br></pre></td></tr></table></figure>
<p>修改权限<br>chown www:www &#x2F;var&#x2F;www&#x2F;html&#x2F;index.php<br>重启nginx和php-fpm</p>
<p>打开浏览器输入地址<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_1_5.png"></p>
<p>至此LNMP搭建完毕。</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>从零开始构建云平台监控(二)安装zabbix</title>
    <url>/2017/10/10/monitor_2/</url>
    <content><![CDATA[<h3 id="安装zabbix-server"><a href="#安装zabbix-server" class="headerlink" title="安装zabbix-server"></a>安装zabbix-server</h3><p>这里我选择zabbix3.4.2正式版,zabbix3.4.2修复前期非常多bug，release文档如下:<br><a href="https://www.zabbix.com/rn3.2.4">https://www.zabbix.com/rn3.2.4</a>  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget https://ncu.dl.sourceforge.net/project/zabbix/ZABBIX%20Latest%20Stable/3.4.2/zabbix-3.4.2.tar.gz</span><br></pre></td></tr></table></figure>
<h3 id="创建zabbix用户和组"><a href="#创建zabbix用户和组" class="headerlink" title="创建zabbix用户和组"></a>创建zabbix用户和组</h3><p>groupadd zabbix<br>useradd -g zabbix zabbix -s &#x2F;sbin&#x2F;nologin  </p>
<h3 id="创建库并授权"><a href="#创建库并授权" class="headerlink" title="创建库并授权"></a>创建库并授权</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; create database zabbix character set utf8 collate utf8_bin;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; grant all privileges on zabbix.* to zabbix@localhost identified by &#x27;passw0rd&#x27;;</span><br><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; flush privileges;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br></pre></td></tr></table></figure>
<p>解压zabbix tar包<br>tar -xvf zabbix-3.4.2.tar.gz -C &#x2F;lnmp&#x2F;</p>
<h3 id="导入数据库"><a href="#导入数据库" class="headerlink" title="导入数据库"></a>导入数据库</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@localhost zabbix-3.4.2]# cd /lnmp/zabbix-3.4.2/database/mysql/</span><br><span class="line"></span><br><span class="line">注意顺序</span><br><span class="line">[root@localhost mysql]# mysql -uzabbix -ppassw0rd zabbix &lt; schema.sql</span><br><span class="line">[root@localhost mysql]# mysql -uzabbix -ppassw0rd zabbix &lt; images.sql</span><br><span class="line">[root@localhost mysql]# mysql -uzabbix -ppassw0rd zabbix &lt; data.sql</span><br></pre></td></tr></table></figure>
<h3 id="安装需要依赖"><a href="#安装需要依赖" class="headerlink" title="安装需要依赖"></a>安装需要依赖</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum install -y net-snmp net-snmp-devel OpenIPMI OpenIPMI-devel libevent libevent-devel unixODBC-devel</span><br></pre></td></tr></table></figure>
<h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><p>注意这里不安装zabbix-java-gateway， zabbix-java-gateway是用来监控tomcat的需要本机有java环境，这里先不装，如果需要，安装java环境后重新编译加上–enable-java就可以了  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./configure  --enable-server --enable-agent --with-mysql --with-net-snmp  --with-libcurl  --with-libxml2  --with-openipmi --with-unixodbc --with-openssl</span><br><span class="line"></span><br><span class="line">make install</span><br></pre></td></tr></table></figure>
<h3 id="配置monitor-server"><a href="#配置monitor-server" class="headerlink" title="配置monitor-server"></a>配置monitor-server</h3><p>创建zabbix日志目录  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@localhost yum.repos.d]# mkdir /var/log/zabbix</span><br><span class="line">[root@localhost yum.repos.d]# chown -R zabbix:zabbix /var/log/zabbix/</span><br></pre></td></tr></table></figure>
<p>创建自定义脚本目录  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@localhost tru64]# mkdir /usr/local/etc/zabbix/alertscripts -pv</span><br><span class="line">[root@localhost yum.repos.d]# chown -R zabbix:zabbix /usr/local/etc/zabbix/alertscripts</span><br></pre></td></tr></table></figure>
<p>配置文件目录<br>&#x2F;usr&#x2F;local&#x2F;etc&#x2F;<br>编辑zabbix-server.conf</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">LogFile=/var/log/zabbix/zabbix_server.log</span><br><span class="line">DBHost=localhost</span><br><span class="line">DBName=zabbix</span><br><span class="line">DBUser=zabbix</span><br><span class="line">DBPassword=passw0rd</span><br><span class="line">ListenIP=0.0.0.0</span><br><span class="line">FpingLocation=/usr/sbin/fping</span><br><span class="line">Timeout=20</span><br><span class="line">CacheSize=1024</span><br><span class="line">AlertScriptsPath=/usr/lib/zabbix/alertscripts</span><br><span class="line">ExternalScripts=/usr/lib/zabbix/externalscript</span><br></pre></td></tr></table></figure>
<p>编辑zabbix-agent.conf  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">LogFile=/var/log/zabbix/zabbix_agentd.log</span><br><span class="line">Server=127.0.0.1</span><br><span class="line">ServerActive=127.0.0.1</span><br><span class="line">Hostname=Zabbix server</span><br></pre></td></tr></table></figure>
<p>拷贝启动脚本  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@localhost tru64]# cp /lnmp/zabbix-3.4.2/misc/init.d/tru64/zabbix_server  /etc/init.d/</span><br><span class="line">[root@localhost tru64]# cp /lnmp/zabbix-3.4.2/misc/init.d/tru64/zabbix_agentd /etc/init.d/</span><br></pre></td></tr></table></figure>
<p>添加执行权限  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@localhost tru64]# chmod a+x /etc/init.d/zabbix_server</span><br><span class="line">[root@localhost tru64]# chmod a+x /etc/init.d/zabbix_agentd</span><br></pre></td></tr></table></figure>
<p>编辑启动脚本<br>vim &#x2F;etc&#x2F;init.d&#x2F;zabbix_server</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/sh</span><br><span class="line">#chkconfig: 345 95 95</span><br><span class="line">#description: Zabbix_Server</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>vim &#x2F;etc&#x2F;init.d&#x2F;zabbix_agent  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/sh</span><br><span class="line">#chkconfig: 345 95 95</span><br><span class="line">#description: Zabbix_agentd</span><br></pre></td></tr></table></figure>
<p>添加服务  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@localhost tru64]# /sbin/chkconfig --add zabbix_agentd</span><br><span class="line">[root@localhost tru64]# /sbin/chkconfig --add zabbix_server</span><br></pre></td></tr></table></figure>
<p>开机自启  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@localhost tru64]# /sbin/chkconfig zabbix_server  on</span><br><span class="line">[root@localhost tru64]# /sbin/chkconfig zabbix_agentd  on</span><br></pre></td></tr></table></figure>

<p>创建目录  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@localhost /]# mkdir /var/www/html/zabbix</span><br></pre></td></tr></table></figure>
<p>拷贝安装页  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp -rf /lnmp/zabbix-3.4.2/frontends/php/* /var/www/html/zabbix/  </span><br></pre></td></tr></table></figure>

<p>重启nginx  </p>
<p>&#x2F;etc&#x2F;init.d&#x2F;nginx restart<br>打开浏览器<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_2_1.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_2_2.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_2_3.png"><br>按提示操作，下载文件放到指定目录<br>设置中文<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_2_4.png"><br>安装完成  </p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>从零开始构建云平台监控(三)配置基础监控平台</title>
    <url>/2017/10/11/monitor_3/</url>
    <content><![CDATA[<p>环境：<br>3台控制节点+2台融合节点 2个osd</p>
<h3 id="给zabbix-server导入模板"><a href="#给zabbix-server导入模板" class="headerlink" title="给zabbix-server导入模板"></a>给zabbix-server导入模板</h3><p>模板为分别对应为control、compute、ceph</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_3_1.png"><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_3_2.png"></p>
<p>导入这3个模板<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_3_3.png"></p>
<h3 id="配置自动注册"><a href="#配置自动注册" class="headerlink" title="配置自动注册"></a>配置自动注册</h3><p>自动注册就是zabbix-server根据zabbix-agent里面配置的HostMetadata参数的值去进行一系列的操作，和自动发现不一样的是，自动发现只能跟据zabbix-server配置的扫描的网段去添加机器，非常不灵活，并且这个操作是zabbix-server发起的机器规模一大，对zabbix-server有很大的负载压力。自动注册是agent端主动将HostMetadata参数的值给server，server根据管理员的配置在做出相应的操作，在这里比如我openstack的控制节点我就在agent端的HostMetadata配置openstack_controler、然后计算节点配置openstack_computer、存储节点openstack_storager、计算和存储融合节点为 openstack_computer&amp;storager,然后在zabbix-server端创建相应的动作如下图(以添加control为例)其他类似。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_3_4.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_3_5.png"></p>
<p>操作都填写这些  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_3_6.png"><br>最后  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_3_7.png">  </p>
<h3 id="配置zabbix-agent在agent端安装zabbix-agent，并导入脚本"><a href="#配置zabbix-agent在agent端安装zabbix-agent，并导入脚本" class="headerlink" title="配置zabbix-agent在agent端安装zabbix-agent，并导入脚本"></a>配置zabbix-agent在agent端安装zabbix-agent，并导入脚本</h3><p>下载zabbix-agent，agent端就不用源码安装了，直接下载rpm包。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget http://repo.zabbix.com/zabbix/3.4/rhel/7/x86_64/zabbix-agent-3.4.2-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure>
<p>安装</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node-6 ~]# rpm -ivh zabbix-agent-3.4.2-1.el7.x86_64.rpm</span><br><span class="line">vim /etc/zabbix/zabbix.conf</span><br><span class="line">PidFile=/var/run/zabbix/zabbix_agentd.pid</span><br><span class="line">LogFile=/var/log/zabbix/zabbix_agentd.log</span><br><span class="line">LogFileSize=0</span><br><span class="line">Server=10.10.1.100</span><br><span class="line">ListenPort=10050</span><br><span class="line">ServerActive=10.10.1.100</span><br><span class="line">HostMetadata=openstack_controler</span><br><span class="line">Timeout=15</span><br><span class="line">Include=/etc/zabbix/zabbix_agentd.d/*.conf</span><br></pre></td></tr></table></figure>

<p>解压  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tar -xvf controller.agent.tar.gz  -C /etc/zabbix/</span><br><span class="line">[root@node-6 zabbix_agentd.d]# chown zabbix:zabbix *</span><br><span class="line">[root@node-6 zabbix_agentd.d]# chmod a+x *</span><br></pre></td></tr></table></figure>
<p>cd &#x2F;etc&#x2F;zabbix&#x2F;script<br>[root@node-6 script]# chown zabbix:zabbix *<br>其他几个节点一样</p>
<p>注意两个融合节点metadata配置<br>HostMetadata&#x3D;openstack_computer&amp;storager<br>重启zabbix-agent，等待自动注册。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl restart zabbix-agent</span><br><span class="line">systemctl enable zabbix-agent</span><br></pre></td></tr></table></figure>
<p>可以看见机器都自动注册进来了。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_3_8.png">  </p>
<h3 id="配置邮件告警"><a href="#配置邮件告警" class="headerlink" title="配置邮件告警"></a>配置邮件告警</h3><h4 id="脚本配置"><a href="#脚本配置" class="headerlink" title="脚本配置"></a>脚本配置</h4><p>将发送邮件脚本放置到<br>&#x2F;usr&#x2F;local&#x2F;etc&#x2F;zabbix&#x2F;alertscripts</p>
<p>vim sendmail.py</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_3_21.png"></p>
<p>chmod a+x sendmail.py  </p>
<h4 id="zabbix-server-web端配置"><a href="#zabbix-server-web端配置" class="headerlink" title="zabbix-server web端配置"></a>zabbix-server web端配置</h4><p>创建报警媒介类型<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_3_10.png"></p>
<p>名字、类型、注意下面三个参数要传递到脚本里面。  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_3_11.png"></p>
<p>配置用户<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_3_12.png"></p>
<p>报警媒介<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_3_13.png"></p>
<p>输入联系人<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_3_14.png"><br>更新</p>
<p>创建动作，zabbix在3.4.2版本中对创建动作这块有较大改动，多了确认操作，以前只有故障和恢复时发邮件，触发动作的条件可以自己配置。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_3_15.png"></p>
<p>添加新的动作  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_3_16.png"></p>
<p>添加用户、添加组、选择刚刚创建的发送媒介  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_3_17.png"></p>
<p>恢复操作通知。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_3_18.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_3_19.png">  </p>
<p>测试将zabbix-agent关闭等待5分钟。动作日志。  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_3_20.png">  </p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>从零开始构建云平台监控(四)图形化展示监控效果</title>
    <url>/2017/10/13/monitor_4/</url>
    <content><![CDATA[<h3 id="组件安装和配置"><a href="#组件安装和配置" class="headerlink" title="组件安装和配置"></a>组件安装和配置</h3><p>zabbix本功能非常强大，自定义监控项，自动发现、自动注册等，但zabbix-server获取的zabbix数据看起来不是特别直观。</p>
<p>grafana的出现正好弥补了zabbix绘图能力上的不足，grafana是基于js开发的图形编辑器。<br>telegraf<br>telegraf是一个go语言编写的收集监控项的agentTelegraf内存占用小的特点，通过插件来实现不同的监控需求，这里我使用ceph插件。</p>
<p>influxdb<br>InfluxDB 是一个开源分布式时序|、事件和指标数据库。使用 Go 语言编写，无需外部依赖。其设计目标是实现分布式和水平伸缩扩展。<br>它有三大特性：  </p>
<ol>
<li>Time Series （时间序列）：你可以使用与时间有关的相关函数（如最大，最小，求和等）  </li>
<li>Metrics（度量）：你可以实时对大量数据进行计算  </li>
<li>Eevents（事件）：它支持任意的事件数据<br>特点<br>schemaless(无结构)，可以是任意数量的列<br>Scalable<br>min, max, sum, count, mean, median 一系列函数，方便统计<br>Native HTTP API, 内置http支持，使用http读写<br>Powerful Query Language 类似sql<br>Built-in Explorer 自带管理工具</li>
</ol>
<p>通过grafana调用zabbix的接口的实现，通过自定义模板和key去拉去数据，在grafana上进行展示，grafana只做统一的监控展示平台ceph的一些监控数据由telegraf收集存到influxdb，grafana去读取。</p>
<p>为什么选择telegraf而不直接通过zabbix写item获取这些数据？<br>1，telegraf是一个go语言写的小程序，占用资源非常小，并且本身有监控ceph的插件<br>2，telegraf监控ceph的插件，监控的数据非常全，osd数mon数，细致点，journal盘的速率、pgmap的数率，cluster的iops、pool池的使用趋势等等，如果用zabbix的话我们想获取这些数据要写非常多的item。  </p>
<h3 id="monitor-server端操作"><a href="#monitor-server端操作" class="headerlink" title="monitor-server端操作"></a>monitor-server端操作</h3><p>安装influxdb<br>influxdb的安装<br>配置yum源</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/influxdb.repo</span><br><span class="line">[influxdb]</span><br><span class="line">name = InfluxDB Repository - RHEL \$releasever</span><br><span class="line">baseurl = https://repos.influxdata.com/rhel/\$releasever/\$basearch/stable</span><br><span class="line">enabled = 1</span><br><span class="line">gpgcheck = 1</span><br><span class="line">gpgkey = https://repos.influxdata.com/influxdb.key</span><br><span class="line">EOF</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>yum install influxdb<br>systemctl enable influxdb<br>systemctl restart influxdb  </p>
<p>默认influxdb的web管理界面是关闭的开启方法<br> vim &#x2F;etc&#x2F;influxdb&#x2F;influxdb.conf<br> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[admin]</span><br><span class="line">    enabled = true</span><br><span class="line">    bind-address = &quot;:8083&quot;</span><br></pre></td></tr></table></figure><br>重启influxdb<br>浏览器服务<a href="http://ip:8083/">http://ip:8083</a></p>
<h3 id="在ceph-mon节点上安装telegraf"><a href="#在ceph-mon节点上安装telegraf" class="headerlink" title="在ceph-mon节点上安装telegraf"></a>在ceph-mon节点上安装telegraf</h3><p>telegraf通过读取ceph的asok文件获取里面的信息来达到监控目的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget https://dl.influxdata.com/telegraf/releases/telegraf-1.2.1.x86_64.rpm</span><br></pre></td></tr></table></figure>
<p>yum localinstall telegraf-1.2.1.x86_64.rpm</p>
<p>修改telegraf的启动用户，不然会读取ceph asok文件权限不足<br>vim &#x2F;lib&#x2F;systemd&#x2F;system&#x2F;telegraf.service<br>将User&#x3D;telegraf改为User&#x3D;root<br>systemctl daemon-reload<br>systemctl restart telegraf<br>systemctl enable telegraf  </p>
<p>配置telegraf<br>vim &#x2F;etc&#x2F;telegraf&#x2F;telegraf.conf  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[log]</span><br><span class="line">logfile = &quot;/var/log/telegraf/telegraf.log&quot;</span><br><span class="line">[[outputs.influxdb]]</span><br><span class="line">urls = [&quot;http://10.10.1.100:8086&quot;] # required #填写对应的influxdb的地址</span><br><span class="line">[[inputs.ceph]]</span><br><span class="line">interval = &#x27;1m&#x27;</span><br><span class="line">ceph_binary = &quot;/usr/bin/ceph&quot;</span><br><span class="line">socket_dir = &quot;/var/run/ceph&quot;</span><br><span class="line">mon_prefix = &quot;ceph-mon&quot;</span><br><span class="line">osd_prefix = &quot;ceph-osd&quot;</span><br><span class="line">ceph_user = &quot;client.admin&quot;</span><br><span class="line">ceph_config = &quot;/etc/ceph/ceph.conf&quot;</span><br><span class="line">gather_admin_socket_stats = true</span><br><span class="line">gather_cluster_stats = true</span><br></pre></td></tr></table></figure>

<p>另外3个控制节点同步配置<br>重启telegraf<br>测试数据是否拿到</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">telegraf -test -config /etc/telegraf/telegraf.conf -config-directory /etc/telegraf/telegraf.d -input-filter ceph</span><br></pre></td></tr></table></figure>

<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_1.png"><br>在monitor-server的influxdb上可以看见创建好的库<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_2.png"></p>
<p>进入influxdb<br>influxd<br>列出全部库<br>show databases;<br>进入指定的库<br>use telegraf<br>列出库中全部表<br>show measurements;<br>查询表<br>select * from ceph;  </p>
<p>默认influxdb的数据存储时间为168小时也就是7天如果要修改方法为<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_3.png"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create retention policy &quot;rp_name&quot; on &quot;db_name&quot; duration 3w replication 1 default</span><br><span class="line">rp_name：策略名</span><br><span class="line">db_name：具体的数据库名</span><br><span class="line">3w：保存3周，3周之前的数据将被删除，influxdb具有各种事件参数，比如：h（小时），d（天），w（星期）</span><br><span class="line">replication 1：副本个数，一般为1就可以了</span><br><span class="line">default：设置为默认策略</span><br></pre></td></tr></table></figure>

<p>对已有的策略修改  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">alter retention policy &quot;rp_name&quot; on &quot;db_name&quot; duration 30d default  </span><br></pre></td></tr></table></figure>
<p>删除已有策略  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">drop retention policy &quot;rp_name&quot;  </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>安装grafana<br>配置安装grafana<br>下载软件包  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana-4.1.2-1486989747.x86_64.rpm</span><br></pre></td></tr></table></figure>
<p>安装软件  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum localinstall /root/grafana-4.1.2-1486989747.x86_64.rpm</span><br></pre></td></tr></table></figure>
<p>启动grafana<br>systemctl start grafana-server<br>查看端口<br>lsof -i:3000  </p>
<p>安装pie chart插件</p>
<p>ceph这一块用饼图描述比较直观点，需要安装grafana Pie Chart插件  </p>
<p>grafana-cli plugins install grafana-piechart-panel  </p>
<p>在线安装方式<br>grafana-cli plugins install grafana-piechart-panel   </p>
<p>离线安装方式<br>将下载好的plugins解压到<br>&#x2F;var&#x2F;lib&#x2F;grafana&#x2F;plugins<br>grafana-cli plugins install grafana-piechart-pane  </p>
<p>重启grafana<br>systemctl restart grafana-server<br>登录grafana<br><a href="http://localhost:3000/">http://localhost:3000</a><br>默认帐号密码为admin&#x2F;admin<br>将插件激活<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_4.png"></p>
<p>enable zabbix<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_5.png"></p>
<p>右侧可以看见已经安装的插件<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_6.png"></p>
<p>添加Datasource<br>选zabbix<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_7.png"></p>
<p>url输入<br><a href="http://192.168.122.100/zabbix/api_jsonrpc.php">http://192.168.122.100/zabbix/api_jsonrpc.php</a>  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_8.png"></p>
<p>测试api连接<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_9.png"></p>
<p>然后就可以在dashboard里面看见两个新模板了<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_10.png"></p>
<p>将Template Linux Server删除<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_11.png"></p>
<p>查看zabbix-server模板  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_12.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_13.png"></p>
<p>添加influxdb DataSource<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_14.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_15.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_16.png"></p>
<p>创建以下dashboard<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_17.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_18.png"></p>
<p>创建好后，我们从一个个dashboard里面添加模块</p>
<h3 id="openstack物理节点性能监控"><a href="#openstack物理节点性能监控" class="headerlink" title="openstack物理节点性能监控"></a>openstack物理节点性能监控</h3><h4 id="配置template"><a href="#配置template" class="headerlink" title="配置template"></a>配置template</h4><p>因为这是一个集群，有很多台机器，每台机器所对应的角色是不一样的，比如node-1到node-3是控制节点node-4到node6是存储节点 node-7到node-10是计算节点，不同角色的监控项也有略微不同。但我们想在一个dashboard展示多台机器的监控值<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_19.png"></p>
<p>所以我们需要定义Templating<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_20.png"></p>
<p>定义group的templating<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_21.png"></p>
<p>然后在zabbix里面定义的group就出来了<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_22.png"></p>
<p>selection options host可以选多个，然后在一个图上绘出，我建议，这里还是一一对应的显示吧<br>定义host的templating<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_23.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_24.png"></p>
<p>匹配完的显示<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_25.png"></p>
<h4 id="创建row"><a href="#创建row" class="headerlink" title="创建row"></a>创建row</h4><p>row是一组panel的集合  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_26.png"><br>这里面的uptime、memeory free、free disk space on &#x2F; 、system load(1min）这些panel就是属于主机情况这个row的。<br>以创建主机情况row为例<br>dashboard下面有一个add row的按钮<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_27.png"></p>
<p>编辑row<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_28.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_29.png"></p>
<p>编辑名称显示名字<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_30.png"></p>
<p>按照上述方法创建另外3个row，资源使用情况，网卡信息，<br>然后ctrl+s保存这个dashboard的改变。<br>创建panel<br>将panel创建到对应的row中<br>创建uptime时间panel<br>点击创建panel  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_31.png"><br>类型为Singlestat panel<br>general配置 标题大小<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_32.png">   </p>
<p>metrics配置数据来源  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_33.png">   </p>
<p>配置字体颜色<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_34.png">   </p>
<p>ctrl+s保存，uptime 的panel就完成了。  </p>
<h4 id="定义memory-free-panel"><a href="#定义memory-free-panel" class="headerlink" title="定义memory free panel"></a>定义memory free panel</h4><p>在主机情况row创建Singlestat类型的panel<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_35.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_36.png">   </p>
<p>options定义背景色，和定义值范围，根据值的不同范围，背景色会自动调整</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_37.png">   </p>
<p>这里需要注意下Thresholds这个参数，这个参数定义值的范围，然后granafa会根据这个范围自动调整颜色，如7,9如果值小于7则显示红色，在7<del>9之间则显示橙色，大于9则显示绿色，但这个值不支持单位换算后的值，所以这也是为什么我这里是10737418240,21474836480，是10,20 G内存空闲小于10G变红色，10G</del>20G之间橙色，大于20G绿色。</p>
<p>ctrl+s保存  </p>
<h4 id="定义根分区剩余空间panel"><a href="#定义根分区剩余空间panel" class="headerlink" title="定义根分区剩余空间panel"></a>定义根分区剩余空间panel</h4><p>在主机情况row创建Singlestat类型的panel<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_38.png">   </p>
<p>配置metrics<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_39.png">   </p>
<p>options定义背景色，和定义值范围，根据值的不同范围，背景色会自动调整<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_40.png">   </p>
<p>这里也是定义小于10G时背景色变红，10G~20G范围内为橙色，大于20G为绿色。<br>ctrl+s保存<br>定义cpu负载panel<br>不过这里首先需要修改我们zabbix的 item，zabbix item里面定义的 per cpu而不是all cpu，所以得出来的值会比操作系统里面直接uptime看起来小 ，需要修改zabbix 里面Template OpenStack Compute、和Template OpenStack Controller里面，找到这三个健值修改成all<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_41.png">   </p>
<p>测试<br>zabbix_agentd -t “system.cpu.load[all,avg1]”  </p>
<p>在主机情况row创建Singlestat类型的panel<br>这里我选择15分钟负载情况  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_42.png"><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_43.png"><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_44.png">   </p>
<p>ctrl+s保存  </p>
<h4 id="定义cpu空闲率panel"><a href="#定义cpu空闲率panel" class="headerlink" title="定义cpu空闲率panel"></a>定义cpu空闲率panel</h4><p>在资源使用情况row创建Graph类型的panel<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_45.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_46.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_47.png">   </p>
<p>单位为percent</p>
<p>保存<br>ctrl+s  </p>
<h4 id="定义CPU时间片使用情况"><a href="#定义CPU时间片使用情况" class="headerlink" title="定义CPU时间片使用情况"></a>定义CPU时间片使用情况</h4><p>在资源使用情况rom创建Table panel<br>Genterl配置panel 名字<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_48.png">   </p>
<p>配置Metrics<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_49.png">   </p>
<p>配置Options<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_50.png">   </p>
<p>ctrl+s保存</p>
<h4 id="定义可用内存panel"><a href="#定义可用内存panel" class="headerlink" title="定义可用内存panel"></a>定义可用内存panel</h4><p>在资源使用情况rom创建Graph panel<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_51.png"><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_52.png"><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_53.png">   </p>
<h4 id="定义网桥流量监控panel"><a href="#定义网桥流量监控panel" class="headerlink" title="定义网桥流量监控panel"></a>定义网桥流量监控panel</h4><p>这里需要注意，我们直接对ovs桥进行监控，因为通过ovs桥我们可以确认这部分流量的用途，直接通过物理网卡的话不直观，所以这里需要在zabbix里面将对应的item加上。  </p>
<p>这里将Incoming和Outcoming拆分了开来用两张图显示，并且只监控traffic流量。<br>在网卡信息row创建Graph panel<br>先配置Incoming_Network_traffic</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_54.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_55.png">   </p>
<p><code>注意item为&quot;/Incoming network traffic on br-.*/&quot;  </code><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_56.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_57.png">   </p>
<p>在配置Outcoming_Network_traffic<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_58.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_59.png">   </p>
<p>item写这个   </p>
<p><code>/Outgoing network traffic on br-.*/  </code><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_60.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_61.png">   </p>
<p>ctrl+s  </p>
<p>最终效果<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_62.png">   </p>
<h3 id="添服务状态dashboard，"><a href="#添服务状态dashboard，" class="headerlink" title="添服务状态dashboard，"></a>添服务状态dashboard，</h3><p>这里我拆分为3个dashboard，分别是OpenStack控制节点服务状态、OpenStack计算节点服务状态、OpenStack储存节点服务状态  </p>
<p>每个dashboard设置不同的Templating以OpenStack控制节点服务状态为例，后面都一样<br>创建group，指定为controller这样就不会选别的了，计算节点就指定为computer、储存节点就指定为ceph  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_63.png">   </p>
<p>创建host<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_64.png">   </p>
<p>ctrl+s保存  </p>
<h3 id="OpenStack控制节点服务状态"><a href="#OpenStack控制节点服务状态" class="headerlink" title="OpenStack控制节点服务状态"></a>OpenStack控制节点服务状态</h3><h4 id="定义nova-api-panel"><a href="#定义nova-api-panel" class="headerlink" title="定义nova-api panel"></a>定义nova-api panel</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_65.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_66.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_67.png"><br>配置 value mapping，因为默认抓取的值为数字我们需要将数字映射为更好看的值，比如大于1映射为running，其他映射为down，这里测试发现down后值经常为0.0x所以这里设置映射为值的范围，比如0~0.9为down，1为up  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_126.png"></p>
<p>效果如下<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_69.png"><br>我把nova-api关掉  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_70.png">   </p>
<p>其他控制节点和计算节点监控项目可以直接duplicate了，只需要修改下panel和metrics<br>控制节点需要显示的panel</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nova-api</span><br><span class="line">nova-scheduler</span><br><span class="line">nova-conductor</span><br><span class="line">Neutron-server</span><br><span class="line">Neutron-l3-agent</span><br><span class="line">Neutron-ovs-agent</span><br><span class="line">Neutron-metadata</span><br><span class="line">Neutron-DHCP</span><br><span class="line">Glance-api</span><br><span class="line">Glance-Registry</span><br><span class="line">Keystone</span><br><span class="line">Rabbitmq_cluster</span><br><span class="line">Rabbitmq_liste_port</span><br><span class="line">Rabbitmq_BEAM_process</span><br><span class="line">Rabbitmq_EPMD_listen_port</span><br><span class="line">Rabbitmq_EPMD_process</span><br><span class="line">Mysql_cluster</span><br><span class="line">Cinder_api</span><br><span class="line">Cinder_scheduler</span><br><span class="line">Cinder_volumer</span><br><span class="line">Ceilometer_process</span><br><span class="line">Ceilometer_API</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>计算节点需要显示的panel  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">openstack-compute</span><br><span class="line">libvirtd</span><br><span class="line">openvswitch-agent</span><br><span class="line">ceilometer-agent-compute</span><br></pre></td></tr></table></figure>
<h3 id="zabbix-altert-dashboard"><a href="#zabbix-altert-dashboard" class="headerlink" title="zabbix-altert dashboard"></a>zabbix-altert dashboard</h3><p>alter-dashboard主要显示整个集群的问题  </p>
<p>创件panel，类型为zabbix-Triggers<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_71.png">   </p>
<p>添加后，它自动会将zabbix的Triggers同步过来<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_72.png">   </p>
<h3 id="ceph-dashboard"><a href="#ceph-dashboard" class="headerlink" title="ceph dashboard"></a>ceph dashboard</h3><h4 id="创建template"><a href="#创建template" class="headerlink" title="创建template"></a>创建template</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_73.png">   </p>
<p>这里需要注意把group中固定住控制节点  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_74.png">   </p>
<p>host配置<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_75.png">   </p>
<h4 id="添加ceph健康状态监控panel"><a href="#添加ceph健康状态监控panel" class="headerlink" title="添加ceph健康状态监控panel"></a>添加ceph健康状态监控panel</h4><p>在状态row中创建 Singlestat  panel  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_76.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_77.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_78.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_79.png">   </p>
<h4 id="添加ceph-mon总数监控panel"><a href="#添加ceph-mon总数监控panel" class="headerlink" title="添加ceph mon总数监控panel"></a>添加ceph mon总数监控panel</h4><p>在状态row中创建一个singlestat的panel，General修改名字，metrics配置数据源<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_80.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_81.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_82.png">   </p>
<h4 id="添加ceph-mon-up数panel"><a href="#添加ceph-mon-up数panel" class="headerlink" title="添加ceph mon up数panel"></a>添加ceph mon up数panel</h4><p>在状态row中创建一个singlestat的panel，General修改名字，metrics配置数据源<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_83.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_84.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_85.png">   </p>
<p>thresholds要根据实际集群mon数写，ceph-mon不能挂掉总个数的一半，3个mon只能挂1个，所以这里写2,3小于2时就变红。</p>
<h4 id="添加ceph-osd总数监控panel"><a href="#添加ceph-osd总数监控panel" class="headerlink" title="添加ceph osd总数监控panel"></a>添加ceph osd总数监控panel</h4><p>在状态row中创建一个singlestat的panel，General修改名字，metrics配置数据源  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_86.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_87.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_88.png">   </p>
<h4 id="添加ceph-osd-up数监控panel"><a href="#添加ceph-osd-up数监控panel" class="headerlink" title="添加ceph osd up数监控panel"></a>添加ceph osd up数监控panel</h4><p>在状态row中创建一个singlestat的panel，General修改名字，metrics配置数据源</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_89.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_90.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_91.png">   </p>
<h4 id="添加ceph总容量panel"><a href="#添加ceph总容量panel" class="headerlink" title="添加ceph总容量panel"></a>添加ceph总容量panel</h4><p>在容量row中创建一个singlestat的panel，General修改名字，metrics配置数据源<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_92.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_93.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_94.png">   </p>
<p>注意单位</p>
<h4 id="添加ceph剩余容量panel"><a href="#添加ceph剩余容量panel" class="headerlink" title="添加ceph剩余容量panel"></a>添加ceph剩余容量panel</h4><p>在容量row中创建一个singlestat的panel，General修改名字，metrics配置数据源<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_95.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_96.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_97.png">   </p>
<h4 id="添加ceph已用容量panel"><a href="#添加ceph已用容量panel" class="headerlink" title="添加ceph已用容量panel"></a>添加ceph已用容量panel</h4><p>在容量row中创建一个singlestat的panel，General修改名字，metrics配置数据源</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_98.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_99.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_100.png">   </p>
<p>注意单位</p>
<h4 id="添加ceph容量使用率-饼图-panel"><a href="#添加ceph容量使用率-饼图-panel" class="headerlink" title="添加ceph容量使用率(饼图)panel"></a>添加ceph容量使用率(饼图)panel</h4><p>在容量row中创建一个 Pie Chart 的panel，General修改名字，metrics配置数据源<br>已经使用和剩余可用百分比<br>利用饼图展示已用空间占总空间的比例<br>新建个panel，类型选择pie chart<br>general设置名字  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_101.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_102.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_104.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_103.png">   </p>
<h4 id="添加pool容量使用率-已compute-pool为例-panel"><a href="#添加pool容量使用率-已compute-pool为例-panel" class="headerlink" title="添加pool容量使用率(已compute pool为例)panel"></a>添加pool容量使用率(已compute pool为例)panel</h4><p>在容量row中创建一个 Pie Chart 的panel，General修改名字，metrics配置数据源  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_105.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_106.png">   </p>
<p>option这里根刚才一样<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_107.png">   </p>
<p>另外几个pool根这个一样，需要注意的是 where name里面的pool name要更改。</p>
<h4 id="添加pool池的使用趋势图-以compute-pool为例-panel"><a href="#添加pool池的使用趋势图-以compute-pool为例-panel" class="headerlink" title="添加pool池的使用趋势图(以compute pool为例)panel"></a>添加pool池的使用趋势图(以compute pool为例)panel</h4><p>在pool池中的使用情况row中创建Graph panel</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_108.png">  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_109.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_110.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_111.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_112.png">   </p>
<p>显示30天的数据<br>剩下pool的使用情况图根上面有一，只需要修改metrics里面的pool名就可以了  </p>
<h4 id="添加性能panel"><a href="#添加性能panel" class="headerlink" title="添加性能panel"></a>添加性能panel</h4><p>在性能row中创将graph panel  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_113.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_114.png">   </p>
<p>注意这里的datasource是 zabbix  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_115.png">   </p>
<p>单位选择IOPS</p>
<h4 id="添加ceph-pg-read-rate-panel"><a href="#添加ceph-pg-read-rate-panel" class="headerlink" title="添加ceph pg read rate panel"></a>添加ceph pg read rate panel</h4><p>在性能row中创将graph panel<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_116.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_117.png">   </p>
<p>单位为bites<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_118.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_119.png">   </p>
<p>显示一天的</p>
<h4 id="添加ceph-pg-write-rate-panel"><a href="#添加ceph-pg-write-rate-panel" class="headerlink" title="添加ceph pg write rate panel"></a>添加ceph pg write rate panel</h4><p>在性能row中创将graph panel<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_120.png">   </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_121.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_122.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_123.png">   </p>
<p>最终效果  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_124.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_4_125.png">   </p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>从零开始搭建云平台监控(五)监控自动化部署</title>
    <url>/2017/10/16/monitor_5/</url>
    <content><![CDATA[<p>前面四篇文章讲的更多是说这套监控平台是如何一步搭建起来的，但在实际生产中，我们要给客户搭建很多套私有云环境，不可能每个环境手动去装一边，因为zabbix的监控项和grafana的dashboard都可以做成模板导入。剩下就是一些软件安装和zabbix-agent的配置问题了，目前我采用的做法是将monitor-server封装成一个镜像，镜像里面将软件都安装好了monitor-server上有写好的ansible的playbook，agent端通，过monitor-server上的ansible的playbook去推送安装。</p>
<p>ansible是一个python写的基于ssh的轻量级的自动化运维工具，它有以下优点<br>优点：<br>(1)、轻量级，无需在客户端安装agent，更新时，只需在操作机上进行一次更新即可；<br>(2)、批量任务执行可以写成脚本，而且不用分发到远程就可以执行；<br>(3)、使用python编写，维护更简单，ruby语法过于复杂；<br>(4)、支持sudo；<br>(5)、基于ssh无需agent端就可以实现自动化配置。  </p>
<p>缺点：<br>(1)、ansible默认才用轮询的方式，如果节点数量一多，撑不住，相比较基于c&#x2F;s架构的saltstack 中间还有消息队列辅助在大规模集群中性能确实弱。</p>
<p>下面主要介绍自己写的playbook，目录结构如下  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/etc/ansible/</span><br><span class="line">|-- ansible.cfg</span><br><span class="line">|-- hosts</span><br><span class="line">|-- readme</span><br><span class="line">|-- roles</span><br><span class="line">|   |-- ceph</span><br><span class="line">|   |   |-- handlers</span><br><span class="line">|   |   |   `-- main.yml</span><br><span class="line">|   |   |-- tasks</span><br><span class="line">|   |   |   `-- main.yml</span><br><span class="line">|   |   |-- templates</span><br><span class="line">|   |   |   `-- zabbix_agentd.conf.j2</span><br><span class="line">|   |   `-- vars</span><br><span class="line">|   |       `-- main.yml</span><br><span class="line">|   |-- common</span><br><span class="line">|   |   |-- 1</span><br><span class="line">|   |   |-- handlers</span><br><span class="line">|   |   |   `-- main.yml</span><br><span class="line">|   |   |-- tasks</span><br><span class="line">|   |   |   |-- 1</span><br><span class="line">|   |   |   `-- main.yml</span><br><span class="line">|   |   |-- templates</span><br><span class="line">|   |   |   `-- zabbix_agentd.conf.j2</span><br><span class="line">|   |   `-- vars</span><br><span class="line">|   |       `-- main.yml</span><br><span class="line">|   `-- telegraf</span><br><span class="line">|       |-- handlers</span><br><span class="line">|       |   `-- main.yml</span><br><span class="line">|       |-- tasks</span><br><span class="line">|       |   `-- main.yml</span><br><span class="line">|       |-- templates</span><br><span class="line">|       |   `-- telegraf.conf.j2</span><br><span class="line">|       `-- vars</span><br><span class="line">|           `-- main.yml</span><br><span class="line">|-- server_ip.sh</span><br><span class="line">|-- site.retry</span><br><span class="line">|-- site.yml</span><br><span class="line">`-- zabbix_agent.tar</span><br></pre></td></tr></table></figure>
<p>我这里分为3个roles ，ceph、common、telegraf<br>ceph主要做的操作有<br>1、拷贝配置文件  </p>
<p>common 主要做的操作有<br>1、环境初始化<br>2、拷贝软件包到对应节点<br>3、解压tar包<br>4、安装软件<br>5、修改对应的配置文件<br>6、修改执行权限<br>7、修改防火墙开放端口<br>8、启动服务、设置开机自启  </p>
<p>telegraf主要做的操作有<br>1、安装telegraf<br>2、修改telegraf systemd的启动用户<br>3、拷贝telegraf配置文件<br>4、重启telegraf并设置开机启动  </p>
<p>因为每个节点所对应的角色不一样，ansible所跑的脚本也是不一样的，所以定义这3个roles，服务器的角色通过hosts文件控制  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_5_1.png"></p>
<p>定义了三组角色，控制节点、计算节点、存储节点、融合节点、每组角色设置了不同的metadata，这个metadata是通过templates&#x2F;zabbix_agentd.conf.j2传给对应节点zabbix-agent里面的HostMetadata参数，然后zabbix-server根据不同的metadata去调用不同的模板。定义group是为了在task里面调用不同的命令。  </p>
<p>通过site.yml来控制不同的host调用不同的role<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_5_2.png"></p>
<p>可以看见common的hosts是all表示在hosts定义的所有组都会调用common， telegraf roles是只有control的host会去调用 ceph roles是storage的 hosts去调用</p>
<p>每个roles下面分handlers、tasks、templates、vars 其中，<br>handlers：也是task，但只有其关注的条件满足时，才会被触发执行 ，  </p>
<p>templates：定义的是配置文件模板，比如我们这里定义好了zabbix_agent、和telegraf的配置文件模板  </p>
<p>vars：定义变量，我在templates里面定义了zabbix_agent变量，里面的server 和server_active我都是以变量方式存储的，然后在执行ansible前先把变量改好，这样在不同的环境配置文件里的ip也会根着变。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/monitor_5_3.png"></p>
<p>项目代码先以开源：<br><a href="https://github.com/wanshaoyuan/ansible_monitor">https://github.com/wanshaoyuan/ansible_monitor</a></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>常见网络分析工具使用（持续更新整理)</title>
    <url>/2018/09/24/network_tools/</url>
    <content><![CDATA[<h3 id="概述："><a href="#概述：" class="headerlink" title="概述："></a>概述：</h3><p>在日常工作中经常会遇到一些网络问题，如何去排查网络问题，需要针对性的运用一些工具，以下就是常用的一些网络工具的介绍和简单使用教程，涉及从网络端口探测到网络质量检测、网络数据包分析等。</p>
<h3 id="nc工具"><a href="#nc工具" class="headerlink" title="nc工具"></a>nc工具</h3><p>nc是netcat的简写，非常小巧，功能丰富强大，经常用在各种网络分析中。</p>
<h4 id="主要作用："><a href="#主要作用：" class="headerlink" title="主要作用："></a>主要作用：</h4><p>1、实现任意TCP&#x2F;UDP端口的侦听，nc可以作为server以TCP或UDP方式侦听指定端口。<br>2、端口的探测，nc可以作为client发起TCP或UDP连接。<br>3、机器之间传输文件。</p>
<h4 id="nc常用参数"><a href="#nc常用参数" class="headerlink" title="nc常用参数"></a>nc常用参数</h4><p> -l：用于指定nc将处于侦听模式。指定该参数，则意味着nc被当作server，侦听并接受连接，而非向其它地址发起连接。<br> -s ：指定发送数据的源IP地址，适用于多网卡机<br> -u：指定nc使用UDP协议，默认为TCP<br> -v：输出详细信息<br>-w：超时时间单位为秒<br>-z：表示zero，表示扫描时不发送任何数据</p>
<h4 id="常见用法"><a href="#常见用法" class="headerlink" title="常见用法"></a>常见用法</h4><p>启动任意端口侦听</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nc -l port_number (tcp)</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/network_tools_1.png"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nc -ul port_name （udp)</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/network_tools_2.png"></p>
<p>端口探测</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nc -vzw 2 47.104.162.0 443 (tcp)</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/network_tools_3.png"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nc -vuzw 2 47.104.162.0 9000(udp)</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/network_tools_4.png"></p>
<p>-v 显示详细过程<br>-z只扫描不发送数据<br>-w超时时间（单位为秒）<br>-u表示udp默认不指定为TCP</p>
<p>传输文件</p>
<p>nc传输文件根传统scp和rsync相比比较方便不用输密码，只要对端有通过nc启动对应服务有暴露端口就可以，还可以通过压缩方式传输文件，提高传输效率。<br>方式一：<br>先接收端启动nc准备接收文件，比如我们传输1.py这个文件<br>接收端执行命令，这里命令意思就是监听TCP 9000端口，将收取到的文件重定向到当前目录sendmail.py文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nc -l 9000 &gt; sendmail.py</span><br></pre></td></tr></table></figure>

<p>发送端执行发送命令</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nc 47.104.162.0 9000 &lt; sendmai.py</span><br></pre></td></tr></table></figure>
<p>将sendmai.py重定向发送到47.104.162.0 TCP 9000端口<br>发送完毕后，接收端和发送端会自动退出监听。</p>
<p>方式二：<br>前面那种方式是需要先将接受端启动侦听，然后发送端在将文件通过接受端侦听的端口发送过去，方式二是反过来的是发送端先启动发送指令，然后接受端在接收<br>发送端执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nc -l 9000 &lt; sendmai.py</span><br></pre></td></tr></table></figure>
<p>表示通过本机9000端口发送<br>接收端</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nc 47.104.162.1 9000 &gt; sendmai.py</span><br></pre></td></tr></table></figure>
<p>从47.104.162.1 9000端口或取文件，然后写入到sendmai.py文件<br>方式三：<br>压缩传输，可以将整个目录文件传输到对端<br>接收端启动，管道后必须接的是-不能是其他</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nc -l 9000 |tar xfvz -</span><br></pre></td></tr></table></figure>
<p>发送端将目录下文件全部打包压缩发送到47.104.162.0 9000端口</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tar cfz - * |nc 47.104.162.0 9000</span><br></pre></td></tr></table></figure>
<p>例<br>在发送发端创建test目录并创建文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir /root/test</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">touch ~/test &#123;1..100&#125;</span><br></pre></td></tr></table></figure>
<p>接收端执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nc -l 9000 |tar xfvz -</span><br></pre></td></tr></table></figure>
<p>发送端切换test目录，执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tar cfz - * |nc 47.104.162.0 9000</span><br></pre></td></tr></table></figure>

<h3 id="nmap工具"><a href="#nmap工具" class="headerlink" title="nmap工具"></a>nmap工具</h3><p>全称Network Mapper是linux系统下网络扫描和嗅探工具。</p>
<h4 id="主要作用"><a href="#主要作用" class="headerlink" title="主要作用"></a>主要作用</h4><p>1、端口扫描，并自动分析端口对应的服务和对应的软件版本<br>2、主机存活检测</p>
<h4 id="nmap常用参数"><a href="#nmap常用参数" class="headerlink" title="nmap常用参数"></a>nmap常用参数</h4><p>-s：扫描模式<br>-S：指定源IP<br>-p：指定端口范围扫描<br>-sV：暴露端口的软件的版本<br>-n：不做dns解析<br>-sn：不做端口扫描只检测主机存活<br>-e ：指定网卡<br>-Pn：跳过主机存活性检测<br>-Su：仅扫描UDP端口<br>-sS：半连接方式扫描，不需要完成完整的tcp三次握手简单来说就是一个TCP_SYN扫描，扫描器对客户端发送syn包，得到ack响应，扫描器发送一个RST包断开连接，这样就不用完成三次握手，建立正常的TCP连接。</p>
<h4 id="常见用法："><a href="#常见用法：" class="headerlink" title="常见用法："></a>常见用法：</h4><p>端口扫描</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nmap ip_addr</span><br></pre></td></tr></table></figure>
<p>直接不接任何参数扫描，默认户将tcp，udp这些全部列出来</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/network_tools_5.png"></p>
<p>加了-sS以后就是嗅探tcp时效率高点，其他都一样<br>可以对域名进行扫描也可以对IP进行扫描</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nmap  -sS ip_addr</span><br></pre></td></tr></table></figure>
<p>端口状态<br>open：端口处于开放状态，可以建立正常的连接。<br>closed：端口处于关闭状态，没有应用在监听这个端口。<br>Filtered：发出去的探测报文被过滤了，返回的ICMP报文状态是目的主机不可达，nmap无法确定端口状态，同时端口无法被nmap访问。<br>Unfiltered：未被过滤状态，nmap不确定端口状态，但端口能被nmap访问。<br>open|filtered：无法确定端口是open还是filtered状态，比如常见的开放的端口不响应发出去的请求就是这种状态。</p>
<p>按照上述命令扫描端口默认是扫描1-1024+nmap-server数据库里面收录的一些已知应用端口，所以要扫描大范围需要使用-p参数如</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nmap ip_addr -p1-65535</span><br></pre></td></tr></table></figure>
<p>软件版本检测<br>列出扫描出的端口对应的应用的软件版本信息，这里就直接显示了openssh的版本和对应的操作系统信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nmap -sV 172.31.164.57</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/network_tools_6.png"></p>
<p>通过ping扫描指定范围内机器是否up<br>经常要测试一个地址范围内ip的联通性就可以使用这个命令</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nmap -sn 172.31.164.57-60</span><br></pre></td></tr></table></figure>

<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/network_tools_7.png"></p>
<p>伪装源ip检测<br>在用些场景我们需要伪装我们的源IP去进行嗅探，或我们的源IP被防火墙拉入了黑名单此时可以通过伪装ip的方式进行检测，</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nmap -e eth0 10.0.1.161 -S 10.0.1.167 -Pn</span><br></pre></td></tr></table></figure>
<p>使用-S伪装自己源地址进行扫描的话，你必须另外使用-e 指定网卡和-Pn参数才能伪装</p>
<h3 id="iperf工具"><a href="#iperf工具" class="headerlink" title="iperf工具"></a>iperf工具</h3><p>iperf是一款网络性能测试工具，主要用来测试网络之间的带宽和丢包率。</p>
<h4 id="主要作用-1"><a href="#主要作用-1" class="headerlink" title="主要作用"></a>主要作用</h4><p>1、网络带宽性能测试<br>2、网络丢包率测试（UDP关注点)  </p>
<h4 id="iperf3常用参数"><a href="#iperf3常用参数" class="headerlink" title="iperf3常用参数"></a>iperf3常用参数</h4><p>iperf3常用参数<br>-f, –format [bkmaBKMA]<br>格式化带宽数输出。支持的格式有：<br>‘b’ &#x3D; bits&#x2F;sec ‘B’ &#x3D; Bytes&#x2F;sec<br>‘k’ &#x3D; Kbits&#x2F;sec ‘K’ &#x3D; KBytes&#x2F;sec<br>‘m’ &#x3D; Mbits&#x2F;sec ‘M’ &#x3D; MBytes&#x2F;sec<br>‘g’ &#x3D; Gbits&#x2F;sec ‘G’ &#x3D; GBytes&#x2F;sec<br>‘a’ &#x3D; adaptive bits&#x2F;sec ‘A’ &#x3D; adaptive Bytes&#x2F;sec  </p>
<p>-i：设置每次报告之间的时间间隔，单位为秒。如果设置为非零值，就会按照此时间间隔输出测试报告。默认值为零。<br>-l,[KM]：设置读写缓冲区的长度。TCP方式默认为8KB，UDP方式默认为1470字节。<br>-m, –print_mss：输出TCP MSS值（通过TCP_MAXSEG支持）。MSS值一般比MTU值小40字节。<br>-p：设置端口，与服务器端的监听端口一致。默认是5001端口，与tcp的一样。<br>-u：使用UDP方式而不是TCP方式(iperf3不需要)<br>-b:  指定带宽（测试UDP时使用）<br>-B：绑定到主机的多个地址中的一个。对于客户端来说，这个参数设置了出站接口。对于服务器端来说，这个参数设置入栈接口。这个参数只用于具有多网络接口的主机。在Iperf的UDP模式下，此参数用于绑定和加入一个多播组。使用范围在224.0.0.0至239.255.255.255的多播地址。参考-T参数。<br>-M, –mss #[KM}：通过TCP_MAXSEG选项尝试设置TCP最大信息段的值。MSS值的大小通常是TCP&#x2F;IP头减去40字节。在以太网中，MSS值 为1460字节（MTU1500字节）。许多操作系统不支持此选项。<br>-s：Server模式运行<br>-c：Client模式运行，后面接上连接server的ip<br>-P：测试数据流并发数量  </p>
<h4 id="iper3常见用法"><a href="#iper3常见用法" class="headerlink" title="iper3常见用法"></a>iper3常见用法</h4><p>测试性能<br>服务器端</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">iperf3 -s -i2  -p 5001</span><br></pre></td></tr></table></figure>
<p>客户端</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">iperf3 -i 1 -c x.x.x.x -t 60</span><br></pre></td></tr></table></figure>
<p>-i多久显示一次，单位S<br>-c后跟服务器IP<br>-t持续多久，单位秒</p>
<p>需要在主机上打开5001端口</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">iptables -I INPUT -p tcp --dport 5001 -j ACCEPT</span><br></pre></td></tr></table></figure>
<p>测试，可以发现，每秒都会显示网络带宽，这个网络是个千兆网络,默认是以4个并发进行测试的，如果发现带宽根实际物理带宽差距太大可以调整并发数在测试。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/network_tools_8.png"></p>
<h3 id="tcpdump工具"><a href="#tcpdump工具" class="headerlink" title="tcpdump工具"></a>tcpdump工具</h3><p>tcpdump是linux下最常用的抓包分析工具。</p>
<h4 id="主要作用：-1"><a href="#主要作用：-1" class="headerlink" title="主要作用："></a>主要作用：</h4><p>1、抓取网卡数据包<br>tcpdump参数<br>-i：指定监听的端口<br>-w：将结果保存到指定位置，可以保存成cap文件用wireshark做数据包分析<br>-vnn：以ip和端口方式输出详细信息<br>-c：抓取包的数量，默认不指定会一直不断的抓直到ctrl+c<br>-nn：以ip和端口方式显示，不以主机名和服务名方式显示，可以看起来直观<br>-v：输出详细信息<br>-X 直接输出package data数据，默认不设置，只能通过-w指定文件进行输出</p>
<h4 id="常见用法：-1"><a href="#常见用法：-1" class="headerlink" title="常见用法："></a>常见用法：</h4><p>监视指定网络接口的数据包</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcpdump -i eth1</span><br></pre></td></tr></table></figure>
<p>如果不指定网卡，默认tcpdump只会监视第一个网络接口，一般是eth0，下面的例子都没有指定网络接口。　</p>
<p>监视指定主机的数据包(如rke-node1)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcpdump host rke-node1</span><br></pre></td></tr></table></figure>
<p>输出test-1 与 test-2或者与 test-3 之间通信的数据包</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcpdump host test-1 and \( test-2 or test-3 \)</span><br></pre></td></tr></table></figure>
<p>打印test-1与任何其他主机之间通信的IP 数据包, 但不包括与test-2之间的数据包.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcpdump host test-1 and ! test-2</span><br></pre></td></tr></table></figure>
<p>截获源ip为xxx.xxx.xxx.xxx发送的所有数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcpdump -i eth0 src host xxx.xxx.xxx.xxx</span><br></pre></td></tr></table></figure>
<p>监视所有送到目的主机的数据包</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcpdump -i eth0 dst host ipaddr or hostname</span><br></pre></td></tr></table></figure>
<p>抓取100个数据包信息保存为cap文件可以用wireshark分析</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcpdump  -i eth0 -c 100 -w /tmp/capture2.cap</span><br></pre></td></tr></table></figure>
<p>抓取指定协议的数据包</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcpdump -i eth0 -vnn icmp -w /tmp/1</span><br></pre></td></tr></table></figure>
<p>抓取udp协议的123端口的数据包</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcpdump udp port 123</span><br></pre></td></tr></table></figure>
<p>抓取host为192.168.1.1并且udp端口为23的数据包</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcpdump tcp port 23 and host 192.168.1.1</span><br></pre></td></tr></table></figure>
<h3 id="netstat工具"><a href="#netstat工具" class="headerlink" title="netstat工具"></a>netstat工具</h3><p>netstat是一款本机网络状态分析工具  </p>
<h4 id="作用："><a href="#作用：" class="headerlink" title="作用："></a>作用：</h4><p>分析本机端口监听状态，连接状态，连接数等</p>
<h4 id="netstat常用参数"><a href="#netstat常用参数" class="headerlink" title="netstat常用参数"></a>netstat常用参数</h4><p>-a :显示所有状态连接<br>-n：显示ip地址<br>-t：显示TCP协议连接<br>-l：显示LISTEN状态连接<br>-u：显示UDP状态连接<br>-p：显示socket的PID和程序名<br>-i：显示网卡信息  </p>
<p>TCP连接的常见状态<br>LISTENING：侦听状态<br>ESTABLISHED：建立连接状态<br>CLOSE_WAIT：等待关闭状态。<br>TIME_WAIT：主动关闭连接的一方会进入TIME_WAIT状态。为了防止最终的ACK包未到达，活动的TCP连接等待2MSL时间释放连接。<br>SYN_SENT：表示请求连接，发送SYN报文。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/network_tools_9.png">  </p>
<h4 id="常见用法：-2"><a href="#常见用法：-2" class="headerlink" title="常见用法："></a>常见用法：</h4><p>列出本机所有tcp连接  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">netstat -at</span><br></pre></td></tr></table></figure>
<p>列出本机所有udp连接  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">netstat -au</span><br></pre></td></tr></table></figure>
<p>显示所有数据包的统计情况  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">netstat -s</span><br></pre></td></tr></table></figure>
<p>显示网络接口收发报情况  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">netstat -i</span><br></pre></td></tr></table></figure>

<p>列出本机LISTEN状态的TCP端口  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">netstat -lnt</span><br></pre></td></tr></table></figure>

<p>列出本机所有LISTEN状态的UDP端口  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">netstat -lnu</span><br></pre></td></tr></table></figure>


<p>统计产生TIME_WAIT最多的几个IP  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">netstat -ptan | grep TIME_WAIT |awk &#x27;&#123;print $5&#125;&#x27; |awk -F : &#x27;&#123;print $1&#125;&#x27;|sort |uniq -c|sort -r</span><br></pre></td></tr></table></figure>
<p><a href="https://my.oschina.net/u/1024767/blog/757465">https://my.oschina.net/u/1024767/blog/757465</a>  </p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenStack vlan模式下网络流量走向</title>
    <url>/2017/09/17/neutron_vlan/</url>
    <content><![CDATA[<h3 id="使用openvswitch-vlan的组网"><a href="#使用openvswitch-vlan的组网" class="headerlink" title="使用openvswitch+vlan的组网"></a>使用openvswitch+vlan的组网</h3><p>以生产网vlan为101和102为例在openstack上创建个私有网络，<br>创建了 OVS Integration bridge br-int。它的四个 Access 端口中，两个打上了内部 Tag 1，连接接入 network 1 的两个网卡；另两个端口的 VLAN Tag 为 2。<br>创建了一对 patch port，连接 br-int 和 br-eth0。<br>设置 br-int 中的 flow rules。对从 access ports 进入的数据帧，加上相应的 VLAN Tag，转发到 patch port；从 patch port 进入的数据帧，将 VLAN ID 101 修改为 1, 102 修改为 2，再转发到相应的 Access ports。<br>设置 br-eth0 中的 flow rules。从 patch port 进入的数据帧，将内部 VLAN ID 1 修改为 101，内部 VLAN ID 2 修改为 102，再从 eth1 端口发出。对从 eth1 进入的数据帧做相反的处理。</p>
<p>br-int上的local_id和vlan_id的转换是实现多租户的主要技术。</p>
<p>以下介绍在云平台三中情况数据流量的走向  </p>
<h4 id="相同物理机上的虚拟机"><a href="#相同物理机上的虚拟机" class="headerlink" title="相同物理机上的虚拟机"></a>相同物理机上的虚拟机</h4><p>vm1和vm2在同一个宿主机上同一个项目下同一个子网时，两个虚机之间的流量是不需要经过交换机的，直接通过ovs的br-int桥就可以做转发了,不同项目是不通的因为br-int上有local_id用于多租户隔离。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/vlan_1.png"></p>
<h4 id="不同物理机上的虚拟机"><a href="#不同物理机上的虚拟机" class="headerlink" title="不同物理机上的虚拟机"></a>不同物理机上的虚拟机</h4><p>vm1和vm2在同一个宿主机上同一个项目下同一个子网时，两个虚拟机之间的流量是需要通过物理交换机进行转发的<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/vlan_2.png"></p>
<p>vm的流量首先经过tap网卡到qbr桥上，qbr桥是linux-bridge桥，曾经的ovs不支持安全组的实现所以openstack只能加一个qbr桥在上面通过iptables来实现安全组，(mitaka版本中已经没有了因为ovs已经可以通过openflow来实现了),通过veth口将br-int和qbr桥连接起来，流量到br-int上，br-int上有对应的ovs规则转发到br-eth0上，，r-eth0 中的 flow rules。从 patch port 进入的数据帧，将内部 ocal_id 修改为vlan_id，再从 eth0 端口发出。通过交换机到达另外一个计算机节点。</p>
<h4 id="不同租户的不同子网的通信或虚机根外部通信"><a href="#不同租户的不同子网的通信或虚机根外部通信" class="headerlink" title="不同租户的不同子网的通信或虚机根外部通信"></a>不同租户的不同子网的通信或虚机根外部通信</h4><p>需要经过网络节点，由网络节点的qroute做三层转发，也可以直接使用物理交换机。不同子网的通信，虚机会将流量丢给默认网关然后到达网络节点的qrouter通过qrouter的三层转发通讯。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/vlan_3.png"></p>
<p>使用内部vlan号是为了实现多租户网络隔离，也就是说计算节点上将数据包隔离通过br-int的loca_id，假设没有这个local_id的话，租户a的192.168.1.0&#x2F;24和租户b的192.168.1.0&#x2F;24在同一个计算节点的同一个br-int的网桥上，如果没有local_id隔离，网络就串了。这种是openvswitch的实现方式，</p>
<h3 id="linuxbridge的实现方式"><a href="#linuxbridge的实现方式" class="headerlink" title="linuxbridge的实现方式"></a>linuxbridge的实现方式</h3><p>比如租户a的私有网络是vlan100，租户b的私户网络是vlan200，然后linuxbridge会将在这个计算节点上创建两个网桥。然后用vconfig将对应的网卡创建出子接口，挂到刚刚创的桥上，在将对应的虚机tap网卡挂载到对应的桥上。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/vlan_4.png"></p>
]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title>Neuvector介绍和部署</title>
    <url>/2022/03/17/neuvector_1/</url>
    <content><![CDATA[<h3 id="Neuvector介绍"><a href="#Neuvector介绍" class="headerlink" title="Neuvector介绍"></a>Neuvector介绍</h3><p>NeuVector 是最早开发 Docker&#x2F;Kubernetes 安全产品的公司，是 Kubernetes 网络安全的领导<br>者，NeuVector 致力于保障企业级容器平台安全，产品适用于各种云环境、跨云或者本地部署等容器生产环境。NeuVector 提供实时深入的容器网络可视化、东西向容器网络监控、主动隔离和保护、容器主机安全以及容器内部安全。和容器管理平台无缝集成并且实现应用级容器安全的自动化。<br>2021年SUSE收购Neuvector，并将其开源。</p>
<p>项目地址：<br><a href="https://github.com/neuvector/neuvector">https://github.com/neuvector/neuvector</a></p>
<h4 id="架构解析"><a href="#架构解析" class="headerlink" title="架构解析"></a>架构解析</h4><p><img src="https://open-docs.neuvector.com/user/pages/01.basics/01.overview/architecture.png"></p>
<p>NeuVector本身包含Controller、Enforcer、Manager、Scanner、Updater模块。</p>
<p>Controller：整个Neuvector的控制模块，API入口，包括配置下发，高可用主要考虑Controller的HA，通常建议部署3个Controller模块组成集群。</p>
<p>Enforcer：主要用于安全策略部署下发和执行，DaemonSet类型会在每个节点部署。<br>Manager：提供web-UI(仅HTTPS)和CLI控制台，供用户管理NeuVector。<br>Scanner:对节点、容器、Kubernetes、镜像进行CVE漏洞扫描<br>Updater:cronjob，用于定期更新CVE漏洞库</p>
<h4 id="功能介绍"><a href="#功能介绍" class="headerlink" title="功能介绍"></a>功能介绍</h4><ul>
<li>安全漏洞扫描</li>
<li>容器网络流量可视化</li>
<li>网络安全策略定义</li>
<li>L7防火墙</li>
<li>CICD安全扫描</li>
<li>合规分析<br>本篇文档更多侧重安装部署，实际功能介绍在后续文章进行深入介绍</li>
</ul>
<h3 id="NeuVector安装"><a href="#NeuVector安装" class="headerlink" title="NeuVector安装"></a>NeuVector安装</h3><p>安装环境<br>软件版本：<br>OS：Ubuntu18.04<br>Kubernetes：1.20.14<br>Rancher：2.5.12<br>Docker：19.03.15<br>NeuVector：5.0.0-b1</p>
<h4 id="快速部署"><a href="#快速部署" class="headerlink" title="快速部署"></a>快速部署</h4><p>创建namespace</p>
<pre><code>kubectl create namespace neuvector
</code></pre>
<p>部署CRD(Kubernetes 1.19+版本)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/crd-k8s-1.19.yaml</span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/waf-crd-k8s-1.19.yaml</span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/dlp-crd-k8s-1.19.yaml</span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/admission-crd-k8s-1.19.yaml</span><br></pre></td></tr></table></figure>

<p>部署CRD(Kubernetes 1.18或更低版本)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/crd-k8s-1.16.yaml</span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/waf-crd-k8s-1.16.yaml</span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/dlp-crd-k8s-1.16.yaml</span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/admission-crd-k8s-1.16.yaml</span><br></pre></td></tr></table></figure>

<p>配置RBAC</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create clusterrole neuvector-binding-app --verb=get,list,watch,update --resource=nodes,pods,services,namespaces</span><br><span class="line">kubectl create clusterrole neuvector-binding-rbac --verb=get,list,watch --resource=rolebindings.rbac.authorization.k8s.io,roles.rbac.authorization.k8s.io,clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io</span><br><span class="line">kubectl create clusterrolebinding neuvector-binding-app --clusterrole=neuvector-binding-app --serviceaccount=neuvector:default</span><br><span class="line">kubectl create clusterrolebinding neuvector-binding-rbac --clusterrole=neuvector-binding-rbac --serviceaccount=neuvector:default</span><br><span class="line">kubectl create clusterrole neuvector-binding-admission --verb=get,list,watch,create,update,delete --resource=validatingwebhookconfigurations,mutatingwebhookconfigurations</span><br><span class="line">kubectl create clusterrolebinding neuvector-binding-admission --clusterrole=neuvector-binding-admission --serviceaccount=neuvector:default</span><br><span class="line">kubectl create clusterrole neuvector-binding-customresourcedefinition --verb=watch,create,get,update --resource=customresourcedefinitions</span><br><span class="line">kubectl create clusterrolebinding  neuvector-binding-customresourcedefinition --clusterrole=neuvector-binding-customresourcedefinition --serviceaccount=neuvector:default</span><br><span class="line">kubectl create clusterrole neuvector-binding-nvsecurityrules --verb=list,delete --resource=nvsecurityrules,nvclustersecurityrules</span><br><span class="line">kubectl create clusterrolebinding neuvector-binding-nvsecurityrules --clusterrole=neuvector-binding-nvsecurityrules --serviceaccount=neuvector:default</span><br><span class="line">kubectl create clusterrolebinding neuvector-binding-view --clusterrole=view --serviceaccount=neuvector:default</span><br><span class="line">kubectl create rolebinding neuvector-admin --clusterrole=admin --serviceaccount=neuvector:default -n neuvector</span><br><span class="line">kubectl create clusterrole neuvector-binding-nvwafsecurityrules --verb=list,delete --resource=nvwafsecurityrules</span><br><span class="line">kubectl create clusterrolebinding neuvector-binding-nvwafsecurityrules --clusterrole=neuvector-binding-nvwafsecurityrules --serviceaccount=neuvector:default</span><br><span class="line">kubectl create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=list,delete --resource=nvadmissioncontrolsecurityrules</span><br><span class="line">kubectl create clusterrolebinding neuvector-binding-nvadmissioncontrolsecurityrules --clusterrole=neuvector-binding-nvadmissioncontrolsecurityrules --serviceaccount=neuvector:default</span><br><span class="line">kubectl create clusterrole neuvector-binding-nvdlpsecurityrules --verb=list,delete --resource=nvdlpsecurityrules</span><br><span class="line">kubectl create clusterrolebinding neuvector-binding-nvdlpsecurityrules --clusterrole=neuvector-binding-nvdlpsecurityrules --serviceaccount=neuvector:default</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>检查是否有以下RBAC对象</p>
<pre><code>kubectl get clusterrolebinding  | grep neuvector
kubectl get rolebinding -n neuvector | grep neuvector



kubectl get clusterrolebinding  | grep neuvector

neuvector-binding-admission                            ClusterRole/neuvector-binding-admission                            44h
neuvector-binding-app                                  ClusterRole/neuvector-binding-app                                  44h
neuvector-binding-customresourcedefinition             ClusterRole/neuvector-binding-customresourcedefinition             44h
neuvector-binding-nvadmissioncontrolsecurityrules      ClusterRole/neuvector-binding-nvadmissioncontrolsecurityrules      44h
neuvector-binding-nvsecurityrules                      ClusterRole/neuvector-binding-nvsecurityrules                      44h
neuvector-binding-nvwafsecurityrules                   ClusterRole/neuvector-binding-nvwafsecurityrules                   44h
neuvector-binding-rbac                                 ClusterRole/neuvector-binding-rbac                                 44h
neuvector-binding-view                                 ClusterRole/view                                                   44h



kubectl get rolebinding -n neuvector | grep neuvector
neuvector-admin         ClusterRole/admin            44h
</code></pre>
<p>部署NeuVector<br>底层runtime为Docker</p>
<pre><code>kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/neuvector-docker-k8s.yaml
</code></pre>
<p>底层runtime为containerd（对于k3s和rke2可以使用此yaml文件）</p>
<pre><code>kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/neuvector-containerd-k8s.yaml
</code></pre>
<p>1.21以下的Kubernetes版本会提示以下错误，将yaml文件下载将batch&#x2F;v1修改为batch&#x2F;v1beta1</p>
<pre><code>error: unable to recognize &quot;https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/neuvector-docker-k8s.yaml&quot;: no matches for kind &quot;CronJob&quot; in version &quot;batch/v1&quot;
</code></pre>
<p>1.20.x cronjob还处于beta阶段没有正式GA,1.21版本开始cronjob才进入正式版。</p>
<p>默认部署web-ui使用的是loadblance类型的Service，为了方便访问修改为NodePort，也可以通过Ingress对外提供服务</p>
<pre><code>kubectl patch  svc neuvector-service-webui  -n neuvector --type=&#39;json&#39; -p &#39;[&#123;&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;/spec/type&quot;,&quot;value&quot;:&quot;NodePort&quot;&#125;,&#123;&quot;op&quot;:&quot;add&quot;,&quot;path&quot;:&quot;/spec/ports/0/nodePort&quot;,&quot;value&quot;:30888&#125;]&#39;
</code></pre>
<p>访问https:&#x2F;&#x2F;node_ip:30888</p>
<p>默认密码为admin&#x2F;admin</p>
<p>点击头像旁的My profile页面进入设置页面，设置密码和语言<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector-1.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector-2.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector-3.png"></p>
<h4 id="Helm部署"><a href="#Helm部署" class="headerlink" title="Helm部署"></a>Helm部署</h4><p>添加repo</p>
<pre><code>helm repo add neuvector https://neuvector.github.io/neuvector-helm/



helm search repo neuvector/core
</code></pre>
<p>创建namespace</p>
<pre><code>kubectl create namespace neuvector
</code></pre>
<p>创建ServiceAccount</p>
<pre><code>kubectl create serviceaccount neuvector -n neuvector
</code></pre>
<p>helm安装</p>
<pre><code>helm install neuvector --namespace neuvector neuvector/core  --set registry=docker.io  --set tag=5.0.0-preview.1 --set=controller.image.repository=neuvector/controller.preview --set=enforcer.image.repository=neuvector/enforcer.preview --set manager.image.repository=neuvector/manager.preview --set cve.scanner.image.repository=neuvector/scanner.preview --set cve.updater.image.repository=neuvector/updater.preview 
</code></pre>
<p>Helm-chart参数查看<br><a href="https://github.com/neuvector/neuvector-helm/tree/master/charts/core">https://github.com/neuvector/neuvector-helm/tree/master/charts/core</a></p>
<h3 id="高可用架构设计"><a href="#高可用架构设计" class="headerlink" title="高可用架构设计"></a>高可用架构设计</h3><p><img src="https://open-docs.neuvector.com/user/pages/01.basics/01.overview/architecture.png"></p>
<p>NeuVector-HA主要需要考虑Controller模块的HA，只要有一个Controller处于打开状态，所有数据都将在3个副本之间之间同步。<br>Controller数据主要存储在 &#x2F;var&#x2F;neuvector&#x2F; 目录中，但出现POD重建或集群重新部署时，会自动从此目录加载备份文件，进行集群恢复。</p>
<h4 id="部署策略"><a href="#部署策略" class="headerlink" title="部署策略"></a>部署策略</h4><p>NeuVector官方提供四种HA部署模式<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector-14.png"></p>
<p>方式一：不进行任何调度限制，由Kubernetes进行自由调度管理管理。</p>
<p>方式二：NeuVector control组件(manager,controller）+enforce、scanner组件配置调度label限制和污点容忍，与Kubernetes master节点部署一起。</p>
<p>方式三：给Kubernetes集群中通过Taint方式建立专属的NeuVector节点，只允许NeuVector control组件部署。</p>
<p>方式四：NeuVector control组件(manager,controller）配置调度label限制和污点容忍，与Kubernetes master节点部署一起。k8s-master不部署enforce和scanner组件，意味着master节点不在接受扫描和策略下发。</p>
<p>以方式二为例，进行部署<br>给master节点打上特定标签</p>
<pre><code>kubectl label nodes nodename nvcontroller=true
</code></pre>
<p>获取节点Taint</p>
<pre><code>kubectl get node nodename -o yaml|grep -A 5 taint
</code></pre>
<p>以rancher部署的节点master节点为例</p>
<pre><code> taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/controlplane
    value: &quot;true&quot;
  - effect: NoExecute
    key: node-role.kubernetes.io/etcd
</code></pre>
<p>编辑部署的yaml给NeuVector-control组件（manager,controller）添加nodeSelector和tolerations给enforce、scanner组件只添加tolerations。</p>
<p>例如以manager组件为例：</p>
<pre><code>kind: Deployment
metadata:
  name: neuvector-manager-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-manager-pod
  replicas: 1
  template:
    metadata:
      labels:
        app: neuvector-manager-pod
    spec:
      nodeSelector:
        nvcontroller: &quot;true&quot;
      containers:
        - name: neuvector-manager-pod
          image: neuvector/manager.preview:5.0.0-preview.1
          env:
            - name: CTRL_SERVER_IP
              value: neuvector-svc-controller.neuvector
      restartPolicy: Always
      tolerations:
      - effect: NoSchedule
        key: &quot;node-role.kubernetes.io/controlplane&quot;
        operator: Equal
        value: &quot;true&quot;
      - effect: NoExecute
        operator: &quot;Equal&quot;
        key: &quot;node-role.kubernetes.io/etcd&quot;
        value: &quot;true&quot;
</code></pre>
<h4 id="数据持久化"><a href="#数据持久化" class="headerlink" title="数据持久化"></a>数据持久化</h4><p>配置环境变量启用配置数据持久化</p>
<pre><code>- env:
  - name: CTRL_PERSIST_CONFIG
</code></pre>
<p>配置此环境变量后，默认情况下Neuvector-Controller会将数据存储在&#x2F;var&#x2F;neuvector目录内，默认此目录是hostpath映射在POD所在宿主机的&#x2F;var&#x2F;neuvector目录内。</p>
<p>若需要更高级别数据可靠性也可以通过PV对接nfs或其他支出多读写的存储中。<br>这样当出现Neuvector-Controller三个POD副本同时都销毁，宿主机都完全不可恢复时，也不会有数据配置数据丢失。<br>以下以NFS为例。<br>部署nfs</p>
<p>创建pv和pvc</p>
<pre><code>cat &lt;&lt;EOF | kubectl apply -f -

apiVersion: v1
kind: PersistentVolume
metadata:
  name: neuvector-data
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany 
  nfs:
    path: /nfsdata
    server: 172.16.0.195 

EOF



cat &lt;&lt;EOF | kubectl apply -f -

kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: neuvector-data
  namespace: neuvector
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
EOF
</code></pre>
<p>修改NeuVector-Controller部署yaml，添加pvc信息，将&#x2F;var&#x2F;neuvector目录映射到nfs中（默认是hostpath映射到本地)</p>
<pre><code>spec:
  template:
    spec:
      volumes:
        - name: nv-share
#         hostPath:                        // replaced by persistentVolumeClaim
#           path: /var/neuvector        // replaced by persistentVolumeClaim
          persistentVolumeClaim:
            claimName: neuvector-data
</code></pre>
<p>或直接在NeuVector部署yaml中挂载nfs目录</p>
<pre><code>      volumes:
      - name: nv-share
        nfs:
          path: /opt/nfs-deployment
          server: 172.26.204.144
</code></pre>
<h3 id="多云安全管理"><a href="#多云安全管理" class="headerlink" title="多云安全管理"></a>多云安全管理</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector-5.png"><br>在实际生产应用中，会存在对多个集群进行安全进行管理，NeuVector支持集群联邦功能。<br>需要在一个集群上暴露Federation Master服务，在每个远端集群上部署Federation Worker服务。为了更好的灵活性，可以在每个集群同时启用Federation Master和Federation Worker服务。<br>在每个集群部署此yaml</p>
<pre><code>apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-controller-fed-master
  namespace: neuvector
spec:
  ports:
  - port: 11443
    name: fed
    nodePort: 30627
    protocol: TCP
  type: NodePort
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-controller-fed-worker
  namespace: neuvector
spec:
  ports:
  - port: 10443
    name: fed
    nodePort: 31783
    protocol: TCP
  type: NodePort
  selector:
    app: neuvector-controller-pod
</code></pre>
<p>将其中一个集群升级为主集群<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector-6.png"><br>将其中一个集群升级为主集群，配置连接远程暴露ip和对remot cluster可达的端口。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector-7.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector-8.png"><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector-9.png"><br>在主集群中，生成token，用于其他remote cluster连接。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector-10.png"><br>在remote cluster中配置加入主集群，配置token和连接端子<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector-11.png"><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector-12.png"></p>
<p>在界面可以对多个Neuvector集群进行管理<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector-13.png"></p>
<h3 id="其他配置"><a href="#其他配置" class="headerlink" title="其他配置"></a>其他配置</h3><h4 id="升级"><a href="#升级" class="headerlink" title="升级"></a>升级</h4><p>若是采用yaml文件方式部署的NeuVector直接更新对应的组件镜像tag即可完成升级。如</p>
<pre><code>kubectl set image deployment/neuvector-controller-pod neuvector-controller-pod=neuvector/controller:2.4.1 -n neuvector



kubectl set image -n neuvector ds/neuvector-enforcer-pod neuvector-enforcer-pod=neuvector/enforcer:2.4.1
</code></pre>
<p>若是采用Helm部署的NeuVector，则直接执行helm update配置对应参数即可即可。</p>
<h4 id="卸载"><a href="#卸载" class="headerlink" title="卸载"></a>卸载</h4><p>删除部署的组件</p>
<pre><code>kubectl delete -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/neuvector-docker-k8s.yaml
</code></pre>
<p>删除配置的RBAC</p>
<pre><code>kubectl get clusterrolebinding  | grep neuvector|awk &#39;&#123;print $1&#125;&#39;|xargs kubectl delete clusterrolebinding



kubectl get rolebinding -n neuvector | grep neuvector|awk &#39;&#123;print $1&#125;&#39;|xargs kubectl delete rolebinding -n neuvector
</code></pre>
<p>删除对应的CRD</p>
<pre><code>kubectl delete -f  https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/crd-k8s-1.19.yaml



kubectl delete -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/waf-crd-k8s-1.19.yaml



kubectl delete -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/admission-crd-k8s-1.19.yaml
</code></pre>
<p>总结：<br>SUSE此次开源的NeuVector是一个成熟稳定的容器安全管理平台，未来NeuVector会和Rancher产品更好的融合。</p>
]]></content>
      <categories>
        <category>安全</category>
      </categories>
      <tags>
        <tag>安全</tag>
      </tags>
  </entry>
  <entry>
    <title>云原生安全平台NeuVector基础使用</title>
    <url>/2022/04/17/neuvector_2/</url>
    <content><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>上一篇NeuVector文章主要以安装部署为主，本篇将实际结合NeuVector的基础功能进行操作演示，主要包含对于NeuVector安全漏洞管理、合规性和机密性检查、策略管理、准入控制策略、动态安全响应和行为监控。本篇文档适用版本为以NeuVector首个开源版NeuVector:5.0.0-preview.1为主</p>
<h3 id="安全漏洞管理"><a href="#安全漏洞管理" class="headerlink" title="安全漏洞管理"></a>安全漏洞管理</h3><p>集成CVE漏洞库，每天自动更新，支持对平台（Kubernetes）、主机、容器、镜像仓库进行安全漏洞扫描。</p>
<p><a href="">配置</a>自动扫描，当平台漏洞库有更新或有新的节点和容器加入时会自动进行扫描。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-1.png"></p>
<p>针对不同漏洞有不同风险级别，以及对应的组件版本和修复版本提示<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-2.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-3.png"></p>
<p>每个漏洞可以展示对应的漏洞发布时间、漏洞影响范围、对应的组件影响版本。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-4.png"></p>
<p>对漏洞进行过滤，是否已经修复，漏洞等级、发布时间等<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-5.png"></p>
<h4 id="配置对接镜像仓库扫描"><a href="#配置对接镜像仓库扫描" class="headerlink" title="配置对接镜像仓库扫描"></a>配置对接镜像仓库扫描</h4><p>支持对接多种镜像仓库如（docker-registry（harbor）、JFrog Artifactory、Nexus等）<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-6.png"></p>
<p>以对接Harbor为例，配置连接方式，填写连接方式和认证信息，过滤器表示你需要扫描的范围如扫描uat项目下全部镜像则<code>uat/*</code>,如果需要扫描整个Harbor内全部镜像则*。测试设置可以验证编写的表达式的关联情况。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-7.png"></p>
<h3 id="合规性检查和机密性检查"><a href="#合规性检查和机密性检查" class="headerlink" title="合规性检查和机密性检查"></a>合规性检查和机密性检查</h3><p>NeuVector的合规性审核包括 CIS 基线测试、自定义检查、机密审核以及 PCI、GDPR 和其他法规的行业标准模板扫描。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-8.png"></p>
<p>类型这表示对应的那个基线标准如K.4.1.1对应Kubernetes CIS基线测试4.1.1<br>容器对应的基线标准为D开头的，镜像对应的基线标准为I开头</p>
<p>注：《通用数据保护条例》（General Data Protection Regulation，简称GDPR）为欧洲联盟的条例</p>
<p>在合规性检查中也会检查是否存在密文泄漏情况<br><img src="https://open-docs.neuvector.com/user/pages/06.scanning/01.scanning/02.compliance/secrets_image_4.png"></p>
<p>包括如</p>
<pre><code>General Private Keys
General detection of credentials including &#39;apikey&#39;, &#39;api_key&#39;, &#39;password&#39;, &#39;secret&#39;, &#39;passwd&#39; etc.
General passwords in yaml files including &#39;password&#39;, passwd&#39;, &#39;api_token&#39; etc.
General secrets keys in key/value pairs
Putty Private key
Xml Private key
AWS credentials / IAM
Facebook client secret
Facebook endpoint secret
Facebook app secret
Twitter client Id
Twitter secret key
Github secret
Square product Id
Stripe access key
Slack API token
Slack web hooks
LinkedIn client Id
LinkedIn secret key
Google API key
SendGrid API key
Twilio API key
Heroku API key
MailChimp API key
MailGun API key
</code></pre>
<h3 id="策略管理"><a href="#策略管理" class="headerlink" title="策略管理"></a>策略管理</h3><p>在NeuVector中通过组的方式对容器和主机进行管理。通过对组进行合规性检查、网络规则、进程和文件访问规则、DLP&#x2F;WAF的检测配置。</p>
<p>NeuVector会自动将当前集群主机加入到nodes组，对于集群内容器会自动创建以nv.开头的组<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-9.png"><br>NeuVector的组支持3种模式，学习模式、监控模式、保护模式。各个模式实现作用如下。<br>学习模式：<br>学习和记录容器、主机间网络连接情况和进程执行信息。<br>自动构建网络规则白名单，保护应用网络正常行为。<br>为每个服务的容器中运行的进程设定安全基线，并创建进程配置文件规则白名单</p>
<p>监控模式：<br>NeuVector监视容器和主机网络和进程运行情况，遇到非学习模式下记录的行为将在NeuVector中进行告警。</p>
<p>保护模式：</p>
<p>NeuVector监视容器和主机网络和进程运行情况，遇到非学习模式下记录的行为直接拒绝。</p>
<p>新建的容器业务被自动发现默认为学习模式，也可以通过设置将默认模式设置为监控模式或保护模式。</p>
<p>不同组下策略冲突情况下，适用的有效模式如下表：</p>
<table>
<thead>
<tr>
<th>源组模式</th>
<th>目的组模式</th>
<th>有效模式</th>
</tr>
</thead>
<tbody><tr>
<td>学习模式</td>
<td>监控模式</td>
<td>学习模式</td>
</tr>
<tr>
<td>学习模式</td>
<td>保护模式</td>
<td>学习模式</td>
</tr>
<tr>
<td>监控模式</td>
<td>学习模式</td>
<td>学习模式</td>
</tr>
<tr>
<td>监控模式</td>
<td>保护模式</td>
<td>监控模式</td>
</tr>
<tr>
<td>保护模式</td>
<td>学习模式</td>
<td>学习模式</td>
</tr>
<tr>
<td>保护模式</td>
<td>监控模式</td>
<td>监控模式</td>
</tr>
</tbody></table>
<p>为了保证业务的稳定运行，当出现模式不一致时，有效模式以限制最小的模式运行。</p>
<p>生产环境最佳实践使用：<br>使用路径可以是：1、上新业务时先学习模式运行一段时间，进行完整的功能测试和调用测试得到实际运行此业务的网络连接情况和进程执行情况信息。2、监控模式运行一段时间，看看有没有额外的特殊情况，进行判断添加规则。3、最后全部容器都切换到保护模式确定最终形态。</p>
<h4 id="动态微隔离"><a href="#动态微隔离" class="headerlink" title="动态微隔离"></a>动态微隔离</h4><p>使用场景一：POD间通过网络策略互相隔离<br>在Kubernetes平台中创建四个Nginx。名称和用途如下。<br>workload_name：test-web1 image:nginx  用途：web服务器。<br>workload_name：test-con1 image:nginx   用途：连接客户端1<br>workload_name：test-con2 image:nginx   用途：连接客户端2<br>workload_name：test-con3 image:nginx   用途：连接客户端3</p>
<p>创建workload</p>
<pre><code>kubectl create deployment test-web1 --image=nginx
kubectl expose deployment/test-web1 --port=80 --type=NodePort 
kubectl create deployment test-con1 --image=nginx
kubectl create deployment test-con2 --image=nginx
kubectl create deployment test-con3 --image=nginx
</code></pre>
<p>此时在NeuVector中会自动生成这几个组。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-10.png"></p>
<p>在test-con1中通过curl访问test-web1</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-11.png"></p>
<p>此时可以正常访问，因为在学习模式下。NeuVector也会自动添加此访问规则。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-12.png"></p>
<p>将test-web1和test-con2都设置为监控模式<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-13.png"></p>
<p>然后在test-con2中curl访问test-web1<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-14.png"></p>
<p>此时test-con2可以正常访问test-web1，但在NeuVector中会生成告警<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-15.png"></p>
<p>同时对应的在网络活动拓扑图中也可以看见对应的连接链路变为红色。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-16.png"></p>
<p>将test-web1和test-con2都设置为保护模式，在通过test-con2去curl test-web1</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-17.png"></p>
<p>因为curl在学习模式时没有使用，也不是NeuVector默认允许的可执行进程，所以进程直接就无法访问了。</p>
<p>将test-con1设置为保护模式，此时test-con1无法访问外部网络，</p>
<p>可以通过自定义添加网络规则方式开通访问。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-18.png"></p>
<p>在网络规则页，此处规则已经是在学习模式下生成的规则列表。</p>
<p>添加外部访问规则<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-19.png"></p>
<p>NeuVector深度了解应用程序行为，并将分析有效负载以确定应用程序协议。协议包括HTTP，HTTPS，SSL，SSH，DNS，DNCP，NTP，TFTP，ECHO，RTSP，SIP，MySQL，Redis，Zookeeper，Cassandra，MongoDB，PostgresSQL，Kafka，Couchbase，ActiveMQ，ElasticSearch，RabbitMQ，Radius，VoltDB，Consul，Syslog，Etcd，Spark，Apache，Nginx，Jetty，NodeJS，Oracle，MSSQL和gRPC。</p>
<p>现在test-con1的curl去访问www.baidu.com以正常访问。</p>
<p>总结：<br>除上述策略外，NeuVector也内置网络威胁检测，能够快速识别常用网络攻击，保护业务容器安全运行。</p>
<p>无论保护模式如何。在”学习和监视”模式下，将发出警报，并且可以在”通知”-&gt;安全事件中找到这些威胁。在保护模式下，这些将收到警报和阻止。还可以根据威胁检测创建响应规则。</p>
<p>包含的威胁检测如下：</p>
<pre><code>SYN flood attack
ICMP flood attack
IP Teardrop attack
TCP split handshake attack
PING death attack
DNS flood DDOS attack
Detect SSH version 1, 2 or 3
Detect SSL TLS v1.0
SSL heartbeed attack
Detect HTTP negative content-length buffer overflow
HTTP smugging attack
HTTP Slowloris DDOS attack
TCP small window attack
DNS buffer overflow attack
Detect MySQL access deny
DNS zone transfer attack
ICMP tunneling attack
DNS null type attack
SQL injection attack
Apache Struts RCE attack
DNS tunneling attack
TCP Small MSS attack
Cipher Overflow attack
Kubernetes man-in-the-middle attack per CVE-2020-8554
</code></pre>
<h4 id="进程管理"><a href="#进程管理" class="headerlink" title="进程管理"></a>进程管理</h4><p>NeuVector支持对容器和主机内进程进行管理<br>在学习模式下，运行的进程和命令会自动添加到规则中<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-20.png"></p>
<p>此时在test-con1中执行<code>df -h</code>会发现报错<code>bash: /bin/df: Operation not permitted</code><br>在<code>nv.test-con1.default</code>组中添加<code>df</code>进程规则</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-21.png"><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-22.png"></p>
<p>然后在重新执行即可执行。</p>
<p>进程管理也支持对node节点，可以在node组中进行限制，约束宿主机进程执行。如限制执行docker cp 执行，通过学习模式得知是<code>docker-tar</code>进程在后端执行<br>将节点切换到保护模式，限制<code>docker-tar</code>进程即可。</p>
<p>这些在节点就无法执行<code>docker cp</code></p>
<h3 id="准入策略控制"><a href="#准入策略控制" class="headerlink" title="准入策略控制"></a>准入策略控制</h3><p>NeuVector支持与Kubernetes准入控制（admission-control）功能对接，实现UI配置准入控制规则，对请求进行拦截，用于对请求的资源对象进行校验。<br>NeuVector支持多种准入控制策率配置如镜像CVE漏洞情况限制、部署特权模式、镜像内使用root用户、特定标签等。</p>
<p>在策略-准入控制中开启此功能，注意：需要Kubernetes集群提前开启admission-control功能<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-24.png"></p>
<p>NeuVector准入策略控制，支持两种模式，监控模式和保护模式，对应含义和组的模式一样的。这里我们直接切换到保护模式，添加策略。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-25.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-26.png"></p>
<p>添加完后，在Rancher中部署特权模式容器会提示解决，策略生效。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-27.png"></p>
<h3 id="动态安全响应"><a href="#动态安全响应" class="headerlink" title="动态安全响应"></a>动态安全响应</h3><p>NeuVector事件响应机制可以配置响应规则根据安全事件情况进行动态响应，包括以下事件：漏洞扫描结果、CIS基准测试、准入控制事件等。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-28.png"></p>
<p>响应动作包括隔离、webhook通知、日志抑制<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-29.png"><br>隔离模式：对应的容器网络进出流量将全部被切断。<br>webhook通知：将触发信息通过webhook方式进行告警。<br>日志抑制：对触发告警信息进行抑制。</p>
<p>以CVE漏洞配置为例，配置包含CVE漏洞名称为CVE-2020-16156的容器进入隔离模式。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-30.png"></p>
<p>组名对应的是影响范围，如果为空，表示对全部的组都生效，填写组名可以设置对特定组生效。</p>
<p>配置策略后，在集群去curl nginx容器，发现无法访问，在NeuVector中查看容器状态为隔离状态。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-31.png"></p>
<p>删除策略时，也可以配置将对应隔离状态容器解除隔离。</p>
<p>注意：<br>1、隔离操作不适用于为主机事件触发的规则<br>2、每个规则可以有多个操作。</p>
<h3 id="行为监控"><a href="#行为监控" class="headerlink" title="行为监控"></a>行为监控</h3><h4 id="网络流量可视化"><a href="#网络流量可视化" class="headerlink" title="网络流量可视化"></a>网络流量可视化</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-32.png"></p>
<p>网络流量可视化，可以清晰可见容器集群内网络连接关系，当前容器连接会话并且可以过滤网络连接信息，进行图标展示。能够快速进行网络问题定位。</p>
<h4 id="流量抓包"><a href="#流量抓包" class="headerlink" title="流量抓包"></a>流量抓包</h4><p>针对容器可进行网络抓包，方便故障不需要进入主机获取高权限，就能使进行网络问题深入排查。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-33.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/neuvector2-34.png"></p>
<p>采集到的数据包可直接下载通过Wireshark进行解包分析。</p>
]]></content>
      <categories>
        <category>安全</category>
      </categories>
      <tags>
        <tag>安全</tag>
      </tags>
  </entry>
  <entry>
    <title>OPA概述和介绍</title>
    <url>/2021/07/08/opa_1/</url>
    <content><![CDATA[<h3 id="OPA是什么？"><a href="#OPA是什么？" class="headerlink" title="OPA是什么？"></a>OPA是什么？</h3><p>OPA是一个开源的通用策略引擎，由Styra公司贡献给CNCF基金会，目前也已经成CNCF基金会毕业了。</p>
<p>OPA通过声明式开发语言Rego，来编写控制策略，对支持的平台进行策略控制。类似于在请求入口和后端服务中间部署的一个组件，对请求的指令进行判断是否与配置的策略相匹配。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/opa1-1.png"></p>
<h3 id="OPA能做什么？"><a href="#OPA能做什么？" class="headerlink" title="OPA能做什么？"></a>OPA能做什么？</h3><p>通过OPA编写的策略可以对microservices, Kubernetes, CI&#x2F;CD pipelines, API gateways进行一些策略控制。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/opa1-2.png"><br>例如与K8s集成，实现灵活的安全策略控制，在Kubernetes中也是可以取代Pod Security Policy实现更灵活的控制，目前k8s社区也在后面版本将Pod Security Policy去除，计划用OPA进行取代。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/opa1-4.png"></p>
<h3 id="OPA与Kubernetes结合是发展历程和实现方式"><a href="#OPA与Kubernetes结合是发展历程和实现方式" class="headerlink" title="OPA与Kubernetes结合是发展历程和实现方式"></a>OPA与Kubernetes结合是发展历程和实现方式</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/opa1-5.png"><br>Gatekeeper v1.0 - 使用 OPA 作为带有 kube-mgmt sidecar 的许可控制器，用来强制执行基于 configmap 的策略。这种方法实现了验证和转换许可控制(admission controller)。贡献方：Styra</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/opa1-6.png"><br>Gatekeeper v2.0 - 使用 Kubernetes 策略控制器（admission controller)作为许可控制器，OPA 和 kube-mgmt sidecar 实施基于 configmap 的策略。这种方法实现了验证和转换准入控制和审核功能。贡献方：Microsoft</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/opa1-7.png"></p>
<p>Gatekeeper v3.0 - 准入控制器(adminission controller)与 OPA Constraint Framework 集成在一起，用来实施基于 CRD 的策略，并可以可靠地共享已完成声明配置的策略。使用 kubebuilder 进行构建，实现了验证以及最终转换（待完成）为许可控制和审核功能。这样就可以为 Rego 策略创建策略模板，将策略创建为 CRD 并存储审核结果到策略 CRD 上。该项目是 Google，Microsoft，Red Hat 和 Styra 合作完成的。</p>
<p>上面提到了多次准入控制器(adminission controller)这里也简单一起介绍一下，为什么有了准入控制器还需要OPA。<br>首先<br>准入(Admission Control)机制是一种请求拦截机制，用于对请求的资源对象进行校验。<br>包含两个控制器：</p>
<ul>
<li>变更（Mutating）准入控制：修改请求的对象</li>
<li>验证（Validating）准入控制：验证请求的对象<br>当请求到达 API Server 的时候首先执行变更准入控制，然后再执行验证准入控制。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/opa1-8.png"></li>
</ul>
<p>准入控制器实现的策略<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/opa1-9.png"></p>
<p>为什么有了准入控制器还需要OPA-Gatekeeper ？<br>在Kubernetes中，admission在创建，更新和删除操作期间强制执行对象的语义验证，需要重新编译或重新配置Kubernetes API服务器。但使用OPA，直接可以在Kubernetes对象上实施自定义策略</p>
<p>配合强大的声明式策略语言Rego，直接通过K8S对象配置规则。</p>
<p>Service可以是以下任意：</p>
<ul>
<li>Kubernetes API server</li>
<li>API gateway</li>
<li>Custom service</li>
<li>CI&#x2F;CD pipeline</li>
</ul>
<p>总结来说，以前玩admission Controller太复杂了，要定义很多多个webhook-server，但现在Gatekeeper就相当于一个总的webhook-server，根据Rego+CRD配置对应规则即可。</p>
<p>Gatekeeper与Kubernetes集成实现方式<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/opa1-10.png"></p>
<p>1、Gatekeeper部署在Kubernetes 中启动后注册 API Server 的注册 Dynamic Admission Controller。</p>
<p>2、将Gatekeeper做为总的Webhook-server，当连接Api-server时，通过RBAC认证后，请求到Admission controller，发送到Gatekeeper进行规则匹配和决策。</p>
<p>3、然后将响应结果在返回到Api-server。</p>
<p>参考链接：</p>
<p><a href="https://www.openpolicyagent.org/">https://www.openpolicyagent.org/</a><br><a href="https://kubernetes.io/zh/blog/2019/08/06/opa-gatekeeper-policy-and-governance-for-kubernetes/">https://kubernetes.io/zh/blog/2019/08/06/opa-gatekeeper-policy-and-governance-for-kubernetes/</a></p>
]]></content>
      <categories>
        <category>安全</category>
      </categories>
      <tags>
        <tag>安全</tag>
      </tags>
  </entry>
  <entry>
    <title>openssl证书生成</title>
    <url>/2017/07/20/openssl_ca/</url>
    <content><![CDATA[<p>X.509是一种非常通用的证书格式。所有的证书都符合ITU-T X.509国际标准，因此(理论上)为一种应用创建的证书可以用于任何其他符合X.509标准的应用。<br>x.5.09证书通常包含三类文件<br>key文件，密钥文件用于对发送和接收到的数据进行加密和解密<br>csr是签名请求文件，用于提交给CA进行签名<br>crt是ca签名后颁发的证书<br>pem是 用于导出和导入证书时的证书格式，通常是crt+key的结合</p>
<p>在内部使用时通常使用自签名的证书，简单来说是，就是自己通过一些工具如openssl或cfssl生成CA然后去进行签名证书，如果要对外提供公共服务，就需要去购买正规的证书颁发机构的CA进行签名。这里我们讨论的是自签名证书</p>
<p>首先我们需要生成CA根证书<br>1、生成CA 私钥，这里使用的是RSA算法</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">openssl genrsa -out ca.key 2048</span><br></pre></td></tr></table></figure>
<p>2、生成根证书请求文件csr</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">openssl req -new -key ca.key -out ca.csr</span><br></pre></td></tr></table></figure>

<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ca_1.png"></p>
<p>需要依次输入国家，地区，城市，组织，组织单位，Common Name和Email。其中Common Name，可以写自己的名字或者域名，如果要支持https，Common Name应该与域名保持一致，否则会引起浏览器警告。</p>
<p>3、自签名得到ca证书</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">openssl x509 -req -days 36500  -in ca.csr -signkey ca.key -out ca.crt</span><br></pre></td></tr></table></figure>

<p>得到自签名的CA证书以后我们要用CA证书对用户的证书进行签名，这样才能正常使用<br>生成私钥 key文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">openssl genrsa -out domain.rancher.com.key 1024</span><br></pre></td></tr></table></figure>

<p>生成证书请求csr文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">openssl req -new -key domain.rancher.com.key -out domain.rancher.com.csr</span><br></pre></td></tr></table></figure>

<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ca_2.png"></p>
<p>需要依次输入国家，地区，城市，组织，组织单位，Common Name和Email。其中Common Name，可以写自己的名字或者域名，如果要支持https，Common Name应该与域名保持一致，否则会引起浏览器警告。</p>
<p>用CA证书签名用户证书</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">openssl ca -in domain.rancher.com.csr -out domain.rancher.com.crt -cert ca.crt -keyfile ca.key</span><br></pre></td></tr></table></figure>
<p>注意: 此时会出错：Using configuration from &#x2F;usr&#x2F;share&#x2F;ssl&#x2F;openssl.cfg I am unable to access the .&#x2F;demoCA&#x2F;newcerts directory .&#x2F;demoCA&#x2F;newcerts: No such file or directory</p>
<p>解决方法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir -p ./demoCA/newcerts</span><br><span class="line">touch demoCA/index.txt  #空文件，生成证书时会将数据记录写入</span><br><span class="line">touch demoCA/serial</span><br><span class="line">echo 01 &gt; demoCA/serial #写入01，然后生成证书会以此递增</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ca_3.png"></p>
<p>然后会对应的ca证书和用户证书放到对应的web服务器的目录就可以了。</p>
<p>如果需要pem证书文件 ，就把证书文件(crt)和私钥文件(key)文件合并即可</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat domain.rancher.com.crt domain.rancher.com.key &gt; domain.rancher.com.pem</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>openstack N版对接ceph jewel版</title>
    <url>/2017/02/06/openstac&amp;ceph/</url>
    <content><![CDATA[<h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p>网络配置：</p>
<table>
<thead>
<tr>
<th></th>
<th align="center">public_network</th>
<th align="center">cluster_network</th>
</tr>
</thead>
<tbody><tr>
<td>控制节点&#x2F;ceph-mon</td>
<td align="center">192.168.4.6</td>
<td align="center">192.168.3.5</td>
</tr>
<tr>
<td>ceph-osd1</td>
<td align="center">192.168.4.12</td>
<td align="center">192.168.3.9</td>
</tr>
<tr>
<td>ceph-osd2</td>
<td align="center">192.168.4.8</td>
<td align="center">192.168.3.7</td>
</tr>
</tbody></table>
<p>硬件配置：<br>ceph-mon<br>cpu：4核<br>内存：4G  </p>
<p>ceph-osd1<br>cpu：4核<br>内存：4G<br>硬盘：3块100G磁盘  </p>
<p>ceph-osd2<br>cpu：4核<br>内存：4G<br>硬盘：3块100G磁盘<br>将selinux和firewalld关闭，或配置防火墙规则  </p>
<p>配置软件源：<br>163 yum源<br><code>wget -O /etc/yum.repo/ CentOS7-Base-163.repo http://mirrors.163.com/.help/CentOS7-Base-163.repo</code></p>
<p>配置epel源<br><code>wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</code></p>
<p>增加ceph源<br>vim &#x2F;etc&#x2F;yum.repos.d&#x2F;ceph.repo</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[ceph]</span><br><span class="line">name=ceph</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/x86_64/</span><br><span class="line">gpgcheck=0</span><br><span class="line">[ceph-noarch]</span><br><span class="line">name=cephnoarch</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/noarch/</span><br><span class="line">gpgcheck=0</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>清理yum缓存<br>yum clean all<br>创建缓存<br>yum makecache<br>所有节点安装ceph<br>yum install ceph –y  </p>
<h3 id="开始部署"><a href="#开始部署" class="headerlink" title="开始部署"></a>开始部署</h3><p>在部署节点安装我这里的是openstack的controller节点安装ceph-deploy，手动部署参考上一篇文章 <a href="http://www.bladewan.com/2017/01/01/manual_ceph/#more">http://www.bladewan.com/2017/01/01/manual_ceph/#more</a></p>
<p>yum install ceph-deploy –y</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ceph&openstack_1.png"></p>
<p>在部署节点创建部署目录<br>mkdir &#x2F;etc&#x2F;ceph<br>cd &#x2F;etc&#x2F;ceph&#x2F;<br>ceph-deploy new control-node1<br>没有erro继续向下</p>
<p>此时目录下有<br>ceph.conf、ceph-deploy-ceph.log、ceph.mon.keyring  </p>
<p>修改ceph.conf添加public_network和cluster_network，同时增加允许时钟偏移<br>vim &#x2F;etc&#x2F;ceph&#x2F;ceph.conf<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ceph&openstack_2.png"></p>
<p>开始monitor<br>在controller上执行<br>ceph-deploy mon create-initial  </p>
<p>……</p>
<p>部署目录多了以下文件<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ceph&openstack_3.png">  </p>
<p>查看ceph状态<br>ceph -s<br>此时ceph状态应该是ERROR的<br>health HEALTH_ERR<br>no osds<br>Monitor clock skew detected   </p>
<p>部署osd</p>
<p><code>ceph-deploy --overwrite-conf osd prepare ceph-osd1:/dev/vdb /dev/vdc /dev/vdd ceph-osd2:/dev/vdb /dev/vdbc /dev/vdd --zap-disk</code><br>部署完后查看ceph状态  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ceph&openstack_4.png"></p>
<p>查看osd tree</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ceph&openstack_5.png"></p>
<p>推送配置<br><code>ceph-deploy --overwrite-conf config push ceph-osd1 ceph-osd2</code></p>
<p>重启ceph进程<br>mon节点<br>systemctl restart <a href="mailto:&#x63;&#101;&#112;&#x68;&#45;&#x6d;&#111;&#110;&#64;&#99;&#111;&#x6e;&#x74;&#114;&#111;&#108;&#x2d;&#110;&#111;&#100;&#x65;&#49;&#x2e;&#x73;&#x65;&#114;&#118;&#105;&#99;&#x65;">&#x63;&#101;&#112;&#x68;&#45;&#x6d;&#111;&#110;&#64;&#99;&#111;&#x6e;&#x74;&#114;&#111;&#108;&#x2d;&#110;&#111;&#100;&#x65;&#49;&#x2e;&#x73;&#x65;&#114;&#118;&#105;&#99;&#x65;</a>  </p>
<p>osd节点重启<br>systemctl restart ceph-osd@x  </p>
<p>查看public_network和cluster_network配置是否生效<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ceph&openstack_6.png"></p>
<h3 id="根openstack对接"><a href="#根openstack对接" class="headerlink" title="根openstack对接"></a>根openstack对接</h3><h4 id="Ceph"><a href="#Ceph" class="headerlink" title="Ceph"></a>Ceph</h4><p>创建pool<br>根据公式计算出每个pool的合适pg数  </p>
<p>PG Number<br>PG和PGP数量一定要根据OSD的数量进行调整，计算公式如下，但是最后算出的结果一定要接近或者等于一个2的指数。<br><code>Total PGs = (Total_number_of_OSD * 100) / max_replication_count</code></p>
<p>例：<br>有6个osd，2副本，3个pool  </p>
<p>Total PGs &#x3D;6*100&#x2F;2&#x3D;300<br>每个pool 的PG&#x3D;300&#x2F;3&#x3D;100，那么创建pool的时候就指定pg为128<br>ceph osd pool create pool_name 128<br>ceph osd pool create pool_name 128    </p>
<p>创建3个pool<br>ceph osd pool create volumes 128<br>ceph osd pool create images 128<br>ceph osd pool create vms 128  </p>
<p>创建nova、cinder、glance、backup用户并授权  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ceph auth get-or-create client.cinder mon &#x27;allow r&#x27; osd &#x27;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rwx pool=images&#x27;</span><br><span class="line"></span><br><span class="line">ceph auth get-or-create client.glance mon &#x27;allow r&#x27; osd &#x27;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rwx pool=images&#x27;</span><br><span class="line"></span><br><span class="line">ceph auth get-or-create client.nova mon &#x27;allow r&#x27; osd &#x27;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rwx pool=images&#x27;</span><br></pre></td></tr></table></figure>

<p>生成keyring文件<br>控制节点  </p>
<p>ceph auth get-or-create client.cinder | tee &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyring<br>ceph auth get-or-create client.glance | tee &#x2F;etc&#x2F;ceph&#x2F;ceph.client.glance.keyring    </p>
<p>修改文件属组<br>chown cinder:cinder &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyring<br>chown glance:glance &#x2F;etc&#x2F;ceph&#x2F;ceph.client.glance.keyring  </p>
<p>计算节点<br>ceph auth get-or-create client.cinder |tee &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyring<br>ceph auth get-or-create client.nova |tee &#x2F;etc&#x2F;ceph&#x2F;ceph.client.nova.keyring<br>ceph auth get-or-create client.glance |tee &#x2F;etc&#x2F;ceph&#x2F;ceph.client.glance.keyring  </p>
<p>修改文件属组<br>chown cinder:cinder &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyring<br>chown nova:nova &#x2F;etc&#x2F;ceph&#x2F;ceph.client.nova.keyring  </p>
<p>在计算节点上生成uuidgen（所有计算节点用一个就可以）<br>uuidgen<br>f77169a0-7d56-4fc3-a436-35298081f9f9  </p>
<p>创建secret.xml<br>vim secret.xml</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;secret ephemeral=&#x27;no&#x27; private=&#x27;no&#x27;&gt;</span><br><span class="line">  &lt;uuid&gt;f77169a0-7d56-4fc3-a436-35298081f9f9&lt;/uuid&gt;</span><br><span class="line">  &lt;usage type=&#x27;ceph&#x27;&gt;</span><br><span class="line">    &lt;name&gt;client.nova secret&lt;/name&gt;</span><br><span class="line">  &lt;/usage&gt;</span><br><span class="line">&lt;/secret&gt;</span><br></pre></td></tr></table></figure>
<p>导出nova的keyring</p>
<p>ceph auth get-key client.nova |  tee client.nova.key  </p>
<p>virsh secret-define –file secret.xml<br>virsh  secret-set-value –secret f77169a0-7d56-4fc3-a436-35298081f9f9 –base64 $(cat client.nova.key )</p>
<p>查看secret-value<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ceph&openstack_7.png"></p>
<p>另外一台计算节点一样</p>
<p>修改openstack组件配置  </p>
<h4 id="glance"><a href="#glance" class="headerlink" title="glance"></a>glance</h4><p>cp &#x2F;etc&#x2F;glance&#x2F;glance-api.conf  &#x2F;etc&#x2F;glance&#x2F;glance-api.conf.bak<br>vim &#x2F;etc&#x2F;glance&#x2F;glance-api.conf  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">...</span><br><span class="line">show_image_direct_url = True</span><br><span class="line">...</span><br><span class="line">[glance_store]</span><br><span class="line">stores=glance.store.rbd.Store</span><br><span class="line">default_store = rbd</span><br><span class="line">rbd_store_pool = images</span><br><span class="line">rbd_store_user = glance</span><br><span class="line">rbd_store_ceph_conf = /etc/ceph/ceph.conf</span><br><span class="line">rbd_store_chunk_size = 8</span><br></pre></td></tr></table></figure>

<p>重启glance-api和glance-registry<br>systemctl restart openstack-glance-api<br>systemctl restart  openstack-glance-registry  </p>
<h4 id="cinder"><a href="#cinder" class="headerlink" title="cinder"></a>cinder</h4><p>cp &#x2F;etc&#x2F;cinder&#x2F;cinder.conf  &#x2F;etc&#x2F;cinder&#x2F;cinder.conf.bak<br>vim &#x2F;etc&#x2F;cinder&#x2F;cinder.conf  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">enabled_backends = rbd</span><br><span class="line">[rbd]</span><br><span class="line">volume_driver = cinder.volume.drivers.rbd.RBDDriver</span><br><span class="line">rbd_pool = volumes</span><br><span class="line">rbd_ceph_conf = /etc/ceph/ceph.conf</span><br><span class="line">rbd_flatten_volume_from_snapshot = false</span><br><span class="line">rbd_max_clone_depth = 5</span><br><span class="line">rbd_store_chunk_size = 4</span><br><span class="line">rados_connect_timeout = -1</span><br><span class="line">glance_api_version = 1</span><br><span class="line">rbd_user = cinder</span><br><span class="line">rbd_secret_uuid = f77169a0-7d56-4fc3-a436-35298081f9f9</span><br><span class="line">volume_backend_name = rbd</span><br></pre></td></tr></table></figure>
<p>重启cinder-api、cinder-schedule、cinder-volume  </p>
<p>systemctl restart openstack-cinder-api<br>systemctl restart openstack-cinder-volume<br>systemctl restart openstack-cinder-scheduler  </p>
<h4 id="nova"><a href="#nova" class="headerlink" title="nova"></a>nova</h4><p>修改nova-compute<br>cp &#x2F;etc&#x2F;nova&#x2F;nova.conf  &#x2F;etc&#x2F;nova&#x2F;nova.conf.bak<br>修改nova.conf<br>添加如下配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[libvirt]</span><br><span class="line">virt_type = qemu</span><br><span class="line">images_type = rbd</span><br><span class="line">images_rbd_pool = vms</span><br><span class="line">images_rbd_ceph_conf = /etc/ceph/ceph.conf</span><br><span class="line">rbd_user = nova</span><br><span class="line">rbd_secret_uuid = f77169a0-7d56-4fc3-a436-35298081f9f9</span><br><span class="line">live_migration_flag=&quot;VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE,VIR_MIGRATE_PERSIST_DEST,VIR_MIGRATE_TUNNELLED</span><br></pre></td></tr></table></figure>
<p>重启nova-compute<br>systemctl restart openstack-nova-compute</p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><h4 id="glance-1"><a href="#glance-1" class="headerlink" title="glance"></a>glance</h4><p>上传镜像，在ceph pool中查看是否存在<br><code>openstack image create &quot;cirros&quot; --file cirros-0.3.4-x86_64-disk.img  --disk-format qcow2 --container-format bare –public</code></p>
<p>存在说明对接正常<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ceph&openstack_8.png"></p>
<h4 id="cinder-1"><a href="#cinder-1" class="headerlink" title="cinder"></a>cinder</h4><p>在控制台创建个云硬盘，创建成功后在ceph的volumes pool池中可以看见刚刚创建的云硬盘说明创建成功<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ceph&openstack_9.png"></p>
<h4 id="nova-1"><a href="#nova-1" class="headerlink" title="nova"></a>nova</h4><p>在控制台创建个云主记，创建成功后在ceph的vm pool池中可以看见刚刚创建的云主机说明创建成功<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/ceph&openstack_10.png"></p>
]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title>openstack 本地yum源搭建</title>
    <url>/2016/12/12/openstack-localyumbuild/</url>
    <content><![CDATA[<p>我们在部署openstack时用国外yum源的快，经常会很慢导致等待时间太久，所以建议使用本地yum源安装  </p>
<p>这里以newton版centos7平台为例  </p>
<h1 id="首先下载官方repo"><a href="#首先下载官方repo" class="headerlink" title="首先下载官方repo"></a>首先下载官方repo</h1><p>yum install <a href="https://rdoproject.org/repos/rdo-release.rpm">https://rdoproject.org/repos/rdo-release.rpm</a><br>这时侯&#x2F;etc&#x2F;yum.repos.d里面会产生3个文件  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@test yum.repos.d]# ls  </span><br><span class="line">rdo-qemu-ev.repo  rdo-release.repo  rdo-testing.repo</span><br></pre></td></tr></table></figure>

<p>我这里打算用http搭建我的本地yum服务器 </p>
<h1 id="先安装httpd"><a href="#先安装httpd" class="headerlink" title="先安装httpd"></a>先安装httpd</h1><p>yum install httpd</p>
<p>mkdir &#x2F;var&#x2F;www&#x2F;html&#x2F;newton  </p>
<p>待会将同步下来的包放这个目录<br>cd &#x2F;vaw&#x2F;www&#x2F;html&#x2F;newton<br> yum repolist    –列出你所有的仓库<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/yumrepo.png"></p>
<p>前面是repo id不包含x86_64  </p>
<p>这里我只需要openstack-newton、和rdo-qemu-ev这两个软件库  </p>
<p>先同步openstack-newton<br><code>reposync --repoid=openstack-newton</code></p>
<p>指定要下载的仓库id，会通过网络全部下载到当前目录下载下来。  </p>
<p>同步完第一个继续同步第二个<br><code> reposync --repoid=rdo-qemu-ev</code></p>
<p>同步完后<br>这时查看 &#x2F;vaw&#x2F;www&#x2F;html&#x2F;newton里面已经有很多包了，只有软件包，没有repodate清单，所以需要自己重新createrepo来创建清单没有createrepo自己安装，创建软件清单<br><code> createrepo /var/www/html/newton/</code></p>
<p>然后启动httpd服务，其他机器通过httpd服务来访问yum源  </p>
<p>例如控制节点yum源配置<br>vim &#x2F;etc&#x2F;yum.repos.d&#x2F;openstack.repo</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[openstack]</span><br><span class="line">name=openstack</span><br><span class="line">baseurl=http://192.168.4.3/newton</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">~                </span><br></pre></td></tr></table></figure>
<p>yum makecache  </p>
<p>其他节点一样。  </p>
]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title>openstack使用ISO镜像</title>
    <url>/2017/11/29/openstack_iso/</url>
    <content><![CDATA[<p>openstack默认使用ISO镜像创建虚机时，nova从glance那获取镜像，并将根磁盘设置的磁盘类型设置为cdrom类型，ide总线类型，导致到安装系统界面时，会发现找不到磁盘。</p>
<p>这种情况是正常的，因为你的根分区mout了这个系统的cdrom。当我们在直接使用kvm或vmware时，使用cdrom做为安装介质时，我们也是在选完安装介质后，还需要在创建硬盘进行安装，总好像pc的cdrom是 cd或dvd设备，创建的磁盘是pc的硬盘一样。  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/openstack_iso_3.png"><br>解决方法<br>1、修改flavor，增加临时磁盘空间，这样安装时就可以看见硬盘了<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/openstack_iso_2.png"></p>
<p>弊端是<br>openstack的快照功能只能针对根磁盘，所以想通过此方法创建主机，在做快照，通过快照做成镜像是不行的。<br>可以通过修改libvirt代码将cdrom mount到临时磁盘，根分区用来安装系统就解决了这个问题。    </p>
<p>2、给云主机挂载块云硬盘，将系统安装在云硬盘内  </p>
<p>将此云硬盘做为镜像方法<br>通过glance create创建一个空镜像，记录uuid占个坑<br>glance image-create  </p>
<p>在ceph底层将这个volume卷cp到glance读取image的pool  </p>
<p>rbd cp volume&#x2F;volume-xxxxx images&#x2F;image_uuid  </p>
<p>创建snap打上protect<br>rbd snap create images&#x2F;image_uuid@snap<br>rbd snap protet image&#x2F;image_uuid@snap  </p>
<p>修改刚刚创建的空的glance镜像   </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ceph_id=`ceph -s | grep cluster | awk &#x27;&#123;print $2&#125;&#x27;`  </span><br></pre></td></tr></table></figure>
<p>设置后端存储URL  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">glance location-add --url rbd://$&#123;ceph_id&#125;/images/$&#123;image_uuid&#125;/snap $image_uuid</span><br></pre></td></tr></table></figure>
<p>images为ceph中存放镜像的pool名，根据实际环境修改。<br>更新镜像元数据  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">glance image-update --name=&quot;test_image&quot; --disk-format=raw --container-format=bare image_uuid</span><br></pre></td></tr></table></figure>

<p>然后就可以基于新的镜像创建云主机了。  </p>
]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title>openstack newton 安装</title>
    <url>/2016/12/12/openstack-newton-install/</url>
    <content><![CDATA[<h1 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h1><p>机器配置：<br>3台8v8G的虚拟机，1台做控制节点2台做融合节点。  </p>
<p>网络划分：<br>192.168.122.0&#x2F;24 public网络<br>192.168.3.0&#x2F;24 存储网络<br>192.168.4.0&#x2F;24管理网络、sdn隧道网络  </p>
<p>我这里配置了本地源，就不用在手动配置官网源本地源的搭建和配置会在另外一个文档说明。<br>节点网络信息 ：  </p>
<table>
<thead>
<tr>
<th></th>
<th align="center">管理网络和随道网络</th>
<th align="center">存储网络</th>
<th align="center">公网</th>
</tr>
</thead>
<tbody><tr>
<td>控制节点</td>
<td align="center">192.168.4.6</td>
<td align="center">192.168.3.5</td>
<td align="center">192.168.122.2</td>
</tr>
<tr>
<td>计算节点</td>
<td align="center">192.168.4.7</td>
<td align="center">192.168.3.6</td>
<td align="center">192.168.125.5</td>
</tr>
<tr>
<td>计算节点</td>
<td align="center">192.168.4.8</td>
<td align="center">192.168.3.7</td>
<td align="center">192.168.122.6</td>
</tr>
</tbody></table>
<p>网络拓扑<br><img src="http://ohx02qrb8.bkt.clouddn.com/tuopu.png" alt="tuopu"></p>
<h1 id="安装chrony"><a href="#安装chrony" class="headerlink" title="安装chrony"></a>安装chrony</h1><p>控制节点向外同步时间，其他节点如计算节点都直接同步控制节点<br>yum install chrony  </p>
<p>修改配置文件<br>vim &#x2F;etc&#x2F;chrony.conf<br>添加下面这两条<br>server cn.ntp.org.cn iburst<br>allow 192.168.4.0&#x2F;24  </p>
<p>设置开机启动<br>systemctl enable chrony<br>systemctl start chrony  </p>
<p>其他节点：<br>yum install chrony  </p>
<p>修改配置文件<br>vim &#x2F;etc&#x2F;chrony.conf  </p>
<p>添加下面这两条<br>server 192.168.4.6 iburst  </p>
<p>设置开机启动<br>systemctl enable chrony  </p>
<p>启动进程<br>systemctl start chrony  </p>
<p>安装openstack客户端<br>yum install python-openstackclient</p>
<h1 id="安装Mariadb（数据库服务）"><a href="#安装Mariadb（数据库服务）" class="headerlink" title="安装Mariadb（数据库服务）"></a>安装Mariadb（数据库服务）</h1><p>vim &#x2F;etc&#x2F;my.cnf.d&#x2F;openstack.cnf  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[mysqld]      </span><br><span class="line">bind-address = 192.168.4.6 #填写管理网段ip  </span><br><span class="line">default-storage-engine = innodb  </span><br><span class="line">innodb_file_per_table  </span><br><span class="line">max_connections = 4096</span><br><span class="line">collation-server = utf8_general_ci</span><br><span class="line">character-set-server = utf8</span><br></pre></td></tr></table></figure>
<p>设置开机启动  </p>
<p>systemctl enable mariadb  </p>
<p>启动Mariadb  </p>
<p>systemctl start mariadb</p>
<h1 id="安装rabbitmq（用于消息队列）"><a href="#安装rabbitmq（用于消息队列）" class="headerlink" title="安装rabbitmq（用于消息队列）"></a>安装rabbitmq（用于消息队列）</h1><p>yum install rabbitmq-server  </p>
<p>设置开机启动  </p>
<p>systemctl enable rabbitmq-server  </p>
<p>开启rabbitmq  </p>
<p>systemctl start rabbitmq-server  </p>
<p>创建openstack用户和配置密码  </p>
<p>rabbitmqctl add_user openstack 123456  </p>
<p>给openstack用户配置读和写权限  </p>
<p><code>rabbitmqctl set_permissions openstack &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; </code></p>
<h1 id="安装memcache（缓存token）"><a href="#安装memcache（缓存token）" class="headerlink" title="安装memcache（缓存token）"></a>安装memcache（缓存token）</h1><p>yum install memcached phython-memcached  </p>
<p>systemctl enable memcached  </p>
<p>systemctl start memcached  </p>
<h1 id="安装keystone（认证服务）"><a href="#安装keystone（认证服务）" class="headerlink" title="安装keystone（认证服务）"></a>安装keystone（认证服务）</h1><p>连接数据库  </p>
<p>[root@control-node1 yum.repos.d]# mysql  </p>
<p>创建keystone数据库  </p>
<p>create database keystone;  </p>
<p>数据库授权 密码自己设置，这里为了方便设置123456<br><code>grant all privileges on keystone.* to &#39;keystone&#39;@&#39;localhost&#39; identified by &#39;123456&#39;;  </code></p>
<p><code>grant all privileges on keystone.* to &#39;keystone&#39;@&#39;%&#39; identified by &#39;123456&#39;;  </code></p>
<p>keystone使用httpd的mod_wsgi运行在端口5000和35357处理认证服务请求。默认情况下，keystone服务依然监听在5000和35357端口。    </p>
<p>安装 keystone和wsgi  </p>
<p>yum install openstack-keystone httpd mod_wsgi  </p>
<p>修改keystone配置文件  </p>
<p>vim &#x2F;etc&#x2F;keystone&#x2F;keystone.conf  </p>
<p>connection &#x3D; mysql+pymysql:&#x2F;&#x2F;keystone:<a href="mailto:&#49;&#50;&#51;&#x34;&#53;&#54;&#64;&#x31;&#x39;&#x32;&#46;&#49;&#x36;&#56;&#46;&#52;&#x2e;&#54;">&#49;&#50;&#51;&#x34;&#53;&#54;&#64;&#x31;&#x39;&#x32;&#46;&#49;&#x36;&#56;&#46;&#52;&#x2e;&#54;</a>&#x2F;keystone #加入连接数据库配置  </p>
<p>配置使用哪种产生token方式目前keystone支持4种(UUID、PKI、PKIZ、Fernet)这里我们配置fernet<br><a href="http://www.tuicool.com/articles/jQJNFrn">http://www.tuicool.com/articles/jQJNFrn</a> 这篇文章有几种模式的详细介绍。  </p>
<p>[token]<br>provider &#x3D; fernet  </p>
<p>同步数据库  </p>
<p>su -s &#x2F;bin&#x2F;sh -c “keystone-manage db_sync” keystone #发现同步数据库就是错了也没有反应，需要检查keystone的日志文件查看是否正确  </p>
<p>初始化fernet key<br>keystone-manage fernet_setup –keystone-user keystone –keystone-group keystone</p>
<p>keystone-manage credential_setup –keystone-user keystone –keystone-group keystoen</p>
<p>创建Bootstrap the Identity service(就是创建admin用户的帐号信息)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">keystone-manage bootstrap --bootstrap-password 123456 \  </span><br><span class="line">--bootstrap-admin-url http://192.168.4.6:35357/v3/ \  </span><br><span class="line">--bootstrap-internal-url http://192.168.4.6:35357/v3/ \  </span><br><span class="line">--bootstrap-public-url http://192.168.122.2:5000/v3/ \  </span><br><span class="line">--bootstrap-region-id RegionOne  </span><br></pre></td></tr></table></figure>

<p>配置apache服务器<br>vim &#x2F;etc&#x2F;httpd&#x2F;conf&#x2F;httpd.conf  </p>
<p>配置成管理网段的ip<br>ServerName 192.168.4.6  </p>
<p>将keystone的配置文件软链接到apache的配置文件<br>ln -s &#x2F;usr&#x2F;share&#x2F;keystone&#x2F;wsgi-keystone.conf  &#x2F;etc&#x2F;httpd&#x2F;conf.d&#x2F;  </p>
<p>设置开机启动<br>systemctl enable httpd  </p>
<p>启动httpd<br>systemctl start httpd<br>检查端口   </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lsof -i:5000</span><br><span class="line">COMMAND   PID   USER   FD   TYPE DEVICE SIZE/OFF NODE NAME</span><br><span class="line">httpd   18883   root    6u  IPv6  57978      0t0  TCP *:commplex-main (LISTEN)</span><br><span class="line">httpd   18894 apache    6u  IPv6  57978      0t0  TCP *:commplex-main (LISTEN)</span><br><span class="line">httpd   18895 apache    6u  IPv6  57978      0t0  TCP *:commplex-main (LISTEN)</span><br><span class="line">httpd   18896 apache    6u  IPv6  57978      0t0  TCP *:commplex-main (LISTEN)</span><br><span class="line">httpd   18897 apache    6u  IPv6  57978      0t0  TCP *:commplex-main (LISTEN)</span><br><span class="line">httpd   18898 apache    6u  IPv6  57978      0t0  TCP *:commplex-main (LISTEN)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>到root下创建环境变量文件  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim /root/openrc</span><br><span class="line">#!/bin/bash</span><br><span class="line">export OS_USERNAME=admin</span><br><span class="line">export OS_PASSWORD=123456 #这个密码是上面Bootstrap the Identity service填的密码</span><br><span class="line">export OS_PROJECT_NAME=admin</span><br><span class="line">export OS_USER_DOMAIN_NAME=default</span><br><span class="line">export OS_PROJECT_DOMAIN_NAME=default</span><br><span class="line">export OS_AUTH_URL=http://192.168.4.6:35357/v3</span><br><span class="line">export OS_IDENTITY_API_VERSION=3</span><br></pre></td></tr></table></figure>

<p>创建域、项目、用户<br>创建service project<br><code>openstack project create --domain default   --description &quot;Service Project&quot; service</code>  </p>
<p>创建user角色<br><code>openstack role create user</code><br>这里不创建普通用户了<br>测试admin用户获取token<br><code>openstack --os-auth-url http://192.168.4.6:35357/v3 token issue</code>  </p>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/keystone.png">  </p>
<h1 id="安装glance镜像服务"><a href="#安装glance镜像服务" class="headerlink" title="安装glance镜像服务"></a>安装glance镜像服务</h1><p>连接Mariadb创建数据库  </p>
<p>create database glance;  </p>
<p>授权</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">grant all privileges on glance.* to &#x27;glance&#x27;@&#x27;localhost&#x27; identified by &#x27;123456&#x27;;  </span><br><span class="line">grant all privileges on glance.* to &#x27;glance&#x27;@&#x27;%&#x27; identified by &#x27;123456&#x27;;  </span><br><span class="line">grant all privileges on glance.* to &#x27;glance&#x27;@&#x27;control-node1.novalocal&#x27; identified by &#x27;123456&#x27;;   </span><br></pre></td></tr></table></figure>
<p>control-xxx换成主机名，我这里就算api.conf里面配置的ip默认还是去连接host是主机名，所以只能在加个主机名授权。</p>
<p>创建glance用户并设置密码<br>openstack user create –domain default –password-prompt glance  </p>
<p>给glance用户添加admin角色权限<br>openstack role add –project service –user glance admin  </p>
<p>创建glance service<br>openstack service create –name glance –description “OpenStack Image” image  </p>
<p>创建glance endpoint  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne image public http://192.168.122.2:9292</span><br><span class="line">openstack endpoint create --region RegionOne image internal http://192.168.4.6:9292</span><br><span class="line">openstack endpoint create --region RegionOne image admin  http://192.168.4.6:9292</span><br></pre></td></tr></table></figure>
<p>安装软件包<br>yum install openstack-glance   </p>
<p>配置glance<br>vim &#x2F;etc&#x2F;glance&#x2F;glance-api.conf  </p>
<p>配置数据库  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[database]</span><br><span class="line">connection = mysql+pymysql://glance:123456@192.168.4.6/glance</span><br></pre></td></tr></table></figure>

<p>配置glance<br>vim &#x2F;etc&#x2F;glance&#x2F;glance-api.conf<br>配置数据库  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[database]</span><br><span class="line">connection = mysql+pymysql://glance:123456@192.168.4.6/glance</span><br><span class="line">配置keystone</span><br><span class="line">[keystone_authtoken]</span><br><span class="line">auth_uri = http://192.168.4.6:5000</span><br><span class="line">auth_url = http://192.168.4.6:35357</span><br><span class="line">memcached_servers = 192.168.4.6:11211</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = default</span><br><span class="line">user_domain_name = default</span><br><span class="line">project_name = service</span><br><span class="line">username = glance</span><br><span class="line">password = 123456</span><br><span class="line"></span><br><span class="line">[paste_deploy]</span><br><span class="line">flavor = keystone</span><br></pre></td></tr></table></figure>

<p>配置镜像存储  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[glance_store]</span><br><span class="line">stores = file,http</span><br><span class="line">default_store = file</span><br><span class="line">filesystem_store_datadir = /var/lib/glance/images/</span><br></pre></td></tr></table></figure>

<p>vim &#x2F;etc&#x2F;glance&#x2F;glance-registry.conf  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[database]</span><br><span class="line">connection = mysql+pymysql://glance:123456@192.168.4.6/glance</span><br><span class="line">配置keystone</span><br><span class="line">[keystone_authtoken]</span><br><span class="line">auth_uri = http://192.168.4.6:5000</span><br><span class="line">auth_url = http://192.168.4.6:35357</span><br><span class="line">memcached_servers = 192.168.4.6:11211</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = default</span><br><span class="line">user_domain_name = default</span><br><span class="line">project_name = service</span><br><span class="line">username = glance</span><br><span class="line">password = 123456</span><br><span class="line"></span><br><span class="line">[paste_deploy]</span><br><span class="line">flavor = keystone</span><br></pre></td></tr></table></figure>

<p>同步数据库<br>su -s &#x2F;bin&#x2F;sh -c “glance-manage db_sync” glance  </p>
<p>设置开机启动<br>systemctl enable openstack-glance-api  </p>
<p>启动服务<br>systemctl start openstack-glance-api  </p>
<p>这些做完后最好在查看下日志，看看是否有错误，每部署完一个组件都这样，这样出错的可以很快定位。  </p>
<p>下载cirros测试一下  </p>
<p>wget <a href="http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img">http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img</a><br><code>openstack image create &quot;cirros&quot; --file cirros-0.3.4-x86_64-disk.img --disk-format qcow2 --container-format bare --public</code>    </p>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/glance.png">  </p>
<p>glance image-list 检查一下镜像是否上传成功  </p>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/glance-2.png">  </p>
<h1 id="安装nova组件"><a href="#安装nova组件" class="headerlink" title="安装nova组件"></a>安装nova组件</h1><h2 id="控制节点安装"><a href="#控制节点安装" class="headerlink" title="控制节点安装"></a>控制节点安装</h2><p>创建数据库<br>create database nova_api;<br>create database nova;  </p>
<p>授权  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">GRANT ALL PRIVILEGES ON nova_api.* TO &#x27;nova&#x27;@&#x27;localhost&#x27;  IDENTIFIED BY &#x27;123456&#x27;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON nova_api.* TO &#x27;nova&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;123456&#x27;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON nova_api.* TO &#x27;nova&#x27;@&#x27;control-node1.novalocal&#x27; IDENTIFIED BY &#x27;123456&#x27;;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON nova.* TO &#x27;nova&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;123456&#x27;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON nova.* TO &#x27;nova&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;123456&#x27;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON nova.* TO &#x27;nova&#x27;@&#x27;control-node1.novalocal&#x27; IDENTIFIED BY &#x27;123456&#x27;;  </span><br></pre></td></tr></table></figure>
<p>创建为nova组件创建用户、service、endpoint<br><code>openstack user create --domain default  --password-prompt nova</code>    </p>
<p>给nova用户添加admin角色权限<br><code>openstack role add --project service --user nova admin</code>    </p>
<p>创建service<br><code>openstack service create --name nova  --description &quot;OpenStack Compute&quot; compute</code>  </p>
<p>创建endpoint</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne compute public http://192.168.122.2:8774/v2.1/%\(tenant_id\)s  </span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne compute internal http://192.168.4.6:8774/v2.1/%\(tenant_id\)s  </span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne compute admin http://192.168.4.6:8774/v2.1/%\(tenant_id\)s  </span><br></pre></td></tr></table></figure>

<p>安装nova-api组件<br><code>yum install openstack-nova-api openstack-nova-conductor openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler</code>  </p>
<p>配置nova<br>vim &#x2F;etc&#x2F;nova&#x2F;nova.conf  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">transport_url = rabbit://openstack:123456@192.168.4.6 #配置rabbitmq帐户和密码</span><br><span class="line">my_ip = 192.168.4.6</span><br><span class="line">use_neutron = True</span><br><span class="line">firewall_drive = nova.virt.firewall.NoopFirewallDriver</span><br><span class="line">enabled_apis=osapi_compute,metadata</span><br><span class="line">auth_strategy=keystone</span><br><span class="line"></span><br><span class="line">[api_database]</span><br><span class="line">connection = mysql+pymysql://nova:123456@192.168.4.6/nova_api #配置nova连接数据库</span><br><span class="line"></span><br><span class="line">[database]</span><br><span class="line">connection = mysql+pymysql://nova:123456@192.168.4.6/nova</span><br><span class="line"></span><br><span class="line">[keystone_authtoken]</span><br><span class="line">auth_uri = http://192.168.4.6:5000</span><br><span class="line">auth_url = http://192.168.4.6:35357</span><br><span class="line">memcached_servers = 192.168.4.6:11211</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = default</span><br><span class="line">user_domain_name = default</span><br><span class="line">project_name = service</span><br><span class="line">username = nova</span><br><span class="line">password = 123456</span><br><span class="line">配置novnc</span><br><span class="line">novncproxy_port=6080</span><br><span class="line">novncproxy_base_url=http://211.156.182.144:6080/vnc_auto.html</span><br><span class="line">vncserver_listen=192.168.4.6</span><br><span class="line">[glance]</span><br><span class="line">api_servers = http://192.168.4.6:9292</span><br><span class="line"></span><br><span class="line">[oslo_concurrency]</span><br><span class="line">lock_path = /var/lib/nova/tmp</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>同步数据库<br>su -s &#x2F;bin&#x2F;sh -c “nova-manage api_db sync” nova<br>su -s &#x2F;bin&#x2F;sh -c “nova-manage db sync” nova  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl enable openstack-nova-api.service \</span><br><span class="line">openstack-nova-consoleauth.service \</span><br><span class="line">openstack-nova-scheduler.service \</span><br><span class="line">openstack-nova-conductor.service \</span><br><span class="line">openstack-nova-novncproxy.service</span><br><span class="line"></span><br><span class="line">systemctl start openstack-nova-api.service \</span><br><span class="line">openstack-nova-consoleauth.service \</span><br><span class="line">openstack-nova-scheduler.service \</span><br><span class="line">openstack-nova-conductor.service \</span><br><span class="line">openstack-nova-novncproxy.service</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="计算机节点安装"><a href="#计算机节点安装" class="headerlink" title="计算机节点安装"></a>计算机节点安装</h2><p>安装nova-compute<br>yum install openstack-nova-compute  </p>
<p>配置nova-compute<br>vim &#x2F;etc&#x2F;nova&#x2F;nova.conf  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">enabled_apis = osapi_compute,metadata</span><br><span class="line">transport_url = rabbit://openstack:123456@192.168.4.6 #配置rabbitmq帐号和密码</span><br><span class="line">auth_strategy = keystone</span><br><span class="line">my_ip = 192.168.4.7</span><br><span class="line">use_neutron = True</span><br><span class="line">firewall_driver = nova.virt.firewall.NoopFirewallDriver</span><br><span class="line"></span><br><span class="line">[keystone_authtoken]</span><br><span class="line">auth_uri = http://192.168.4.6:5000</span><br><span class="line">auth_url = http://192.168.4.6:35357</span><br><span class="line">memcached_servers = 192.168.4.6:11211</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = default</span><br><span class="line">user_domain_name = default</span><br><span class="line">project_name = service</span><br><span class="line">username = nova</span><br><span class="line">password = 123456</span><br><span class="line">[vnc]</span><br><span class="line">enabled=true</span><br><span class="line">vncserver_listen=0.0.0.0</span><br><span class="line">vncserver_proxyclient_address=192.168.4.7 #填写本机ip</span><br><span class="line">novncproxy_base_url=http://211.156.182.144:6080/vnc_auto.html #这个填你要用控制节点的public ip</span><br><span class="line"></span><br><span class="line">[glance]</span><br><span class="line">api_servers = http://192.168.4.6:9292</span><br><span class="line"></span><br><span class="line">配置锁路径</span><br><span class="line">[oslo_concurrency]</span><br><span class="line">lock_path = /var/lib/nova/tmp</span><br><span class="line"></span><br><span class="line">[libvirt]</span><br><span class="line">virt_type = qemu #物理服务器就配置kvm虚拟机就配置qemu</span><br></pre></td></tr></table></figure>
<p>设置开机启动<br>systemctl enable libvirtd.service openstack-nova-compute.service  </p>
<p>启动nova-compute<br>systemctl start libvirtd.service openstack-nova-compute.service  </p>
<p>在控制节点查看检查一下compute进程根控制节点连接<br><img src="http://ohx02qrb8.bkt.clouddn.com/nova-compute.png">  </p>
<h1 id="配置neutron"><a href="#配置neutron" class="headerlink" title="配置neutron"></a>配置neutron</h1><h2 id="控制节点安装-1"><a href="#控制节点安装-1" class="headerlink" title="控制节点安装"></a>控制节点安装</h2><p>我这里使用openvswitch不使用linux bridge，因为openvswitch功能比linux Brige功能强太多了。但配置稍微复杂点。  </p>
<p>创建数据库<br>create database neutron;  </p>
<p>授权   </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">GRANT ALL PRIVILEGES ON neutron.* TO &#x27;neutron&#x27;@&#x27;localhost&#x27;  IDENTIFIED BY &#x27;123456&#x27;;    </span><br><span class="line">GRANT ALL PRIVILEGES ON neutron.* TO &#x27;neutron&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;123456&#x27;;    </span><br><span class="line">GRANT ALL PRIVILEGES ON neutron.* TO &#x27;neutron&#x27;@&#x27;control-node1.novalocal&#x27; IDENTIFIED BY &#x27;123456&#x27;;   </span><br></pre></td></tr></table></figure>

<p>创建neutron用户并设置密码<br><code>openstack user create --domain default --password-prompt neutron</code>    </p>
<p>给neutron用户添加admin角色权限<br><code>openstack role add --project service --user neutron admin</code>     </p>
<p>创建neutron service<br><code>openstack service create --name neutron --description &quot;OpenStack Networking&quot; network</code>  </p>
<p>创建neutron endpoint  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne network public http://192.168.122.2:9696  </span><br><span class="line">openstack endpoint create --region RegionOne network admin http://192.168.4.6:9696  </span><br><span class="line">openstack endpoint create --region RegionOne network internal http://192.168.4.6:9696  </span><br></pre></td></tr></table></figure>
<p>安装neutron组件<br>yum -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch  </p>
<p>vim &#x2F;etc&#x2F;neutron&#x2F;neutron.conf  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">[DEFAULT]</span><br><span class="line">service_plugins = router</span><br><span class="line">transport_url = rabbit://openstack:123456@192.168.4.6</span><br><span class="line">auth_strategy = keystone</span><br><span class="line">notify_nova_on_port_status_changes = True</span><br><span class="line">notify_nova_on_port_data_changes = True</span><br><span class="line">state_path = /var/lib/neutron</span><br><span class="line">use_syslog = True</span><br><span class="line">syslog_log_facility = LOG_LOCAL4</span><br><span class="line">log_dir =/var/log/neutron</span><br><span class="line">core_plugin = neutron.plugins.ml2.plugin.Ml2Plugin</span><br><span class="line">base_mac = fa:16:3e:00:00:00</span><br><span class="line">mac_generation_retries = 32</span><br><span class="line">dhcp_lease_duration = 600</span><br><span class="line">dhcp_agent_notification = True</span><br><span class="line">allow_bulk = True</span><br><span class="line">allow_pagination = False</span><br><span class="line">allow_sorting = False</span><br><span class="line">allow_overlapping_ips = True</span><br><span class="line">advertise_mtu = True</span><br><span class="line">agent_down_time = 30</span><br><span class="line">router_scheduler_driver = neutron.scheduler.l3_agent_scheduler.ChanceScheduler</span><br><span class="line">allow_automatic_l3agent_failover = True</span><br><span class="line">dhcp_agents_per_network = 2</span><br><span class="line">api_workers = 9</span><br><span class="line">rpc_workers = 9</span><br><span class="line">network_device_mtu=1450</span><br><span class="line"></span><br><span class="line">[database]</span><br><span class="line">connection = mysql+pymysql://neutron:123456@192.168.4.6/neutron</span><br><span class="line"></span><br><span class="line">[keystone_authtoken]</span><br><span class="line">auth_uri = http://192.168.4.6:5000</span><br><span class="line">auth_url = http://192.168.4.6:35357</span><br><span class="line">memcached_servers = 192.168.4.6:11211</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = default</span><br><span class="line">user_domain_name = default</span><br><span class="line">project_name = service</span><br><span class="line">username = neutron</span><br><span class="line">password = 123456</span><br><span class="line"></span><br><span class="line">[nova]</span><br><span class="line">auth_url = http://192.168.4.6:35357</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = default</span><br><span class="line">user_domain_name = default</span><br><span class="line">region_name = RegionOne</span><br><span class="line">project_name = service</span><br><span class="line">username = nova</span><br><span class="line">password = 123456</span><br><span class="line">[oslo_concurrency]</span><br><span class="line">lock_path = /var/lib/neutron/tmp</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>配置modular layer 2（ml2）插件  </p>
<p>vim &#x2F;etc&#x2F;neutron&#x2F;plugins&#x2F;ml2&#x2F; ml2_conf.ini  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">type_drivers = flat,vxlan</span><br><span class="line">tenant_network_types = vxlan</span><br><span class="line">mechanism_drivers = openvswitch,l2population</span><br><span class="line">extension_drivers = port_security</span><br><span class="line"></span><br><span class="line">[ml2]</span><br><span class="line">path_mtu = 1450</span><br><span class="line">type_drivers = flat,vxlan</span><br><span class="line">tenant_network_types = vxlan</span><br><span class="line">physical_network_mtus =physnet1:1500</span><br><span class="line"></span><br><span class="line">[ml2_type_flat]</span><br><span class="line">flat_networks =*</span><br><span class="line"></span><br><span class="line">[ml2_type_vxlan]</span><br><span class="line">vni_ranges =2:65535</span><br><span class="line">vxlan_group =224.0.0.1</span><br><span class="line"></span><br><span class="line">[securitygroup]</span><br><span class="line">enable_security_group = True</span><br><span class="line">firewall_driver=neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver</span><br><span class="line"></span><br><span class="line">[ovs]</span><br><span class="line">local_ip=192.168.4.6</span><br><span class="line">tunnel_bridge=br-tun</span><br><span class="line">enable_tunneling=True</span><br><span class="line">integration_bridge=br-int</span><br><span class="line">bridge_mappings=physnet1:br-ex</span><br></pre></td></tr></table></figure>

<p>配置l3-agent<br>vim &#x2F;etc&#x2F;neutron&#x2F;l3_agent.ini  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">debug = False</span><br><span class="line">interface_driver =neutron.agent.linux.interface.OVSInterfaceDriver</span><br><span class="line">handle_internal_only_routers = True</span><br><span class="line">metadata_port = 8775</span><br><span class="line">send_arp_for_ha = 3</span><br><span class="line">periodic_interval = 40</span><br><span class="line">periodic_fuzzy_delay = 5</span><br><span class="line">enable_metadata_proxy = True</span><br><span class="line">router_delete_namespaces = True</span><br></pre></td></tr></table></figure>

<p>配置dhcp_agent<br>vim &#x2F;etc&#x2F;neutron&#x2F;dhcp_agent.ini</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">resync_interval = 30</span><br><span class="line">interface_driver =neutron.agent.linux.interface.OVSInterfaceDriver</span><br><span class="line">enable_isolated_metadata = True</span><br><span class="line">enable_metadata_network = False</span><br><span class="line">dhcp_domain = openstacklocal</span><br><span class="line">dhcp_broadcast_reply = False</span><br><span class="line">dhcp_delete_namespaces = True</span><br><span class="line">root_helper=sudo neutron-rootwrap /etc/neutron/rootwrap.conf</span><br><span class="line">state_path=/var/lib/neutron</span><br></pre></td></tr></table></figure>
<p>vim &#x2F;etc&#x2F;neutron&#x2F;plugins&#x2F;ml2&#x2F;openvswitch_agent.ini  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[agent]</span><br><span class="line">polling_interval = 2</span><br><span class="line">tunnel_types = vxlan</span><br><span class="line">vxlan_udp_port = 4789</span><br><span class="line">l2_population = True</span><br><span class="line">prevent_arp_spoofing = False</span><br><span class="line">extensions =</span><br><span class="line">[ovs]</span><br><span class="line">local_ip=192.168.4.6</span><br><span class="line">tunnel_bridge=br-tun</span><br><span class="line">enable_tunneling=True</span><br><span class="line">integration_bridge=br-int</span><br><span class="line">bridge_mappings=physnet1:br-ex</span><br><span class="line"></span><br><span class="line">[securitygroup]</span><br><span class="line">firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver</span><br><span class="line">enable_security_group = true</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>设置openvswitch开机启动<br>systemctl enable openvswitch.service  </p>
<p>启动openvswitch<br>systemctl start openvswitch  </p>
<p>创建br-ex br-tun br-int<br>ovs-vsctl add-br br-int<br>ovs-vsctl add-br br-ex<br>ovs-vsctl add-br br-tun  </p>
<p>将上外网网卡挂载到br-ex上<br>ovs-vsctl add-port br-ex eth2</p>
<p>设置开机启动项<br>systemctl enable neutron-openvswitch-agent.service<br>启动进程<br>systemctl start neutron-openvswitch-agent.service  </p>
<h2 id="配置计算节点neutron"><a href="#配置计算节点neutron" class="headerlink" title="配置计算节点neutron"></a>配置计算节点neutron</h2><p>配置一下内核参数<br>修改配置文件 &#x2F;etc&#x2F;sysctl.conf</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">net.ipv4.conf.all.rp_filter=0</span><br><span class="line">net.ipv4.conf.default.rp_filter=0</span><br></pre></td></tr></table></figure>
<p>sysctl –p<br>yum -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch   </p>
<p>vim &#x2F;etc&#x2F;neutron&#x2F;neutron.conf</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">transport_url = rabbit://openstack:123456@192.168.4.6</span><br><span class="line">auth_strategy = keystone</span><br><span class="line"></span><br><span class="line">[keystone_authtoken]</span><br><span class="line">auth_uri = http://192.168.4.6:5000</span><br><span class="line">auth_url = http://192.168.4.6:35357</span><br><span class="line">memcached_servers = 192.168.4.6:11211</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = default</span><br><span class="line">user_domain_name = default</span><br><span class="line">project_name = service</span><br><span class="line">username = neutron</span><br><span class="line">password = 123456</span><br><span class="line"></span><br><span class="line">[oslo_concurrency]</span><br><span class="line">lock_path = /var/lib/neutron/tmp</span><br></pre></td></tr></table></figure>

<p>vim &#x2F;etc&#x2F;neutron&#x2F;plugins&#x2F;ml2&#x2F;ml2_conf.ini</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[ml2]</span><br><span class="line">path_mtu = 1450</span><br><span class="line">type_drivers = flat,vxlan</span><br><span class="line">tenant_network_types = vxlan</span><br><span class="line">physical_network_mtus =physnet1:1500</span><br><span class="line">mechanism_drivers = openvswitch,l2population</span><br><span class="line">extension_drivers = port_security</span><br><span class="line"></span><br><span class="line">[securitygroup]</span><br><span class="line">enable_ipset = true</span><br><span class="line">firewall_driver=neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver</span><br></pre></td></tr></table></figure>

<p>vim &#x2F;etc&#x2F;neutron&#x2F;plugins&#x2F;ml2&#x2F;openvswitch_agent.ini  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[ovs]</span><br><span class="line">local_ip=192.168.4.7</span><br><span class="line">tunnel_bridge=br-tun</span><br><span class="line">enable_tunneling=True</span><br><span class="line">integration_bridge=br-int</span><br><span class="line">bridge_mappings=physnet1:br-ex</span><br><span class="line"></span><br><span class="line">[agent]</span><br><span class="line">enable_distributed_routing=True</span><br><span class="line">prevent_arp_spoofing=True</span><br><span class="line">arp_responder=True</span><br><span class="line">polling_interval=2</span><br><span class="line">drop_flows_on_start=False</span><br><span class="line">vxlan_udp_port=4789</span><br><span class="line">l2_population=True</span><br><span class="line">tunnel_types=vxlan</span><br><span class="line"></span><br><span class="line">[securitygroup]</span><br><span class="line">firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver</span><br><span class="line">enable_security_group = true</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>systemctl enable openvswitch.service<br>systemctl start openvswitch.service   </p>
<p>创建br-ex、br-int、br-tun<br>ovs-vsctl add-br br-int<br>ovs-vsctl add-br br-ex<br>ovs-vsctl add-br br-tun  </p>
<p>vim &#x2F;etc&#x2F;nova&#x2F;nova.conf  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">network_api_class = nova.network.neutronv2.api.API</span><br><span class="line">security_group_api = neutron</span><br><span class="line">linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver</span><br><span class="line"></span><br><span class="line">[neutron]</span><br><span class="line">url = http://192.168.4.6:9696</span><br><span class="line">auth_url = http://192.168.4.6:35357</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = default</span><br><span class="line">user_domain_name = default</span><br><span class="line">region_name = RegionOne</span><br><span class="line">project_name = service</span><br><span class="line">username = neutron</span><br><span class="line">password = 123456</span><br></pre></td></tr></table></figure>

<p>systemctl restart neutron-openvswitch-agent.service<br>systemctl restart openstack-nova-compute  </p>
<p>安装完后可以在在控制节点检查是否安装成功<br><img src="http://ohx02qrb8.bkt.clouddn.com/neutron.png"></p>
<h1 id="安装控制台"><a href="#安装控制台" class="headerlink" title="安装控制台"></a>安装控制台</h1><p>yum install openstack-dashboard<br>vim &#x2F;etc&#x2F;openstack-dashboard&#x2F;local_settings  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">这里配置控制节点ip</span><br><span class="line">OPENSTACK_HOST = &quot;192.168.4.6&quot;</span><br><span class="line">配置允许所有节点访问</span><br><span class="line">ALLOWED_HOSTS = [&#x27;*&#x27;, ]</span><br><span class="line">配置memcache</span><br><span class="line">SESSION_ENGINE = &#x27;django.contrib.sessions.backends.cache&#x27;</span><br><span class="line">CACHES = &#123;</span><br><span class="line">    &#x27;default&#x27;: &#123;</span><br><span class="line">         &#x27;BACKEND&#x27;: &#x27;django.core.cache.backends.memcached.MemcachedCache&#x27;,</span><br><span class="line">         &#x27;LOCATION&#x27;: &#x27;192.168.4.6:11211&#x27;,</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">配置keystone v3验证</span><br><span class="line">OPENSTACK_KEYSTONE_URL = &quot;http://%s:5000/v3&quot; % OPENSTACK_HOST</span><br><span class="line">配置域</span><br><span class="line">OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = &#x27;default&#x27;</span><br><span class="line"></span><br><span class="line">配置api版本</span><br><span class="line"></span><br><span class="line">OPENSTACK_API_VERSIONS = &#123;</span><br><span class="line">    &quot;identity&quot;: 3,</span><br><span class="line">    &quot;image&quot;: 2,</span><br><span class="line">    &quot;volume&quot;: 2,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">设置通过控制台默认创建用户的角色是user</span><br><span class="line">OPENSTACK_KEYSTONE_DEFAULT_ROLE = &quot;user&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>重启服务<br>systemctl restart httpd.service memcached.service  </p>
<p>通过<a href="http://control_ip/dashboard%E5%8F%AF%E4%BB%A5%E8%AE%BF%E9%97%AE">http://control_ip/dashboard可以访问</a>  </p>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/dashboard.png"></p>
<p>Admin 登录，密码是你通过keystone创建的，如果不记得查看openrc  </p>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/dashboard-2.png"></p>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/dashboard-2.png"></p>
<p>创建flat网络做float_ip池<br>管理员—&gt;网络——&gt;创建网络  </p>
<p>Phynet1是在ml2.ini里面bridge_mappings定义的br-ex对应的名字，创建完后增加子网，然后在创建个普通网络，创建个路由器，路由器绑定普通子网，创建个主机配置，然后创建vm加入到你创建的普通网络  </p>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/dashboard-4.png">  </p>
<p>这时在vm所在的计算节点或控制节点 ovs-vsctl show  </p>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/dashboard-5.png">  </p>
<p>可以看见计算节点根网络节道隧道已经建立。  </p>
<h1 id="Cinder配置"><a href="#Cinder配置" class="headerlink" title="Cinder配置"></a>Cinder配置</h1><h2 id="配置控制节点"><a href="#配置控制节点" class="headerlink" title="配置控制节点"></a>配置控制节点</h2><p>创建数据库<br>create database cinder;<br>用户授权  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">GRANT ALL PRIVILEGES ON cinder.* TO &#x27;cinder&#x27;@&#x27;localhost&#x27; identified by &#x27;123456&#x27;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON cinder.* TO &#x27;cinder&#x27;@&#x27;%&#x27; identified by &#x27;123456&#x27;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON cinder.* TO &#x27;cinder&#x27;@&#x27;control-node1.novalocal&#x27; identified by &#x27;123456&#x27;;  </span><br></pre></td></tr></table></figure>
<p>创建用户<br><code>openstack user create --domain default --password-prompt cinder</code>  </p>
<p>给cinder用户赋予admin权限<br><code>openstack role add --project service --user cinder admin</code>    </p>
<p><code>openstack service create --name cinder --description &quot;OpenStack Block Storage&quot; volume</code>    </p>
<p><code>openstack service create --name cinderv2 --description &quot;OpenStack Block Storage&quot; volumev2</code>    </p>
<p>创建endpoint  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne \</span><br><span class="line">volume public http://192.168.122.2:8776/v1/%\(tenant_id\)s</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne \</span><br><span class="line">volume internal http://192.168.4.6:8776/v1/%\(tenant_id\)s</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne \</span><br><span class="line">volume admin http://192.168.4.6:8776/v1/%\(tenant_id\)s</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne \</span><br><span class="line">volumev2 public http://192.168.122.2:8776/v2/%\(tenant_id\)s</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne \</span><br><span class="line">volumev2 internal http://192.168.4.6:8776/v2/%\(tenant_id\)s</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne \</span><br><span class="line">volumev2 admin http://192.168.4.6:8776/v2/%\(tenant_id\)s</span><br></pre></td></tr></table></figure>

<p>安装cinder<br>yum install openstack-cinder  </p>
<p>vim &#x2F;etc&#x2F;cinder&#x2F;cinder.conf</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">transport_url = rabbit://openstack:123456@192.168.4.6</span><br><span class="line">auth_strategy = keystone</span><br><span class="line"></span><br><span class="line">[database]</span><br><span class="line">connection = mysql+pymysql://cinder:123456@192.168.4.6/cinder</span><br><span class="line"></span><br><span class="line">[keystone_authtoken]</span><br><span class="line">auth_uri = http://192.168.4.6:5000</span><br><span class="line">auth_url = http://192.168.4.6:35357</span><br><span class="line">memcached_servers = 192.168.4.6:11211</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = default</span><br><span class="line">user_domain_name = default</span><br><span class="line">project_name = service</span><br><span class="line">username = cinder</span><br><span class="line">password = 123456</span><br><span class="line"></span><br><span class="line">[oslo_concurrency]</span><br><span class="line">lock_path = /var/lib/cinder/tmp</span><br></pre></td></tr></table></figure>

<p>同步数据库<br>su -s &#x2F;bin&#x2F;sh -c “cinder-manage db sync” cinder  </p>
<h2 id="配置计算机节点使用cinder"><a href="#配置计算机节点使用cinder" class="headerlink" title="配置计算机节点使用cinder"></a>配置计算机节点使用cinder</h2><p>vim &#x2F;etc&#x2F;nova&#x2F;nova.conf  </p>
<p>[cinder]<br>os_region_name &#x3D; RegionOne  </p>
<p>重启服务<br>systemctl restart openstack-nova-api.service  </p>
<p>设置开机自启cinder<br>systemctl enable openstack-cinder-api.service openstack-cinder-scheduler.service  </p>
<p>systemctl start openstack-cinder-api.service openstack-cinder-scheduler.service  </p>
<h2 id="配置一个存储节点"><a href="#配置一个存储节点" class="headerlink" title="配置一个存储节点"></a>配置一个存储节点</h2><p>安装lvm<br>yum install lvm2  </p>
<p>systemctl enable lvm2-lvmetad.service<br>systemctl start  lvm2-lvmetad.service  </p>
<p>创建个lvm卷<br>pvcreate &#x2F;dev&#x2F;vdb  </p>
<p>创建vg<br>vgcreate cinder-volumes &#x2F;dev&#x2F;vdb  </p>
<p>vim &#x2F;etc&#x2F;lvm&#x2F;lvm.conf</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">devices &#123;</span><br><span class="line">filter = [ &quot;a/vdb/&quot;, &quot;r/.*/&quot;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>安装软件<br>yum install openstack-cinder targetcli python-keystone  </p>
<p>修改cinder配置文件<br>vim &#x2F;etc&#x2F;cinder&#x2F;cinder.conf  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">transport_url = rabbit://openstack:123456@192.168.4.6</span><br><span class="line">verbose = True</span><br><span class="line">auth_strategy = keystone</span><br><span class="line">enabled_backends = lvm</span><br><span class="line">glance_api_servers = http://192.168.4.6:9292</span><br><span class="line">[database]</span><br><span class="line">connection = mysql+pymysql://cinder:123456@192.168.4.6/cinder</span><br><span class="line"></span><br><span class="line">[keystone_authtoken]</span><br><span class="line">auth_uri = http://192.168.4.6:5000</span><br><span class="line">auth_url = http://192.168.4.6:35357</span><br><span class="line">memcached_servers = 192.168.4.6:11211</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = default</span><br><span class="line">user_domain_name = default</span><br><span class="line">project_name = service</span><br><span class="line">username = cinder</span><br><span class="line">password = 123456</span><br><span class="line">[lvm]</span><br><span class="line">volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver</span><br><span class="line">volume_group = cinder-volumes</span><br><span class="line">iscsi_protocol = iscsi</span><br><span class="line">iscsi_helper = lioadm</span><br><span class="line"></span><br><span class="line">[oslo_concurrency]</span><br><span class="line">lock_path = /var/lib/cinder/tmp</span><br><span class="line">systemctl enable openstack-cinder-volume.service target.service  </span><br><span class="line">systemctl start openstack-cinder-volume.service target.service  </span><br></pre></td></tr></table></figure>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/cinder.png"></p>
<p>到控制台创建个卷，并挂载到云主机。  </p>
<h1 id="Ceilometer配置"><a href="#Ceilometer配置" class="headerlink" title="Ceilometer配置"></a>Ceilometer配置</h1><p>Ceilometer使用Mongdb存储meter数据，所以需要先在控制节点安装Mongdb</p>
<p>在控制节点安装Mongdb<br>yum install mongodb-server mongodb  </p>
<p>配置Mongdb<br>vim &#x2F;etc&#x2F;mongod.conf<br>smallfiles &#x3D; true #限制日志大小<br>创建Mongdb数据库和帐户授权<br><img src="http://ohx02qrb8.bkt.clouddn.com/ceilometer.png"><br>替换123456为你自己设置的密码<br>创建用户<br>openstack user create –domain default –password-prompt ceilometer  </p>
<p>给ceilometer用户添加admin角色权限<br>openstack role add –project service –user ceilometer admin<br>创建ceilometer service<br>openstack service create –name ceilometer –description “Telemetry” metering  </p>
<p>创建ceilometer endpoint</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne metering public http://192.168.122.2:8777</span><br><span class="line">openstack endpoint create --region RegionOne metering admin  http://192.168.4.6:8777</span><br><span class="line">openstack endpoint create --region RegionOne metering internal  http://192.168.4.6:8777</span><br></pre></td></tr></table></figure>
<p>安装包<br>yum install openstack-ceilometer-api \<br>openstack-ceilometer-collector \<br>openstack-ceilometer-notification \<br>openstack-ceilometer-central \<br>python-ceilometerclient  </p>
<p>配置ceilometer<br>vim &#x2F;etc&#x2F;ceilometer&#x2F;ceilometer.conf  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">rpc_backend = rabbit</span><br><span class="line">auth_strategy = keystone</span><br><span class="line"></span><br><span class="line">[oslo_messaging_rabbit]</span><br><span class="line">rabbit_host = 192.168.4.6</span><br><span class="line">rabbit_userid = openstack</span><br><span class="line">rabbit_password = 123456</span><br><span class="line"></span><br><span class="line">[keystone_authtoken]</span><br><span class="line">auth_uri = http://192.168.4.6:5000</span><br><span class="line">auth_url = http://192.168.4.6:35357</span><br><span class="line">memcached_servers = 192.168.4.6:11211</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = default</span><br><span class="line">user_domain_name = default</span><br><span class="line">project_name = service</span><br><span class="line">username = ceilometer</span><br><span class="line">password = 123456</span><br><span class="line"></span><br><span class="line">[service_credentials]</span><br><span class="line">auth_type = password</span><br><span class="line">auth_url = http://192.168.4.6:5000/v3</span><br><span class="line">project_domain_name = default</span><br><span class="line">user_domain_name = default</span><br><span class="line">project_name = service</span><br><span class="line">username = ceilometer</span><br><span class="line">password = 123456 ##密码替换成在keystone创建ceilometer时设置的密码</span><br><span class="line">interface = internalURL</span><br><span class="line">region_name = RegionOne</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>创建ceilometer的vhost  </p>
<p>vim &#x2F;etc&#x2F;httpd&#x2F;conf.d&#x2F;wsgi-ceilometer.conf  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Listen 8777</span><br><span class="line"></span><br><span class="line">&lt;VirtualHost *:8777&gt;</span><br><span class="line">    WSGIDaemonProcess ceilometer-api processes=2 threads=10 user=ceilometer group=ceilometer display-name=%&#123;GROUP&#125;</span><br><span class="line">    WSGIProcessGroup ceilometer-api</span><br><span class="line">    WSGIScriptAlias / /usr/lib/python2.7/site-packages/ceilometer/api/app.wsgi</span><br><span class="line">    WSGIApplicationGroup %&#123;GLOBAL&#125;</span><br><span class="line">    ErrorLog /var/log/httpd/ceilometer_error.log</span><br><span class="line">    CustomLog /var/log/httpd/ceilometer_access.log combined</span><br><span class="line">&lt;/VirtualHost&gt;</span><br><span class="line"></span><br><span class="line">WSGISocketPrefix /var/run/httpd</span><br></pre></td></tr></table></figure>
<p>重启httpd<br>systemctl reload httpd.service<br>设置服务开机启动<br>systemctl enable openstack-ceilometer-notification.service \<br>openstack-ceilometer-central.service \<br>openstack-ceilometer-collector.service  </p>
<p>启动进程<br>systemctl start openstack-ceilometer-notification.service \<br>openstack-ceilometer-central.service \<br>openstack-ceilometer-collector.service  </p>
<p>配置glance的ceilometer统计<br>vim &#x2F;etc&#x2F;glance&#x2F;glance-api.conf  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">rpc_backend = rabbit</span><br><span class="line"></span><br><span class="line">[oslo_messaging_amqp]</span><br><span class="line">driver = messagingv2</span><br><span class="line"></span><br><span class="line">[oslo_messaging_rabbit]</span><br><span class="line">rabbit_host = 192.168.4.6</span><br><span class="line">rabbit_userid = openstack</span><br><span class="line">rabbit_password = 123456</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>重启进程<br>systemctl restart openstack-glance-api.service openstack-glance-registry.service  </p>
<p>配置nova的ceilometer统计<br>安装软件<br>yum install openstack-ceilometer-compute python-ceilometerclient python-pecan  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">rpc_backend = rabbit</span><br><span class="line">auth_strategy = keystone</span><br><span class="line"></span><br><span class="line">[oslo_messaging_rabbit]</span><br><span class="line">rabbit_host = 192.168.4.6</span><br><span class="line">rabbit_userid = openstack</span><br><span class="line">rabbit_password = 123456</span><br><span class="line"></span><br><span class="line">[keystone_authtoken]</span><br><span class="line">auth_uri = http://192.168.4.6:5000</span><br><span class="line">auth_url = http://192.168.4.6:35357</span><br><span class="line">memcached_servers = 192.168.4.6:11211</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = default</span><br><span class="line">user_domain_name = default</span><br><span class="line">project_name = service</span><br><span class="line">username = ceilometer</span><br><span class="line">password = 123456  #将密码替换成keystone创建ceilometer用户时设置的密码</span><br><span class="line"></span><br><span class="line">[service_credentials]</span><br><span class="line">auth_url = http://192.168.4.6:5000</span><br><span class="line">project_domain_id = default</span><br><span class="line">user_domain_id = default</span><br><span class="line">auth_type = password</span><br><span class="line">username = ceilometer</span><br><span class="line">project_name = service</span><br><span class="line">password = 123456 #将密码替换成keystone创建ceilometer用户时设置的密码</span><br><span class="line">interface = internalURL</span><br><span class="line">region_name = RegionOne</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>修改nova-compute配置文件  </p>
<p>vim &#x2F;etc&#x2F;nova&#x2F;nova.conf  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">instance_usage_audit = True</span><br><span class="line">instance_usage_audit_period = hour</span><br><span class="line">notify_on_state_change = vm_and_task_state</span><br><span class="line"></span><br><span class="line">[oslo_messaging_amqp]</span><br><span class="line">driver = messagingv2</span><br></pre></td></tr></table></figure>
<p>设置开机启动<br>systemctl enable openstack-ceilometer-compute.service<br>启动ceilometer-compute进程<br>systemctl start openstack-ceilometer-compute.service<br>重启nova-compute<br>systemctl restart openstack-nova-compute.service  </p>
<p>配置块设备使用ceilometer计量服务  </p>
<p>验证<br>ceilometer meter-list  </p>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/ceilometer-2.png"></p>
<p>正常情况下是会出现如上图一些资源的数据的，但我这里默认报  </p>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/ceilometer-3.png"></p>
<p>打debug  </p>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/ceilometer-4.png"></p>
<p>访问被拒绝<br>解决办法：<br>修改httpd.conf<br><img src="http://ohx02qrb8.bkt.clouddn.com/ceilometer-5.png"></p>
<p>systemctl restart httpd<br>在测试应该没问题了。  </p>
<h2 id="Aodh报警服务："><a href="#Aodh报警服务：" class="headerlink" title="Aodh报警服务："></a>Aodh报警服务：</h2><p>创件aodh数据库<br>create database aodh;  </p>
<p>授权  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">GRANT ALL PRIVILEGES ON aodh.* TO &#x27;aodh&#x27;@&#x27;localhost&#x27; identified by &#x27;123456&#x27;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON aodh.* TO &#x27;aodh&#x27;@&#x27;%&#x27; identified by &#x27;123456&#x27;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON aodh.* TO &#x27;aodh&#x27;@&#x27;control-node1.novalocal&#x27; identified by &#x27;123456&#x27;;  </span><br></pre></td></tr></table></figure>
<p>创建用户<br><code>openstack user create --domain default   --password-prompt aodh</code>  </p>
<p>给adoh帐户添加admin权限<br><code>openstack role add --project service --user aodh admin</code>    </p>
<p>添加服务<br><code>openstack service create --name aodh   --description &quot;Telemetry&quot; alarming</code>    </p>
<p>创建endpoint   </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne alarming public http://192.168.122.2:8042  </span><br><span class="line">openstack endpoint create --region RegionOne alarming internal http://192.168.4.6:8042  </span><br><span class="line">openstack endpoint create --region RegionOne alarming admin http://192.168.4.6:8042  </span><br></pre></td></tr></table></figure>
<p>安装软件<br>yum install openstack-aodh-api \<br>openstack-aodh-evaluator \<br>openstack-aodh-notifier \<br>openstack-aodh-listener \<br>openstack-aodh-expirer \<br>python-aodhclient  </p>
<p>修改配置文件  </p>
<p>vim &#x2F;etc&#x2F;aodh&#x2F;aodh.conf  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">transport_url = rabbit://openstack:123456@192.168.4.6</span><br><span class="line">auth_strategy = keystone</span><br><span class="line"></span><br><span class="line">[database]</span><br><span class="line">connection = mysql+pymysql://aodh:123456@192.168.4.6/aodh</span><br><span class="line"></span><br><span class="line">[keystone_authtoken]</span><br><span class="line">auth_uri = http://192.168.4.6:5000</span><br><span class="line">auth_url = http://192.168.4.6:35357</span><br><span class="line">memcached_servers = 192.168.4.6:11211</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = default</span><br><span class="line">user_domain_name = default</span><br><span class="line">project_name = service</span><br><span class="line">username = aodh</span><br><span class="line">password = 123456 #填写通过keystone创建帐户时设置的帐号和密码</span><br><span class="line"></span><br><span class="line">[service_credentials]</span><br><span class="line">auth_type = password</span><br><span class="line">auth_url = http://192.168.4.6:5000/v3</span><br><span class="line">project_domain_name = default</span><br><span class="line">user_domain_name = default</span><br><span class="line">project_name = service</span><br><span class="line">username = aodh</span><br><span class="line">password = 123456 #填写通过keystone创建帐户时设置的帐号和密码</span><br><span class="line">interface = internalURL</span><br><span class="line">region_name = RegionOne</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>systemctl enable openstack-aodh-api.service \<br>openstack-aodh-evaluator.service \<br>openstack-aodh-notifier.service \<br>openstack-aodh-listener.service  </p>
<p>systemctl start openstack-aodh-api.service \<br>openstack-aodh-evaluator.service \<br>openstack-aodh-notifier.service \<br>openstack-aodh-listener.service   </p>
]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title>openstack添加多个不同物理出口的private网络</title>
    <url>/2017/09/13/openstack_multi_private/</url>
    <content><![CDATA[<p>在生产环境中我们有些虚拟机需要同时需要接入两个二层网络，一个生产网络，一个DMZ网络，通常情况下这两个网络也是对应两个不同的物理交换机的，所以对应的计算节点也应该是两个不同的物理出口。  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/openstack_multi_private_1.png"></p>
<p>以上图的环境为例<br>compute-2上的虚拟机需要同时接入两个网络，生产网和DMZ网络，所以在对compute-2接完线后，还需要配置neutron，让br-int根对应的br-dmz桥通过ovs的patch口连接起来，同时生成对应的流表控制，修改openvswitch配置文件使得physnetX根br-dmz mapping。<br>需要注意的是<br>这里control-1接dmz的网络主要是为了dhcp 地址、metadata注入、和根三层网络的NAT。你可以通过neutron availability zone的方式把把dhcp和metadata和l3部署在网络能通的计算节点，这样就不需要控制节点接线了。  </p>
<h3 id="配置方法（组件为openvswitch"><a href="#配置方法（组件为openvswitch" class="headerlink" title="配置方法（组件为openvswitch)"></a>配置方法（组件为openvswitch)</h3><p>这里假设compute-2的br-dmz物理出口为br-ovs-bond2<br>控制节点和计算节点都执行<br>创建ovs桥  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ovs-vsctl add-br br-dmz</span><br></pre></td></tr></table></figure>
<p>添加端口</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ovs-ovsctl add-port br-dmz br-dmz--br-ovs-bond2</span><br></pre></td></tr></table></figure>
<p>设置端口类型为patch</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ovs-vsctl set interface br-dmz--br-ovs-bond2 type=patch</span><br></pre></td></tr></table></figure>
<p>设置patch口的peer端</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ovs-vsctl set interface br-dmz--br-ovs-bond2 options:peer=br-ovs-bond2--br-dmz</span><br></pre></td></tr></table></figure>

<p>在br-ovs-bond2上配置另外一个path口<br>添加端口  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ovs-vsctl add-port br-ovs-bond2 br-ovs-bond2--br-dmz</span><br></pre></td></tr></table></figure>
<p>设置端口类型为patch</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ovs-vsctl set interface br-ovs-bond2--br-dmz type=patch</span><br></pre></td></tr></table></figure>
<p>设置patch口的peer端</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ovs-vsctl set interface br-ovs-bond2--br-dmz options:peer=br-dmz--br-ovs-bond2</span><br></pre></td></tr></table></figure>
<h3 id="控制节点配置"><a href="#控制节点配置" class="headerlink" title="控制节点配置"></a>控制节点配置</h3><p>vim &#x2F;etc&#x2F;neutron&#x2F;plugins&#x2F;ml2&#x2F;ml2_conf.ini  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">network_vlan_ranges =physnet2:1000:2000,physnet3   #在原来基础上新增个physnet3为我新增的，没有明确vlan-id范围可以不写</span><br></pre></td></tr></table></figure>
<p>vim &#x2F;etc&#x2F;neutron&#x2F;plugins&#x2F;ml2&#x2F;openvswitch_agent.ini  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bridge_mappings=physnet2:br-prv,physnet3:br-dmz  #在原来基础上新增个physnet3:br-dmz</span><br></pre></td></tr></table></figure>

<p>重启neutron-server和neutron-openvswitch-agent  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl restart neutron-server  </span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl restart neutron-openvswitch-agent</span><br></pre></td></tr></table></figure>
<h3 id="计算节点配置"><a href="#计算节点配置" class="headerlink" title="计算节点配置"></a>计算节点配置</h3><p>vim &#x2F;etc&#x2F;neutron&#x2F;plugins&#x2F;ml2&#x2F;openvswitch_agent.ini</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bridge_mappings=physnet2:br-prv,physnet3:br-dmz  #在原来基础上新增个physnet3:br-dmz</span><br></pre></td></tr></table></figure>
<p>重启neutron-openvswitch-agent  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl restart neutron-openvswitch-agent</span><br></pre></td></tr></table></figure>
<p>创建网络，输入对应的physnet  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">neutron net-create dmz_net --router:external=true --provider:network_type=vlan --provider:physical_network=physnet3 --provider:segmentation_id=108</span><br></pre></td></tr></table></figure>


<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">neutron subnet-create --gateway 20.52.15.254 --allocation-pool start=20.52.15.30,end=20.52.15.40 --disable-dhcp dmz_net 20.52.15.0/24</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title>openstack nova server_group</title>
    <url>/2017/08/12/openstack_nova_server_group/</url>
    <content><![CDATA[<h3 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h3><p>openstack在I版中增加了server group的概念，设置server group策略可以做为虚机调度的策略，目前支持的策略有”affinity”和”Anti-Affinity” 意思就是亲和性和非亲和性。</p>
<h3 id="使用场景："><a href="#使用场景：" class="headerlink" title="使用场景："></a>使用场景：</h3><p>我们知道nova创建虚机调度是通过nova-schedule的算法去筛选出对应的计算节点，然后调度到上面。<br>在openstack上创建个app高可用集群，需要虚机分布在不同的本nova availability zone的不同的计算节点内，那么我们只需要创建个server group然后配置策略为anti-affinity，创建虚机时选择这个策略，这样创建出来的虚机就会落到不同的计算节点上。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/nova_server_group_1.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/nova_server_group_2.png"></p>
<p>虽然默认的nova-schedule也支持affinity和anti-affinity但nova-schedule并不能支持持久化，只在虚机创建时生效，如果后面执行migration、evacute、resize那高可用就被破坏了。  </p>
<p>server group会将group的信息持久化，每次重新调度都能获得正确的虚机。  </p>
<h3 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h3><p>在nova库中增加了以下三张表instance_group_member、instance_group_policy             instance_groups。  </p>
<p>1，在创建云主机时如果加入了server-group这将instances加入到server_group_member中；<br>2，conductor 从数据库中取得group信息，解析 group 的 policy 并设置调度参数，通过RPC让 scheduler 选择合适的宿主机；<br>3，nova-scheduler将创建请求通过rpc传递给对应的nova-compute；<br>4，nova-comput去访问nova-conductor去获取创建虚机的一些信息；  </p>
<p>启用server_group<br>修改控制节点nova.conf<br>将scheduler_default_filters改成如下。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scheduler_default_filters=RetryFilter,AvailabilityZoneFilter,RamFilter,CoreFilter,DiskFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter</span><br></pre></td></tr></table></figure>
<p>目前dashboard没有Server group接口，只能通过cli的方式。<br>CLI命令创建server_group<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/nova_server_group_5.png"></p>
<p>创建server-group 其中test-2为group的名字 affinity为policy<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/nova_server_group_3.png"></p>
<p>查看哪些云主机使用了此server_group<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/nova_server_group_4.png"></p>
<p>创建云主机时要指定server-group的方法<br>nova boot时加上–hint group&#x3D;server_group_id<br>如  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nova boot --image IMAGE_ID --flavor flavor_id --hint group=group-1  test3  </span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title>POD间网络不通的排查思路</title>
    <url>/2020/06/26/pod_network/</url>
    <content><![CDATA[<h3 id="POD间网络不通"><a href="#POD间网络不通" class="headerlink" title="POD间网络不通"></a>POD间网络不通</h3><h4 id="VXLAN网络模式"><a href="#VXLAN网络模式" class="headerlink" title="VXLAN网络模式"></a>VXLAN网络模式</h4><p>canal网络插件<br>测试以下场景  </p>
<p><img src="https://pic.downk.cc/item/5ed27dbdc2a9a83be5fb3d85.jpg"></p>
<ul>
<li>通过Rancher UI检查system项目下canal网络插件是否运行正常。</li>
<li>在对应项目下以daemonset方式启动多个POD，分布到不同节点，POD内进行ping测试。</li>
<li>POD内直接ping对端宿主机的flannel.1网卡。</li>
<li>在POD所在宿主机上ping对应POD ip。</li>
<li>主机上udp 8472端口是否监听</li>
<li></li>
</ul>
<h4 id="macvlan网络插件"><a href="#macvlan网络插件" class="headerlink" title="macvlan网络插件"></a>macvlan网络插件</h4><p><img src="https://pic.downk.cc/item/5ed27f6fc2a9a83be5fe1778.jpg"></p>
<p>端口单vlan通过场景验证<br>找2台机器worker1和worker2，在2台机器中，分别给需要支持macvlan的网卡手动添加一个IP，假如网卡为eth0。 添加IP 在worker1中执行如下命令</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ip address add 192.168.1.2/24 dev eth0</span><br></pre></td></tr></table></figure>
<p>在worker2中执行如下命令</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ip address add 192.168.1.3/24 dev eth0</span><br></pre></td></tr></table></figure>
<p>检查IP是否添加成功 以worker1为例，添加前  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ip a</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc mq state UP group default qlen 1000</span><br><span class="line">    link/ether 0a:06:7c:64:b7:f6 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.31.15.100/20 brd 172.31.15.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::806:7cff:fe64:b7f6/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>

<p>添加后  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc mq state UP group default qlen 1000</span><br><span class="line">    link/ether 0a:06:7c:64:b7:f6 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.31.15.100/20 brd 172.31.15.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 192.168.1.2/24 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::806:7cff:fe64:b7f6/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>

<p>检查路由规则 给网卡添加IP时，会自动添加路由规则，可通过route -n查看。</p>
<p>添加前</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">route -n</span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">0.0.0.0         172.31.0.1      0.0.0.0         UG    0      0        0 eth0</span><br><span class="line">172.31.0.0      0.0.0.0         255.255.240.0   U     0      0        0 eth0</span><br></pre></td></tr></table></figure>


<p>添加后  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">route -n</span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">0.0.0.0         172.31.0.1      0.0.0.0         UG    0      0        0 eth0</span><br><span class="line">172.31.0.0      0.0.0.0         255.255.240.0   U     0      0        0 eth0</span><br><span class="line">192.168.1.0     0.0.0.0         255.255.255.0   U     0      0        0 eth0</span><br></pre></td></tr></table></figure>
<p>跨主机互ping验证 只要跨主机在网卡上添加ip且互相可以ping通，说明iaas层就没问题，此时worker1和worker2上的eth0网卡详情如下  </p>
<p>worker1 eth0  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc mq state UP group default qlen 1000</span><br><span class="line">    link/ether 0a:06:7c:64:b7:f6 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.31.15.100/20 brd 172.31.15.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 192.168.1.2/24 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::806:7cff:fe64:b7f6/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>

<p>worker2 eth0  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc mq state UP group default qlen 1000</span><br><span class="line">    link/ether 0a:16:e4:de:a5:68 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.31.1.41/20 brd 172.31.15.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 192.168.1.3/24 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::816:e4ff:fede:a568/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>


<p>从worker1上ping worker2机器使用macvlan ip进行互ping</p>
<h4 id="端口多vlan通过场景验证"><a href="#端口多vlan通过场景验证" class="headerlink" title="端口多vlan通过场景验证"></a>端口多vlan通过场景验证</h4><p>验证方式与单端口一致，唯一区别在于多vlan情况下，交换机端口配置的是trunk模式，允许多个vlan通过，也就是交换机本身不会对端口打vlan-tag了，需要通过软件层面给对应的网卡的数据包打上vlan-tag，否则不能通信，在linux操作系统中可以通过建立子vlan口的方式将IP地址配置在子vlan口上，对应的数据包经过子vlan口后会自动打上vlan-tag 实现使用以下方式：</p>
<p>在需要测试的两台worker节点上按下述方法创建好子vlan口，然后在子vlan口上配置ip，互相ping测试。 测试跨子网通信，则需要提前配置好路由，手动添加静态路由由即可。</p>
<p>安装vconfig命令  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum install -y vlan</span><br></pre></td></tr></table></figure>
<p>给eth0设置vlan 404</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vconfig add eth0 404    </span><br><span class="line">//添加eth0.404子接口</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ip address add 192.168.1.101  dev eth0.404  //设置IP</span><br></pre></td></tr></table></figure>
<p>测试路由</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ip route add 192.168.1.0/24 dev eth0.404   </span><br></pre></td></tr></table></figure>
<p>&#x2F;&#x2F;设置路由,192.168.1.0网段的从eth0.404发出</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ifconfig eth0.404 up</span><br></pre></td></tr></table></figure>

<p>删除方式  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vconfig rem eth0.404</span><br></pre></td></tr></table></figure>


<p>实际生产环境中建议配置使用多网卡做Bond方式，保证网卡的高可用，一般推荐模式为Bond1模式即为主备模式，需要提前验证Bond配置是否生效。 </p>
<p>验证</p>
<p>查看bond状态</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat /proc/net/bonding/bond0  </span><br><span class="line">Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)</span><br><span class="line"></span><br><span class="line">Bonding Mode: fault-tolerance (active-backup)</span><br><span class="line">Primary Slave: None</span><br><span class="line">Currently Active Slave: eth1   #当前主接口</span><br><span class="line">MII Status: up</span><br><span class="line">MII Polling Interval (ms): 100</span><br><span class="line">Up Delay (ms): 0</span><br><span class="line">Down Delay (ms): 0</span><br><span class="line"></span><br><span class="line">Slave Interface: eth1</span><br><span class="line">MII Status: up</span><br><span class="line">Speed: Unknown  //openstack虚拟化网卡默认不显示速率</span><br><span class="line">Duplex: Unknown //openstack虚拟化网卡默认不显示双工</span><br><span class="line">Link Failure Count: 0 </span><br><span class="line">Permanent HW addr: fa:16:3e:47:c7:a0</span><br><span class="line">Slave queue ID: 0</span><br></pre></td></tr></table></figure>

<p>测试方法 Down掉当前主接口，进行测试，看是否会进行网卡切换，网络是否会中断。</p>
<p>模拟容器内macvlan网络测试 </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 创建namespace</span><br><span class="line">ip netns add ns1</span><br><span class="line">ip netns add ns2</span><br><span class="line"></span><br><span class="line"># 基于物理网卡创建macvlan网卡</span><br><span class="line">ip link add mv1 link eno1 type macvlan mode bridge</span><br><span class="line">ip link add mv2 link eno1 type macvlan mode bridge</span><br><span class="line"></span><br><span class="line"># 将网卡挂载到对应namespace中(若trunk模式，需要配置子vlan口，将子vlan口加入)  </span><br><span class="line">ip link set mv1 netns ns1</span><br><span class="line">ip link set mv2 netns ns2</span><br><span class="line"></span><br><span class="line"># 启动网卡 </span><br><span class="line">ip netns exec ns1 ip link set dev mv1 up</span><br><span class="line">ip netns exec ns2 ip link set dev mv2 up</span><br><span class="line"></span><br><span class="line"># 设置IP地址  </span><br><span class="line">ip netns exec ns1 ifconfig mv1 192.168.1.50/24 up</span><br><span class="line">ip netns exec ns2 ifconfig mv2 192.168.1.60/24 up</span><br><span class="line"></span><br><span class="line"># 查看端口详细情况  </span><br><span class="line">ip netns exec ns1 ip a</span><br><span class="line">ip netns exec ns2 ip a</span><br><span class="line"></span><br><span class="line"># ping测试</span><br><span class="line">ip netns exec ns1 ping -c 4 192.168.1.60</span><br><span class="line"></span><br><span class="line"># 清除</span><br><span class="line">ip netns del ns1</span><br><span class="line">ip netns del ns2</span><br><span class="line"></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>prometheus监控部署与使用</title>
    <url>/2020/01/03/prometheus_1/</url>
    <content><![CDATA[<h2 id="prometheus部署"><a href="#prometheus部署" class="headerlink" title="prometheus部署"></a>prometheus部署</h2><h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构:"></a>整体架构:</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/prometheus1_9.jpeg"></p>
<h3 id="组件作用："><a href="#组件作用：" class="headerlink" title="组件作用："></a>组件作用：</h3><p>prometheus：用于收集监控指标。<br>grafana：对接prometheus，将收集到的监控指标，图标化展示。<br>altermanager：用于对触发阈值的监控指标进行告警。<br>kube-state-metrics：部署在kubernetes集群中，用于收集kubernetes组件信息和对应的POD信息监控信息。<br>node-export：用于收集节点监控指标。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/stefanprodan/k8s-prom-hpa  </span><br></pre></td></tr></table></figure>
<p>切换到k8s-prom-hpa目录  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f ./prometheus</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>目录内包含了prometheus的配置文件和部署文件<br>查看prometheus  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get pod -n monitoring</span><br><span class="line">NAME                                        READY   STATUS    RESTARTS   AGE</span><br><span class="line">prometheus-64bc56989f-qcs4p                 1/1     Running   1          22h</span><br><span class="line"></span><br><span class="line">kubectl  get svc -n monitoring</span><br><span class="line">NAME                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">prometheus                 NodePort    10.104.215.207   &lt;none&gt;        9090:31190/TCP   22h</span><br></pre></td></tr></table></figure>
<p>访问prometheus<br><a href="http://node_ip:31190/">http://node_ip:31190</a></p>
<p>prometheus内prometheus-cfg.yaml 配置了自动发生规则，会自动将组件注册。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/prometheus1_10.jpeg">  </p>
<h3 id="部署kube-state-metric监控Kubernetes集群"><a href="#部署kube-state-metric监控Kubernetes集群" class="headerlink" title="部署kube-state-metric监控Kubernetes集群"></a>部署kube-state-metric监控Kubernetes集群</h3><p>参考：<br><a href="https://github.com/kubernetes/kube-state-metrics/tree/master/docs">https://github.com/kubernetes/kube-state-metrics/tree/master/docs</a></p>
<p>kube-state-metric对应的版本支持的Kubernetes版本信息<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/prometheus1_3.png"></p>
<p>clone代码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/kubernetes/kube-state-metrics.git -b v1.8.0</span><br></pre></td></tr></table></figure>
<p>部署kube-state-metric<br>在kube-state-metrics&#x2F;kubernetes目录执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f kube-state-metrics-service-account.yaml,kube-state-metrics-cluster-role.yaml,kube-state-metrics-cluster-role-binding.yaml,kube-state-metrics-deployment.yaml,kube-state-metrics-service.yaml   </span><br></pre></td></tr></table></figure>
<p>检查kube-state-metrices部署  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl  get pod -n kube-system -o wide</span><br><span class="line">metrics-server-7575d9677b-fl5rc       1/1     Running   1          21h    10.244.0.14    k8s-master  </span><br><span class="line"></span><br><span class="line">curl --insecure https://10.244.0.14/healthz</span><br><span class="line">ok</span><br></pre></td></tr></table></figure>
<p>默认prometheus会自动发现kube-state-metrices创建的Service对应的端口和指标获取路径  </p>
<p>在prometheus上可以targets上可以看见对应的点  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/prometheus1_4.png"></p>
<h3 id="部署node-export用于节点监控数据收集"><a href="#部署node-export用于节点监控数据收集" class="headerlink" title="部署node-export用于节点监控数据收集"></a>部署node-export用于节点监控数据收集</h3><p>使用helm-chart部署</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">helm install --name node-exporter  --namespace kube-system  prometheus-node-exporter/ </span><br></pre></td></tr></table></figure>

<p>验证是否能取到指标  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl http://127.0.0.1:9100/metrics</span><br></pre></td></tr></table></figure>



<h2 id="grafana部署"><a href="#grafana部署" class="headerlink" title="grafana部署"></a>grafana部署</h2><h3 id="对接prometheus"><a href="#对接prometheus" class="headerlink" title="对接prometheus"></a>对接prometheus</h3><h3 id="添加对应的监控指标展示模板"><a href="#添加对应的监控指标展示模板" class="headerlink" title="添加对应的监控指标展示模板"></a>添加对应的监控指标展示模板</h3><h2 id="配置grafana对监控指标进行展示"><a href="#配置grafana对监控指标进行展示" class="headerlink" title="配置grafana对监控指标进行展示"></a>配置grafana对监控指标进行展示</h2><p>安装grafan</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -d --name=grafana -p 3000:3000 grafana/grafana</span><br></pre></td></tr></table></figure>
<p>访问主机3000端口<br>默认帐号密码：admin&#x2F;admin  </p>
<p>添加DataSource，类型选择prometheus，添加prometheus的地址  </p>
<p>添加监控展示模板，添加主机监控模板和Kubernetes集群监控模板</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/prometheus1_5.png"></p>
<p>导入ID为8919的node监控dashboard<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/prometheus1_6.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/prometheus1_7.png"></p>
<p>导入ID为6417的Kubernetes集群监控dashboard</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/prometheus1_8.png">  </p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Python logging模块</title>
    <url>/2017/09/27/python_logging/</url>
    <content><![CDATA[<p>默认logging函数输出的warning级别的日志。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/logging_1.png"></p>
<h3 id="日志级别"><a href="#日志级别" class="headerlink" title="日志级别"></a>日志级别</h3><p>日志级别大小关系为：CRITICAL &gt; ERROR &gt; WARNING &gt; INFO &gt; DEBUG &gt; NOTSET</p>
<p>logging库提供了多个组件：Logger、Handler、Filter、Formatter：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Logger      对象提供应用程序可直接使用的接口，供应用代码使用；</span><br><span class="line">Handler     发送日志到适当的目的地，比如socket和文件等</span><br><span class="line">Filter      提供了过滤日志信息的方法，控制输出；</span><br><span class="line">Formatter  指定日志输出和显示的具体格式。</span><br></pre></td></tr></table></figure>


<p>通过logging.basicConfig对日志的输出格式配置<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/logging_2.png"></p>
<p>cat test.log<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/logging_3.png"></p>
<p>需要注意的是<br>只有等级大于等于basicConfig定义的level的log才会被输出，比如这里定义的等级为DEBUG、debug、info、warning、error日志等级都大于等于debug</p>
<p>logging.basicConfig各参数<br>level：日志等级<br>format格式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">format: 指定输出的格式和内容，format可以输出很多有用信息，如上例所示:</span><br><span class="line"> %(levelno)s: 打印日志级别的数值</span><br><span class="line"> %(levelname)s: 打印日志级别名称</span><br><span class="line"> %(pathname)s: 打印当前执行程序的路径，其实就是sys.argv[0]</span><br><span class="line"> %(filename)s: 打印当前执行程序名</span><br><span class="line"> %(funcName)s: 打印日志的当前函数</span><br><span class="line"> %(lineno)d: 打印日志的当前行号</span><br><span class="line"> %(asctime)s: 打印日志的时间</span><br><span class="line"> %(thread)d: 打印线程ID</span><br><span class="line"> %(threadName)s: 打印线程名称</span><br><span class="line"> %(process)d: 打印进程ID</span><br><span class="line"> %(message)s: 打印日志信息</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> filename：输出文件名</span><br><span class="line">filemode：写入模式w为直接写入，a为追加</span><br><span class="line">datafmt：输出的时间格式  这里用%Y-</span><br><span class="line">python中时间日期格式化符号：</span><br><span class="line">%y 两位数的年份表示（00-99）</span><br><span class="line">%Y 四位数的年份表示（000-9999）</span><br><span class="line">%m 月份（01-12）</span><br><span class="line">%d 月内中的一天（0-31）</span><br><span class="line">%H 24小时制小时数（0-23）</span><br><span class="line">%I 12小时制小时数（01-12）</span><br><span class="line">%M 分钟数（00=59）</span><br><span class="line">%S 秒（00-59）</span><br><span class="line">%a 本地简化星期名称</span><br><span class="line">%A 本地完整星期名称</span><br><span class="line">%b 本地简化的月份名称</span><br><span class="line">%B 本地完整的月份名称</span><br><span class="line">%c 本地相应的日期表示和时间表示</span><br><span class="line">%j 年内的一天（001-366）</span><br><span class="line">%p 本地A.M.或P.M.的等价符</span><br><span class="line">%U 一年中的星期数（00-53）星期天为星期的开始</span><br><span class="line">%w 星期（0-6），星期天为星期的开始</span><br><span class="line">%W 一年中的星期数（00-53）星期一为星期的开始</span><br><span class="line">%x 本地相应的日期表示</span><br><span class="line">%X 本地相应的时间表示</span><br><span class="line">%Z 当前时区的名称</span><br></pre></td></tr></table></figure>
<h3 id="将日志同时输出到屏幕和文件"><a href="#将日志同时输出到屏幕和文件" class="headerlink" title="将日志同时输出到屏幕和文件"></a>将日志同时输出到屏幕和文件</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/logging_4.png"></p>
<p>输出</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/logging_4_1.png"></p>
<h3 id="logging-日志轮询"><a href="#logging-日志轮询" class="headerlink" title="logging 日志轮询"></a>logging 日志轮询</h3><p>使用TimedRotatingFileHandler设置日志轮转，轮转的方式有两种一种是基于时间的轮转，一种是基于日志文件大小的轮转<br>TimedRotatingFileHandler函数参数说明</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">logging.handlers.TimedRotatingFileHandler(file_name,when=时间单位, interval=&#x27;时间间隔&#x27;,backupCount=&quot;保留的文件个数&quot;)</span><br></pre></td></tr></table></figure>
<p>interval:表示等待多长时间文件重建，重建的文件名等于file_name+suffix</p>
<p>以下面例子说明</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/logging_5.png"></p>
<p>myadd.addHandler(filehandler)的意思是给myapp这个logger添加filehandler这个handler。</p>
<p>执行脚本可以看见每隔一秒会自动生成一个新的日志文件，到满3个时会自动进行一次新的轮转</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/logging_6.png"><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/logging_7.png"></p>
<p>TimedRotatingFileHandler设置的重建时间间隔后，suffix就需要按装下面的进行配置不然删除不了，比如设置的为S则suffix为%Y-%m-%d_%H-%M-%S</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/logging_8.png"></p>
<h3 id="RotatingFileHandler安文件大小切分"><a href="#RotatingFileHandler安文件大小切分" class="headerlink" title="RotatingFileHandler安文件大小切分"></a>RotatingFileHandler安文件大小切分</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/logging_9.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/logging_10.png"></p>
<h3 id="logger实例的父子关系"><a href="#logger实例的父子关系" class="headerlink" title="logger实例的父子关系"></a>logger实例的父子关系</h3><p>通过前面几个例子对比你应该发现了前面我用logging.basicConfig()去设置format，后面我是通过getlogger创建一个logger后，通过setformat方法去给他对应的handler设置format。</p>
<p>root logger是处于最顶层的logger，同时也是默认的logger，如果不创建logger实例默认调用logger.info(),logger.debug(),logger.error()使用</p>
<p>如何得到root logger<br>通过logging.getLogger()和logging.getLogger(“”)得到root logger实例。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/logging_11.png"><br>logger的父子关系<br>logger以name命名方式来表达父子关系<br>比如<br>父<br>logging.getLogger(foo)<br>子<br>logging.getLogger(foo.tar)</p>
<h3 id="effective-level"><a href="#effective-level" class="headerlink" title="effective level"></a>effective level</h3><p>一个looger如果没有指定level，就继承父level，如果父logger也没有就直接继承root的level。<br>handler同样，子没有就继承父的，父也没有的话就继承root的<br>例<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/logging_12.png"></p>
<p>root logger这里没设置logger的setLevel默认是warning，但父logger设置了，所以父logger会将自己的logger setlevel传递给root logger</p>
<h3 id="调用配置好的logging"><a href="#调用配置好的logging" class="headerlink" title="调用配置好的logging"></a>调用配置好的logging</h3><p>正常写程序中只要配置好一个logging，其他程序只要调用他就可以了一种是通过logging.config，一种是通过模块导入</p>
<p>介绍方法二：<br>比如我将配置好的logging写在test.py里面<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/logging_13.png"><br>在另外一个程序中调用它</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import test</span><br><span class="line">test.myapp.info(&quot;test&quot;)</span><br></pre></td></tr></table></figure>
<p>这样输出的就是按test.py里面myapp这个logger配置好的log了</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">http://blog.csdn.net/lizhe_dashuju/article/details/72579705</span><br><span class="line">http://blog.csdn.net/z_johnny/article/details/50812878</span><br><span class="line">http://kenby.iteye.com/blog/1162698</span><br><span class="line">http://blog.csdn.net/chosen0ne/article/details/7319306</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Python socket模块</title>
    <url>/2017/10/01/python_socket/</url>
    <content><![CDATA[<p>socket俗称套接字，应用程序的进程和进程的之间的沟通是通过套接字来进行的。<br>在python中，socket模块来创建套节字。  </p>
<p>在同一台机器上，不同进程之间，通过进程号进行区分，但在不同的机器上，会存在相同的进程号，比如cenos7的PID为1的进程为systemd，在另外一个机器的centos7也是同样的，所以在网络环境下PID并不能唯一标识一个进程，比如主机A也有PID为1的进程，主机B也有PID为1的进程，但此问题，tcp&#x2F;ip协议族已经帮我们解决了，网络层的ip地址可以唯一的确定一台主机，传输层的协议和端口可以唯一确定这台主机上的进程。这样利用三元组(ip+协议+端口）可以与其进程进行交互。  </p>
<h3 id="python-socket编程思路"><a href="#python-socket编程思路" class="headerlink" title="python socket编程思路"></a>python socket编程思路</h3><p>tcp服务端  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">创建socket  [socket.socket(socket.AF_INET,socket.SOCK_STREAM)]</span><br><span class="line">        |</span><br><span class="line">        |</span><br><span class="line">将ip和端口与socket绑定 [socket.bind((host,port))]</span><br><span class="line">        |</span><br><span class="line">        |</span><br><span class="line">监听端口 [socket.listen()]</span><br><span class="line">        |</span><br><span class="line">        |</span><br><span class="line">接收连接，建立连接请求[socket.accept()]</span><br><span class="line">        |</span><br><span class="line">        |</span><br><span class="line">接收数据，并发送数据[socket.recv(),socket.send()]</span><br><span class="line">         |</span><br><span class="line">         |</span><br><span class="line">连接结束，关闭连接。[socket.close()]</span><br></pre></td></tr></table></figure>

<p>tcp客户端  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">创建socket</span><br><span class="line">      |</span><br><span class="line">      |</span><br><span class="line">建立连接[socket.socket(socket.AF_INET,socket.SOCK_STREAM)]</span><br><span class="line">      |</span><br><span class="line">      |</span><br><span class="line">接收数据，同时也发送数据。[socket.recv(),socket.send()]</span><br><span class="line">      |</span><br><span class="line">      |</span><br><span class="line">关闭连接[socket.close()]</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/socket_1.png"></p>
<h3 id="socket功能列表"><a href="#socket功能列表" class="headerlink" title="socket功能列表"></a>socket功能列表</h3><p>socket(family,type[,protocal]) 使用给定的地址族、套接字类型、协议编号（默认为0）来创建套接字。  </p>
<table>
<thead>
<tr>
<th>socket类型</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
<td>socket.AF_UNIX</td>
<td align="center">只能够用于单一的Unix系统进程间通信</td>
</tr>
<tr>
<td>socket.AF_INET</td>
<td align="center">服务器之间网络通信</td>
</tr>
<tr>
<td>socket.AF_INET6</td>
<td align="center">IPv6</td>
</tr>
<tr>
<td>socket.SOCK_STREAM</td>
<td align="center">流式socket , for TCP</td>
</tr>
<tr>
<td>socket.SOCK_DGRAM</td>
<td align="center">数据报式socket , for UDP</td>
</tr>
<tr>
<td>socket.SOCK_RAW</td>
<td align="center">原始套接字，普通的套接字无法处理ICMP、IGMP等网络报文，而SOCK_RAW可以；其次，SOCK_RAW也可以处理特殊的IPv4报文；此外，利用原始套接字，可以通过IP_HDRINCL套接字选项由用户构造IP头</td>
</tr>
<tr>
<td>socket.SOCK_SEQPACKET</td>
<td align="center">可靠的连续数据包服务</td>
</tr>
<tr>
<td>创建TCP Socket：</td>
<td align="center">s&#x3D;socket.socket(socket.AF_INET,socket.SOCK_STREAM)</td>
</tr>
<tr>
<td>创建UDP Socket：</td>
<td align="center">s&#x3D;socket.socket(socket.AF_INET,socket.SOCK_DGRAM)</td>
</tr>
</tbody></table>
<p>socket函数<br>服务端函数  </p>
<table>
<thead>
<tr>
<th>socket函数</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
<td>s.bind()</td>
<td align="center">将套接字绑定到地址, 在AF_INET下,以元组（host,port）的形式表示地址</td>
</tr>
<tr>
<td>s.listen(backlog)</td>
<td align="center">开始监听TCP传入连接。backlog指定在拒绝连接之前，操作系统可以挂起的最大连接数量。该值至少为1，大部分应用程序设为5就可以了。</td>
</tr>
<tr>
<td>s.accept()</td>
<td align="center">接受TCP连接并返回（conn,address）,中conn是新的套接字对象，可以用来接收和发送数据。address是连接客户端的地址。 accept默认是阻塞，当有connect过来时才会打开</td>
</tr>
</tbody></table>
<p>客户端socket函数    </p>
<table>
<thead>
<tr>
<th>socket函数</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
<td>s.connect(address)</td>
<td align="center">连接到address处的套接字。一般address的格式为元组（hostname,port），如果连接出错，返回socket.error错误。</td>
</tr>
<tr>
<td>s.connect_ex(adddress)</td>
<td align="center">功能与connect(address)相同，但是成功返回0，失败返回errno的值。</td>
</tr>
</tbody></table>
<p>公共socket函数  </p>
<table>
<thead>
<tr>
<th>socket函数</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
<td>s.recv(bufsize[,flag])</td>
<td align="center">接受TCP套接字的数据。数据以字符串形式返回，bufsize指定要接收的最大数据量。flag提供有关消息的其他信息，通常可以忽略。</td>
</tr>
<tr>
<td>s.send(string[,flag])</td>
<td align="center">发送TCP数据。将string中的数据发送到连接的套接字。返回值是要发送的字节数量，该数量可能小于string的字节大小。</td>
</tr>
<tr>
<td>s.sendall(string[,flag])</td>
<td align="center">完整发送TCP数据。将string中的数据发送到连接的套接字，但在返回之前会尝试发送所有数据。成 功返回None，失败则抛出异常。</td>
</tr>
<tr>
<td>s.recvfrom(bufsize[.flag])</td>
<td align="center">接受UDP套接字的数据。与recv()类似，但返回值是（data,address）。其中data是包含接收数据的字符串，address是发送数据的套接字地址。</td>
</tr>
<tr>
<td>s.sendto(string[,flag],address)</td>
<td align="center">发送UDP数据。将数据发送到套接字，address是形式为（ipaddr，port）的元组，指定远程地址。返回值是发送的字节数。</td>
</tr>
<tr>
<td>s.close()</td>
<td align="center">关闭套接字。</td>
</tr>
<tr>
<td>s.getpeername()</td>
<td align="center">返回连接套接字的远程地址。返回值通常是元组（ipaddr,port）。</td>
</tr>
<tr>
<td>s.getsockname()</td>
<td align="center">返回套接字自己的地址。通常是一个元组(ipaddr,port)</td>
</tr>
<tr>
<td>s.setsockopt(level,optname,value)</td>
<td align="center">设置给定套接字选项的值。</td>
</tr>
<tr>
<td>s.getsockopt(level,optname[.buflen])</td>
<td align="center">返回套接字选项的值。</td>
</tr>
<tr>
<td>s.settimeout(timeout)</td>
<td align="center">设置套接字操作的超时期，timeout是一个浮点数，单位是秒值为None表示没有超时期。一般，超时期应该在刚创建套接字时设置，因为它们可能用于连接的操作（如connect()）</td>
</tr>
<tr>
<td>s.fileno()</td>
<td align="center">返回套接字的文件描述符。</td>
</tr>
<tr>
<td>s.setblocking(flag)</td>
<td align="center">如果flag为0，则将套接字设为非阻塞模式，否则将套接字设为阻塞模式（默认值）。非阻塞模式下，如果调用recv()没有发现任何数据，或send()调用无法立即发送数据，那么将引起socket.error异常。</td>
</tr>
<tr>
<td>s.makefile()</td>
<td align="center">创建一个与该套接字相关连的文件</td>
</tr>
</tbody></table>
<h3 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h3><p>server端<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/socket_2.png"></p>
<p>1、使用一个死循环while True：将会使server端一直处于监听状态。<br>2、children,addr&#x3D;s.accept 将创建一个新的socket，这样原先的socket继续侦听，而新的socket将接收client端的数据,addr返回的是客户端的ip。那么问题来了，当客户端发送数据关联时是与哪个socket进行连接呢，首先我们需要知道的是客户端发送的数据有两种，一种是请求建立连接的，一种是已经建立好连接后的数据传输，就如上所说tcp&#x2F;ip有接收缓存和发送缓存，当收到建立连接的请求时，则传给正在监听端口的socket调用accept，当收到连接好连接后的数据传输时，将输据放入接收缓冲区，这样当服务器需要读取数据时调用accept建立的新socket，的recv函数从接收缓冲区读取。<br>3、将socket.accept写在循环里面client新的连接一次将重新创建一个新的socket。  </p>
<p>client端<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/socket_3.png"></p>
<p>1，s.recv(1024)一次最大接收1024字节<br>2,  当收到连接好连接后的数据传输时，将输据放入接收缓冲区，这样当服务器需要读取数据时调用accept建立的新socket，的recv函数从接收缓冲区读取。<br>输出<br>server端  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/socket_4.png"></p>
<p>client端<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/socket_5.png"><br>socket.recv()<br>tcp&#x2F;ip socket在内核中都有一个接收缓冲区和发送缓冲区，当socket接收到数据时，并不是马上调用socket.recv(),而是将数据拷贝到socket中的接收缓冲区中，调用socket.recv()后就是将接收缓冲区的数据，移动到应用层的buff中，并返回。当接收窗口满了后发生的操作是，收端通知发端，停止发送。<br>socket.send()<br>socket.send()是将应用层的buff拷贝到tcp-socket的发送缓冲区中</p>
<p>UDP<br>server端<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/socket_6.png">  </p>
<p>client<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/socket_7.png"></p>
<p>输出<br>server端  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/socket_8.png"></p>
<p>client端<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/socket_9.png"></p>
<h3 id="socket实现tcp简单聊天"><a href="#socket实现tcp简单聊天" class="headerlink" title="socket实现tcp简单聊天"></a>socket实现tcp简单聊天</h3><p>server端<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/socket_10.png"></p>
<p>需要注意的是这里的socket.accept()是写在while循怀外的，因为写在死循怀里，脚本一直在执行，client调用一次accept后就进入阻塞状态了，而client端用的还是旧的connect所以，如果写在循怀里，就是执行client后建立一个连接后只能进行一次对话，因为server端的accept又重新回到阻塞状态了,重新执行client生成一个新的connec又可以一次对话。</p>
<p>client端<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/socket_11.png"></p>
<p>结果<br>server端</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/socket_12.png"></p>
<p>client端<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/socket_13.png"></p>
<p>优化点：<br>1，目前程序没有多线程，IO复用，还是半双工状态，一次只能一个用户说话，一个发时，另一个只能收，server在说话，client就不能说。</p>
<p>参考链接  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">http://www.oschina.net/question/12_76126?sort=default&amp;p=1</span><br><span class="line">https://segmentfault.com/q/1010000000591930</span><br><span class="line">http://www.cnblogs.com/aylin/p/5572104.html</span><br><span class="line">http://blog.csdn.net/rebelqsp/article/details/22109925</span><br><span class="line">http://blog.csdn.net/hguisu/article/details/7445768/</span><br><span class="line">http://blog.csdn.net/rebelqsp/article/details/22191409</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Rancher2.6 Monitoring使用</title>
    <url>/2022/06/29/rancher_monitor/</url>
    <content><![CDATA[<table>
<thead>
<tr>
<th>软件</th>
<th>版本</th>
</tr>
</thead>
<tbody><tr>
<td>Rancher</td>
<td>.9</td>
</tr>
<tr>
<td>Kubernetes</td>
<td>1.23.7+rke2r2</td>
</tr>
</tbody></table>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>Rancher 2.6监控启用方式与之前版本存在较大差异，属于原生的Prometheus-Operator，通过抽象化一些Kubernetes CRD资源，可以更好的把监控告警功能整合起来，提高易用性。Prometheus-operator包括以下CRD资源对象：</p>
<p>PrometheusRules ：定义告警规则</p>
<p>Alert Managers ：Altermanager启动CRD，用于Altermanager启动副本。</p>
<p>Receivers：配置告警接收媒介CRD</p>
<p>Routers： 将告警规则和告警媒介进行匹配。</p>
<p>ServiceMonitor：定义Prometheus采集的监控指标地址</p>
<p>Pod Monitor：更细粒化的对POD进行监控。</p>
<p><img src="https://github.com/prometheus-operator/prometheus-operator/raw/main/Documentation/user-guides/images/architecture.png"></p>
<h3 id="配置使用"><a href="#配置使用" class="headerlink" title="配置使用"></a>配置使用</h3><h4 id="启用监控"><a href="#启用监控" class="headerlink" title="启用监控"></a>启用监控</h4><p>具体方法如下</p>
<p>切换到对应集群，选择左下角clusterTools启用Prometheus</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/2.6-16.png"></p>
<p>部署到System项目中，勾选自定义helm参数</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/2.6-17.png"></p>
<p>根据实际需求修改部署要求</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/2.6-18.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/2.6-19.png"></p>
<p>如果需要对接远端存储如infuxdb需要修改yaml，修改配置指向influxdb。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">remoteRead:</span><br><span class="line">  - url: http://192.168.0.7:8086/api/v1/prom/read?db=prometheus</span><br><span class="line">remoteWrite:</span><br><span class="line">  - url: http://192.168.0.7:8086/api/v1/prom/write?db=prometheus</span><br></pre></td></tr></table></figure>
<p>默认node-Exporter资源limit配置较低，长时间运行后容易被OOMKILL掉，需要修改默认的内存限制为150Mi。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">podLabels:</span><br><span class="line">   jobLabel: node-exporter</span><br><span class="line"> resources:</span><br><span class="line">   limits:</span><br><span class="line">     cpu: 200m</span><br><span class="line">     memory: 150Mi</span><br><span class="line">   requests:</span><br><span class="line">     cpu: 100m</span><br><span class="line">     memory: 30Mi</span><br></pre></td></tr></table></figure>

<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/2.6monitor-1.png"></p>
<p>在此页面可以点击进入对应的组件配置页面。<br>如：</p>
<ul>
<li>Altermanager：进入的是告警信息查看页。</li>
<li>Grafana：查看监控数据图标</li>
<li>Prometheus Graph：Prometheus表达式执行页</li>
<li>Prometheus Rules：查看Prometheus配置的告警表达式页</li>
<li>Prometheus Targets：监控采集数据采集点</li>
</ul>
<h4 id="配置自定义监控指标"><a href="#配置自定义监控指标" class="headerlink" title="配置自定义监控指标"></a>配置自定义监控指标</h4><p>默认启用监控会会自动添加一些ServiceMonitor监控规则和Prometheus Rules 告警规则，主要是针对平台组件监控和集群内节点状态监控和告警</p>
<p>如针对java应用的jmx监控</p>
<p>Jmx有官方的prometheus-export，我们只需要将其jar包下载让java应用程序加载jar包和加载其配置即可。<br>以一个应用为例，整体流程如下：<br>利用JMX exporter，在Java进程内启动一个小型的Http server<br>配置Prometheus抓取那个Http server提供的metrics。<br>配置Grafana连接Prometheus，配置Dashboard。<br>创建文件夹：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir  -p /Dockerfile/jmx-exporter/</span><br></pre></td></tr></table></figure>
<p>下载jmx-export.jar包放到此目录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://github.com/prometheus/jmx_exporter</span><br><span class="line">https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.12.0/jmx_prometheus_javaagent-0.12.0.jar</span><br></pre></td></tr></table></figure>
<p>编写jvm-export配置文件放置&#x2F;root&#x2F;jmx-exporter&#x2F;目录<br>创建simple-config.yml内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">rules:</span><br><span class="line">- pattern: &quot;.*&quot;</span><br></pre></td></tr></table></figure>
<p>这里意思表示将全部监控信息抓取出来。<br>将jvm-export集成到tomcat中，重新编写Dockerfile</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM tomcat</span><br><span class="line">COPY ./jmx_prometheus_javaagent-0.12.0.jar /jmx_prometheus_javaagent-0.12.0.jar</span><br><span class="line">ENV CATALINA_OPTS=&quot;-Xms64m -Xmx128m -javaagent:/jmx-exporter/jmx_prometheus_javaagent-0.12.0.jar=6060:/jmx-exporter/simple-config.yml&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>重新docker build，build后执行以下docker run命令可以查看收集的监控指标，这里6060端口就是我们的jmx-export端口</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker build -t tomcat:v1.0 .</span><br><span class="line">docker run -itd -p 8080:8080 -p 6060:6060 tomcat:v1.0</span><br></pre></td></tr></table></figure>

<p>访问查看：<br><a href="http://host_ip:6060/">http://host_ip:6060</a><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/2.6monitor-2.png"></p>
<p>部署到Rancher平台</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/2.6monitor-3.png"></p>
<p>给Service打上label，用于ServiceMonitor关联</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl label  svc tomcat app=tomcat</span><br></pre></td></tr></table></figure>

<p>创建ServiceMonitor</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: monitoring.coreos.com/v1</span><br><span class="line">kind: ServiceMonitor</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat-app</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  endpoints:</span><br><span class="line">  - port: exporter</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: tomcat</span><br></pre></td></tr></table></figure>

<p>创建成功后通过Prometheus可以查看到对应的Target<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/2.6monitor-4.png"></p>
<p>对应的监控指标也已经抓取</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/2.6monitor-5.png"></p>
<p>进入grafana页面添加dashboard，默认账号密码为admin&#x2F;prom-operator<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/2.6monitor-6.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/2.6monitor-7.png"></p>
<p>添加dashboard</p>
<p>输入dashboard-id，8878，离线环境需要提前将Dashboard下载好，通过json方式导入。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/2.6monitor-8.png"></p>
<h4 id="配置告警"><a href="#配置告警" class="headerlink" title="配置告警"></a>配置告警</h4><p>PrometheusRule用于定义告警规则，默认已经包含针对平台组件和节点的一些告警策略。可以通过配置Router和Receivers配置告警媒介将对应告警通知到相应的人员。采用Routing Tree的告警结构能够快速的将告警进行分类，然后发送到指定的人员进行处理。通过配置AlertmanagerConfig统一实现Rooter和Recivers配置<br>创建AlertmanagerConfig<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/2.6-21.png"></p>
<p>选择Email告警<br>Receivers配置告警媒介<br>填写SMTP地址和配置的账号&#x2F;密码，默认接收的邮箱。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/2.6-35.png"></p>
<p>邮箱密码创建Opaque类型的secret</p>
<p>Routes配置用于告警媒介和告警规则进行匹配，默认创建的root规则，用于匹配全部的告警规则，配置上对应创建的告警媒介。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/2.6-22.png"><br>此时全部的告警规则都会发送给配置的告警媒介</p>
<p>若要细分告警规则创建新的Routes通过Label与Prometheus Rules内对应的Alter name对接</p>
<p>如匹配alert:etcdNoLeader这条告警规则</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/2.6-23.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/2.6-24.png"></p>
<p>也可以使用正则表达式匹配多个规则如</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/2.6-25.png"></p>
<p>Grouping配置主要用于告警规则分类、抑制避免大量无用告警的干扰</p>
<p>group_by：用于配置告警分组，达到告警抑制效果，同一个group的告警只会聚合到一起发送一次，例如host01上运行了数据库，那么对应的告警包含了host down、mysql down。他们配置在一个group内，那么如果host down了对应的mysql肯定也是down了，那么因为他们配置在一个group中，所以host down和mysql down的告警会聚合到一起发送出。</p>
<p>group_wait：新建的AlterGroup等待多久后触发第一次告警。</p>
<p>group_interval：AlterGroup内产生的不同告警触发间隔时间。</p>
<p>repeat_interval：AlterGroup内如果一直是同样的告警，Altermanager为了避免长时间的干扰，进行告警去重的等待时间。 </p>
<p>匹配后，告警触发，可以收到对应的告警邮件</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/2.6-26.png"></p>
<h4 id="自定义告警"><a href="#自定义告警" class="headerlink" title="自定义告警"></a>自定义告警</h4><p>当默认的告警规则不能满足需求时，可以根据实际情况添加自定义告警，实际就是添加对应的PrometheusRule。如以下例子，添加pod非running状态的告警。</p>
<p>UI配置</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/2.6monitor-9.png"></p>
<p>对应yaml配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: monitoring.coreos.com/v1</span><br><span class="line">kind: PrometheusRule</span><br><span class="line">metadata:</span><br><span class="line">  name: podmonitor</span><br><span class="line">  namespace: cattle-monitoring-system</span><br><span class="line">spec:</span><br><span class="line">  groups:</span><br><span class="line">  - name: pod_node_ready</span><br><span class="line">    rules:</span><br><span class="line">    - alert: pod_not_ready</span><br><span class="line">      annotations:</span><br><span class="line">        message: &#x27;&#123;&#123; $labels.namespace &#125;&#125;/&#123;&#123; $labels.pod &#125;&#125; is not ready.&#x27;</span><br><span class="line">      expr: &#x27;sum by (namespace, pod) (kube_pod_status_phase&#123;phase!~&quot;Running|Succeeded&quot;&#125;)</span><br><span class="line">        &gt; 0 &#x27;</span><br><span class="line">      for: 180s</span><br><span class="line">      labels:</span><br><span class="line">        severity: 严重</span><br></pre></td></tr></table></figure>

<p>for：表示持续时间<br>message：表示告警通知内的信息。<br>label.severity：表示告警级别<br>expr：指标获取表达式</p>
<p>配置告警接收者</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/2.6monitor-10.png"></p>
<p>根据标签匹配到这个PrometheusRule<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/2.6monitor-11.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/2.6monitor-12.png"></p>
<h3 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h3><p>1、触发告警后，邮箱收不到告警邮件<br>使用163邮箱SMTP的465端口<br>altermanager报错<br><code>msg=&quot;Notify for alerts failed&quot; num_alerts=1 err=&quot;cattle-monitoring-system-test-test/email[0]: notify retry canceled after 16 attempts: send STARTTLS command: 454 Command not permitted when TLS active&quot;</code></p>
<p>修改</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">spec:</span><br><span class="line">  receivers:</span><br><span class="line">  - emailConfigs:</span><br><span class="line">    - authPassword:</span><br><span class="line">        key: password</span><br><span class="line">        name: altermanager</span><br><span class="line">      authUsername: xx@163.com</span><br><span class="line">      from: xx@163.com</span><br><span class="line">      requireTLS: false</span><br><span class="line">      sendResolved: false</span><br><span class="line">      smarthost: smtp.163.com:465</span><br><span class="line">      tlsConfig: &#123;&#125;</span><br><span class="line">      to: xx@qq.com</span><br></pre></td></tr></table></figure>
<p>添加<code>requireTLS: false</code></p>
<p>2、内部邮件服务器使用非权威证书</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">email_configs:</span><br><span class="line">  - to: &#x27;xxx&#x27;</span><br><span class="line">    insecure_skip_verify: true</span><br></pre></td></tr></table></figure>
<p>添加insecure_skip_verify: true<br>参考链接：</p>
<p><a href="https://mp.weixin.qq.com/s/fT-AXnPP8rrWxTposbi-9A">https://mp.weixin.qq.com/s/fT-AXnPP8rrWxTposbi-9A</a></p>
<p><a href="https://github.com/prometheus-operator/prometheus-operator">https://github.com/prometheus-operator/prometheus-operator</a></p>
<p><a href="https://rancher.com/docs/rancher/v2.6/en/monitoring-alerting/guides/enable-monitoring/">https://rancher.com/docs/rancher/v2.6/en/monitoring-alerting/guides/enable-monitoring/</a><br><a href="https://mp.weixin.qq.com/s/c9QGlwQrhLgptNsnQ1m6-w">https://mp.weixin.qq.com/s/c9QGlwQrhLgptNsnQ1m6-w</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Rancher容器集群跨主机网络模式切换</title>
    <url>/2020/04/28/rancher_network/</url>
    <content><![CDATA[<h3 id="Flannel或canal网络插件"><a href="#Flannel或canal网络插件" class="headerlink" title="Flannel或canal网络插件"></a>Flannel或canal网络插件</h3><h4 id="切换host-gw模式"><a href="#切换host-gw模式" class="headerlink" title="切换host-gw模式"></a>切换host-gw模式</h4><p>默认部署出来是canal的vxlan的模式<br>查看宿主机网卡，会发现多了个flannel.1网卡，这个是建立vxlan隧道的网卡</p>
<p><img src="https://pic.downk.cc/item/5eb5806bc2a9a83be5dbb150.png"></p>
<p>跨主机的网卡都是走flannel.1网卡出去，因为要经过vxlan封包<br><img src="https://pic.downk.cc/item/5eb58081c2a9a83be5dbc218.png"></p>
<p>切换到host-gw模式的方法<br>编辑集群</p>
<p><img src="https://pic.downk.cc/item/5eb57fd8c2a9a83be5db3b64.jpg"></p>
<p><img src="https://pic.downk.cc/item/5eb580d6c2a9a83be5dc03be.png"></p>
<p>将flannel_backend_type改为host-gw<br>保存，等待集群进行更新  </p>
<p>继续修改canal默认的configmap文件</p>
<p>切换到System项目——&gt;资源——&gt;配置映射修改canal-config</p>
<p><img src="https://pic.downk.cc/item/5eb5811fc2a9a83be5dc4127.png"></p>
<p>保存，然后将机器重启<br>在次查看flannel.1网卡已经消失了,所有流量出去通过对应的明细路由进行访问 。</p>
<p><img src="https://pic.downk.cc/item/5eb5816cc2a9a83be5dc7d77.png"></p>
<p><img src="https://pic.downk.cc/item/5eb5817dc2a9a83be5dc8afe.png"></p>
<p>验证<br>创建workload，多个副本，分布到不同主机上，进行跨主机ping测试</p>
<p><img src="https://pic.downk.cc/item/5eb581b4c2a9a83be5dcb6d8.png"></p>
<p><img src="https://pic.downk.cc/item/5eb581c8c2a9a83be5dcc520.png"></p>
<p>在不同主机上互相ping测试，看看跨主机网络是否正常，这里需要注意的是，在公有云和openstack上是ping不通的因为host-gw模式公有云默认都开启了防ARP欺骗，分配的Mac地址和IP都绑定了，非平台分配的mac地址和IP出去会被drop掉。    </p>
<h3 id="calico网络插件"><a href="#calico网络插件" class="headerlink" title="calico网络插件"></a>calico网络插件</h3><p>Calico是一种开源网络和网络安全解决方案，适用于容器，虚拟机和基于主机的本机工作负载。Calico支持广泛的平台，包括Kubernetes，docker，OpenStack和裸机服务。Calico后端支持多种网络模式。</p>
<ul>
<li>BGP模式：将节点做为虚拟路由器通过BGP路由协议来实现集群内容器之间的网络访问。</li>
<li>IPIP模式：在原有IP报文中封装一个新的IP报文，新的IP报文中将源地址IP和目的地址IP都修改为对端宿主机IP。</li>
<li>cross-subnet：Calico-ipip模式和calico-bgp模式都有对应的局限性，对于一些主机跨子网而又无法使网络设备使用BGP的场景可以使用cross-subnet模式，实现同子网机器使用calico-BGP模式，跨子网机器使用calico-ipip模式。</li>
</ul>
<h4 id="calico切换BGP模式"><a href="#calico切换BGP模式" class="headerlink" title="calico切换BGP模式"></a>calico切换BGP模式</h4><p>部署完成后默认使用calico-ipip的模式，通过在节点的路由即可得知，通往其他节点路由通过tunl0网卡出去</p>
<p><img src="https://pic.downk.cc/item/5eb58365c2a9a83be5de25d1.png"></p>
<p>修改为BGP网络模式，在system项目中修改calico-node daemonset</p>
<p><img src="https://pic.downk.cc/item/5eb583bdc2a9a83be5de6a23.png"></p>
<p><img src="https://pic.downk.cc/item/5eb584b0c2a9a83be5df35b0.png"></p>
<p>修改<code>CALICO_IPV4POOL_IPIP</code>改为off，添加新环境变量<code>FELIX_IPINIPENABLED</code>为false  </p>
<p><img src="https://pic.downk.cc/item/5eb5853cc2a9a83be5df9db0.png"></p>
<p>修改完成后对节点进行重启，等待恢复后查看主机路由，与ipip最大区别在于去往其他节点的路由，由Tunnel0走向网络网卡。</p>
<p><img src="https://pic.downk.cc/item/5eb585c1c2a9a83be5e043cb.png"></p>
<h4 id="calico切换cross-subne模式"><a href="#calico切换cross-subne模式" class="headerlink" title="calico切换cross-subne模式"></a>calico切换cross-subne模式</h4><p>Calico-ipip模式和calico-bgp模式都有对应的局限性，对于一些主机跨子网而又无法使网络设备使用BGP的场景可以使用cross-subnet模式，实现同子网机器使用calico-BGP模式，跨子网机器使用calico-ipip模式。</p>
<p>部署集群网络选择calico网络插件<br><img src="https://pic.downk.cc/item/5eb5869bc2a9a83be5e0f0fa.png"></p>
<p>默认部署出来是calico的ip-in-ip的模式<br>查看宿主机网卡，会发现多了个tunl0网卡，这个是建立ip隧道的网卡</p>
<p><img src="https://pic.downk.cc/item/5eb586c3c2a9a83be5e10f13.png"></p>
<p>去其他主机的路由都是走tunl0网卡出去</p>
<p><img src="https://pic.downk.cc/item/5eb58710c2a9a83be5e15062.png"></p>
<p>切换到cross-subnet模式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl edit ipPool/default-ipv4-ippool </span><br></pre></td></tr></table></figure>

<p>将ipipMode改为crossSubnet</p>
<p><img src="https://pic.downk.cc/item/5eb58738c2a9a83be5e1782a.png"><br>在UI将calico-node的POD删了重建</p>
<p><img src="https://pic.downk.cc/item/5eb587fac2a9a83be5e21475.png"></p>
<p>重启检查calico网络<br><img src="https://pic.downk.cc/item/5eb588c7c2a9a83be5e2abbe.png"></p>
<p>可以看见同子网的主机出口走的是bgp，不同子网主机走的是tunl0网卡走ipip模式<br>验证<br>创建应用测试跨主机网络，在不同主机上互相ping测试，看看跨主机网络是否正常。 </p>
<h4 id="配置Route-reflector"><a href="#配置Route-reflector" class="headerlink" title="配置Route reflector"></a>配置Route reflector</h4><p>安装方式<br>Single host上面binary安装<br>Single host上面continer安装<br>作为k8s pod运行<br>实际经验：<br>Binary方式在集群里面的一台worker节点安装（比如RR）<br>calicoctl会检测bird&#x2F;felix的运行状态<br>在非calico node节点运行只能使用部分命令，不能运行calico node相关命令<br>通过配置calicoctl来对calico进行控制，通常情况下建议将  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -O -L  https://github.com/projectcalico/calicoctl/releases/download/v3.13.3/calicoctl</span><br></pre></td></tr></table></figure>
<p>配置可执行权限</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">chmod +x calicoctl</span><br></pre></td></tr></table></figure>
<p>复制的&#x2F;usr&#x2F;bin&#x2F;目录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp calicoctl /usr/bin/</span><br></pre></td></tr></table></figure>
<p>配置calicoctl连接Kubernetes集群</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export CALICO_DATASTORE_TYPE=kubernetes</span><br><span class="line">export CALICO_KUBECONFIG=~/.kube/config</span><br><span class="line">calicoctl node status</span><br></pre></td></tr></table></figure>

<p><img src="https://pic.downk.cc/item/5eb58997c2a9a83be5e34deb.png"></p>
<p>calico node-to-node mesh   </p>
<p>默认情况下calico采用node-to-node mesh方式 ，为了防止BGP路由环路，BGP协议规定在一个AS（自治系统）内部，IBGP路由器之间只能传一跳路由信息，所以在一个AS内部，IBGP路由器之间为了学习路由信息需要建立全互联的对等体关系，但是当一个AS规模很大的时候，这种全互联的对等体关系维护会大量消耗网络和CPU资源，所以这种情况下就需要建立路由反射器以减少IBGP路由器之间的对等体关系数量。<br><img src="https://pic.downk.cc/item/5eb589c7c2a9a83be5e37132.png"></p>
<p>Route reflector角色介绍  </p>
<p>早期calico版本提供专门的route reflector镜像，在新版本calico node内置集成route reflector功能。<br>Route reflector可以是以下角色：  </p>
<ul>
<li>集群内部的node节点</li>
<li>集群外部节点运行calico node</li>
<li>其他支持route reflector的软件或者设备。</li>
</ul>
<p>这里以一个集群内部的node节点为例：</p>
<p>关闭node-to-node mesh    </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | calicoctl apply -f -</span><br><span class="line"></span><br><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">kind: BGPConfiguration</span><br><span class="line">metadata:</span><br><span class="line"> name: default</span><br><span class="line">spec:</span><br><span class="line">  logSeverityScreen: Info</span><br><span class="line">  nodeToNodeMeshEnabled: false</span><br><span class="line">  asNumber: 63400</span><br><span class="line">EOF</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>设置Route reflector<br>配置Route reflector支持多种配置方式如：1、支持配置全局BGP peer，。2、支持针对单个节点进行配置BGP Peer。也可以将calico节点充当Route reflector 这里以配置calico节点充当Router reflector为例。  </p>
<p>配置节点充当BGP Route Reflector  </p>
<p>可将Calico节点配置为充当路由反射器。为此，要用作路由反射器的每个节点必须具有群集ID-通常是未使用的IPv4地址。  </p>
<p>要将节点配置为集群ID为244.0.0.1的路由反射器，请运行以下命令。这里将节点名为rke-node4的节点配置为Route Reflector，若一个集群中要配置主备rr，为了防止rr之间的路由环路，需要将集群ID配置成一样</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">calicoctl patch node rke-node4 -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;bgp&quot;: &#123;&quot;routeReflectorClusterID&quot;: &quot;244.0.0.1&quot;&#125;&#125;&#125;&#x27;</span><br></pre></td></tr></table></figure>
<p>给节点打上对应的label标记该节点以表明它是Route Reflector，从而允许BGPPeer资源选择它。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl label node rke-node4 route-reflector=true</span><br></pre></td></tr></table></figure>
<p>创建BGPPeer</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export CALICO_DATASTORE_TYPE=kubernetes</span><br><span class="line">export CALICO_KUBECONFIG=~/.kube/config</span><br><span class="line">cat &lt;&lt;EOF | calicoctl apply -f -</span><br><span class="line">kind: BGPPeer</span><br><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">metadata:</span><br><span class="line">  name: peer-with-route-reflectors</span><br><span class="line">spec:</span><br><span class="line">  nodeSelector: all()</span><br><span class="line">  peerSelector: route-reflector == &#x27;true&#x27;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p>查看BGP节点状态<br>node上查看，peer type由node-to-node mesh变为 node specific</p>
<p><img src="https://pic.downk.cc/item/5eb58a75c2a9a83be5e3ec4e.png"></p>
<p>Route Reflector上节点查看，节点已正常建立连接<br><img src="https://pic.downk.cc/item/5eb58a98c2a9a83be5e4079e.png"></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>RKE2的简单使用</title>
    <url>/2022/01/01/rke2/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>软件版本：rke2 version v1.22.5+rke2r1<br>os：ubuntu18.04</p>
<p>RKE2是Rancher  Kubernetes新的发行版，结合和k3s和RKE1的一些特性。与RKE1相比主要特性在于安全性，符合美国联邦政府部门的安全性和合规性，完整通过CIS安全基线标准，符合FIPS-140-2 标准和定期的镜像安全扫描。<br>比如结合k3s的一个单体二进制文件启动，底层runtime集成containerd。</p>
<h3 id="与其他Kubernetes部署工具对比"><a href="#与其他Kubernetes部署工具对比" class="headerlink" title="与其他Kubernetes部署工具对比"></a>与其他Kubernetes部署工具对比</h3><table>
<thead>
<tr>
<th></th>
<th>组件集成度</th>
<th>安全性</th>
<th>组件容器化</th>
<th>部署简易性</th>
</tr>
</thead>
<tbody><tr>
<td>kubeadm</td>
<td>低，需要单独部署kubelet、runtime等组件，然后在通过static-pod启动其他组件。</td>
<td>中，默认安全配置</td>
<td>除kubelet外全部容器化</td>
<td>低，组件HA需要用户自己完成。</td>
</tr>
<tr>
<td>RKE-1</td>
<td>低，单独部署runtime然后在通过rke部署集群。</td>
<td>中，默认安全配置</td>
<td>全部容器化</td>
<td>高，一键部署，组件HA自动完成</td>
</tr>
<tr>
<td>RKE-2</td>
<td>高，单体二进制文件集成runtime和kubelet，一键启动。</td>
<td>高,专为安全而生，符合各项安全测试规范</td>
<td>除kubelet外全部容器化</td>
<td>中，每台节点需要单独操作安装，组件HA自动完成</td>
</tr>
</tbody></table>
<h2 id="RKE2部署"><a href="#RKE2部署" class="headerlink" title="RKE2部署"></a>RKE2部署</h2><h3 id="部署前提："><a href="#部署前提：" class="headerlink" title="部署前提："></a>部署前提：</h3><p>Linux部署前提条件：</p>
<ul>
<li>关闭swap。</li>
<li>关闭NetworkManager（若有），或配置NetworkManager忽略 calico&#x2F;flannel 相关网络接口。</li>
<li>关闭Selinux，或参考下述链接配置Selinux规则。</li>
<li>节点主机名采用标准FQDN格式。</li>
</ul>
<p>若需要开启NetworkManager和Selinux，策略配置NetworkManager和Selinux策略链接：<br><a href="https://rancher2.docs.rancher.cn/docs/rke2/known_issues/_index#networkmanager">https://rancher2.docs.rancher.cn/docs/rke2/known_issues/_index#networkmanager</a></p>
<h3 id="通过完整兼容性测试的操作系统："><a href="#通过完整兼容性测试的操作系统：" class="headerlink" title="通过完整兼容性测试的操作系统："></a>通过完整兼容性测试的操作系统：</h3><p>Ubuntu 18.04 (amd64)<br>Ubuntu 20.04 (amd64)<br>CentOS&#x2F;RHEL 7.8 (amd64)<br>CentOS&#x2F;RHEL 8.2 (amd64)<br>SLES 15 SP2 (amd64) (v1.18.16+rke2r1 和更新版本)</p>
<p>注：使用Cilium网络插件时，因为ebpf依赖内核技术，所以需要保证以下内核版本<br>1 、kernel版本 &gt;&#x3D; 4.9.17</p>
<h3 id="通过RKE2单机方式快速部署Kubernetes"><a href="#通过RKE2单机方式快速部署Kubernetes" class="headerlink" title="通过RKE2单机方式快速部署Kubernetes"></a>通过RKE2单机方式快速部署Kubernetes</h3><p>部署Server</p>
<p>下载rke2二进制可执行文件，和自动配置rke2-server</p>
<pre><code>curl -sfL http://rancher-mirror.rancher.cn/rke2/install.sh | INSTALL_RKE2_MIRROR=cn sh -
</code></pre>
<p>设置rke2-server开机自启</p>
<pre><code>systemctl enable rke2-server.service
</code></pre>
<p>启动rke2-server</p>
<pre><code>systemctl start rke2-server.service
</code></pre>
<p>此时，将会通过rke2自动拉起kubelet，然后以static-pod方式启动api-server、Controller-manager、etcd、scheduler</p>
<p>日志查看：</p>
<pre><code>journalctl -u rke2-server -f
</code></pre>
<p>默认情况下rke2将创建以下目录:</p>
<p>&#x2F;var&#x2F;lib&#x2F;rancher&#x2F;rke2&#x2F;:存放额外部署的集群插件（core-dns、网络插件、Ingress-Controller）、etcd数据库存放路径、其他worker连接的token。<br>&#x2F;etc&#x2F;rancher&#x2F;rke2&#x2F;：连接集群的kubeconfig文件，以及集群组件参数配置信息。</p>
<p>将常用CLI配置软链接</p>
<pre><code>ln -s /var/lib/rancher/rke2/bin/kubectl  /usr/bin/kubectl
ln -s /var/lib/rancher/rke2/bin/ctr /usr/bin/ctr
ln -s /var/lib/rancher/rke2/bin/crictl /usr/bin/crictl
</code></pre>
<p>配置kubeconfig</p>
<pre><code>mkdir -p ~/.kube/
cp /etc/rancher/rke2/rke2.yaml ~/.kube/config
</code></pre>
<p>验证查看:</p>
<pre><code>kubectl get node
NAME        STATUS   ROLES                       AGE   VERSION
rke-node6   Ready    control-plane,etcd,master   72m   v1.22.5+rke2r1
</code></pre>
<p>获取worker注册到server的token文件</p>
<pre><code>cat /var/lib/rancher/rke2/server/token 
</code></pre>
<p>部署worker<br>下载rke2二进制可执行文件，和自动配置rke2-server</p>
<pre><code>curl -sfL http://rancher-mirror.rancher.cn/rke2/install.sh | INSTALL_RKE2_MIRROR=cn INSTALL_RKE2_TYPE=&quot;agent&quot;  sh -
</code></pre>
<p>启动rke2-agent服务</p>
<pre><code>systemctl enable rke2-agent.service
</code></pre>
<p>配置rke2-agent服务</p>
<pre><code>mkdir -p /etc/rancher/rke2/
vim /etc/rancher/rke2/config.yaml
</code></pre>
<p>配置文件内容如下：</p>
<pre><code>server: https://&lt;server&gt;:9345
token: &lt;token from server node&gt;
</code></pre>
<p>注：<br>rke2 server 进程通过端口 9345 监听新节点的注册。Kubernetes API 仍然监听端口 6443。</p>
<p>启动服务,等待服务启动注册成功。</p>
<pre><code>systemctl start rke2-agent.service
</code></pre>
<p>日志查看</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">journalctl -u rke2-agent -f</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>查看最终部署</p>
<pre><code>kubectl get node
NAME        STATUS   ROLES                       AGE   VERSION
rke-node6   Ready    control-plane,etcd,master   81m   v1.22.5+rke2r1
rke-node7   Ready    &lt;none&gt;                      70m   v1.22.5+rke2r1
</code></pre>
<p>测试验证</p>
<pre><code>kubectl create deployment test --image=busybox:1.28  --replicas=2   -- sleep 30000 
</code></pre>
<h3 id="通过RKE2高可用方式部署Kubernetes"><a href="#通过RKE2高可用方式部署Kubernetes" class="headerlink" title="通过RKE2高可用方式部署Kubernetes"></a>通过RKE2高可用方式部署Kubernetes</h3><p>前提条件：</p>
<ul>
<li><p>Apiserver统一入口（可选），为了方便外部访问集群，需要在集群实现统一入口，可以通过L4负载均衡器或vip地址或智能轮询DNS。集群内部已经通过rke2-agent实现了worker访问api-server的多入口反向代理。</p>
</li>
<li><p>奇数个（推荐三个）的 server节点，运行 etcd、Kubernetes API 和其他控制节点服务。</p>
</li>
</ul>
<p>部署顺序</p>
<ul>
<li>启动第一个 server 节点</li>
<li>加入其他 server 节点</li>
<li>加入 agent 节点</li>
</ul>
<p>部署负载均衡器（可选）<br>以nginx为例，配置转发到9345和后端6443端口<br>创建nginx.conf文件</p>
<pre><code>events &#123;
  worker_connections  1024;  ## Default: 1024
&#125; 
stream &#123;
    upstream kube-apiserver &#123;
        server host1:6443     max_fails=3 fail_timeout=30s;
        server host2:6443     max_fails=3 fail_timeout=30s;
        server host3:6443     max_fails=3 fail_timeout=30s;
    &#125;
    upstream rke2 &#123;
        server host1:9345     max_fails=3 fail_timeout=30s;
        server host2:9345     max_fails=3 fail_timeout=30s;
        server host3:9345     max_fails=3 fail_timeout=30s;
    &#125;
    server &#123;
        listen 6443;
        proxy_connect_timeout 2s;
        proxy_timeout 900s;
        proxy_pass kube-apiserver;
    &#125;
    server &#123;
        listen 9345;
        proxy_connect_timeout 2s;
        proxy_timeout 900s;
        proxy_pass rke2;
    &#125;
&#125;
</code></pre>
<p>将对应的3个ip地址修改为实际server节点ip地址</p>
<p>启动nginx</p>
<pre><code>docker run -itd -p 9345:9345  -p 6443:6443 -v ~/nginx.conf:/etc/nginx/nginx.conf nginx
</code></pre>
<p>实际生产环境部署建议部署两个nginx，中间通过keepalived维持vip实现统一入口。</p>
<p>部署第一个Server<br>下载rke2二进制可执行文件，和自动配置rke2-server</p>
<pre><code>curl -sfL http://rancher-mirror.rancher.cn/rke2/install.sh | INSTALL_RKE2_MIRROR=cn sh -
</code></pre>
<p>设置rke2-server开机自启</p>
<pre><code>systemctl enable rke2-server.service
</code></pre>
<p>配置config.yaml文件</p>
<pre><code>mkdir /etc/rancher/rke2/ -p 
</code></pre>
<!---->

<pre><code>touch config.yaml
</code></pre>
<p>输入以内容</p>
<pre><code>tls-san:
  - xxx.xxx.xxx.xxx
  - www.xxx.com
</code></pre>
<p>此处填写LB的统一入口ip地址或域名，如果有多个换行分组方式隔开。</p>
<p>启动rke2-server</p>
<pre><code>systemctl start rke2-server.service
</code></pre>
<p>将常用CLI配置软链接</p>
<pre><code>ln -s /var/lib/rancher/rke2/bin/kubectl  /usr/bin/kubectl
ln -s /var/lib/rancher/rke2/bin/ctr /usr/bin/ctr
ln -s /var/lib/rancher/rke2/bin/crictl /usr/bin/crictl
</code></pre>
<p>配置kubeconfig</p>
<pre><code>mkdir -p ~/.kube/
cp /etc/rancher/rke2/rke2.yaml ~/.kube/config
</code></pre>
<p>可以将kubeconfig文件中的中的ip地址由127.0.0.1替换为实际LB的IP地址。</p>
<p>获取注册到server的token文件</p>
<pre><code>cat /var/lib/rancher/rke2/server/token 
</code></pre>
<p>配置其他Server<br>配置rke2-agent服务</p>
<pre><code>mkdir -p /etc/rancher/rke2/
vim /etc/rancher/rke2/config.yaml
</code></pre>
<p>配置文件内容如下：</p>
<pre><code>server: https://&lt;server&gt;:9345
token: &lt;token from server node&gt;
tls-san:
  - xxx.xxx.xxx.xxx
  - www.xxx.com
</code></pre>
<p>注：</p>
<ul>
<li>server地址可以填写第一台Server的地址，也可以填写外部统一入口的地址，最佳实践是填写统一入口地址，这样当第一个Server出现问题后，agent还可以通过统一入口地址通过其他Server获取集群信息。</li>
<li>token填写第一台server的token</li>
<li>tls-san跟第一台server一样，一般填写统一入口的ip地址或域名，用于TLS证书注册。</li>
</ul>
<p>下载rke2二进制可执行文件，和自动配置rke2-server</p>
<pre><code>curl -sfL http://rancher-mirror.rancher.cn/rke2/install.sh | INSTALL_RKE2_MIRROR=cn sh -
</code></pre>
<p>设置rke2-server开机自启</p>
<pre><code>systemctl enable rke2-server.service
</code></pre>
<p>启动rke2-server</p>
<pre><code>systemctl start rke2-server.service
</code></pre>
<p>等待注册和集群启动</p>
<p>验证：</p>
<pre><code>kubectl get node
NAME        STATUS   ROLES                       AGE    VERSION
rke-node4   Ready    control-plane,etcd,master   140m   v1.22.5+rke2r1
rke-node5   Ready    control-plane,etcd,master   138m   v1.22.5+rke2r1
rke-node6   Ready    control-plane,etcd,master   19h    v1.22.5+rke2r1
rke-node7   Ready    &lt;none&gt;                      19h    v1.22.5+rke2r1
</code></pre>
<p>进入etcd-pod，查看etcd集群状态。</p>
<pre><code>etcdctl --cert /var/lib/rancher/rke2/server/tls/etcd/server-client.crt --key /var/lib/rancher/rke2/server/tls/etcd/server-client.key --endpoints https://127.0.0.1:2379 --cacert /var/lib/rancher/rke2/server/tls/etcd/server-ca.crt member list
e19d2834bb177be1, started, rke-node4-896165c9, https://192.168.0.25:2380, https://192.168.0.25:2379, false
ec67af24a94fb07c, started, rke-node6-fed10843, https://192.168.0.32:2380, https://192.168.0.32:2379, false
f7e9f28da0a6e5e6, started, rke-node5-4a4b6af5, https://192.168.0.29:2380, https://192.168.0.29:2379, false
</code></pre>
<p>按单机操作加入agent节点。</p>
<h3 id="通过RKE2离线部署kubernetes集群"><a href="#通过RKE2离线部署kubernetes集群" class="headerlink" title="通过RKE2离线部署kubernetes集群"></a>通过RKE2离线部署kubernetes集群</h3><h4 id="Tarball模式"><a href="#Tarball模式" class="headerlink" title="Tarball模式"></a>Tarball模式</h4><p>RKE2的离线部署方式与k3s比较相似，都是提前将对应的离线介质下载放置到对应的目录，启动二进制进程执行。</p>
<p>在RKE2对应的Release页下载对应的离线安装介质<br><a href="https://github.com/rancher/rke2/releases">https://github.com/rancher/rke2/releases</a><br>主要为以下离线安装介质</p>
<ul>
<li>rke2-images.linux-amd64.tar</li>
<li>rke2.linux-amd64.tar.gz</li>
<li>sha256sum-amd64.txt<br>根据所需要的不同网络插件，下载对应的镜像包</li>
<li>rke2-images-canal.linux-amd64.tar.gz</li>
<li>离线安装脚本</li>
</ul>
<p>将这些下载后的安装介质放置在节点的一个统一目录如&#x2F;root&#x2F;images</p>
<p>下载离线安装脚本</p>
<pre><code>curl -sfL https://get.rke2.io --output install.sh
</code></pre>
<p>部署安装</p>
<pre><code>INSTALL_RKE2_ARTIFACT_PATH=/root/images sh install.sh
</code></pre>
<p>执行此脚本，将自动对离线介质进行解压到对应目录。<br>接下来就跟在线安装一样，启动RKE2的进程，进行部署server和agent<br>启动rke2</p>
<p>设置rke2-server开机自启</p>
<pre><code>systemctl enable rke2-server.service
</code></pre>
<p>启动rke2-server</p>
<pre><code>systemctl start rke2-server.service
</code></pre>
<p>等待注册和集群启动</p>
<h4 id="Private-Registry"><a href="#Private-Registry" class="headerlink" title="Private Registry"></a>Private Registry</h4><p>将镜像上传到镜像仓库<br>可以使用rancher的rancher-load-images.sh脚本结合rke2-images-all.linux-amd64.txt文件进行镜像上传。</p>
<p>下载rke2可执行文件rke2.linux-amd64.tar.gz<br>解压，将systemctl文件和rke2可执行文件复制到对应目录</p>
<pre><code>cp lib/systemd/system/* /usr/local/lib/systemd/system/
</code></pre>
<!---->

<pre><code>cp bin/* /usr/local/bin/
</code></pre>
<!---->

<pre><code>cp share/* /usr/local/share/ -rf
</code></pre>
<p>配置config.yaml，指定默认拉取镜像</p>
<pre><code>system-default-registry: xxx.xxx.xxx.xxx
</code></pre>
<p>若私有镜像仓库为http或自签名https需要在<code>/etc/rancher/rke2 /registries.yaml</code>进行配置<br>但这里我配置的insecure-registry没有生效，具体issue查看：<a href="https://github.com/rancher/rke2/issues/2317">https://github.com/rancher/rke2/issues/2317</a></p>
<h3 id="通过RKE2部署Kubernetes高可用实现原理"><a href="#通过RKE2部署Kubernetes高可用实现原理" class="headerlink" title="通过RKE2部署Kubernetes高可用实现原理"></a>通过RKE2部署Kubernetes高可用实现原理</h3><p>RKE2部署的Kubernetes和其他Kubernetes的组件需要HA的方式是一致的.<br>Kubernetes 集群的高可用是针对：</p>
<ul>
<li>etcd</li>
<li>controller-manager</li>
<li>scheduler</li>
<li>apiserver</li>
</ul>
<p>etcd：通过本身的 Raft 算法 Leader 选主机制，组成ETCD集群，实现 etcd 高可用。</p>
<p>controller manager：leader election 选举竞争锁的机制来保证高可用。</p>
<p>scheduler：leader election 选举竞争锁的机制来保证高可用。</p>
<p>apiserver：无状态，通过前端负载均衡实现高可用。</p>
<p>另外一个在于在rke2集群中，containerd、kubelet组件集成到了rke2服务中，这点和k3s非常相式，同时在rke2服务中还集成了nginx服务，主要用于做为kubelet连接api-server的方向代理。</p>
<p>HA的主要区别在于API-server统一入口，因为RKE2会帮助其他组件自动做HA，</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/RKE-1.png"></p>
<p>当有统一入口时，跟kubeadm和其他原生Kubernetes一样，所有请求都会通过统一负载均衡器连接到后端的rke2-server。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/RKE-2.png"></p>
<p>如果api-server没有统一入口，kubelet和rke2-agent去连接rke2-server时，会用一个server地址去注册即可，然后agent会获取 所有rke2 server 的地址，然后存储到 &#x2F;var&#x2F;lib&#x2F;rancher&#x2F;rke2&#x2F;agent&#x2F;etc&#x2F;rke2-api-server-agent-load-balancer.json中，生成nginx反向代理配置<br>比如：</p>
<pre><code>cat rke2-agent-load-balancer.json
&#123;
  &quot;ServerURL&quot;: &quot;https://192.168.3.10:9345&quot;,
  &quot;ServerAddresses&quot;: [
    &quot;192.168.3.11:9345&quot;,
    &quot;192.168.3.12:9345&quot;
  ],
  &quot;Listener&quot;: null
</code></pre>
<p>当192.168.3.10 挂掉之后，会自动切换到另一个rke2 server 去连接。当192.168.3.10恢复后，回重新连接192.168.3.10。</p>
<p>另外在前面也提到，rke2里面也集成了containerd，那么问题来了，如果rke2-agent进程出现问题down了，是否会影响平台上业务正常运行呢？<br>答案是，不会影响业务正常运行，因为containerd创建容器是通过containerd-shim-runc-v2调用runc创建，当containerd出现问题时containerd-shim-runc-v2会被init进程托管，不会导致退出影响现有业务POD。但需要注意的是rke2-agent退出后kubelet也退出了，对应的业务状态探测就没有了，在默认超时5分钟后，Controller-manager会将业务pod重建。</p>
<h2 id="其他使用技巧"><a href="#其他使用技巧" class="headerlink" title="其他使用技巧"></a>其他使用技巧</h2><h3 id="使用RKE2部署Kubernetes使用其他网络插件"><a href="#使用RKE2部署Kubernetes使用其他网络插件" class="headerlink" title="使用RKE2部署Kubernetes使用其他网络插件"></a>使用RKE2部署Kubernetes使用其他网络插件</h3><p>默认情况下rke2部署使用的是canal做为网络插件，还支持calico和cilium网络插件，若想使用其他网络插件只需要进行配置即可。</p>
<p>如cilium<br>cilium依赖内核bfp特性，在启用前需要先进行挂载。<br>检查是否有进行挂载</p>
<pre><code>mount | grep /sys/fs/bpf
</code></pre>
<p>进行挂载</p>
<pre><code>sudo mount bpffs -t bpf /sys/fs/bpf
sudo bash -c &#39;cat &lt;&lt;EOF &gt;&gt; /etc/fstab
none /sys/fs/bpf bpf rw,relatime 0 0
EOF&#39;
</code></pre>
<p>在次检查</p>
<pre><code>mount | grep /sys/fs/bpf
bpffs on /sys/fs/bpf type bpf (rw,relatime)
bpffs on /sys/fs/bpf type bpf (rw,relatime)
</code></pre>
<p>在start rke2-server和agent服务前先配置config.yaml</p>
<pre><code>mkdir -p /etc/rancher/rke2/
vim /etc/rancher/rke2/config.yaml
</code></pre>
<p>添加以下参数</p>
<pre><code>cni: cilium
</code></pre>
<p>启动rke2-server</p>
<pre><code>systemctl start rke2-server.service
</code></pre>
<p>查看是否部署成功</p>
<pre><code>kubectl get pod -A
NAMESPACE     NAME                                                    READY   STATUS              RESTARTS   AGE
kube-system   cilium-6rfzw                                            1/1     Running             0          52s
kube-system   cilium-node-init-998vd                                  1/1     Running             0          52s
kube-system   cilium-operator-85f67b5cb7-nw7n8                        1/1     Running             0          52s
kube-system   cilium-operator-85f67b5cb7-qc2vh                        0/1     Pending             0          52s
kube-system   cloud-controller-manager-rke-node4                      1/1     Running             0          65s
kube-system   etcd-rke-node4                                          1/1     Running             0          73s
</code></pre>
<h3 id="组件参数配置"><a href="#组件参数配置" class="headerlink" title="组件参数配置"></a>组件参数配置</h3><p>在&#x2F;etc&#x2F;rancher&#x2F;rke2&#x2F;config.yaml 文件中，按照对应组件，添加对应的参数，如apiserver对应为kube-apiserver-arg，组件对应参数为etcd-arg。kube-controller-manager-arg、kube-scheduler-arg、kubelet-arg、kube-proxy-arg。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">etcd-arg:</span><br><span class="line">  - &quot;quota-backend-bytes=858993459&quot;</span><br><span class="line">  - &quot;max-request-bytes=33554432&quot;</span><br><span class="line">kube-apiserver-arg:</span><br><span class="line">  - &quot;watch-cache=true&quot;</span><br><span class="line">kubelet-arg:</span><br><span class="line">  - &quot;system-reserved=cpu=1,memory=2048Mi&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>配置完成后启动rke2-server。agent节点要同步时配置，否则kubelet和kube-proxy参数将不生效</p>
<p>检查参数是否生效<br>如：</p>
<pre><code>ps aux|grep system-reserved
</code></pre>
<h3 id="集群备份和还原"><a href="#集群备份和还原" class="headerlink" title="集群备份和还原"></a>集群备份和还原</h3><p>rke2备份文件保存在每个拥有etcd角色的节点的&#x2F;var&#x2F;lib&#x2F;rancher&#x2F;rke2&#x2F;server&#x2F;db&#x2F;snapshots目录内，拥有多副本保存。<br>默认每隔12小时备份一次，保留5份。</p>
<p>注：目前版本只能通过定时备份，没有立刻备份的选型。<br>将</p>
<p>指定备份文件恢复</p>
<p>关闭rke2-server进程</p>
<pre><code>systemctl stop rke2-server
</code></pre>
<p>指定文件恢复</p>
<pre><code>rke2 server \
  --cluster-reset \
  --cluster-reset-restore-path=&lt;PATH-TO-SNAPSHOT&gt;
</code></pre>
<p>若是HA集群，还原成功后在其他server节点将执行<code>rm -rf /var/lib/rancher/rke2/server/db</code>然后重新启动server，加入集群。</p>
<p>rke2跟rke1一样也支持将备份文件在一个新集群进行还原。</p>
<h3 id="常见操作"><a href="#常见操作" class="headerlink" title="常见操作"></a>常见操作</h3><p>参考链接：<br><a href="https://gist.github.com/superseb/3b78f47989e0dbc1295486c186e944bf">https://gist.github.com/superseb/3b78f47989e0dbc1295486c186e944bf</a></p>
<h4 id="查看本机运行的容器"><a href="#查看本机运行的容器" class="headerlink" title="查看本机运行的容器"></a>查看本机运行的容器</h4><p>ctr命令</p>
<pre><code>/var/lib/rancher/rke2/bin/ctr --address /run/k3s/containerd/containerd.sock --namespace k8s.io container ls
</code></pre>
<p>crictl命令</p>
<pre><code>export CRI_CONFIG_FILE=/var/lib/rancher/rke2/agent/etc/crictl.yaml
/var/lib/rancher/rke2/bin/crictl ps
</code></pre>
<!---->

<pre><code>/var/lib/rancher/rke2/bin/crictl --config /var/lib/rancher/rke2/agent/etc/crictl.yaml ps
</code></pre>
<!---->

<pre><code>/var/lib/rancher/rke2/bin/crictl --runtime-endpoint unix:///run/k3s/containerd/containerd.sock ps -a
</code></pre>
<p>最终都是连接到containerd的socket文件</p>
<h4 id="查看日志"><a href="#查看日志" class="headerlink" title="查看日志"></a>查看日志</h4><pre><code>journalctl -f -u rke2-server
/var/lib/rancher/rke2/agent/containerd/containerd.log
/var/lib/rancher/rke2/agent/logs/kubelet.log
</code></pre>
<h4 id="etcd操作"><a href="#etcd操作" class="headerlink" title="etcd操作"></a>etcd操作</h4><p>etcdctl check perf</p>
<pre><code>for etcdpod in $(kubectl -n kube-system get pod -l component=etcd --no-headers -o custom-columns=NAME:.metadata.name); do kubectl -n kube-system exec $etcdpod -- sh -c &quot;ETCDCTL_ENDPOINTS=&#39;https://127.0.0.1:2379&#39; ETCDCTL_CACERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt&#39; ETCDCTL_CERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.crt&#39; ETCDCTL_KEY=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.key&#39; ETCDCTL_API=3 etcdctl check perf&quot;; done
</code></pre>
<p>etcdctl endpoint status</p>
<pre><code>for etcdpod in $(kubectl -n kube-system get pod -l component=etcd --no-headers -o custom-columns=NAME:.metadata.name); do kubectl -n kube-system exec $etcdpod -- sh -c &quot;ETCDCTL_ENDPOINTS=&#39;https://127.0.0.1:2379&#39; ETCDCTL_CACERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt&#39; ETCDCTL_CERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.crt&#39; ETCDCTL_KEY=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.key&#39; ETCDCTL_API=3 etcdctl endpoint status&quot;; done
</code></pre>
<p>etcdctl endpoint health</p>
<pre><code>for etcdpod in $(kubectl -n kube-system get pod -l component=etcd --no-headers -o custom-columns=NAME:.metadata.name); do kubectl -n kube-system exec $etcdpod -- sh -c &quot;ETCDCTL_ENDPOINTS=&#39;https://127.0.0.1:2379&#39; ETCDCTL_CACERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt&#39; ETCDCTL_CERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.crt&#39; ETCDCTL_KEY=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.key&#39; ETCDCTL_API=3 etcdctl endpoint health&quot;; done
</code></pre>
<p>etcdctl compact</p>
<pre><code>rev=$(kubectl -n kube-system exec $(kubectl -n kube-system get pod -l component=etcd --no-headers -o custom-columns=NAME:.metadata.name | head -1) -- sh -c &quot;ETCDCTL_ENDPOINTS=&#39;https://127.0.0.1:2379&#39; ETCDCTL_CACERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt&#39; ETCDCTL_CERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.crt&#39; ETCDCTL_KEY=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.key&#39; ETCDCTL_API=3 etcdctl endpoint status --write-out fields | grep Revision | cut -d: -f2&quot;)
kubectl -n kube-system exec $(kubectl -n kube-system get pod -l component=etcd --no-headers -o custom-columns=NAME:.metadata.name | head -1) -- sh -c &quot;ETCDCTL_ENDPOINTS=&#39;https://127.0.0.1:2379&#39; ETCDCTL_CACERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt&#39; ETCDCTL_CERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.crt&#39; ETCDCTL_KEY=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.key&#39; ETCDCTL_API=3 etcdctl compact \&quot;$(echo $rev)\&quot;&quot;
</code></pre>
<p>etcdctl defrag</p>
<pre><code>kubectl -n kube-system exec $(kubectl -n kube-system get pod -l component=etcd --no-headers -o custom-columns=NAME:.metadata.name | head -1) -- sh -c &quot;ETCDCTL_ENDPOINTS=&#39;https://127.0.0.1:2379&#39; ETCDCTL_CACERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt&#39; ETCDCTL_CERT=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.crt&#39; ETCDCTL_KEY=&#39;/var/lib/rancher/rke2/server/tls/etcd/server-client.key&#39; ETCDCTL_API=3 etcdctl defrag --cluster&quot;
</code></pre>
<p>对应的，直接操作etcdctl<br>参考：<a href="https://gist.github.com/superseb/3b78f47989e0dbc1295486c186e944bf">https://gist.github.com/superseb/3b78f47989e0dbc1295486c186e944bf</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>rke部署kubernetes</title>
    <url>/2018/03/26/rke%E9%83%A8%E7%BD%B2kubernetes/</url>
    <content><![CDATA[<p>RKE（rancher kubernetes engine)是rancher 发布的一款轻量级的kubernetes部署工具，部署整个集群只需要一个命令和一个配置文件，就可以轻松部署kubernetes集群，还支持kubernetes的ha部署，各类网络plugin。<br>环境:<br>rke-node1 172.31.164.57 （部署节点)<br>rke-node2 172.31.164.58  (master节点、etcd节点)<br>rke-node3 172.31.164.59  (node节点)</p>
<p>软件版本<br>rke 0.1.6<br>docker 17.03</p>
<p>操作系统<br>ubuntu 16.04</p>
<p>准备工作<br>配置hosts<br>172.31.164.57 rke-node1<br>172.31.164.58 rke-node2<br>172.31.164.59 rke-node3</p>
<p>配置免密码登录<br>我们在操作系统是ubuntu 16.04，rke可以直接使用root，centos的话就不行，必须要普通用户，并且属组为docker<br>ssh-copy-id rke-node1<br>ssh-copy-id rke-node2<br>ssh-copy-id rke-node3</p>
<p>关闭selinux<br>关闭防火墙</p>
<p>配置路由转发<br>&#x2F;etc&#x2F;sysctl.conf<br>net.ipv4.ip_forward &#x3D; 1<br>net.bridge.bridge-nf-call-ip6tables &#x3D; 1<br>net.bridge.bridge-nf-call-iptables &#x3D; 1</p>
<p>安装docker（全部节点)<br>因为目前rke部署的kubernetes还是1.8版本的，kubernetes 1.8只支持docker1.12.6、1.13.1、17.03</p>
<p>更新软件源</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apt-get update</span><br></pre></td></tr></table></figure>

<p>安装软件包，允许apt使用https</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apt-get install apt-transport-https ca-certificates curl software-properties-common</span><br></pre></td></tr></table></figure>

<p>导入docker官方key</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -</span><br></pre></td></tr></table></figure>
<p>添加软件源</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot;</span><br></pre></td></tr></table></figure>
<p>在刷新下</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apt-get update</span><br></pre></td></tr></table></figure>
<p>安装指定版本docker<br>列出软件版本</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apt-cache policy docker-ce</span><br></pre></td></tr></table></figure>
<p>安装docker-17.03</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apt-get install docker-ce=17.03.0~ce-0~ubuntu-xenial</span><br></pre></td></tr></table></figure>
<p>启动</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl enable docker &amp;&amp; systemctl start docker</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>安装rke<br><a href="https://github.com/rancher/rke/releases/">https://github.com/rancher/rke/releases/</a></p>
<p>下载<br>rke_linux-amd64</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mv rke_linux-amd64.dms rke</span><br></pre></td></tr></table></figure>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/rke_1.png"></p>
<p>cluster.yml的来源有两种，一种是直接从github上下载完整的<br>下载cluster.yml<br>rke主要依靠cluster.yml去对集群角色进行划分和配置<br>下载</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://github.com/rancher/rke/blob/master/cluster.yml</span><br><span class="line">cluster.yml</span><br></pre></td></tr></table></figure>
<p>修改</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nodes:</span><br><span class="line">- address: 172.31.164.58</span><br><span class="line">  internal_address: &quot;&quot;</span><br><span class="line">  role:</span><br><span class="line">  - controlplane</span><br><span class="line">  - etcd</span><br><span class="line">  hostname_override: rke-node2</span><br><span class="line">  user: root</span><br><span class="line">  docker_socket: /var/run/docker.sock</span><br><span class="line">  ssh_key: &quot;&quot;</span><br><span class="line">  ssh_key_path: ~/.ssh/id_rsa</span><br><span class="line">  labels: &#123;&#125;</span><br><span class="line">- address: 172.31.164.59</span><br><span class="line">  internal_address: &quot;&quot;</span><br><span class="line">  role:</span><br><span class="line">  - worker</span><br><span class="line">  hostname_override: rke-node3</span><br><span class="line">  user: root</span><br><span class="line">  docker_socket: /var/run/docker.sock</span><br><span class="line">  ssh_key: &quot;&quot;</span><br><span class="line">  ssh_key_path: ~/.ssh/id_rsa</span><br><span class="line">  labels: &#123;&#125;</span><br><span class="line">services:</span><br><span class="line">  etcd:</span><br><span class="line">    image: rancher/etcd:v3.0.17</span><br><span class="line">    extra_args: &#123;&#125;</span><br><span class="line">  kube-api:</span><br><span class="line">    image: rancher/k8s:v1.8.7-rancher1-1</span><br><span class="line">    extra_args: &#123;&#125;</span><br><span class="line">    service_cluster_ip_range: 10.233.0.0/18</span><br><span class="line">    pod_security_policy: false</span><br><span class="line">  kube-controller:</span><br><span class="line">    image: rancher/k8s:v1.8.7-rancher1-1</span><br><span class="line">    extra_args: &#123;&#125;</span><br><span class="line">    cluster_cidr: 10.233.64.0/18</span><br><span class="line">    service_cluster_ip_range: 10.233.0.0/18</span><br><span class="line">  scheduler:</span><br><span class="line">    image: rancher/k8s:v1.8.7-rancher1-1</span><br><span class="line">    extra_args: &#123;&#125;</span><br><span class="line">  kubelet:</span><br><span class="line">    image: rancher/k8s:v1.8.7-rancher1-1</span><br><span class="line">    extra_args: &#123;&#125;</span><br><span class="line">    cluster_domain: cluster.local</span><br><span class="line">    infra_container_image: registry.cn-shenzhen.aliyuncs.com/rancher_cn/pause-amd64:3.0</span><br><span class="line">    cluster_dns_server: 10.233.0.3</span><br><span class="line">    fail_swap_on: false</span><br><span class="line">  kubeproxy:</span><br><span class="line">    image: rancher/k8s:v1.8.7-rancher1-1</span><br><span class="line">    extra_args: &#123;&#125;</span><br><span class="line">network:</span><br><span class="line">  plugin: flannel</span><br><span class="line">  options: &#123;&#125;</span><br><span class="line">authentication:</span><br><span class="line">  strategy: x509</span><br><span class="line">  options: &#123;&#125;</span><br><span class="line">addons: &quot;&quot;</span><br><span class="line">system_images:</span><br><span class="line">  etcd: &quot;&quot;</span><br><span class="line">  alpine: &quot;&quot;</span><br><span class="line">  nginx_proxy: &quot;&quot;</span><br><span class="line">  cert_downloader: &quot;&quot;</span><br><span class="line">  kubernetes_services_sidecar: &quot;&quot;</span><br><span class="line">  kubedns: &quot;&quot;</span><br><span class="line">  dnsmasq: &quot;&quot;</span><br><span class="line">  kubedns_sidecar: &quot;&quot;</span><br><span class="line">  kubedns_autoscaler: &quot;&quot;</span><br><span class="line">  kubernetes: &quot;&quot;</span><br><span class="line">  flannel: &quot;&quot;</span><br><span class="line">  flannel_cni: &quot;&quot;</span><br><span class="line">  calico_node: &quot;&quot;</span><br><span class="line">  calico_cni: &quot;&quot;</span><br><span class="line">  calico_controllers: &quot;&quot;</span><br><span class="line">  calico_ctl: &quot;&quot;</span><br><span class="line">  canal_node: &quot;&quot;</span><br><span class="line">  canal_cni: &quot;&quot;</span><br><span class="line">  canal_flannel: &quot;&quot;</span><br><span class="line">  wave_node: &quot;&quot;</span><br><span class="line">  weave_cni: &quot;&quot;</span><br><span class="line">  pod_infra_container: &quot;&quot;</span><br><span class="line">ssh_key_path: ~/.ssh/id_rsa</span><br><span class="line">authorization:</span><br><span class="line">  mode: rbac</span><br><span class="line">  options: &#123;&#125;</span><br><span class="line">ignore_docker_version: false</span><br><span class="line">kubernetes_version: &quot;&quot;</span><br><span class="line">private_registries: []</span><br><span class="line">ingress:</span><br><span class="line">  provider: &quot;&quot;</span><br><span class="line">  options: &#123;&#125;</span><br><span class="line">  node_selector: &#123;&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>注意几个点<br>1、nodes内定义的就是节点的ip、角色、key位置，有以下几种角色<br>controlplane：也就是kubernetes中的master节点部署kube-api、kube-controller-manger、kube-scheduler组件<br>etcd：部署etcd，kubernetes集群存储后端数据信息<br>worker：kubernetes的node节点，承载应用</p>
<p>2、默认是会直接部署ingress的要disable的将ingress的provider弄为node</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ingress:</span><br><span class="line">  provider: none</span><br><span class="line">  options: &#123;&#125;</span><br><span class="line">  node_selector: &#123;&#125;</span><br></pre></td></tr></table></figure>
<p>还有种办法直接生成,按提示一步步输入。<br> .&#x2F;rke config #生成名为cluster.yml文件<br>.&#x2F;rke config –name mycluster.yml #生成指定名字的配置文件</p>
<p>当配置文件和rke命令在一个文件夹下时可以直接使用直接部署命令，否则需要指定yml配置文件<br>直接部署</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./rke up</span><br></pre></td></tr></table></figure>
<p>指定配置文件部署</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rke up --config cluster.yml</span><br></pre></td></tr></table></figure>
<p>部署成功</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/rke_2.png"></p>
<p>默认rke是没有安装kubectl的<br>手动安装kubectl</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget https://dl.k8s.io/v1.8.7/kubernetes-client-linux-amd64.tar.gz</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp kubernetes/client/bin/kube* /usr/local/bin/</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">chmod a+x /usr/local/bin/kube*</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export PATH=/usr/local/bin:$PATH</span><br></pre></td></tr></table></figure>
<p>默认在当前目录生成了kubeconfig文件<br>创建kube文件夹</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir /root/.kube/</span><br></pre></td></tr></table></figure>
<p>拷贝文件到&#x2F;root&#x2F;.kube内</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp kube_config_cluster.yml  /root/.kube/config</span><br></pre></td></tr></table></figure>
<p>测试<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/rke_3.png"></p>
<p>部署应用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl run nginx --image=nginx:1.7.9 --replicas=3</span><br></pre></td></tr></table></figure>
<p> 验证<br> <img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/rke_4.png"></p>
<p> HA部署</p>
<p>rke部署ha的方式非传统的vip的架构，而是通过在每个host上部署个nginx-proxy的container，然后通过这个container做反向代理，本地的kube进程直接连接127.0.0.1:6443</p>
<p>注意事项</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://github.com/rancher/rke</span><br><span class="line">http://blog.51cto.com/10321203/2071396?utm_source=oschina-app</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Rook-ceph</title>
    <url>/2020/05/25/rook_ceph/</url>
    <content><![CDATA[<h3 id="Rook概述"><a href="#Rook概述" class="headerlink" title="Rook概述"></a>Rook概述</h3><p>Rook（<a href="https://rook.io/%EF%BC%89">https://rook.io/）</a><br>是由CNCF社区管理的云原生存储编排系统，rook并不是一个实际的存储软件，它做的是将存储软件的部署和运维动作通过Kubernetes来实现自动化。比如rook-ceph项目实际上是在Kubernetes中定义了对应的operator和CRD资源对象来对ceph集群进行操作。</p>
<p>rook目前支持的存储</p>
<ul>
<li>Ceph</li>
<li>EdgeFS</li>
<li>CockroachDB</li>
<li>Cassandra</li>
<li>NFS</li>
<li>Yugabyte DB</li>
</ul>
<p>目前比较成熟的是rook-ceph。通过rook-ceph可以将ceph非常简单方便的部署到Kubernetes，通过Kubernetes的资源对象来对ceph进行控制。  </p>
<p>rook架构<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/rook-1.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/rook-2.png"></p>
<h3 id="使用Rook部署ceph"><a href="#使用Rook部署ceph" class="headerlink" title="使用Rook部署ceph"></a>使用Rook部署ceph</h3><p>环境概述</p>
<table>
<thead>
<tr>
<th>软件</th>
<th>版本</th>
</tr>
</thead>
<tbody><tr>
<td>centos</td>
<td>7.7</td>
</tr>
<tr>
<td>Kubernetes</td>
<td>1.17.4</td>
</tr>
<tr>
<td>rook</td>
<td>v1.3</td>
</tr>
</tbody></table>
<p>每个节点预留了块50G的磁盘做为osd节点</p>
<p>节点磁盘信息如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> lsblk -f</span><br><span class="line">NAME   FSTYPE LABEL UUID                                 MOUNTPOINT</span><br><span class="line">vda                                                      </span><br><span class="line">└─vda1 ext4         995d4542-f0dd-47e6-90eb-690de3b64430 /</span><br><span class="line">vdb                                             </span><br></pre></td></tr></table></figure>
<p>将节点vdb磁盘做为ceph-osd节点  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/rook/rook.git -b release-1.3</span><br></pre></td></tr></table></figure>
<p>若clone慢也可以使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone https://gitee.com/wanshaoyuan/rook.git -b release-1.3</span><br></pre></td></tr></table></figure>

<p>部署  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd rook/cluster/examples/kubernetes/ceph</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f common.yaml</span><br><span class="line">kubectl create -f operator.yaml</span><br></pre></td></tr></table></figure>
<p>cluster.yaml文件内包含对ceph初始化的配置  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f cluster.yaml</span><br></pre></td></tr></table></figure>
<p>参数：<br>默认情况会下rook-ceph会将集群内全部节点以及节点上全部磁盘做为osd节点，生产环境不建议这样使用，建议指定节点和指定节点设备。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">useAllDevices: true //将host上全部空余设备做为ceph-osd磁盘  </span><br><span class="line">useAllNodes: true  //将集群内全部节点做为ceph节点</span><br></pre></td></tr></table></figure>

<p>使用指定节点的指定设备配置<br>将<br>useAllNodes和useAllDevices设置为false</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nodes:</span><br><span class="line">- name: &quot;172.24.234.128&quot;</span><br><span class="line">  devices: # specific devices to use for storage can be specified for each node</span><br><span class="line">  - name: &quot;vdb&quot;</span><br><span class="line">- name: &quot;172.24.234.147&quot;</span><br><span class="line">  devices:</span><br><span class="line">  - name: &quot;vdb&quot;</span><br><span class="line">- name: &quot;172.24.234.156&quot;</span><br><span class="line">  devices:</span><br><span class="line">  - name: &quot;vdb&quot;</span><br></pre></td></tr></table></figure>

<p>注意：nodes：name处要与<code>kubectl get node</code>出来的显示一致，若为ip显示ip若为主机名显示主机名</p>
<h4 id="ceph-dashboard访问"><a href="#ceph-dashboard访问" class="headerlink" title="ceph-dashboard访问"></a>ceph-dashboard访问</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f dashboard-external-https.yaml </span><br></pre></td></tr></table></figure>

<p>获取访问端口 </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get svc/rook-ceph-mgr-dashboard-external-https -n rook-ceph</span><br><span class="line"> </span><br><span class="line">NAME                                     TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">rook-ceph-mgr-dashboard-external-https   NodePort   10.43.117.2   &lt;none&gt;        8443:30519/TCP   53s</span><br></pre></td></tr></table></figure>

<p>获取访问密码  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=&#x27;&#123;.data.password&#125;&#x27;  |  base64 --decode</span><br></pre></td></tr></table></figure>


<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/rook-3.png"></p>
<h4 id="查看集群健康状态"><a href="#查看集群健康状态" class="headerlink" title="查看集群健康状态"></a>查看集群健康状态</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get CephCluster -n rook-ceph</span><br><span class="line">NAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE   MESSAGE                        HEALTH</span><br><span class="line">rook-ceph   /var/lib/rook     3          20m   Ready   Cluster updated successfully   HEALTH_OK</span><br></pre></td></tr></table></figure>

<h4 id="创建存储池和storageclass"><a href="#创建存储池和storageclass" class="headerlink" title="创建存储池和storageclass"></a>创建存储池和storageclass</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: ceph.rook.io/v1</span><br><span class="line">kind: CephBlockPool</span><br><span class="line">metadata:</span><br><span class="line">  name: replicapool</span><br><span class="line">  namespace: rook-ceph</span><br><span class="line">spec:</span><br><span class="line">  failureDomain: host</span><br><span class="line">  replicated:</span><br><span class="line">    size: 3</span><br><span class="line">---</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">   name: rook-ceph-block</span><br><span class="line"># Change &quot;rook-ceph&quot; provisioner prefix to match the operator namespace if needed</span><br><span class="line">provisioner: rook-ceph.rbd.csi.ceph.com</span><br><span class="line">parameters:</span><br><span class="line">    # clusterID is the namespace where the rook cluster is running</span><br><span class="line">    clusterID: rook-ceph</span><br><span class="line">    # Ceph pool into which the RBD image shall be created</span><br><span class="line">    pool: replicapool</span><br><span class="line"></span><br><span class="line">    # RBD image format. Defaults to &quot;2&quot;.</span><br><span class="line">    imageFormat: &quot;2&quot;</span><br><span class="line"></span><br><span class="line">    # RBD image features. Available for imageFormat: &quot;2&quot;. CSI RBD currently supports only `layering` feature.</span><br><span class="line">    imageFeatures: layering</span><br><span class="line"></span><br><span class="line">    # The secrets contain Ceph admin credentials.</span><br><span class="line">    csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner</span><br><span class="line">    csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph</span><br><span class="line">    csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node</span><br><span class="line">    csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph</span><br><span class="line"></span><br><span class="line">    # Specify the filesystem type of the volume. If not specified, csi-provisioner</span><br><span class="line">    # will set default as `ext4`.</span><br><span class="line">    csi.storage.k8s.io/fstype: xfs</span><br><span class="line"></span><br><span class="line"># Delete the rbd volume when a PVC is deleted</span><br><span class="line">reclaimPolicy: Delete</span><br></pre></td></tr></table></figure>


<h4 id="部署应用测试"><a href="#部署应用测试" class="headerlink" title="部署应用测试"></a>部署应用测试</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f /root/rook/cluster/examples/kubernetes/mysql.yaml  -f /root/rook/cluster/examples/kubernetes/wordpress.yaml</span><br></pre></td></tr></table></figure>

<h4 id="扩容存储节点"><a href="#扩容存储节点" class="headerlink" title="扩容存储节点"></a>扩容存储节点</h4><p>需要保证磁盘的gpt分区表类型，先对磁盘进行初始化</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">parted -s /dev/xxxx mklabel gpt</span><br><span class="line"></span><br><span class="line">sgdisk --zap-all /dev/xxx </span><br></pre></td></tr></table></figure>
<p>编辑集群资源</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl edit CephCluster/rook-ceph -n rook-ceph</span><br></pre></td></tr></table></figure>
<p>添加对应节点磁盘字段</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">- config: null</span><br><span class="line">  devices:</span><br><span class="line">  - config: null</span><br><span class="line">    name: vdb</span><br><span class="line">  name: rke-node5</span><br><span class="line">  resources: &#123;&#125;</span><br><span class="line">- config: null</span><br><span class="line">  devices:</span><br><span class="line">  - config: null</span><br><span class="line">    name: vdb</span><br><span class="line">  name: rke-node6</span><br><span class="line">  resources: &#123;&#125;</span><br><span class="line">- config: null</span><br><span class="line">  devices:</span><br><span class="line">  - config: null</span><br><span class="line">    name: vdb</span><br><span class="line">  name: rke-node7</span><br><span class="line">  resources: &#123;&#125;</span><br></pre></td></tr></table></figure>

<h3 id="常见问题："><a href="#常见问题：" class="headerlink" title="常见问题："></a>常见问题：</h3><h4 id="1、RKE部署问题"><a href="#1、RKE部署问题" class="headerlink" title="1、RKE部署问题"></a>1、RKE部署问题</h4><p>1、通过Rancher RKE部署的Kubernetes集群<br>因为rancher rke部署的集群kubelet是运行在容器中的，所以需要将flexvolume插件映射到kubelet容器中，不然无法挂载pvc到workload中。</p>
<p>为kubelt添加以下参数：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">extra_args:</span><br><span class="line">        volume-plugin-dir: /usr/libexec/kubernetes/kubelet-plugins/volume/exec</span><br><span class="line">      extra_binds:</span><br><span class="line">        /usr/libexec/kubernetes/kubelet-plugins/volume/exec:/usr/libexec/kubernetes/kubelet-plugins/volume/exec</span><br></pre></td></tr></table></figure>

<h4 id="2、ubuntu16-04操作系统部署问题"><a href="#2、ubuntu16-04操作系统部署问题" class="headerlink" title="2、ubuntu16.04操作系统部署问题"></a>2、ubuntu16.04操作系统部署问题</h4><p>ubuntu16.04默认4.4内核无法挂载rbd块到workload中，提示缺少特性，需要将内核升级到4.15。<br>升级步骤如下：</p>
<p>升级内核到4.15</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">uname -a</span><br><span class="line">Linux kworker2 4.4.0-142-generic #168-Ubuntu SMP Wed Jan 16 21:00:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apt-get install --install-recommends linux-generic-hwe-16.04</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">reboot</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">uname -a</span><br><span class="line">Linux kworker2 4.15.0-60-generic #67~16.04.1-Ubuntu SMP Mon Aug 26 08:57:33 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux</span><br></pre></td></tr></table></figure>


<h3 id="清理集群"><a href="#清理集群" class="headerlink" title="清理集群"></a>清理集群</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl delete -f cluster.yaml </span><br><span class="line">kubectl delete -f operator.yaml </span><br><span class="line">kubectl delete -f common.yaml </span><br></pre></td></tr></table></figure>
<p>清理宿主机目录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/usr/bin/env bash</span><br><span class="line">DISK=&quot;/dev/sdb&quot;</span><br><span class="line"># Zap the disk to a fresh, usable state (zap-all is important, b/c MBR has to be clean)</span><br><span class="line"># You will have to run this step for all disks.</span><br><span class="line">sgdisk --zap-all $DISK</span><br><span class="line">dd if=/dev/zero of=&quot;$DISK&quot; bs=1M count=100 oflag=direct,dsync</span><br><span class="line"></span><br><span class="line"># These steps only have to be run once on each node</span><br><span class="line"># If rook sets up osds using ceph-volume, teardown leaves some devices mapped that lock the disks.</span><br><span class="line">ls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove %</span><br><span class="line"># ceph-volume setup can leave ceph-&lt;UUID&gt; directories in /dev (unnecessary clutter)</span><br><span class="line">rm -rf /dev/ceph-*</span><br><span class="line">rm /var/lib/rook/ -rf</span><br><span class="line">rm /var/lib/kubelet/plugins/ -rf</span><br><span class="line">rm /var/lib/kubelet/plugins_registry/ -rf</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>分布式存储</category>
      </categories>
      <tags>
        <tag>分布式存储</tag>
      </tags>
  </entry>
  <entry>
    <title>零信任与SPIFFE（一）</title>
    <url>/2022/11/17/spiffe/</url>
    <content><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a><a href="#%E6%A6%82%E8%BF%B0" title="概述"></a>概述</h3><p>传统的网络安全模型通过划分不同的网络分区，同一个网络分区是可信的，不同网络分区之间通过防火墙隔断。这种方式在云原生时代已经变得不可适用了。<br>1、同一网络分区内流量无法进行管控，特别是是如今随着容器大规模落地，容器IP又不是二层可见和固定的无法进行安全管控。</p>
<p>2、传统网络边界防火墙采用静态方式配置规则，对于云原生这类动态变化的环境无法适应。<br>零信任安全框架在此背景下提出。<br>零信任是默认不信任使用，除非通过验证。通过身份认证，访问策略控制，实现最小权限访问控制。零信任安全的本质是以身份为中心进行动态访问控制，SPIFFE项目（Secure Product Identity Framework For Everyone）通用安全身份框架 。通过X.509 证书的形式为生产环境中的每个工作负载提供安全身份分发，认证。</p>
<p><a href="https://spiffe.io/">https://spiffe.io/</a><br>SPIFFE本身也是开源项目，目前托管在CNCF基金会，在2022年9月正式毕业。</p>
<p>SPIFFE</p>
<h3 id="架构和概念解析"><a href="#架构和概念解析" class="headerlink" title="架构和概念解析"></a><a href="#%E6%9E%B6%E6%9E%84%E5%92%8C%E6%A6%82%E5%BF%B5%E8%A7%A3%E6%9E%90" title="架构和概念解析"></a>架构和概念解析</h3><p>SPIFFE ( Secure Production Identity Framework For Everyone )：通用安全身份认证框架。<br>SPIRE ( SPIFFE Runtime Environment )：是 SPIFFE 标准的一套生产就绪实现，它执行节点证明和工作负载证明，可以安全地向服务颁发身份凭证，并根据预定义的条件集合验证其他服务的身份。<br><a href="https://github.com/spiffe/spire">https://github.com/spiffe/spire</a></p>
<p><img src="https://spiffe.io/img/server_and_agent.png"></p>
<p>Spire由SPIRE-Server和一个或多个SPIRE-Agent组成。</p>
<p>Server端充当签名机构（CA）通过Agent颁发给工作负载的证书。它还进行证书维护和验证。</p>
<p>Agent运行在每个workload所在节点上，作用是从Server端接受证书，并将其存储在缓存中。另外是对workload暴露SPIFFE Workload API 充当 SDS（secret discovery service）角色处理整个mTLs流量进行证书交互和验证</p>
<p><img src="https://d2908q01vomqb2.cloudfront.net/fe2ef495a1152561572949784c16bf23abb28057/2021/07/21/envoy-spire.png"></p>
<p>SPIFFE安全框架主要包含以下部分：</p>
<p>SPIFFE ID：用于标识对应信任域的工作负载，类似URI格式的字符串包含以下<br><img src="https://pic3.zhimg.com/v2-cb8de4befb4b9fa5a0dd46fbb649d16e_b.jpg"><br>由spiffe:&#x2F;&#x2F;信任域的名字&#x2F;工作负载名字或对应的身份标识</p>
<p>SVID（SPIFFE Veriﬁable Identity Document）：<br>svid可以是两种格式之一\:X.509证书或jwt。证书svid可用于建立端到端相互TLS<br>加密连接。jwt在端到端相互TLS加密不需要或不需要的情况下非常有用，例如当<br>使用负载均衡器。jwt对于已经支持基于jwt的身份验证的各种云服务的身份验证也很有用。无论是使用JWT SVIDs还是X.509 SVIDs, SPIFFE id、信任包格式和工作负载API都是相同的。</p>
<p>Trust Bundle：用于验证svid的公钥集</p>
<p>Workload API：工作负载通过此api获取对应的SPIFFE ID、SVID、Trust Bundle。</p>
<p>SPIFFE 联邦：不同信任域共享SPIFFE信任包，比如数据中心A的Spire环境与数据中心B的Spire环境建立联邦关系就可以互相配置和检查</p>
<p>注：这里写的workload（工作负载）并不等同于k8s里面的workload，主要指的是需要接入SPIFFE的对象可以是docker容器、VM、k8s-pod等等。</p>
<h3 id="演示"><a href="#演示" class="headerlink" title="演示"></a><a href="#%E6%BC%94%E7%A4%BA" title="演示"></a>演示</h3><p>软件版本：<br>1、kubernetes v1.24.8<br>2、Spire：v1.5.3</p>
<h4 id="Spire部署"><a href="#Spire部署" class="headerlink" title="Spire部署"></a><a href="#Spire%E9%83%A8%E7%BD%B2" title="Spire部署"></a>Spire部署</h4><p>部署local-path-provisioner<br>因为Spire-Server为有状态服务，依赖存储，所以这里部署local-path-provisioner<br>并设置为默认StorageClass</p>
<p><a href="https://github.com/rancher/local-path-provisioner.git">https://github.com/rancher/local-path-provisioner.git</a></p>
<p>完成后  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get sc</span><br><span class="line"></span><br><span class="line">NAME                   PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE</span><br><span class="line"></span><br><span class="line">local-path (default)   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  2d7h</span><br></pre></td></tr></table></figure>
<p>部署Spire-server和Spire-agent<br>clone此项目</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/spiffe/spire-tutorials</span><br></pre></td></tr></table></figure>
<p>切换到spire-tutorials&#x2F;k8s&#x2F;quickstart目录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f spire-namespace.yaml</span><br></pre></td></tr></table></figure>
<p>配置spire-server权限  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply \</span><br><span class="line"></span><br><span class="line">    -f server-account.yaml \</span><br><span class="line"></span><br><span class="line">    -f spire-bundle-configmap.yaml \</span><br><span class="line"></span><br><span class="line">    -f server-cluster-role.yaml</span><br></pre></td></tr></table></figure>
<p>部署Spire-server  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply \</span><br><span class="line"></span><br><span class="line">    -f server-configmap.yaml \</span><br><span class="line"></span><br><span class="line">    -f server-statefulset.yaml \</span><br><span class="line"></span><br><span class="line">    -f server-service.yaml</span><br></pre></td></tr></table></figure>
<p>查看部署状态  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get statefulset --namespace spire</span><br><span class="line"></span><br><span class="line">NAME           READY   AGE</span><br><span class="line"></span><br><span class="line">spire-server   1/1     2d8h</span><br></pre></td></tr></table></figure>
<p>部署spire-agent<br>1、配置权限  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply \</span><br><span class="line"></span><br><span class="line">    -f agent-account.yaml \</span><br><span class="line"></span><br><span class="line">    -f agent-cluster-role.yaml</span><br></pre></td></tr></table></figure>
<p>2、部署spire-agent  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply \</span><br><span class="line"></span><br><span class="line">    -f agent-configmap.yaml \</span><br><span class="line"></span><br><span class="line">    -f agent-daemonset.yaml</span><br></pre></td></tr></table></figure>
<p>3、检查  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get daemonset --namespace spire</span><br><span class="line"></span><br><span class="line">NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span><br><span class="line"></span><br><span class="line">spire-agent   2         2         2       2            2           &lt;none&gt;          2d8h</span><br></pre></td></tr></table></figure>
<p>4、注册spire-agent  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl exec -n spire spire-server-0 -- \</span><br><span class="line"></span><br><span class="line">    /opt/spire/bin/spire-server entry create \</span><br><span class="line"></span><br><span class="line">    -spiffeID spiffe://example.org/ns/spire/sa/spire-agent \</span><br><span class="line"></span><br><span class="line">    -selector k8s_sat:cluster:demo-cluster \</span><br><span class="line"></span><br><span class="line">    -selector k8s_sat:agent_ns:spire \</span><br><span class="line"></span><br><span class="line">    -selector k8s_sat:agent_sa:spire-agent \</span><br><span class="line"></span><br><span class="line">    -node</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl exec -n spire spire-server-0 -- \</span><br><span class="line"></span><br><span class="line">    /opt/spire/bin/spire-server entry create \</span><br><span class="line"></span><br><span class="line">    -spiffeID spiffe://example.org/ns/default/sa/default \</span><br><span class="line"></span><br><span class="line">    -parentID spiffe://example.org/ns/spire/sa/spire-agent \</span><br><span class="line"></span><br><span class="line">    -selector k8s:ns:default \</span><br><span class="line"></span><br><span class="line">    -selector k8s:sa:default</span><br></pre></td></tr></table></figure>
<p>5、验证<br>Spire-agent默认会将socket文件映射到k8s集群主机的&#x2F;run&#x2F;spire&#x2F;sockets&#x2F;agent.sock，部署测试容器查看  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f client-deployment.yaml</span><br></pre></td></tr></table></figure>
<p>验证容器是否可以访问socket  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl exec -it $(kubectl get pods -o=jsonpath=&#x27;&#123;.items[0].metadata.name&#125;&#x27; \</span><br><span class="line"></span><br><span class="line">   -l app=client)  -- /opt/spire/bin/spire-agent api fetch -socketPath /run/spire/sockets/agent.sock</span><br></pre></td></tr></table></figure>
<p>如果agent正常运行，将看到一个 SVID 列表。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SPIFFE ID:              spiffe://example.org/ns/default/sa/default</span><br><span class="line"></span><br><span class="line">SVID Valid After:       2022-12-25 11:41:16 +0000 UTC</span><br><span class="line"></span><br><span class="line">SVID Valid Until:       2022-12-25 12:41:26 +0000 UTC</span><br><span class="line"></span><br><span class="line">CA #1 Valid After:      2022-12-23 15:04:07 +0000 UTC</span><br><span class="line"></span><br><span class="line">CA #1 Valid Until:      2022-12-24 15:04:17 +0000 UTC</span><br><span class="line"></span><br><span class="line">CA #2 Valid After:      2022-12-24 03:04:07 +0000 UTC</span><br><span class="line"></span><br><span class="line">CA #2 Valid Until:      2022-12-25 03:04:17 +0000 UTC</span><br><span class="line"></span><br><span class="line">CA #3 Valid After:      2022-12-24 15:04:07 +0000 UTC</span><br><span class="line"></span><br><span class="line">CA #3 Valid Until:      2022-12-25 15:04:17 +0000 UTC</span><br><span class="line"></span><br><span class="line">CA #4 Valid After:      2022-12-25 03:04:07 +0000 UTC</span><br><span class="line"></span><br><span class="line">CA #4 Valid Until:      2022-12-26 03:04:17 +0000 UTC</span><br></pre></td></tr></table></figure>

<p>Demo应用部署<br>本次演示<br>将Envoy与X.509-SVID结合使用保护微服务通信</p>
<p><img src="https://spiffe.io/img/checkouts/spiffe/spire-tutorials/k8s/envoy-x509/images/SPIRE_Envoy_diagram.png"></p>
<p>如图所示，前端服务通过sidecar Envoy执行X.509 SVID 身份验证与实例建立的起mTLS连接，连接到后端服务。</p>
<p>SPIRE Agent原生支持做为Envoy的SDS服务。通过本地socket连接SDS服务。</p>
<p>切换到spire-tutorials&#x2F;k8s&#x2F;envoy-x509目录</p>
<p>部署应用  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -k k8s/.</span><br><span class="line"></span><br><span class="line">configmap/backend-balance-json-data created</span><br><span class="line"></span><br><span class="line">configmap/backend-envoy created</span><br><span class="line"></span><br><span class="line">configmap/backend-profile-json-data created</span><br><span class="line"></span><br><span class="line">configmap/backend-transactions-json-data created</span><br><span class="line"></span><br><span class="line">configmap/frontend-2-envoy created</span><br><span class="line"></span><br><span class="line">configmap/frontend-envoy created</span><br><span class="line"></span><br><span class="line">configmap/symbank-webapp-2-config created</span><br><span class="line"></span><br><span class="line">configmap/symbank-webapp-config created</span><br><span class="line"></span><br><span class="line">service/backend-envoy created</span><br><span class="line"></span><br><span class="line">service/frontend-2 created</span><br><span class="line"></span><br><span class="line">service/frontend created</span><br><span class="line"></span><br><span class="line">deployment.apps/backend created</span><br><span class="line"></span><br><span class="line">deployment.apps/frontend-2 created</span><br><span class="line"></span><br><span class="line">deployment.apps/frontend created</span><br></pre></td></tr></table></figure>
<p>以backend模块为例<br>查看k8s&#x2F;backend&#x2F;config&#x2F;envoy.yaml文件，可以科技Envoy配置的与spire-agent的socket连接</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">clusters:</span><br><span class="line"></span><br><span class="line">- name: spire_agent</span><br><span class="line"></span><br><span class="line">  connect_timeout: 0.25s</span><br><span class="line"></span><br><span class="line">  http2_protocol_options: &#123;&#125;</span><br><span class="line"></span><br><span class="line">  hosts:</span><br><span class="line"></span><br><span class="line">    - pipe:</span><br><span class="line"></span><br><span class="line">        path: /run/spire/sockets/agent.sock</span><br></pre></td></tr></table></figure>
<p>手动将backend、frontend、frontend-2注册到sprie-server，当然SPIFFE也有自动正常功能就是使用SPIRE Controller Manager 模块（<a href="https://github.com/spiffe/spire-controller-manager%EF%BC%89">https://github.com/spiffe/spire-controller-manager）</a>  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bash create-registration-entries.sh</span><br></pre></td></tr></table></figure>

<p>注册完成后可以查看注册的服务  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl exec -n spire spire-server-0 -c spire-server -- /opt/spire/bin/spire-server  entry show -selector k8s:ns:default</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Found 4 entries</span><br><span class="line"></span><br><span class="line">Entry ID         : 3478c441-3e25-40e7-96d9-ef74611f2205</span><br><span class="line"></span><br><span class="line">SPIFFE ID        : spiffe://example.org/ns/default/sa/default</span><br><span class="line"></span><br><span class="line">Parent ID        : spiffe://example.org/ns/spire/sa/spire-agent</span><br><span class="line"></span><br><span class="line">Revision         : 0</span><br><span class="line"></span><br><span class="line">X509-SVID TTL    : default</span><br><span class="line"></span><br><span class="line">JWT-SVID TTL     : default</span><br><span class="line"></span><br><span class="line">Selector         : k8s:ns:default</span><br><span class="line"></span><br><span class="line">Selector         : k8s:sa:default</span><br><span class="line"></span><br><span class="line">Entry ID         : c188d47c-e886-492e-bf67-6a6bf42c3667</span><br><span class="line"></span><br><span class="line">SPIFFE ID        : spiffe://example.org/ns/default/sa/default/backend</span><br><span class="line"></span><br><span class="line">Parent ID        : spiffe://example.org/ns/spire/sa/spire-agent</span><br><span class="line"></span><br><span class="line">Revision         : 0</span><br><span class="line"></span><br><span class="line">X509-SVID TTL    : default</span><br><span class="line"></span><br><span class="line">JWT-SVID TTL     : default</span><br><span class="line"></span><br><span class="line">Selector         : k8s:container-name:envoy</span><br><span class="line"></span><br><span class="line">Selector         : k8s:ns:default</span><br><span class="line"></span><br><span class="line">Selector         : k8s:pod-label:app:backend</span><br><span class="line"></span><br><span class="line">Selector         : k8s:sa:default</span><br><span class="line"></span><br><span class="line">Entry ID         : 6c376401-67d4-499a-a9d9-6ab71caf69c4</span><br><span class="line"></span><br><span class="line">SPIFFE ID        : spiffe://example.org/ns/default/sa/default/frontend</span><br><span class="line"></span><br><span class="line">Parent ID        : spiffe://example.org/ns/spire/sa/spire-agent</span><br><span class="line"></span><br><span class="line">Revision         : 0</span><br><span class="line"></span><br><span class="line">X509-SVID TTL    : default</span><br><span class="line"></span><br><span class="line">JWT-SVID TTL     : default</span><br><span class="line"></span><br><span class="line">Selector         : k8s:container-name:envoy</span><br><span class="line"></span><br><span class="line">Selector         : k8s:ns:default</span><br><span class="line"></span><br><span class="line">Selector         : k8s:pod-label:app:frontend</span><br><span class="line"></span><br><span class="line">Selector         : k8s:sa:default</span><br><span class="line"></span><br><span class="line">Entry ID         : 49f88c69-b4ee-4656-b740-6dbee5bb89a3</span><br><span class="line"></span><br><span class="line">SPIFFE ID        : spiffe://example.org/ns/default/sa/default/frontend-2</span><br><span class="line"></span><br><span class="line">Parent ID        : spiffe://example.org/ns/spire/sa/spire-agent</span><br><span class="line"></span><br><span class="line">Revision         : 0</span><br><span class="line"></span><br><span class="line">X509-SVID TTL    : default</span><br><span class="line"></span><br><span class="line">JWT-SVID TTL     : default</span><br><span class="line"></span><br><span class="line">Selector         : k8s:container-name:envoy</span><br><span class="line"></span><br><span class="line">Selector         : k8s:ns:default</span><br><span class="line"></span><br><span class="line">Selector         : k8s:pod-label:app:frontend-2</span><br><span class="line"></span><br><span class="line">Selector         : k8s:sa:default</span><br></pre></td></tr></table></figure>
<p>可以看见对应的SPIFFE ID</p>
<p>访问服务  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl get svc</span><br><span class="line"></span><br><span class="line">NAME            TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line"></span><br><span class="line">backend-envoy   ClusterIP      None            &lt;none&gt;        9001/TCP         2d8h</span><br><span class="line"></span><br><span class="line">frontend        LoadBalancer   10.43.106.227   &lt;pending&gt;     3000:32082/TCP   2d8h</span><br><span class="line"></span><br><span class="line">frontend-2      LoadBalancer   10.43.203.167   &lt;pending&gt;     3002:30664/TCP   2d8h</span><br><span class="line"></span><br><span class="line">go-demo         NodePort       10.43.120.2     &lt;none&gt;        8080:30007/TCP   2d10h</span><br><span class="line"></span><br><span class="line">kubernetes      ClusterIP      10.43.0.1       &lt;none&gt;        443/TCP          2d10h</span><br></pre></td></tr></table></figure>
<p>frontend对应的NodePort端口为32082</p>
<p>frontend-2对应的NodePort端口为30664</p>
<p>frontend显示Jacob Marley的账户情况<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/spiffe-1.png"><br>frontend-2显示Alex Fergus的账户情况<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/spiffe-2.png"></p>
<p>更新策略只允许frontend服务访问backend访问</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f backend-envoy-configmap-update.yaml</span><br></pre></td></tr></table></figure>

<p>实际上就是更新backend的Envoy配置对应的 k8s&#x2F;backend&#x2F;config&#x2F;envoy.yaml<br>删除了以下条目  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">- exact: &quot;spiffe://example.org/ns/default/sa/default/frontend-2&quot;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">match_subject_alt_names:</span><br><span class="line"></span><br><span class="line">                - exact: &quot;spiffe://example.org/ns/default/sa/default/frontend&quot;</span><br><span class="line"></span><br><span class="line">                - exact: &quot;spiffe://example.org/ns/default/sa/default/frontend-2&quot;</span><br></pre></td></tr></table></figure>
<p>重启backend服务获取最新配置  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl scale deployment backend --replicas=0</span><br><span class="line"></span><br><span class="line">kubectl scale deployment backend --replicas=1</span><br></pre></td></tr></table></figure>
<p>在次访问frontend正常显示，访问frontend-2<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/spiffe-3.png"></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><a href="#%E6%80%BB%E7%BB%93" title="总结"></a>总结</h3><p>SPIFFE支持多种方式集成如和Istio的envoy-sidecar、OPA策略等方式，可以非常灵活细粒化控制应用访问权限。</p>
<p>参考链接：<br><a href="https://jimmysong.io/blog/why-istio-need-spire/">https://jimmysong.io/blog/why-istio-need-spire/</a><br><a href="https://mp.weixin.qq.com/s/4eEEYb8RuOFOmLcdL3N6wA">https://mp.weixin.qq.com/s/4eEEYb8RuOFOmLcdL3N6wA</a><br><a href="https://atbug.com/what-is-spiffe-and-spire/">https://atbug.com/what-is-spiffe-and-spire/</a><br><a href="https://www.nginx-cn.net/blog/mtls-architecture-nginx-service-mesh/">https://www.nginx-cn.net/blog/mtls-architecture-nginx-service-mesh/</a></p>
]]></content>
      <categories>
        <category>安全</category>
      </categories>
      <tags>
        <tag>安全</tag>
      </tags>
  </entry>
  <entry>
    <title>stable diffusion学习系列1（安装部署-Windows环境)</title>
    <url>/2023/10/03/stablediffusion/</url>
    <content><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>stable diffusion做为目前AI绘图内开源的最强王者，本文主要在本地PC上部署使用是由Stability AI、CompVis與Runway合作开发，采用Apache2.0开源协议。<br><a href="https://github.com/Stability-AI/stablediffusion">https://github.com/Stability-AI/stablediffusion</a></p>
<p>本文用的是基于于stable diffusion封装的stable-diffusion-webui项目，简单直观能快速上手。</p>
<h3 id="安装环境"><a href="#安装环境" class="headerlink" title="安装环境"></a>安装环境</h3><table>
<thead>
<tr>
<th>软硬件</th>
<th>版本\型号</th>
</tr>
</thead>
<tbody><tr>
<td>显卡</td>
<td>RTX 3060 12GB</td>
</tr>
<tr>
<td>OS</td>
<td>Windows 11</td>
</tr>
<tr>
<td>Python</td>
<td>3.10.6</td>
</tr>
<tr>
<td>conda</td>
<td>23.5.2</td>
</tr>
<tr>
<td>显卡驱动</td>
<td>537.42—&gt;对应cuda 12.2</td>
</tr>
<tr>
<td>CUDA版本</td>
<td>12.2</td>
</tr>
<tr>
<td>git</td>
<td>2.42.0</td>
</tr>
<tr>
<td>stable-diffusion-webui</td>
<td>1.6</td>
</tr>
</tbody></table>
<h3 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h3><h4 id="基础环境部署"><a href="#基础环境部署" class="headerlink" title="基础环境部署"></a>基础环境部署</h4><h5 id="git安装"><a href="#git安装" class="headerlink" title="git安装"></a>git安装</h5><p><a href="https://git-scm.com/download/win">https://git-scm.com/download/win</a></p>
<p>下载git安装，最新版，下一步就好。</p>
<h5 id="python安装"><a href="#python安装" class="headerlink" title="python安装"></a>python安装</h5><p>通过conda管理和安装python，需要注意的是python版本，不要用超过3.10.x版本的python，我这里是下载的<br>Miniconda3-py310_23.5.2-0-Windows-x86_64<br><a href="https://repo.anaconda.com/miniconda/">https://repo.anaconda.com/miniconda/</a></p>
<p>参考：<a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Dependencies">https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Dependencies</a></p>
<p>下一步安装就好，安装完成后在CMD中可以正常执行python命令</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python --version</span><br><span class="line">Python 3.10.12</span><br></pre></td></tr></table></figure>
<p>配置conda源</p>
<p>开始菜单用管理员身份执行打开miniconda3<br>执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda config --set show_channel_urls yes</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>生成配置文件</p>
<p>编辑配置文件添加清华大学加速地址</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">C:\Users\wansh\.condarc   //wansh替换为你的用户名</span><br></pre></td></tr></table></figure>

<p>粘贴以下内容</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">channels:</span><br><span class="line"> - defaults</span><br><span class="line">show_channel_urls: true</span><br><span class="line">default_channels:</span><br><span class="line"> - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br><span class="line"> - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span><br><span class="line"> - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span><br><span class="line">custom_channels:</span><br><span class="line"> conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line"> msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line"> bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line"> menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line"> pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line"> pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line"> simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br></pre></td></tr></table></figure>

<p>conda3-cmd中执行以下命令配置python pip下载包的软件源，这里指向阿里云</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip config set global.index-url https://mirrors.aliyun.com/pypi/simple</span><br></pre></td></tr></table></figure>

<p>配置后查看</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip config list</span><br><span class="line">global.index-url=&#x27;https://mirrors.aliyun.com/pypi/simple&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="CUDA配置"><a href="#CUDA配置" class="headerlink" title="CUDA配置"></a>CUDA配置</h4><p>查看显卡安装的驱动对应的CUDA版本<br>在conda3-cmd中执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nvidia-smi.exe</span><br><span class="line">Fri Oct  6 15:14:48 2023</span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 537.42                 Driver Version: 537.42       CUDA Version: 12.2     |</span><br><span class="line">|-----------------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                                         |                      |               MIG M. |</span><br><span class="line">|=========================================+======================+======================|</span><br><span class="line">|   0  NVIDIA GeForce RTX 3060      WDDM  | 00000000:2B:00.0  On |                  N/A |</span><br><span class="line">|  0%   53C    P8              16W / 170W |   5441MiB / 12288MiB |      0%      Default |</span><br><span class="line">|                                         |                      |                  N/A |</span><br><span class="line">+-----------------------------------------+----------------------+----------------------+</span><br><span class="line"></span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                            |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |</span><br><span class="line">|        ID   ID                                                             Usage      |</span><br></pre></td></tr></table></figure>

<p>对应的是12.2，去Nvidia官网下载对应的CUDA版本安装<br><a href="https://developer.nvidia.com/cuda-toolkit-archive">https://developer.nvidia.com/cuda-toolkit-archive</a></p>
<h5 id="终端命令配置"><a href="#终端命令配置" class="headerlink" title="终端命令配置"></a>终端命令配置</h5><p>配置代理需要拉取stable-diffusion-webui，需要conda3-cmd能够访问github</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">set https_proxy=http://127.0.0.1:33210</span><br><span class="line">set http_proxy=http://127.0.0.1:33210</span><br></pre></td></tr></table></figure>

<p>验证</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -I www.google.com</span><br></pre></td></tr></table></figure>
<p>状态码返回200表示ok</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Transfer-Encoding: chunked</span><br><span class="line">Cache-Control: private</span><br><span class="line">Connection: keep-alive</span><br><span class="line">Content-Security-Policy-Report-Only: object-src &#x27;none&#x27;;base-uri &#x27;self&#x27;;script-src &#x27;nonce-EsiPFL30vCI9foliBMkTLA&#x27; &#x27;strict-dynamic&#x27; &#x27;report-sample&#x27; &#x27;unsafe-eval&#x27; &#x27;unsafe-inline&#x27; https: http:;report-uri https://csp.withgoogle.com/csp/gws/other-hp</span><br><span class="line">Content-Type: text/html; charset=ISO-8859-1</span><br><span class="line">Date: Fri, 06 Oct 2023 07:32:54 GMT</span><br><span class="line">Expires: Fri, 06 Oct 2023 07:32:54 GMT</span><br><span class="line">Keep-Alive: timeout=4</span><br><span class="line">P3p: CP=&quot;This is not a P3P policy! See g.co/p3phelp for more info.&quot;</span><br><span class="line">Proxy-Connection: keep-alive</span><br><span class="line">Server: gws</span><br><span class="line">Set-Cookie: 1P_JAR=2023-10-06-07; expires=Sun, 05-Nov-2023 07:32:54 GMT; path=/; domain=.google.com; Secure</span><br><span class="line">Set-Cookie: AEC=Ackid1QHNMFx6j8Bfaco7KM-Wc2Il-3JpKjmJcRYM3QqzErZfcup19XB43Y; expires=Wed, 03-Apr-2024 07:32:54 GMT; path=/; domain=.google.com; Secure; HttpOnly; SameSite=lax</span><br><span class="line">Set-Cookie: NID=511=ksuU76xakl0AZHIz-SjvI3pBnThANk3EBkMB7E4ZD1JNMxpQI8pg8rttvpYMdMqJSgfTwVt0Dqv-5V5p4uwnCRgb-KA_iOqHQ9lNPcsi0PjgXVbWAYhVIG2oCxmw_Jfw5XhA6QbDbpQcMq3zS9zkjx9gUwgHS-Howlm5ip9uU84; expires=Sat, 06-Apr-2024 07:32:54 GMT; path=/; domain=.google.com; HttpOnly</span><br><span class="line">X-Frame-Options: SAMEORIGIN</span><br><span class="line">X-Xss-Protection: 0</span><br></pre></td></tr></table></figure>


<h4 id="stable-diffusion-webui部署"><a href="#stable-diffusion-webui部署" class="headerlink" title="stable-diffusion-webui部署"></a>stable-diffusion-webui部署</h4><p>拉取stable-diffusion-webui代码<br>需要电脑<br>在conda3-CMD中执行E: 切换到E盘，按自己环境能提供的磁盘执行，因为装C盘会占用很多空间</p>
<p>clone代码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git -b v1.6.0</span><br></pre></td></tr></table></figure>

<p>下载stable diffusion的训练模型</p>
<p>sd-v1-4.ckpt</p>
<p><a href="https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/tree/main">https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/tree/main</a></p>
<p>模型是用于AI绘图的元素素材库</p>
<p>下载后放置到E:\stable-diffusion-webui\models\Stable-diffusion目录。E盘根据部署盘符替换</p>
<p>在conda3-cmd执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd stable-diffusion-webui</span><br><span class="line">webui-user.bat</span><br></pre></td></tr></table></figure>
<p>系统会自动执行下载对应的依赖包</p>
<p>执行成功会自动打开浏览器访问<a href="http://127.0.0.1:7860/">http://127.0.0.1:7860/</a></p>
<p>输入prompt生成图片</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/sd1-1.png"></p>
<p>当然也可以使用此prompt生成器进行<br><a href="https://tinygeeker.github.io/menu/autocue/#/?from=tencent">https://tinygeeker.github.io/menu/autocue/#/?from=tencent</a></p>
<h3 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h3><p>1、RuntimeError：Torch is not able to use GPU</p>
<p>这个原因主要是因为pytorch没有连接到GPU，cuda与torch版本不兼容导致的<br>网上有通过参数跳过，但这样就变成用CPU生成了，效率太差。所以还是要成根本上解决。<br>可以进行以下操作进行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">先用pip uninstall torch</span><br></pre></td></tr></table></figure>

<p>通过<a href="https://pytorch.org/get-started/locally/%E4%B8%8B%E8%BD%BD%E5%90%88%E9%80%82%E7%9A%84torch%E7%89%88%E6%9C%AC%EF%BC%8C%E6%88%91%E8%BF%99%E9%87%8C%E6%98%AFNVIDIA">https://pytorch.org/get-started/locally/下载合适的torch版本，我这里是NVIDIA</a> cuda12.2，但torch还没有对应的12.2版本，直接用11.8、12.1也能正常运行，目前1.6最高支持到11.8，可以向下兼容，正常pip配置了正常的源可以自动下载。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/sd1-2.png"></p>
<p>验证<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/sd1-3.png"></p>
<p>参考链接：</p>
<p><a href="https://zhuanlan.zhihu.com/p/610628741">https://zhuanlan.zhihu.com/p/610628741</a></p>
<p><a href="https://www.uisdc.com/47-stable-diffusion-models">https://www.uisdc.com/47-stable-diffusion-models</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/622410028">https://zhuanlan.zhihu.com/p/622410028</a><br><a href="https://aitechtogether.com/python/82781.html">https://aitechtogether.com/python/82781.html</a>       </p>
]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>Thanos初体验</title>
    <url>/2020/03/14/thanos/</url>
    <content><![CDATA[<h3 id="概述："><a href="#概述：" class="headerlink" title="概述："></a>概述：</h3><p>Thanos主要用于解决大规模prometheus部署、增强prometheus高可用的工具，由Improbable团队开源，项目地址<br><a href="https://github.com/thanos-io/thanos">https://github.com/thanos-io/thanos</a></p>
<h3 id="架构："><a href="#架构：" class="headerlink" title="架构："></a>架构：</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/thanos_1.png"></p>
<h3 id="组件："><a href="#组件：" class="headerlink" title="组件："></a>组件：</h3><p>thanos-sidecar：  通过Prometheus附加件与Prometheus进行连接，通过http方式在Prometheus的remote-read API基础之上实现了storeAPI接口，query组件可以直接从此接口读取监控数据，并且还支持将数据上传到对应的对象存储。    </p>
<p>thanos-query：  通过thanos-sidecar组件的store-api的grpc接口抓取监控数据，并对监控数据进行聚合去重处理。  </p>
<p>thanos-storage-gateway：  对接后端对象存储，当需要查询对象存储中的历史监控数据时，query与其相连查看。    </p>
<p>thanos-compact： 对存储在对象存储中的监控数据多个较小的块连续合并为较大的块。这显着减少了存储桶中的总存储大小。提高查询效率。      </p>
<p>thanos-rules：  对监控数据进行告警，通知altermanager，并且可以预先计算经常需要或计算量大的表达式，并将其结果保存为一组新的时间序列据提供给query查询和对象存储进行存储。     </p>
<h3 id="Thanos能解决什么问题？"><a href="#Thanos能解决什么问题？" class="headerlink" title="Thanos能解决什么问题？"></a>Thanos能解决什么问题？</h3><h4 id="大规模集群部署问题"><a href="#大规模集群部署问题" class="headerlink" title="大规模集群部署问题"></a>大规模集群部署问题</h4><p>prometheus本身只支持单机部署，没有自带集群模式，所以在目前大规模集群监控主要通过prometheus联邦机制或者做监控指标服务拆分方式去实现并且最难以接受的是prometheus对于监控历史数据的存储问题，在本地不能存储过久的监控数据，只能通过远端存储接口，存储到支持prometheus远端存储接口的数据库中，这带来的问题就是引入新的组件会增加对应的运维工作量。  </p>
<h4 id="prometheus高可用问题和扩展问题"><a href="#prometheus高可用问题和扩展问题" class="headerlink" title="prometheus高可用问题和扩展问题"></a>prometheus高可用问题和扩展问题</h4><p>prometheus官方高可用的方式是通过部署多个prometheus实例采集同一个target，前端通过LB设备做为统一入口，这带来的问题就是，两个prometheus实例内存储的数据会存在差异，特别是当其中一个prometheus宕机后，另外一个prometheus接管服务，此时宕机的prometheus就会丢失宕机期间的监控数据，当LB的请求转发过来会出现数据不一致情况。</p>
<p><img src="https://gblobscdn.gitbook.com/assets%2F-LBdoxo9EmQ0bJP2BuUi%2F-LVSVjCoJ2ZKnv7dYe6V%2F-LPufOd4LgRt7mUDTV8C%2Fpromethues-ha-01.png?generation=1546683354194329&alt=media"></p>
<p>Thanos能够解决上述问题，thanos能够将多个prometheus实例的数据进行聚合去重，来支持prometheus横向扩展和提高prometheus的高可用性，同时也支持将历史监控数据存储到对象存储中，提供监控数据的可靠性，降低运维难度。  </p>
<h3 id="部署架构"><a href="#部署架构" class="headerlink" title="部署架构"></a>部署架构</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/thanos_5.png"></p>
<ul>
<li>部署多个prometheus实例，采集相同或不同的targets。</li>
<li>thanos-sidecar通过sidecar方式与prometheus部署在一起，将数据提供给query查询和将本地落盘的数据上传到兼容S3协议的对象存储中。  </li>
<li>Query进行数据汇总和去重做数据查询的统一入口，grafana通过query接口进行监控展示。  </li>
<li>历史监控数据查询通过store-Gateway进行查询。</li>
<li>compactor组件对存储在对象存储的数据进行压缩，降低采样率，后续查看长时间监控数据提高效率。</li>
</ul>
<h3 id="部署使用："><a href="#部署使用：" class="headerlink" title="部署使用："></a>部署使用：</h3><p>这里以一个快速demo的方式进行thanos的功能展示和部署，实际生产可以结合prometheus-operator的方式去部署更佳。  </p>
<p>提前准备：<br>创建一个storageclass，用于提供给prometheus实例使用</p>
<h4 id="部署prometheus"><a href="#部署prometheus" class="headerlink" title="部署prometheus"></a>部署prometheus</h4><p>创建ServiceAccount并做权限绑定  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - &quot;&quot;</span><br><span class="line">  resources:</span><br><span class="line">  - nodes</span><br><span class="line">  - services</span><br><span class="line">  - endpoints</span><br><span class="line">  - pods</span><br><span class="line">  - nodes/proxy</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups:</span><br><span class="line">  - &quot;extensions&quot;</span><br><span class="line">  resources:</span><br><span class="line">    - ingresses</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups:</span><br><span class="line">  - &quot;&quot;</span><br><span class="line">  resources:</span><br><span class="line">  - configmaps</span><br><span class="line">  - nodes/metrics</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">- nonResourceURLs:</span><br><span class="line">  - /metrics</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: prometheus</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: prometheus</span><br><span class="line">  namespace: default</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<p>创建configmap<br>prometheus-configmap.yaml  </p>
<p>将targets改为实际运行node-exporter节点地址  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus-config</span><br><span class="line">data:</span><br><span class="line">  prometheus.yaml.tmpl: |</span><br><span class="line">    global:</span><br><span class="line">      scrape_interval: 15s</span><br><span class="line">      scrape_timeout: 15s</span><br><span class="line">      external_labels:</span><br><span class="line">        cluster: test-thanos</span><br><span class="line">        replica: $(POD_NAME)</span><br><span class="line">    scrape_configs:</span><br><span class="line">     - job_name: &quot;node&quot;</span><br><span class="line">       static_configs:</span><br><span class="line">         - targets: [&quot;172.16.1.6:9100&quot;]</span><br><span class="line">        </span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus</span><br><span class="line">  labels:</span><br><span class="line">    app: prometheus</span><br><span class="line">spec:</span><br><span class="line">  serviceName: &quot;prometheus&quot;</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: prometheus</span><br><span class="line">      thanos-store-api: &quot;true&quot;</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels: </span><br><span class="line">        app: prometheus</span><br><span class="line">        thanos-store-api: &quot;true&quot;</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: prometheus</span><br><span class="line">      volumes:</span><br><span class="line">      - name: prometheus-config</span><br><span class="line">        configMap:</span><br><span class="line">          name: prometheus-config</span><br><span class="line">      - name: prometheus-config-shared</span><br><span class="line">        emptyDir: &#123;&#125;</span><br><span class="line">      containers:</span><br><span class="line">      - name: prometheus</span><br><span class="line">        image: prom/prometheus:v2.14.0</span><br><span class="line">        args:</span><br><span class="line">        - &quot;--config.file=/etc/prometheus-shared/prometheus.yaml&quot;</span><br><span class="line">        - &quot;--storage.tsdb.path=/prometheus&quot;</span><br><span class="line">        - &quot;--storage.tsdb.retention.time=6h&quot;</span><br><span class="line">        - &quot;--storage.tsdb.no-lockfile&quot;</span><br><span class="line">        - &quot;--storage.tsdb.min-block-duration=2h&quot;  # 每隔2小时将数据压缩成一个block，持久化到硬盘中</span><br><span class="line">        - &quot;--storage.tsdb.max-block-duration=2h&quot; </span><br><span class="line">        - &quot;--web.enable-admin-api&quot;  # thanos可以通过prometheus admin api管理数据</span><br><span class="line">        - &quot;--web.enable-lifecycle&quot;  # 支持热更新  localhost:9090/-/reload 加载</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 9090</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: prometheus-config-shared</span><br><span class="line">          mountPath: /etc/prometheus-shared/</span><br><span class="line">        - name: data</span><br><span class="line">          mountPath: &quot;/prometheus&quot;</span><br><span class="line">      - name: thanos</span><br><span class="line">        image: thanosio/thanos:v0.11.0</span><br><span class="line">        args:</span><br><span class="line">        - sidecar</span><br><span class="line">        - --tsdb.path=/prometheus</span><br><span class="line">        - --prometheus.url=http://localhost:9090</span><br><span class="line">        - --reloader.config-file=/etc/prometheus/prometheus.yaml.tmpl #配置对应的prometheus.yaml.tmpl文件路径</span><br><span class="line">        - --reloader.config-envsubst-file=/etc/prometheus-shared/prometheus.yaml  # 基于配置文件模板生成配置文件</span><br><span class="line">        ports:</span><br><span class="line">        - name: http-sidecar</span><br><span class="line">          containerPort: 10902</span><br><span class="line">        - name: grpc</span><br><span class="line">          containerPort: 10901</span><br><span class="line">        env:</span><br><span class="line">        - name: POD_NAME</span><br><span class="line">          valueFrom:</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: metadata.name</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: prometheus-config</span><br><span class="line">          mountPath: /etc/prometheus</span><br><span class="line">        - name: prometheus-config-shared</span><br><span class="line">          mountPath: /etc/prometheus-shared/</span><br><span class="line">        - name: data</span><br><span class="line">          mountPath: &quot;/prometheus&quot;</span><br><span class="line">  volumeClaimTemplates:  #提供volumesclaimTemplate为每个prometheus实例分配一个PVC</span><br><span class="line">  - metadata:</span><br><span class="line">      name: data</span><br><span class="line">      labels:</span><br><span class="line">        app: prometheus</span><br><span class="line">    spec:</span><br><span class="line">      storageClassName: managed-nfs-storage</span><br><span class="line">      accessModes:</span><br><span class="line">      - ReadWriteOnce</span><br><span class="line">      resources:</span><br><span class="line">        requests:</span><br><span class="line">          storage: 2Gi</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>




<p>注意以下参数：  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">--storage.tsdb.min-block-duration=2h </span><br><span class="line">--storage.tsdb.max-block-duration=2h    </span><br><span class="line">#这两个值使用thanos需要配置成一致，禁用prometheus对监控数据进行压缩，因为使用thanos，compactor组件会对监控数据进行压缩。 </span><br><span class="line">--web.enable-admin-api&quot;  # thanos可以通过prometheus admin api管理数据</span><br><span class="line">--web.enable-lifecycle&quot; #配置prometheus配置热加载  </span><br></pre></td></tr></table></figure>

<p>创建Service</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: thanos-store-gateway</span><br><span class="line">spec:</span><br><span class="line">  type: ClusterIP</span><br><span class="line">  clusterIP: None</span><br><span class="line">  ports:</span><br><span class="line">  - name: grpc</span><br><span class="line">    port: 10901</span><br><span class="line">    targetPort: grpc</span><br><span class="line">  selector:</span><br><span class="line">    thanos-store-api: &quot;true&quot;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>


<h4 id="部署exporter"><a href="#部署exporter" class="headerlink" title="部署exporter"></a>部署exporter</h4><p>用于采集监控数据</p>
<p>部署node-exporter</p>
<p>在节点上部署node-exporter用于收集主机资源信息  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -d   --net=&quot;host&quot;   --pid=&quot;host&quot;   -v &quot;/:/host:ro,rslave&quot;   bitnami/node-exporter   --path.rootfs=/host</span><br></pre></td></tr></table></figure>


<h4 id="部署Query"><a href="#部署Query" class="headerlink" title="部署Query"></a>部署Query</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line"></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: thanos-querier</span><br><span class="line">  namespace: kube-mon</span><br><span class="line">  labels:</span><br><span class="line">    app: thanos-querier</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: thanos-querier</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: thanos-querier</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: thanos</span><br><span class="line">        image: thanosio/thanos:v0.11.0</span><br><span class="line">        args:</span><br><span class="line">        - query</span><br><span class="line">        - --log.level=debug</span><br><span class="line">        - --query.replica-label=replica</span><br><span class="line">        # Discover local store APIs using DNS SRV.</span><br><span class="line">        - --store=dnssrv+thanos-store-gateway:10901</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 10902</span><br><span class="line">        - name: grpc</span><br><span class="line">          containerPort: 10901</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            memory: &quot;2Gi&quot;</span><br><span class="line">            cpu: &quot;1&quot;</span><br><span class="line">          limits:</span><br><span class="line">            memory: &quot;2Gi&quot;</span><br><span class="line">            cpu: &quot;1&quot;</span><br><span class="line">        livenessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /-/healthy</span><br><span class="line">            port: http</span><br><span class="line">          initialDelaySeconds: 10</span><br><span class="line">        readinessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /-/healthy</span><br><span class="line">            port: http</span><br><span class="line">          initialDelaySeconds: 15</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: thanos-querier</span><br><span class="line">  labels:</span><br><span class="line">    app: thanos-querier</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 9090</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: http</span><br><span class="line">    nodePort: 30001</span><br><span class="line">    name: http</span><br><span class="line">  selector:</span><br><span class="line">    app: thanos-querier</span><br><span class="line">  type: NodePort</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>


<p>访问节点<a href="http://ip:30001端口即可打开Query界面">http://ip:30001端口即可打开Query界面</a></p>
<p>可以看见通过prometheus接口抓取过来的数据</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/thanos_4.png"></p>
<p>可以看见对应的监控指标和对应的thanos-sidecar节点  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/thanos_2.png"></p>
<p>通过Stores页面可以看见对应的组件的状态信息<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/thanos_3.png"></p>
<p>以查看节点负载为例</p>
<p>默认情况下会出现两个值，因为有两个prometheus实例对数据进行收集<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/thanos_6.png">  </p>
<p>勾选deduplication选项将对监控数据进行去重处理，query是根据 prometheus.yaml.tmpl内的external_labels标签进行去重处理。Querier 会取时间戳新的数据进行展示。</p>
<h4 id="部署对象存储"><a href="#部署对象存储" class="headerlink" title="部署对象存储"></a>部署对象存储</h4><p>Thanos目前支持多种对象存储接口，包括像国内的阿里OSS、腾讯COS、openstack-swift和兼容S3接口的对象存储如minio等，下图为官方支持的对象存储列表</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/thanos_7.png">  </p>
<p>这里我们主要以Minio为例演示对接</p>
<p>创建pvc</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  # This name uniquely identifies the PVC. This is used in deployment.</span><br><span class="line">  name: minio-pv-claim</span><br><span class="line">spec:</span><br><span class="line">  # Read more about access modes here: http://kubernetes.io/docs/user-guide/persistent-volumes/#access-modes</span><br><span class="line">  storageClassName: managed-nfs-storage</span><br><span class="line">  accessModes:</span><br><span class="line">    # The volume is mounted as read-write by a single node</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  resources:</span><br><span class="line">    # This is the request for storage. Should be available in the cluster.</span><br><span class="line">    requests:</span><br><span class="line">      storage: 10Gi</span><br><span class="line">      </span><br><span class="line">EOF</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>创建Minio</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl create -f https://raw.githubusercontent.com/minio/minio/master/docs/orchestration/kubernetes/minio-standalone-deployment.yaml</span><br></pre></td></tr></table></figure>
<p>创建对外暴露Service</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: minio-service</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports:</span><br><span class="line">    - port: 9000</span><br><span class="line">      targetPort: 9000</span><br><span class="line">      protocol: TCP</span><br><span class="line">      nodePort: 30002</span><br><span class="line">  selector:</span><br><span class="line">    app: minio</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>


<p>访问Minio  </p>
<p><a href="http://ip:30002/">http://ip:30002</a></p>
<p>使用minio&#x2F;minio123访问Minio</p>
<p>创建名为prometheus的bucket<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/thanos_8.png"></p>
<h4 id="将落盘的监控数据存储到对象存储中"><a href="#将落盘的监控数据存储到对象存储中" class="headerlink" title="将落盘的监控数据存储到对象存储中"></a>将落盘的监控数据存储到对象存储中</h4><p>创建secret</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: minio-secret</span><br><span class="line">type: Opaque</span><br><span class="line">stringData:</span><br><span class="line">  thanos-secret.yaml: |-</span><br><span class="line">    type: S3</span><br><span class="line">    config:</span><br><span class="line">     bucket: &quot;prometheus&quot;</span><br><span class="line">     endpoint: &quot;172.16.1.6:30002&quot;</span><br><span class="line">     access_key: &quot;minio&quot;</span><br><span class="line">     insecure: true</span><br><span class="line">     secret_key: &quot;minio123&quot;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p>在prometheus启动的yaml文件中引用此secret<br>添加以下部分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">spec:</span><br><span class="line">      serviceAccountName: prometheus</span><br><span class="line">      volumes:</span><br><span class="line">      - name: minio-secret</span><br><span class="line">        secret:</span><br><span class="line">          secretName: minio-secret</span><br><span class="line">      containers:</span><br><span class="line">      - name: thanos</span><br><span class="line">        image: thanosio/thanos:v0.11.0</span><br><span class="line">        args:</span><br><span class="line">        - sidecar</span><br><span class="line">        - --tsdb.path=/prometheus</span><br><span class="line">        - --prometheus.url=http://localhost:9090</span><br><span class="line">        - --reloader.config-file=/etc/prometheus/prometheus.yaml.tmpl #配置对应的prometheus.yaml.tmpl文件路径</span><br><span class="line">        - --reloader.config-envsubst-file=/etc/prometheus-shared/prometheus.yaml  # 基于配置文件模板生成配置文件</span><br><span class="line">        - --objstore.config-file=/etc/secret/thanos-secret.yaml</span><br><span class="line">    </span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: minio-secret</span><br><span class="line">          mountPath: &quot;/etc/secret/&quot;</span><br></pre></td></tr></table></figure>

<p>等待片刻后可以在Minio上看见thanos-sidecar上传过来的prometheus监控数据</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/thanos_9.png">  </p>
<p>thanos-secret.yaml</p>
<h4 id="部署Store-Gateway"><a href="#部署Store-Gateway" class="headerlink" title="部署Store-Gateway"></a>部署Store-Gateway</h4><p>部署Store-Gateway方便查看历史存储数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line"></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: thanos-store</span><br><span class="line">  name: thanos-store</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app.kubernetes.io/name: thanos-store</span><br><span class="line">  serviceName: thanos-store-gateway</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        thanos-store-api: &quot;true&quot;</span><br><span class="line">        app.kubernetes.io/name: thanos-store</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - args:</span><br><span class="line">        - store</span><br><span class="line">        - --data-dir=/var/thanos/store</span><br><span class="line">        - --grpc-address=0.0.0.0:10901</span><br><span class="line">        - --http-address=0.0.0.0:10902</span><br><span class="line">        - --objstore.config-file=/etc/secret/thanos-secret.yaml</span><br><span class="line">        - --experimental.enable-index-header</span><br><span class="line">        image: quay.io/thanos/thanos:v0.11.0</span><br><span class="line">        livenessProbe:</span><br><span class="line">          failureThreshold: 8</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /-/healthy</span><br><span class="line">            port: 10902</span><br><span class="line">            scheme: HTTP</span><br><span class="line">          periodSeconds: 30</span><br><span class="line">        name: thanos-store</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 10901</span><br><span class="line">          name: grpc</span><br><span class="line">        - containerPort: 10902</span><br><span class="line">          name: http</span><br><span class="line">        readinessProbe:</span><br><span class="line">          failureThreshold: 20</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /-/ready</span><br><span class="line">            port: 10902</span><br><span class="line">            scheme: HTTP</span><br><span class="line">          periodSeconds: 5</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: /var/thanos/store</span><br><span class="line">          name: data</span><br><span class="line">          readOnly: false</span><br><span class="line">        - mountPath: /etc/secret/</span><br><span class="line">          name: minio-secret</span><br><span class="line">      volumes: </span><br><span class="line">      - name: minio-secret</span><br><span class="line">        secret:</span><br><span class="line">          secretName: minio-secret</span><br><span class="line">  volumeClaimTemplates:</span><br><span class="line">  - metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app.kubernetes.io/name: thanos-store</span><br><span class="line">      name: data</span><br><span class="line">    spec:</span><br><span class="line">      storageClassName: managed-nfs-storage</span><br><span class="line">      accessModes:</span><br><span class="line">      - ReadWriteOnce</span><br><span class="line">      resources:</span><br><span class="line">        requests:</span><br><span class="line">          storage: 10Gi</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/thanos_10.png">  </p>
<p>注意label thanos-store-api: “true”这样才能与thanos-store-gateway这个headless service关联，另外与它关联的Querier才能发现它。</p>
<h4 id="部署Compact"><a href="#部署Compact" class="headerlink" title="部署Compact"></a>部署Compact</h4><p>compact需要本地磁盘空间来存储中间数据以进行处理，建议使用大约100GB的空间以保持正常工作。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line"></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/component: database-compactor</span><br><span class="line">    app.kubernetes.io/instance: thanos-compact</span><br><span class="line">    app.kubernetes.io/name: thanos-compact</span><br><span class="line">    app.kubernetes.io/version: v0.11.0</span><br><span class="line">  name: thanos-compact</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app.kubernetes.io/component: database-compactor</span><br><span class="line">      app.kubernetes.io/instance: thanos-compact</span><br><span class="line">      app.kubernetes.io/name: thanos-compact</span><br><span class="line">  serviceName: thanos-compact</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app.kubernetes.io/component: database-compactor</span><br><span class="line">        app.kubernetes.io/instance: thanos-compact</span><br><span class="line">        app.kubernetes.io/name: thanos-compact</span><br><span class="line">        app.kubernetes.io/version: v0.11.0</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - args:</span><br><span class="line">        - compact</span><br><span class="line">        - --wait</span><br><span class="line">        - --objstore.config-file=/etc/secret/thanos-secret.yaml</span><br><span class="line">        - --data-dir=/var/thanos/compact</span><br><span class="line">        - --debug.accept-malformed-index</span><br><span class="line">        image: quay.io/thanos/thanos:v0.11.0</span><br><span class="line">        livenessProbe:</span><br><span class="line">          failureThreshold: 4</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /-/healthy</span><br><span class="line">            port: 10902</span><br><span class="line">            scheme: HTTP</span><br><span class="line">          periodSeconds: 30</span><br><span class="line">        name: thanos-compact</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 10902</span><br><span class="line">          name: http</span><br><span class="line">        readinessProbe:</span><br><span class="line">          failureThreshold: 20</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /-/ready</span><br><span class="line">            port: 10902</span><br><span class="line">            scheme: HTTP</span><br><span class="line">          periodSeconds: 5</span><br><span class="line">        terminationMessagePolicy: FallbackToLogsOnError</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: /var/thanos/compact</span><br><span class="line">          name: data</span><br><span class="line">          readOnly: false</span><br><span class="line">        - mountPath: /etc/secret/</span><br><span class="line">          name: minio-secret</span><br><span class="line">      volumes: </span><br><span class="line">      - name: minio-secret</span><br><span class="line">        secret:</span><br><span class="line">          secretName: minio-secret</span><br><span class="line">  volumeClaimTemplates:</span><br><span class="line">  - metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app.kubernetes.io/component: database-compactor</span><br><span class="line">        app.kubernetes.io/instance: thanos-compact</span><br><span class="line">        app.kubernetes.io/name: thanos-compact</span><br><span class="line">      name: data</span><br><span class="line">    spec:</span><br><span class="line">      storageClassName: managed-nfs-storage</span><br><span class="line">      accessModes:</span><br><span class="line">      - ReadWriteOnce</span><br><span class="line">      resources:</span><br><span class="line">        requests:</span><br><span class="line">          storage: 10Gi</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>



<p>需要本地磁盘空间来存储中间数据以进行处理，建议使用大约100GB的空间以保持正常工作。在重新启动之间可以安全地删除磁盘上的数据</p>
<h4 id="Receiver介绍"><a href="#Receiver介绍" class="headerlink" title="Receiver介绍"></a>Receiver介绍</h4><p>主要解决大规模场景下 Query都调所有 thanos-Sidecar会消耗很多资源，所以统一通过prometheus的remote-write接口将数据传送给thanos-Receiver，Query从thanos-Receiver获取数据<br><a href="https://thanos.io/proposals/201812_thanos-remote-receive.md/">https://thanos.io/proposals/201812_thanos-remote-receive.md/</a></p>
<h4 id="Grafana对接"><a href="#Grafana对接" class="headerlink" title="Grafana对接"></a>Grafana对接</h4><p>使用thanos后grafana对接直接对接Query就可以了</p>
<p>启动grafana</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -d --name=grafana -p 3000:3000 grafana/grafana</span><br></pre></td></tr></table></figure>

<p>添加DataSource这填写Query的地址</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/thanos_11.png">  </p>
<p>添加dashboard，因为我们之有node-exporter的数据所以这块需要使直接使用主机节点的dashboard，导入dashboard输入id号8919</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/thanos_12.png">  </p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p> Thanos做为prometheus的一个附加组件，还是能解决目前在使用prometheus时的一些高可用，历史数据存储和大规模集群的问题，目前社区也在大力发展。  </p>
<p>参考链接：<br><a href="https://github.com/thanos-io/thanos/blob/master/docs/quick-tutorial.md">https://github.com/thanos-io/thanos/blob/master/docs/quick-tutorial.md</a><br><a href="https://engineering.hellofresh.com/monitoring-at-hellofresh-part-1-architecture-677b4bd6b728">https://engineering.hellofresh.com/monitoring-at-hellofresh-part-1-architecture-677b4bd6b728</a><br><a href="https://github.com/thanos-io/kube-thanos/tree/master/manifests">https://github.com/thanos-io/kube-thanos/tree/master/manifests</a><br><a href="https://github.com/thanos-io/thanos/blob/master/docs/components/sidecar.md">https://github.com/thanos-io/thanos/blob/master/docs/components/sidecar.md</a><br><a href="https://www.qikqiak.com/k8strain/monitor/thanos/#ruler">https://www.qikqiak.com/k8strain/monitor/thanos/#ruler</a><br><a href="https://github.com/thanos-io/kube-thanos/tree/release-0.11/examples/all/manifests">https://github.com/thanos-io/kube-thanos/tree/release-0.11/examples/all/manifests</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>trove原理</title>
    <url>/2017/04/17/trove/</url>
    <content><![CDATA[<p>版本：trove4.0<br>openstack版本：liberty</p>
<h3 id="DBaaS是什么？"><a href="#DBaaS是什么？" class="headerlink" title="DBaaS是什么？"></a>DBaaS是什么？</h3><p>字面上理解数据库即是服务，简单来说就是以服务的形式为用户提供数据库服务</p>
<h4 id="在云平台上使用trove有什么优势？"><a href="#在云平台上使用trove有什么优势？" class="headerlink" title="在云平台上使用trove有什么优势？"></a>在云平台上使用trove有什么优势？</h4><p>1，简化IT操作流程，降低使用数据库使用门槛举个例子，曾经我搭建一个LAMP网站，数据库要自己安装，创建，授权，必要的话，还要自己做主从很繁琐，而且不是专业人员也搞不定，有了Dbaas后，我只需要在控制台点几下就弄好。</p>
<p>2，自动化操作，自动的增、删、改、备。</p>
<p>3，更好的资源利用，你可以根据业务量，自由的对数据库实例进行伸缩。</p>
<h3 id="架构解析"><a href="#架构解析" class="headerlink" title="架构解析"></a>架构解析</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_1.png"></p>
<p>trove根其他一些openstak组件一样，它暴露一个public-api，通过这个api访问trove-service，同时也保存着一些数据库实例状态到数据库中。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_2.png"></p>
<h3 id="组件功能"><a href="#组件功能" class="headerlink" title="组件功能"></a>组件功能</h3><p>trove-api<br>用于操作请求的接收和分发操作提供REST风格的API，同时与trove-conductor和trove-taskmanager通信，一些轻量级的请求比如获取实例状态，实例数量等操作都是自身直接处理或访问<br>conductor和trove-taskmanager处理，比较重量级的操作比如创建数据库，创建备份等操作都是通过rpc传递给trove-taskmanager，taskmanager，然后在通过调用nova、swift、neutron、cinder等组件来完成操作。</p>
<p>trove-conductor<br>将vm内trove-guestagent发送的状态信息保存到数据库，与trove-guestagent的通信是通过rpc来实现的，trove-conductor这个组件的目的是为了避免创建的数据库的实例直接访问数据库，它是做为一个trove-guestagent将昨天写入数据库的中间件。</p>
<p>trove-taskmanager<br>执行trove中大部分复杂的操作，请求者发送消息到task manager，task manager在请求者的上下文中调用相应的程序执行这些请求。task manager处理一些操作，包括实例的创建、删除，与其他服务如Nova、Cinder、Swift等的交互，一些更复杂的Trove操作如复制和集群，以及对实例的整个生命周期的管理。trov-taskmanager就像是其他openstak服务的客户端，如nova，swift，cinder等，当要创建数据库实例时就将请求发送给nova，让nova去创建个实例，要备份的话就调用swift接口上传备份。</p>
<p>trove-guestagent<br>trove-guestagent集成在vm镜像里面，通过监听rpc里面task manager发过来的指令，并在本地执行代码完成数据库任务，taskmanager将消息发送到guest agent，guest agent通过调用相应的程序执行这些请求。</p>
<h3 id="功能原理介绍-这里只介绍对mysql数据库的功能实现，因为trove对mysql支持比较成熟）"><a href="#功能原理介绍-这里只介绍对mysql数据库的功能实现，因为trove对mysql支持比较成熟）" class="headerlink" title="功能原理介绍(这里只介绍对mysql数据库的功能实现，因为trove对mysql支持比较成熟）"></a>功能原理介绍(这里只介绍对mysql数据库的功能实现，因为trove对mysql支持比较成熟）</h3><p>这里分别介绍三个功能的原理<br>1，创建数据库实例<br>2，创建数据库备份<br>3，mysql的集群 </p>
<h4 id="创建数据库实例"><a href="#创建数据库实例" class="headerlink" title="创建数据库实例"></a>创建数据库实例</h4><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_3.png"></p>
<p>创建数据库实例时，实际上就是通过trove-taskmanager create_instance()方法去调用nova-api   然后调用 <code>_get_injected_files</code>方法将guet_info和trove-guetagent.conf信息注入到 数据库实例&#x2F;etc&#x2F;trove&#x2F;conf.d&#x2F;里面，提供给guest-agent进行后续的操作。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_4.png"></p>
<p>所以4.0版本的trove并不需要一开始就将trove-guestagent.conf这个配置文件封装在镜像里面，这个配置文件是通过nova注入的，所以镜像只需要配置好guest-agent从哪里读这个配置文件。剩下的就交给trove-guestagent</p>
<p>guest_info_file这个配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">guest_id=7ec35639-5139-4ae4-8388-8101e41cc0f7 #这个ID是trove 分配给这个实例的ID</span><br><span class="line">datastore_manager=mysql             #采用的是哪个datastore</span><br><span class="line">tenant_id=f2f0e038ff0342a3bc99d8971f829ac2 #是哪个租户的</span><br></pre></td></tr></table></figure>


<p>当你在控制台输入需要创建的云硬盘的大小时，实际上是通过调用taskmanager里面的_create_volume方法<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_5.png"></p>
<p>收集齐上面那些信息后，然后调用nova来创建数据库实例  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_6.png"></p>
<p>然后数据库实例里面的guest-agent会去读取通过nova注入的trove-guestagent.conf 去连接rpc读取taskmanager发送过来的操作请求。</p>
<p>剩下的一些操作比如创建数据库、创建用户这些都是taskmanager调用数据库实例里面的guest-agent去实现</p>
<p>guest-agent对mysql的一些操作实现是在</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/usr/lib/python2.7/dist-packages/trove/guestagent/datastore/mysql/service_base.py 他里面包含了</span><br><span class="line"></span><br><span class="line">def _get_actual_db_status() #获取数据库实例状态方法</span><br></pre></td></tr></table></figure>

<p>主要是通过调用&#x2F;usr&#x2F;bin&#x2F;mysqladmin ping” 和ps -C mysqld h 去获取数据库实例状态 </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_7.png"></p>
<p>通过判断pid文件是否存在来判断mysql是否shutdown。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_8.png"></p>
<p>def create_database #创建数据库实例方法  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_9.png"></p>
<p>def create_user #创建用户并且授权方法<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_10.png"></p>
<p>后面还有删除数据库，删除用户，获取binlog，开始slave、关闭slave等方法<br>同时需要注意的是trove创建数据库实例时，会默认为每个数据库实例同时创建一个<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_26.png"></p>
<p>SecGroup-xxx xxx为主机ID的安全组。<br>trove默认是不启动root用户的所以在控制台用户选项卡里面用户名称是不能填root的。    </p>
<p>需要注意的是上述所有操作都是由trove用户来执行，所以必须要确认的是trove用户拥有sudo权限，否则会失败。</p>
<p>在执行完上述操作前此时数据库状态还是building状态的， 那就是 vm 正在启动，创建数据库，创建用户，对用户授权，同步my.cnf配置文件到数据库实例内，重启mysql ，trove-guestagent 发送 rpc 给trove-taskmanager，最后检测数据库成功运行后发送 Active 状态消息给 rpc，trove-taskmanager 收到 Active 消息后，不再发送创建数据库消息，而 trove-conductor 同时收到 trove-guestagent Active 消息后，去数据库里更新 trove instance 的状态，在trove list 就可以看见instance Active 的状态了。</p>
<h3 id="备份还原"><a href="#备份还原" class="headerlink" title="备份还原"></a>备份还原</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_11.png"></p>
<p>目前trove-guestagent只支持mysql的三种备份方式，一种是传统的mysql Dump方式一种是InnoBackupEx 还有增是InnoBackup的增量备份方式InnoBackupExIncremental。</p>
<p>备份的程序放在  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/usr/lib/python2.7/dist-packages/trove/guestagent/strategies/backup</span><br></pre></td></tr></table></figure>
<p>其调用方式也比较简单，就是trove-guestagent.conf里面配置了什么备份方式就调用指定类执行里面的方法，方法内也都是一些软件的命令。<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_12.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_13.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_14.png"></p>
<p>需要注意的是默认不配置是调用Innobackup<br>备份的日志会存在tmp目录下</p>
<p>备份完成后默认是会存储到swift内<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_15.png"></p>
<p>默认备份在swift内的备份文件夹为database_backups 、开启压缩、ssl加密，分片等<br>调用SwiftStorage类里面的save方法上传到Swift中<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_16.png"></p>
<p>其中会进行文件的校验<br>是备份上去的实际上有两个文件，第一个enc文件主要是用来分片使用，第二个文件才是主要的备份文件。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_17.png"></p>
<h3 id="mysql主从"><a href="#mysql主从" class="headerlink" title="mysql主从"></a>mysql主从</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_18.png"></p>
<p>trove-master端先将当前数据备份到Swift—&gt;然后taskmanager重新创建个数据库实例——&gt;新创建的数据库实例将刚刚的备份从Swift拉下来根据里面的bin-log里面的GTID进行还原—-&gt;建立主从关系—检测创建成功taskmanager删除上传到Swift的备份。</p>
<p>备份前会做个检测，发现以前有备份就调用增量备份的方法节省空间，检测到没有就调用全备的方法<br>这里先做个变量定义，定义好增量备份和全备的变量</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_19.png"></p>
<p>if判断调用全备还是增量备份。</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_20.png"></p>
<p>目前trove只支持mysql的主从不支持主主并且还是异步的主从。<br>创建主从时，创建从同样是调用create_instance()方法</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_21.png"></p>
<p>只是这里做了个判断，如果传过来了slave_of_id就调用__create_replication_slave()方法</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_22.png"></p>
<p><code>__create_replication_slave()</code>方法会去获去备份的ID ,然后继续调用nova创建主机</p>
<p>接下来操作会交给数据库实例里面的guest-agent进行操作。</p>
<p>guest-agent会先将备份文件从来Swift 下载下来。然后还原。接下来建立主从关系，这里要说明的是trove建立主从关系的方式有两种一种是传统的bin-log的形式，一种的用GTID的形式。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">在/usr/lib/python2.7/dist-packages/trove/common/cfg.py </span><br></pre></td></tr></table></figure>

<p>这个是定义的两个不同的策略  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_23.png"></p>
<p>同时也会调用不同的方法去执行<br>当你的配置文件为</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">replication_strategy = MysqlBinlogReplication</span><br><span class="line">replication_namespace = trove.guestagent.strategies.replication.mysql_binlog</span><br></pre></td></tr></table></figure>


<p>调用的是</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/usr/lib/python2.7/dist-packages/trove/guestagent/strategies/replication/mysql_binlog.py</span><br></pre></td></tr></table></figure>

<p>当你的配置文件为  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">replication_strategy = MysqlGTIDReplication</span><br><span class="line">replication_namespace = trove.guestagent.strategies.replication.mysql_gtid</span><br></pre></td></tr></table></figure>
<p>调用的是  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/usr/lib/python2.7/dist-packages/trove/guestagent/strategies/replication/mysql_gtid.py</span><br></pre></td></tr></table></figure>

<p>这两个文件有何不同，方法内定义的命令不同</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_24.png"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/trove_25.png"></p>
<p>GTID的概述：</p>
<p>全局事物标识：global transaction identifieds。</p>
<p>GTID事物是全局唯一性的，且一个事务对应一个GTID。</p>
<p>一个GTID在一个服务器上只执行一次，避免重复执行导致数据混乱或者主从不一致。</p>
<p>GTID用来代替classic的复制方法，不在使用binlog+pos开启复制。而是使用master_auto_postion&#x3D;1的方式自动匹配GTID断点进行复制。</p>
<p>MySQL-5.6.5开始支持的，MySQL-5.6.10后开始完善。</p>
<p>在传统的slave端，binlog是不用开启的，但是在GTID中，slave端的binlog是必须开启的，目的是记录执行过的GTID（强制）。</p>
<p>下面介绍一下mysql GTID（忘记了从哪个网上摘录的)</p>
<p>GTID的组成部分：</p>
<p>前面是server_uuid：后面是一个序列号</p>
<p>例如：server_uuid：sequence number</p>
<p>7800a22c-95ae-11e4-983d-080027de205a:10</p>
<p>UUID：每个mysql实例的唯一ID，由于会传递到slave，所以也可以理解为源ID。</p>
<p>Sequence number：在每台MySQL服务器上都是从1开始自增长的序列，一个数值对应一个事务。</p>
<p>GTID比传统复制的优势：</p>
<p>更简单的实现failover，不用以前那样在需要找log_file和log_Pos。</p>
<p>更简单的搭建主从复制。</p>
<p>比传统复制更加安全。</p>
<p>GTID是连续没有空洞的，因此主从库出现数据冲突时，可以用添加空事物的方式进行跳过。</p>
<p>GTID的工作原理：</p>
<p>master更新数据时，会在事务前产生GTID，一同记录到binlog日志中。</p>
<p>slave端的i&#x2F;o 线程将变更的binlog，写入到本地的relay log中。</p>
<p>sql线程从relay log中获取GTID，然后对比slave端的binlog是否有记录。</p>
<p>如果有记录，说明该GTID的事务已经执行，slave会忽略。</p>
<p>如果没有记录，slave就会从relay log中执行该GTID的事务，并记录到binlog。</p>
<p>在解析过程中会判断是否有主键，如果没有就用二级索引，如果没有就用全部扫描。</p>
<p>要点：</p>
<p>1、slave在接受master的binlog时，会校验master的GTID是否已经执行过（一个服务器只能执行一次）。,</p>
<p>2、为了保证主从数据的一致性，多线程只能同时执行一个GTID。</p>
]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title>Vllm学习-部署使用</title>
    <url>/2025/06/15/vllm-1/</url>
    <content><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>VLLM（Vectorized Large Language Model Inference）是由UC Berkeley的系统研究团队开发，专注于优化大模型的推理速度框架，核心亮点在于通过PagedAttention 注意力算法以提高服务的吞吐量。<br>核心原理是能够将kv-cache动态分配在不连续的空间，提高整体显存利用率和并发数。</p>
<p>架构参考：<br><a href="https://docs.vllm.ai/en/latest/design/arch_overview.html">https://docs.vllm.ai/en/latest/design/arch_overview.html</a></p>
<h3 id="安装-Vllm"><a href="#安装-Vllm" class="headerlink" title="安装 Vllm"></a>安装 Vllm</h3><h4 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h4><p>使用VLLM部署Qwen3 0.6B<br>参考Qwen文档<br><a href="https://qwen.readthedocs.io/zh-cn/latest/deployment/vllm.html">https://qwen.readthedocs.io/zh-cn/latest/deployment/vllm.html</a></p>
<p>硬件配置<br>CPU：AMD-5900X<br>内存：128G<br>显卡：RTX-3060-12G</p>
<p>软件：<br>vllm：0.9.1<br>Python：3.12.7<br>Models：Qwen3-0.6B</p>
<p>部署vllm 0.9.1版本，建议Python: 3.9 – 3.12版本，cuda版本12.8</p>
<p>参考Nvidia官方手册安装cuda</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://developer.nvidia.com/cuda-12-8-0-download-archive?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=Ubuntu&amp;target_version=22.04&amp;target_type=deb_local</span><br></pre></td></tr></table></figure>
<p>安装miniconda</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>
<p>运行安装脚本</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bash Miniconda3-latest-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>
<ul>
<li>按提示按 ​<strong>Enter</strong>​ 阅读协议 → 输入 yes 同意</li>
<li>设置安装路径（默认 ~&#x2F;miniconda3 即可）</li>
<li>提示 ​**Do you wish to initialize Miniconda3?**​ 选 yes</li>
</ul>
<p>安装完成后配置bash<br>在&#x2F;root&#x2F;.bashrc添加PATH目录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export PATH=/root/miniconda3/bin:$PATH</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sorce /root/.bashrc</span><br></pre></td></tr></table></figure>
<p>验证</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda --version</span><br><span class="line">conda 25.3.1</span><br></pre></td></tr></table></figure>

<p>创建vllm部署的Python环境</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda create -n vllm python=3.12.7 </span><br><span class="line">conda activate vllm</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span><br><span class="line">conda config --set show_channel_urls yes</span><br></pre></td></tr></table></figure>

<p>安装pytorch<br>因为当前cuda12.2对应的torch版本还没有进入稳定版所以这里用的nightly路径。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install --pre torch==2.7.0.dev20250310+cu128 --index-url https://download.pytorch.org/whl/nightly/cu128</span><br></pre></td></tr></table></figure>

<h4 id="安装vllm引擎"><a href="#安装vllm引擎" class="headerlink" title="安装vllm引擎"></a>安装vllm引擎</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install vllm==0.9.0.1</span><br></pre></td></tr></table></figure>


<p>测试</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> python</span><br><span class="line">Python 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0] on linux</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line">&gt;&gt;&gt; import torch</span><br><span class="line">&gt;&gt;&gt; print(torch.__version__) </span><br><span class="line">2.7.0.dev20250310+cu128</span><br><span class="line">&gt;&gt;&gt; print(torch.cuda.is_available()) </span><br><span class="line">True</span><br></pre></td></tr></table></figure>

<p>验证vllm版本</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vllm --version</span><br><span class="line"></span><br><span class="line">INFO 06-02 14:34:37 [__init__.py:243] Automatically detected platform cuda.</span><br><span class="line">INFO 06-02 14:34:39 [__init__.py:31] Available plugins for group vllm.general_plugins:</span><br><span class="line">INFO 06-02 14:34:39 [__init__.py:33] - lora_filesystem_resolver -&gt; vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver</span><br><span class="line">INFO 06-02 14:34:39 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.</span><br><span class="line">0.9.0.1</span><br></pre></td></tr></table></figure>


<p>下载模型<br>指定下载路径</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir -p /root/models/Qwen/Qwen3 </span><br></pre></td></tr></table></figure>
<p>从modelscope下载比huggingface要快一些<br>先安装modelscope</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install modelscope</span><br></pre></td></tr></table></figure>

<p>下载模型</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">modelscope download --model Qwen/Qwen3-0.6B --local_dir /root/models/Qwen/Qwen3</span><br></pre></td></tr></table></figure>

<p>通过vllm加载启动模型<br>对外暴露的方式有两种LLM Class和OpenAI-Compatible API Server这里使用OpenAI方式对外暴露</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0  python3 -m vllm.entrypoints.openai.api_server --model /root/models/Qwen/Qwen3 --served-model-name=Qwen3-0.6B --dtype=bfloat16 --trust-remote-code --max-model-len=1024 --tensor-parallel-size=1 --gpu-memory-utilization=0.85 --enable-reasoning --reasoning-parser deepseek_r1 --port 8000 --api-key 123456</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>参数作用<br>以下是转换后的参数说明表格（Markdown格式）：</p>
<table>
<thead>
<tr>
<th><strong>参数</strong></th>
<th><strong>作用</strong></th>
<th><strong>值说明</strong></th>
<th><strong>引用来源</strong></th>
</tr>
</thead>
<tbody><tr>
<td><code>--model /data/models/Qwen/Qwen3</code></td>
<td>指定模型路径</td>
<td>本地存储的 Qwen 模型目录，需提前下载（如通过 <code>modelscope download</code>）</td>
<td></td>
</tr>
<tr>
<td><code>--served-model-name=Qwen3-0.6B</code></td>
<td>设置 API 中模型名称</td>
<td>客户端调用时使用的标识符（如 <code>model=&quot;Qwen3-0.6B&quot;</code>）</td>
<td></td>
</tr>
<tr>
<td><code>--dtype=bfloat16</code></td>
<td>指定计算精度</td>
<td><code>bfloat16</code> 减少显存占用，适合低显存 GPU（如 RTX 2080Ti）</td>
<td></td>
</tr>
<tr>
<td><code>--trust-remote-code</code></td>
<td>允许加载自定义代码</td>
<td>用于支持非标准模型架构（如 Qwen 的特殊 tokenizer）</td>
<td></td>
</tr>
<tr>
<td><code>--max-model-len=1024</code></td>
<td>最大上下文长度</td>
<td>限制单次请求的 token 数量（值越大，显存需求越高）</td>
<td></td>
</tr>
<tr>
<td><code>--tensor-parallel-size=1</code></td>
<td>张量并行大小</td>
<td><code>1</code> 表示单 GPU 运行；多卡需设为 GPU 数量（如 <code>--tensor-parallel-size=4</code>）</td>
<td></td>
</tr>
<tr>
<td><code>--gpu-memory-utilization=0.85</code></td>
<td>GPU 显存利用率</td>
<td>预分配 85% 显存给模型和 KV 缓存，避免 OOM（默认 0.9）</td>
<td></td>
</tr>
<tr>
<td><code>--enable-reasoning --reasoning-parser deepseek_r1</code></td>
<td>启用推理功能</td>
<td>使用 DeepSeek 的解析器增强逻辑推理能力（需 vLLM ≥0.7.3）</td>
<td></td>
</tr>
<tr>
<td><code>--port 8000</code></td>
<td>服务监听端口</td>
<td>API 通过 <code>http://&lt;IP&gt;:8801/v1</code> 提供（需防火墙放行）</td>
<td></td>
</tr>
<tr>
<td><code>--api-key 123456</code></td>
<td>设置 API 认证密钥</td>
<td>客户端需在 Header 中添加 <code>Authorization: Bearer 123456</code></td>
<td></td>
</tr>
</tbody></table>
<p>补充说明：</p>
<ol>
<li><strong>模型路径格式</strong>  <ul>
<li>支持本地路径（如 <code>/data/models/Qwen/Qwen3</code>）或 Hugging Face 模型 ID（如 <code>Qwen/Qwen3-0.6B</code>）。</li>
</ul>
</li>
<li><strong>显存优化</strong>  <ul>
<li><code>bfloat16</code> 在低显存 GPU 上可减少约 30% 显存占用，但可能损失少量精度。</li>
</ul>
</li>
<li><strong>推理功能扩展</strong>  <ul>
<li><code>deepseek_r1</code> 解析器需配合 vLLM ≥0.7.3 使用，支持逻辑推理任务的分步解析。</li>
</ul>
</li>
<li><strong>安全认证</strong>  <ul>
<li><code>--api-key</code> 强制客户端通过 <code>Authorization: Bearer</code> 标头认证，防止未授权访问。</li>
</ul>
</li>
</ol>
<blockquote>
<p>注：参数值中的路径、端口和密钥需根据实际环境调整。</p>
</blockquote>
<p>运行成功后可以通过命令行看见<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/vllm1-1.png"></p>
<p>通过curl命令访问</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl http://localhost:8000/v1/chat/completions     -H &quot;Content-Type: application/json&quot;     -H &quot;Authorization: Bearer 123456&quot;     -d &#x27;&#123;</span><br><span class="line">        &quot;model&quot;: &quot;Qwen3-0.6B&quot;,</span><br><span class="line">        &quot;messages&quot;: [</span><br><span class="line">            &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;&#125;,</span><br><span class="line">            &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你是谁？&quot;&#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;&#x27;</span><br><span class="line">&#123;&quot;id&quot;:&quot;chatcmpl-84534513a50f43abaa7a36e047f780a6&quot;,&quot;object&quot;:&quot;chat.completion&quot;,&quot;created&quot;:1748878398,&quot;model&quot;:&quot;Qwen3-0.6B&quot;,&quot;choices&quot;:[&#123;&quot;index&quot;:0,&quot;message&quot;:&#123;&quot;role&quot;:&quot;assistant&quot;,&quot;reasoning_content&quot;:&quot;\n好的，用户问我是谁。作为AI助手，我需要以合适的方式回答。首先，我应该确认用户的问题，然后提供基本信息。同时，要保持礼貌和专业的形象，避免使用可能引起误解的措辞。需要确保回答简洁明了，让用户感到被理解和支持。最后，检查是否有需要补充的信息，以提供更全面的回答。\n&quot;,&quot;content&quot;:&quot;\n\n我是AI助手，可以帮您解答问题。如果您有任何疑问或需要帮助，请随时告诉我！&quot;,&quot;tool_calls&quot;:[]&#125;,&quot;logprobs&quot;:null,&quot;finish_reason&quot;:&quot;stop&quot;,&quot;stop_reason&quot;:null&#125;],&quot;usage&quot;:&#123;&quot;prompt_tokens&quot;:22,&quot;total_tokens&quot;:122,&quot;completion_tokens&quot;:100,&quot;prompt_tokens_details&quot;:null&#125;,&quot;prompt_logprobs&quot;:null,&quot;kv_transfer_params&quot;:null&#125;(base) </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在server端也可以看见输出的token的速度</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">INFO 06-02 15:13:40 [async_llm.py:261] Added request chatcmpl-0d62a87c6c2d4146927d7e704b11ffc7.</span><br><span class="line">INFO 06-02 15:13:40 [loggers.py:116] Engine 000: Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 1.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%</span><br></pre></td></tr></table></figure>



<p>也可以通过Open WebUI或Cherry Studio配置访问</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/vllm1-2.png"></p>
]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenStack对接vmware</title>
    <url>/2017/09/08/vmware&amp;openstack/</url>
    <content><![CDATA[<p>openstack版本：liberty<br>vsphere版本：6.0  </p>
<h3 id="拓扑图"><a href="#拓扑图" class="headerlink" title="拓扑图"></a>拓扑图</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/openstack&vmware_1.png" alt="image"></p>
<h3 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h3><p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/openstack&vmware_2.png" alt="image"></p>
<p>vmware对接openstack有两种驱动<br>1，VCDriver，以nova driver的方式通过vcenter来控制计算和网络<br>2，VMDKDriver，以cinder driver的方式通过vcenter来控制datastore。  </p>
<p>需要注意的是本文档的对接都是对接vmware的标准交换机进行对接的，如果需要对接vmware的dvs的话需要安装另外的包并配置neutron。</p>
<p>这里采用vcdriver的方式<br>这种方式目前有几个问题<br>1，就是已经存在vmware集群的vm是不能被openstack管理了，只有新创建的机器可以被openstack管理。  </p>
<p>2，vlan模式下，VCDriver要求vCenter里被使用的VLAN port group的名字必须和OpenStack里integration bridge（默认是br-int）的名字一样，而且vlan 号也得一样，这里我配置port group为0，表示不做限制。</p>
<p>3，首次创建虚机会花费很长时间，因为需要将镜像从glance通过vcenter 拷贝到cluster后端的datasotre，后面再创建速度就会很快。</p>
<p>要点：<br>一台计算节点对接vsphere，需要这台计算节点能根vcenter通信。  </p>
<h3 id="Vsphere上的配置"><a href="#Vsphere上的配置" class="headerlink" title="Vsphere上的配置"></a>Vsphere上的配置</h3><p>配置好对应的openstack计算节点对应的cluster<br>如这个vsphere的cluster_name为UAT_Lenovo_6.0</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/openstack&vmware_3.png" alt="image"></p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/openstack&vmware_4.png" alt="image"></p>
<p>cluster对应的storage名字  </p>
<p>创建对应的port_group  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/openstack&vmware_5.png" alt="image"></p>
<h3 id="nova配置"><a href="#nova配置" class="headerlink" title="nova配置"></a>nova配置</h3><p>需要注意对应的物理适配器，待会需要写到配置项内。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">openstack平台对接vcenter配置参考</span><br><span class="line">compute_driver #对应的驱动</span><br><span class="line">host_ip #连接vsphere的ip地址</span><br><span class="line">host_username #管理员帐户</span><br><span class="line">host_password #管理员密码</span><br><span class="line">datastore_rege #cluster对应的后端存储</span><br><span class="line">cluster_name #对应的cluster 名字</span><br><span class="line">vlan_interface #port group对应的物理网卡</span><br><span class="line">vmware.integration_bridge #对应的port_group</span><br><span class="line">insecure=True #关闭ssl连接不然报下面错</span><br></pre></td></tr></table></figure>
<p>修改&#x2F;etc&#x2F;nova&#x2F;nova.conf配置文件：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[default]</span><br><span class="line">compute_driver = vmwareapi.VMwareVCDriver</span><br><span class="line">[vmware]</span><br><span class="line">host_ip=10.3.1.11</span><br><span class="line">host_username=administrator@UAT.com</span><br><span class="line">host_password=1qaz@WSX</span><br><span class="line">datastore_regex=Huawei_S5500_01</span><br><span class="line">cluster_name=UAT_Lenovo_6.0</span><br><span class="line">api_retry_count=10</span><br><span class="line">vlan_interface=vmnic0</span><br><span class="line">vmware.integration_bridge=br-int</span><br><span class="line">insecure=True</span><br></pre></td></tr></table></figure>

<p>需要注意这些参数仔细核对。  </p>
<p>启动openstack-nova-compute服务：<br><code>systemctl restart openstack-nova-compute</code></p>
<p>此时应该可以看见vmware 对应的hypervisor<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/openstack&vmware_6.png" alt="image"></p>
<p>转换镜像将一个qcow2的镜像转换成vmware用的vmdk格式</p>
<h3 id="glance配置"><a href="#glance配置" class="headerlink" title="glance配置"></a>glance配置</h3><p>转换镜像格式为vmdk：<br><code>qemu-img convert -f raw -O vmdk  centos_6.5.raw  centos_6.5.vmdk</code></p>
<p>上传镜像：  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">glance image-create  --name=centos-6.5.vmdk  --disk-format=vmdk  --property hypervisor_type=&quot;vmware&quot;  --property vmware_adaptertype=&quot;ide&quot;  --property vmware_disktype=&quot;sparse&quot;  --is-public=True  --property vmware_ostype=&quot;otherLinuxGuest&quot;  --container-format=bare &lt; centos_6.5.vmdk</span><br></pre></td></tr></table></figure>

<p>创建虚拟机进行测试  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/openstack&vmware_7.png" alt="image"><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/openstack&vmware_8.png" alt="image"><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/openstack&vmware_9.png" alt="image"></p>
<p>第一次创建时间比较久因为要将镜像从glance 拷贝到vmware 的storage内<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/openstack&vmware_10.png" alt="image"></p>
<p>可以看见vsphere内已经有以uuid命名的vm了，表示创建成功  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/openstack&vmware_11.png" alt="image"></p>
<p>后面为可选配置，配置cinder对接vmware存储</p>
<h3 id="cinder配置"><a href="#cinder配置" class="headerlink" title="cinder配置"></a>cinder配置</h3><p>编辑&#x2F;etc&#x2F;cinder&#x2F;cinder.conf配置文件，修改如下参数：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">volume_driver=cinder.volume.drivers.vmware.vmdk.VMwareVcVmdkDriver</span><br><span class="line">volume_backend_name=vmware</span><br><span class="line">vmware_host_ip=10.68.35.24</span><br><span class="line">vmware_host_username=administrator</span><br><span class="line">vmware_host_password=admin123</span><br></pre></td></tr></table></figure>
<p>重启cinder-volume服务：<br><code>/etc/init.d/openstack-cinder-volume restart</code><br>创建多类型存储：<br><code>cinder type-create vmware</code><br><code>cinder type-key vmware set volume_backend_name=vmware</code><br><code>cinder type-create ceph</code><br><code>cinder type-key ceph set volume_backend_name=DEFAULT</code>  </p>
<p>创建云硬盘，挂在云主机进行测试  </p>
<h3 id="mitaka版本对接vsphere6-5"><a href="#mitaka版本对接vsphere6-5" class="headerlink" title="mitaka版本对接vsphere6.5"></a>mitaka版本对接vsphere6.5</h3><p>mitaka对接vsphere有个bug就是在https的连接情况下会连接失败<br><a href="https://review.openstack.org/#/c/341816/">https://review.openstack.org/#/c/341816/</a><br>替换<br><code>/usr/lib/python2.7/site-packages/nova/virt/vmwareapi/vmops.py  </code></p>
<p>重启nova-compute<br><code>systemctl restart openstack-nova-compute</code><br>编辑nova.conf</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[vmware]</span><br><span class="line">host_port=443</span><br><span class="line">host_ip=vcenter.xjrccu.com</span><br><span class="line">host_username=administrator@xjrccu.com</span><br><span class="line">host_password=Rayking,135</span><br><span class="line">datastore_regex=Datastore2</span><br><span class="line">cluster_name=LenovoServer-Cluster</span><br><span class="line">api_retry_count=10</span><br><span class="line">vlan_interface=vmnic4</span><br><span class="line">vmware.integration_bridge=br-int</span><br><span class="line">insecure=True</span><br></pre></td></tr></table></figure>
<p>重启nova-compute<br><code>systemctl restart openstack-nova</code><br>输入nova hypervisor-list可以看见vmware这个hypervisor</p>
<h3 id="cinder对接vmware-datastore"><a href="#cinder对接vmware-datastore" class="headerlink" title="cinder对接vmware datastore"></a>cinder对接vmware datastore</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp /usr/lib/python2.7/site-packages/oslo_vmware/wsdl/6.0   /usr/lib/python2.7/site-packages/oslo_vmware/wsdl/6.5</span><br></pre></td></tr></table></figure>
<p>2、修改 enabled_backends &#x3D; ceph为 enabled_backends &#x3D; ceph,vmware</p>
<p>3、创建type-key，其中ceph的也需要创建（web界面也可以做，在全局管理里——云硬盘类型）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cinder type-create vmware</span><br><span class="line">cinder type-key vmware set volume_backend_name=vmware</span><br><span class="line">cinder type-create ceph</span><br><span class="line">cinder type-key ceph set volume_backend_name=ceph</span><br><span class="line">````</span><br><span class="line">4、给vmware type-key添加扩展属性，vmware:storage_profile（可以在web页面里添加），但前提条件需要在vmware那边先创建好“虚机存储规则策略”  </span><br><span class="line"></span><br><span class="line">5、登录vsphere web client，先创建一个标签，在主页里，名字和类别随便起。</span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/openstack&amp;vmware_10)</span><br><span class="line"></span><br><span class="line">6、给要使用datastore分配新建标签  </span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/openstack&amp;vmware_11)</span><br><span class="line"></span><br><span class="line">7、回到主页，点击策略和配置文件，点击虚拟机存储策略，点击创建一个，按照向导做。名字很重要！！</span><br><span class="line">![](https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/openstack&amp;vmware_12)</span><br><span class="line"></span><br><span class="line">8、对到云平台</span><br></pre></td></tr></table></figure>
<p>cinder type-key vmware set vmware:storage_profile&#x3D;cinder-storage-policy</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">即可。=号后是之前创建的策略名字！！</span><br><span class="line">另外，M版对接6.5，创建云硬盘，不会出现volume开头的虚机，存储里也看不到volume-id文件夹，只有挂载后才会出现，注意！！</span><br><span class="line"></span><br><span class="line">参考链接</span><br></pre></td></tr></table></figure>
<p><a href="https://docs.openstack.org/mitaka/config-reference/compute/hypervisor-vmware.html">https://docs.openstack.org/mitaka/config-reference/compute/hypervisor-vmware.html</a><br><a href="http://blog.csdn.net/jmilk/article/details/52102020">http://blog.csdn.net/jmilk/article/details/52102020</a><br><a href="http://blog.csdn.net/halcyonbaby/article/details/37818789">http://blog.csdn.net/halcyonbaby/article/details/37818789</a><br><a href="http://www.cnblogs.com/zhoumingang/p/5514556.html">http://www.cnblogs.com/zhoumingang/p/5514556.html</a><br><a href="http://www.iyunv.com/forum.php?mod=viewthread&tid=86702">http://www.iyunv.com/forum.php?mod=viewthread&amp;tid=86702</a></p>
<pre><code>
</code></pre>
]]></content>
      <categories>
        <category>openstack</category>
      </categories>
  </entry>
  <entry>
    <title>win10启用ubuntu子shell</title>
    <url>/2016/12/22/win10_shell/</url>
    <content><![CDATA[<p>以前想用linux的shell都得装第三方软件实现，比如git bash等，在win10周年更新版14393直接集成了bash on ubuntu了集成新的 Windows Subsystem for Linux 子系统，这样能直接在 Bash on Ubuntu 环境里编译运行 Linux 程序非常爽。</p>
<p>开启win10开发者模式<br>win10设置</p>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/windows-1.png"></p>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/windows-2.png"></p>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/windows-3.png"></p>
<p>打开控制面板，启用或关闭windows功能</p>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/windows-4.png">  </p>
<p>点确定，然后重启<br>然后打开powershell或cmd<br>直接在里面输入bash命令  </p>
<p>它会询问你是否安装canonical 分发的ubuntu，输入y，然后等待。这里推荐使用vpn，然后安装完后，设置用户名，输入root用root权限吧。<br>安装完后，然后你打开powershell和cmd输入bash就直接进入ubuntu子系统了<br><img src="http://ohx02qrb8.bkt.clouddn.com/windows-5.png">  </p>
<p><img src="http://ohx02qrb8.bkt.clouddn.com/windows-6.png">  </p>
<p>可以看出子bash还是非常给力的是ubuntu 14.04 3.4的内核。以后可以直接在上面跑python脚本了。不需要专门虚拟机。</p>
<p>默认字体蓝色不好看清楚，修改powershell或cmd的背景色即可。</p>
]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title>vxlan</title>
    <url>/2017/02/24/vxlan/</url>
    <content><![CDATA[<h3 id="什么是vxlan？"><a href="#什么是vxlan？" class="headerlink" title="什么是vxlan？"></a>什么是vxlan？</h3><p>VXLAN（Virtual Extensible LAN）虚拟可扩展局域网是目前NVO3(Network Virtualization Over Layer 3 基于三层IP  overlay网络构建虚拟网络技术统称NVO3)，它是目前NVO3中影响力最为广泛的一种，它通过L2 over L4 (MAC in UDP)的报文封装方式，实现基于IP overlay的虚拟局域网。</p>
<h3 id="传统网络存在的一些问题"><a href="#传统网络存在的一些问题" class="headerlink" title="传统网络存在的一些问题"></a>传统网络存在的一些问题</h3><p>1，传统的二层网络，交换机通过mac地址进行数据转发，mac地址一多会造成交换机的转发速率变慢，同时交换机的mac地址表的大小是有限制的，在云计算中，mac表容量限制了虚拟机的数量。<br>2，VLAN的vlan id是一个12bit，最大只支持4096个vlan，这在云计算中是远远不够的。</p>
<h3 id="vxlan的提出很好的解决了上述问题"><a href="#vxlan的提出很好的解决了上述问题" class="headerlink" title="vxlan的提出很好的解决了上述问题"></a>vxlan的提出很好的解决了上述问题</h3><p>1，vxlan采用MAC in UDP的方式将vm主机的数据报文封装在UDP中，并使用物理网络VTEP的IP和MAC地址在外层包头封装进行数据传输，对外表现为VTEP之间的mac地址，极大的降低了交换机mac地址使用。  </p>
<p>2，VXLAN报文中拥有一个24bit vni段，vxlan隔离不同租户就是通过vni来进行，一个vni表示一个租户，不同vni之间二层不能直接通信，但可以通过vxlan三层网关进行通信。即使多个终端用户属于同一个vni也属于同一个租户。  </p>
<h3 id="VXLAN的优点"><a href="#VXLAN的优点" class="headerlink" title="VXLAN的优点"></a>VXLAN的优点</h3><p>1，基于ip的overlay网络，仅需要边界VTEP设备间可通信<br>2，ip overlay TTL避免环路。<br>3，overlay+vni构建虚拟网络支持多达16M的虚拟网络，充分满足多租户的需求<br>4，接入交换机只需要学习物理服务器的mac地址，不需要学习每台VM的mac地址减轻了交换机的负担</p>
<h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>NVE(Network virtrualization Edge网络虚拟边缘节点）是实现网络虚拟化功能的实体。<br>VTEP(vxlan tunnel end point vxlan 隧道端点）vxlan网络中的NVE以VTEP为标识；每个NVE至少得有一个VTEP；VTEP使用NVE的IP地址表示；两个VTEP之间可以确立一条VXLAN隧道，VTEP间的这条VXLAN隧道将两个NVE之间的所有的VNI所公用。<br>VTEP可以由专有硬件来实现，也可以使用纯软件实现，硬件的实现是通过一些SDN交换机，软件的实现主要有  </p>
<p>1，带vxlan内核模块的Linux<br>2，openvswitch  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/vxlan_1.png">  </p>
<p>vxlan报文格式<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/vxlan_2.png">  </p>
<p>端口默认使用4789端口<br>报文中源IP为报文的虚拟机所属的VTEP的IP地址，目的IP为目的虚拟机所属的VTEP的IP，目标ip地址可以为单播地址也可以为组播地址  </p>
<p>vxlan网络架构图<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/vxlan_3.png">  </p>
<p>vxlan使用 MAC IN UDP的方式来延伸二层网络，是一种扩展大二层网络的隧道封装技术。  </p>
<p>NVE负责将vm的报文封装，建立隧道网络。如服务器上的openvswitch就是一个NVE。   </p>
<p>VXLAN报文的转发<br>一种是BUM(broadcast&amp;unknown-unicast&amp;multicast)报文转发，一种是已知的单播报文转发  </p>
<p>BUM报文转发：采用头端复制的方式(vm上层VTEP接口收到BUM报文后本地VTEP通过控制平面获取同一VNI的VTEP列表，将收到的BUM报文根据VTEP列表进行复制并发送给属于同一VNI的所有VTEP)并进行报文封装。  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/vxlan_4.png"></p>
<p>1，switch_1收到来终端A发出的报文后，提取报文信息，并判断报文的目的MAC地址是否为BUM MAC.<br>     是，在对应的二层广播域内广播，并跳转2。<br>    不是，走已知单播报文转发流程。<br>2，switch_1根据报文中的VNI信息获取同一VNI的VTEP列表获取对应的二层广播域，进行vxlan封装，基于出端口和vxlan封装信息封装vxlan头和外层IP信息，进行二层广播。<br>3，switch_2和switch_3上VTEP收到VXLAN报文后，根据UDP目的端口号、源和目的IP地址VNI判断VXLAN报文的合法有效性，依据VNI获取对应的二层广播域，然后进行VXLAN解封装，获取内层二层报文，判断报文的目的MAC是否为BUM MAC<br>     是，在对应的二层广播域内非VXLAN侧进行广播处理。<br>     不是，再判断是否是本机的MAC地址。<br>           是，本机MAC，上送主机处理。<br>           不是，在对应的二层广播域内查找出接口和封装信息并跳转4.<br>4，switch_2和switch_3根据查找到的出接口和封装信息，为报文添加VLAN tag，转发给对应的终端B&#x2F;C.   </p>
<p>已知单播报文转发<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/vxlan_5.png"></p>
<p>1，switch_1收到来自终端A的报文，根据报文信息接入端口和VLAN信息获取对应的二层广播域，并判断报文的目的MAC是否为已知的单播MAC<br>       是，在判断是否为本机MAC.<br>               是，上送主机处理。<br>              不是，在对应的二层广播域内查找出接口和封装信息，并跳装到2.  </p>
<p>2， switch_1上VTEP根据查找到的出接口和封装信息进行VXLAN封装和报文转发。<br>3，switch_2上VTEP收到VXLAN报文后，根据UDP目的端口号，源&#x2F;目的IP地址，VNI判断VXLAN报文的合法有效性，依据VNI获取对应的二层广播域，然后进行VXLAN解封装、获取内层二层报文，判断报文的目的MAC是否为已知单播报文MAC。<br>        是，在对应的二层广播域内查找出接口和封装信息，并跳转到4.<br>        不是，在判断是否是本机的mac。<br>                   是，上送主机处理。<br>                  不是，走BUM报文转发流程。<br>4，switch_2根据查找的出接口和封装信息，为报文添加VLAN tag，转发给对应的终端B。  </p>
<p>为了防止不同vm之间通信发送arp广播引发广播风暴，vtep有一个arp proxy的功能也就是说在请求这个VTEP节点上的虚拟机的mac地址都是由VTEP的mac地址应答。也就是l2population。  </p>
<h3 id="vxlan网关"><a href="#vxlan网关" class="headerlink" title="vxlan网关"></a>vxlan网关</h3><p>vxlan下vm之间的通信方式有3种，同VNI下不同VM，不同VNI下跨网访问，vxlan和非vxlan之间访问。  </p>
<p>vxlan网关分为<br>二层网关、三层网关。<br>二层网关：主要用于解决同VNI下不同VM之间通信，一般为vetp IP<br>三层网关：用于解决不同VNI下跨网访问，和vxlan和非vxlan之间访问。  </p>
<p>通俗理解就是<br>在一台实体服务器上可以虚拟出一个交换机来，这个交换机就是VSwitch，而这个VSwitch下挂的不再是实体服务器，而是一个个VM，一个VM其实就是一个租户租用的服务器，不同租户之间肯定是不能互访的，要不然租户数据的安全性如何保障，这个隔离就是靠的VNI这个ID，其实这个你可以向下VLAN是如何隔离的，目的就是为了隔离租户。我一个租户有2个VM的话，那么我这2个之间应该可以互访吧。所以说基于VNI定义的租户，而非基于VM。内部的结构说清楚了再来说上行如何访问，在一个L2交换机你要跨网访问必然要经过网关，这个网关的IP地址就是VTEP IP，在网络上有个概念叫arp-proxy，一般用途是为了保护内部私有网络，外界的所有应答都有网关来代替回答（可以理解为门卫）。在这里外界只需要你的VTEP IP即可，对端报文到达VTEP这个网关后自己在内部走L2进行转发。因此VXLAN报文中的目的IP就是对端的网关（VTEP IP），而源地址自然也是自己的网关（VTEP IP）。而对于不同leaf上的同一VNI的VM来说，他们的VTEP IP肯定要配置相同，想下同一vlan下的服务器的网关是如何配置的就明白了  </p>
<p><a href="http://www.cnblogs.com/hbgzy/p/5279269.html">http://www.cnblogs.com/hbgzy/p/5279269.html</a><br><a href="http://blog.csdn.net/zztflyer/article/details/51883523">http://blog.csdn.net/zztflyer/article/details/51883523</a></p>
<h3 id="vxlan实验"><a href="#vxlan实验" class="headerlink" title="vxlan实验"></a>vxlan实验</h3><p>环境<br>操作系统：centos7.2<br>内核版本：3.10<br>openvswitch版本：2.5  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/vxlan_6.png"></p>
<p>在两台虚拟机上安装好ovs，并启动<br>这个实验目的，就是HOST 1的 br1，没有根物理网卡绑定，但可以通过建利vxlan隧道与HOST2的br1通信。  </p>
<p>配置host1<br>创建两个网桥br0、br1<br>ovs-vsctl add-br br0<br>ovs-vsctl add-br br1<br>将eth0挂载到br0上<br>ovs-vsctl add-port br0 eth0</p>
<p>将eth0的ip分配到br0上<br>ifconfig eth0 0 &amp;&amp; ifconfig br0 192.168.1.2&#x2F;24  </p>
<p>给br1分配一个ip<br>ifconfig br1 192.168.2.2&#x2F;24  </p>
<p>配置host2<br>创建两个网桥br0、br1<br>ovs-vsctl add-br br0<br>ovs-vsctl add-br br1<br>将eth0挂载到br0上  </p>
<p>ovs-vsctl add-port br0 eth0</p>
<p>将eth0的ip分配到br0上<br>ifconfig eth0 0 &amp;&amp; ifconfig br0 192.168.1.3&#x2F;24</p>
<p>给br1分配一个ip<br>ifconfig br1 192.168.2.3&#x2F;24  </p>
<p>此时在hots1上ping host2上的br0的ip是可以通的<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/vxlan_7.png"></p>
<p>ping br1的ip是pint不通的<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/vxlan_8.png"></p>
<p>通过建立vxlan隧道来实现br1之间通信<br>在host1上执行<br>给br1上增加一个vxlan0接口类型为vxlan，远程ip为 host2上的br0 ip， vni为100<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/vxlan_8.1.png"><br>在host2上执行<br>远端ip为host1上br0的ip<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/vxlan_8.2.png"></p>
<p>此时在host1上ping host2的br1的ip  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/vxlan_9.png"><br>就可以ping通了  </p>
<p>在host2上的 eth0上抓包<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/vxlan_10.png">  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/vxlan_11.png">  </p>
<p>有两层报文封装第一层源ip为192.168.2.2 目标ip为192.168.2.3，然后被udp封装，走vxlan隧道，第二层源ip为192.168.1.2 目标ip为192.168.1.3</p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/vxlan_12.png"><br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/vxlan_13.png"></p>
<p>网络包经过 vxlan interface 到达 eth1 的过程中，Linux vxlan 内核模块会将网络包二层帧封装成 UDP 包，因此，vxlan interface 必须设置适当的 MTU 来限制通过它的网络包的大小（vxlan interface 的 MTU 需要比它所绑定的物理网卡的 MTU 小 50），否则，封装后的包会被 eth1 丢弃。</p>
<p>VTEP (vxlan 随道端点）vswitch生成br-tun连接各个节点<br><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/vxlan_14.png">  </p>
<p>像这些  </p>
<p>为什么vlan只支持4096个因为vid只有12bit 2的12次方&#x3D;4096  </p>
<p>为什么用vxlan不用vlan？<br>因为vlan最大只有4096个<br>虚拟化技术的话用vlan运行vm一多，交换机上的mac地址会很多，影响交换机性能。<br>stp算法会产生大量多路路径冗余  </p>
<p>vxlan：建立在物理网络上的虚拟以太网<br>vlxan是一种将二层报文用三层协议进行封装的技术，它进行传输的标识是通过VNI vni包含24bit所以vxlan最大支持2的24次方约16m个，会将二层数据包封装成udp报文通过隧道组播传输，一般配置的组播地址224.0.0.1发送arp，udp端口是4789，共50字节封装报头<br>     UDP校验和：一般为0，非0则此包将会被丢弃。  </p>
<p>数据包都是通过vtep进行封装传输<br>在 OVS 中, 有几个非常重要的概念：  </p>
<p>Bridge: Bridge 代表一个以太网交换机（Switch），一个主机中可以创建一个或者多个 Bridge 设备。  </p>
<p>Port: 端口与物理交换机的端口概念类似，每个 Port 都隶属于一个 Bridge。  </p>
<p>Interface: 连接到 Port 的网络接口设备。在通常情况下，Port 和 Interface 是一对一的关系, 只有在配置 Port 为 bond 模式后，Port 和 Interface 是一对多的关系。  </p>
<p>Controller: OpenFlow 控制器。OVS 可以同时接受一个或者多个 OpenFlow 控制器的管理。  </p>
<p>datapath: 在 OVS 中，datapath 负责执行数据交换，也就是把从接收端口收到的数据包在流表中进行匹配，并执行匹配到的动作。  </p>
<p>Flow table: 每个 datapath 都和一个“flow table”关联，当 datapath 接收到数据之后， OVS 会在<br>flow table 中查找可以匹配的 flow，执行对应的操作, 例如转发数据到另外的端口。  </p>
<p>veth-pair：一对接口，一个接口收另外一个接口同时也能收到，用于连接两个Brigge  </p>
<p>设置网络接口设备的类型为“internal”。对于 internal 类型的的网络接口，OVS 会同时在 Linux 系统中创建一个可以用来收发数据的模拟网络设备。我们可以为这个网络设备配置 IP 地址、进行数据监听等等。  </p>
<p><img src="https://wanshaoyuan.oss-cn-hangzhou.aliyuncs.com/image/vxlan_15.png"></p>
<p>参考链接<br><a href="http://www.360doc.com/content/16/0227/12/3038654_537760576.shtml">http://www.360doc.com/content/16/0227/12/3038654_537760576.shtml</a><br><a href="http://blog.csdn.net/xjtuse2014/article/details/51376123?locationNum=7">http://blog.csdn.net/xjtuse2014/article/details/51376123?locationNum=7</a><br><a href="http://www.cnblogs.com/hbgzy/p/5279269.html">http://www.cnblogs.com/hbgzy/p/5279269.html</a><br><a href="http://blog.csdn.net/zztflyer/article/details/51883523">http://blog.csdn.net/zztflyer/article/details/51883523</a></p>
]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
      </tags>
  </entry>
</search>
