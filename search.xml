<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[从零开始构建云平台监控(一)搭建LNMP]]></title>
      <url>%2F2017%2F10%2F03%2Fmonitor_2%2F</url>
      <content type="text"><![CDATA[搭建LNMP先安装mysql—-&gt;安装php—–&gt;安装nginx版本:mysql 5.7.19php 7.1.7nginx 1.12.1 本次搭建除mysql外全部采用源码方式安装，因为采用源码的方式安装可以更好的对功能模块是否需要做定制。 安装依赖1yum -y install gcc gcc-devel gcc-c++ gcc-c++-devel libaio-devel boost boost-devel autoconf* automake* zlib* libxml* ncurses-devel ncurses libgcrypt* libtool* cmake openssl openssl-devel bison bison-devel unzip numactl-devel 创建mysql软件源vim /etc/yum.repo.d/mysql.repo1234[mysql]name = mysqlbaseurl = https://mirrors.ustc.edu.cn/mysql-repo/yum/mysql-5.7-community/el/7/x86_64/gpgcheck=0 1yum install mysql-community-libs mysql-community-common mysql-community-client mysql-community-devel mysql-community-server mysql初始化mysqld –initialize –user=mysql 查找默认密码grep ‘temporary password’ /var/log/mysqld.log 默认密码策略设置密码需要大写、小写、特殊符号。关闭默认密码策略 编辑/etc/my.cnfvalidate_password=off 重启mysqlsystemctl restart mysqld 用初始密码登录mysql重启密码1alter user root@localhost identified by &apos;passw0rd&apos;; 修改默认编码由下图可见database和server的字符集使用了latin1编码方式，不支持中文，即存储中文时会出现乱码。以下是命令行修改为utf-8编码的过程，以支持中文 修改为utf-8打开，编辑/etc/my.cnf.d/server.cnf在mysqld里面添加character_set_server = utf8 重启mysqlsystemctl restart mysqld 再次查看 安装php7php7在性能上比php5.x有2倍多的提升，同时兼容性也特别好。同时php5.x很多openssl和openssh的漏洞 安装依赖包1yum -y install libmcrypt-devel mcrypt mhash gd-devel ncurses-devel libxml2-devel bzip2-devel libcurl-devel curl-devel libjpeg-devel libpng-devel freetype-devel net-snmp-devel openssl-devel 安装libconv创建目录mkdir /lnmp进入目录cd /lnmpwget http://ftp.gnu.org/pub/gnu/libiconv/libiconv-1.14.tar.gz解压包12345tar xvf libiconv-1.14.tar.gzcd libiconv-1.14./configure --prefix=/usr/local/libiconv1.14 make时包此错 解决方法 123456vi libiconv-1.14/srclib/stdio.in.h将698行的代码：_GL_WARN_ON_USE (gets, &quot;gets is a security hole - use fgets instead&quot;);替换为：#if defined(__GLIBC__) &amp;&amp; !defined(__UCLIBC__) &amp;&amp; !__GLIBC_PREREQ(2, 16) _GL_WARN_ON_USE (gets, &quot;gets is a security hole - use fgets instead&quot;);#endif 重新make &amp;&amp; make install保存动态链接库12echo &quot;/usr/local/lib64/&quot; &gt; /etc/ld.so.conf.d/lnmp.confecho &quot;/usr/local/lib/&quot; &gt;&gt; /etc/ld.so.conf.d/lnmp.conf 刷新ldconfig 下载php7.1.7源码包cd /lnmpwget http://cn2.php.net/distributions/php-7.1.7.tar.gz 解压tar -xvf php-7.1.7.tar.gz 配置和检查依赖php7.1.71./configure --prefix=/usr/local/php7.1.7 --with-config-file-path=/usr/local/php7.1.7/etc --enable-mysqlnd --with-mysqli=mysqlnd --with-pdo-mysql=mysqlnd --with-iconv-dir=/usr/local/libiconv1.14 --with-pcre-regex --with-zlib --with-bz2 --enable-calendar --with-curl --enable-dba --with-libxml-dir --enable-ftp --with-gd --with-jpeg-dir --with-png-dir --with-zlib-dir --with-freetype-dir --enable-gd-native-ttf --with-mhash --enable-mbstring --with-mcrypt --enable-pcntl --enable-xml --disable-rpath --enable-shmop --enable-sockets --enable-zip --enable-bcmath --with-snmp --disable-ipv6 --with-gettext --enable-fpm --with-fpm-user=www --with-fpm-group=www --with-openssl make &amp;&amp; make install复制php配置文件1cp /lnmp/php-7.1.7/php.ini-production /usr/local/php7.1.7/etc/php.ini 配置php修改时区1sed -i &apos;s#;date.timezone =#date.timezone = Asia/Shanghai#g&apos; /usr/local/php7.1.7/etc/php.ini 隐藏php版本号1sed -i &apos;s#expose_php = On#expose_php = Off#g&apos; /usr/local/php7.1.7/etc/php.ini 配置启动脚本12[root@localhost php-7.1.7]# cp /lnmp/php-7.1.7/sapi/fpm/init.d.php-fpm /etc/init.d/php-fpm[root@localhost php-7.1.7]# chmod +x /etc/init.d/php-fpm 配置fast-cgi1cp /usr/local/php7.1.7/etc/php-fpm.d/www.conf.default /usr/local/php7.1.7/etc/php-fpm.d/www.conf 修改fast-cgivim /usr/local/php7.1.7/etc/php-fpm.d/www.conf修改已下参数rlimit_files = 65535 创建www用户12groupadd www useradd -u www -s /sbin/nologin 启动php-fpm /etc/init.d/php-fpm start设置开机自启/sbin/chkconfig –add php-fpm修改php.ini以满足zabbix要求vim /usr/local/php7.1.7/etc/php.ini限制执行目录,nginx网页放在/var/www/html下open_basedir = “/var/www/html/:/tmp”max_execution_time = 300max_input_time = 300post_max_size = 24Mupload_max_filesize = 4M修改php连接mysqlpdo_mysql.default_socket= /var/lib/mysql/mysql.sockmysqli.default_socket = /var/lib/mysql/mysql.sock 安装nginxwget http://nginx.org/download/nginx-1.12.1.tar.gz解压 [root@localhost lnmp]# tar -xvf nginx-1.12.1.tar.gz [root@localhost lnmp]# cd nginx-1.12.11[root@localhost lnmp]#./configure --prefix=/usr/local/nginx1.12.1 --user=www --group=www --with-http_stub_status_module --with-http_gzip_static_module --with-http_ssl_module [root@localhost lnmp]#make &amp;&amp; make install 配置nginx1cp /usr/local/nginx1.12.1/conf/nginx.conf /usr/local/nginx1.12.1/conf/nginx.conf.bak vim /usr/local/nginx1.12.1/conf/nginx.conf123456789101112131415161718192021222324252627282930313233343536373839404142434445user www www;worker_processes 4;error_log logs/error.log info;pid logs/nginx.pid;events &#123; worker_connections 65535; use epoll;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; tcp_nopush on; keepalive_timeout 65; gzip on; log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; access_log logs/access.log main; server &#123; listen 80; server_name localhost; charset utf8; location / &#123; root /var/www/html; index index.php index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /var/www/html; &#125; location ~ \.php$ &#123; root /var/www/html; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125; &#125;&#125; 修改ulimitulimit -n 65535编辑/etc/systemd/system.conf设置DefaultLimitNOFILE=65535 重启系统 启动nginx/usr/local/nginx1.12.1/sbin/nginx重载nginx/usr/local/nginx1.12.1/sbin/nginx -s reload关闭nginx/usr/local/nginx1.12.1/sbin/nginx -s stop重启nginx/usr/local/nginx1.12.1/sbin/nginx -s reopen 设置nginx启动脚本编辑/etc/init.d/nginx注意PATH和NAME变量123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#!/bin/bash# chkconfig: - 85 15PATH=/usr/local/nginx1.12.1DESC=&quot;nginx daemon&quot;NAME=nginxDAEMON=$PATH/sbin/$NAMECONFIGFILE=$PATH/conf/$NAME.confPIDFILE=$PATH/logs/$NAME.pidSCRIPTNAME=/etc/init.d/$NAMEset -e[ -x &quot;$DAEMON&quot; ] || exit 0do_start() &#123;$DAEMON -c $CONFIGFILE || echo -n &quot;nginx already running&quot;&#125;do_stop() &#123;$DAEMON -s stop || echo -n &quot;nginx not running&quot;&#125;do_reload() &#123;$DAEMON -s reload || echo -n &quot;nginx can&apos;t reload&quot;&#125;case &quot;$1&quot; instart)echo -n &quot;Starting $DESC: $NAME&quot;do_startecho &quot;.&quot;;;stop)echo -n &quot;Stopping $DESC: $NAME&quot;do_stopecho &quot;.&quot;;;reload|graceful)echo -n &quot;Reloading $DESC configuration...&quot;do_reloadecho &quot;.&quot;;;restart)echo -n &quot;Restarting $DESC: $NAME&quot;do_stopdo_startecho &quot;.&quot;;;*)echo &quot;Usage: $SCRIPTNAME &#123;start|stop|reload|restart&#125;&quot; &gt;&amp;2exit 3;;esacexit 0 添加执行权限chmod a+x /etc/init.d/nginx注册成服务/sbin/chkconfig –add nginx 添加开机自启/sbin/chkconfig nginx on 查看 重启测试启动nginxsystemctl start nginx重启systemctl restart nginx关闭systemctl stop nginx 测试php在/var/www/html下编写index.php文件 123&lt;?php phpinfo();?&gt; 重启nginx和php-fpm 打开浏览器输入地址 至此LNMP搭建完毕。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[从零开始构建云平台监控(写在前面)]]></title>
      <url>%2F2017%2F10%2F03%2Fmonitor%2F</url>
      <content type="text"><![CDATA[监控的重要性：在整个it系统中，监控是最为重要的一个环节，监控是发现集群问题的关键，没有监控的话，只能等到初了问题才能发现集群的问题，那时为时以晚。 监控的范围在一套完整的openstack私有云中，组件非常复杂因为它不仅仅有openstack的东西还有分布式存储ceph的东西。计算组件：nova-api、nova-scheduler、nova-condutor、nova-novncproxy、nova-compute……网络组件：neutron-server、neutron-dhcp-agent、neutron-metadata-agent、neutron-openvswitch-agent、openvswitch……存储组件：cinder-api、cinder-scheduler 、cinder-volume 其他：等还有认证组件keystone、消息队列、镜像组件、还有底层分布式存储ceph、一个这么庞大的系统可想而知如果没有一个比较完善的监控很多并且在不同的角色上所监控的组件也是不一样的，比如控制节点没有nova-compute服务，计算节点没有控制节点服务，所以监控项还得针对不同的角色做区分。 除了组件上的监控外，其实少不了还有硬件上的监控，比如说，在私有云生产环境中，网卡都是会做bond的，做了bond的好处是当其中一个网卡出现故障时并并不是影响到整个集群，但你也得通过监控发现这个挂了的网卡，然后准备更换，ceph的osd down了，有可能是osd进程被kill掉了，也有可能是硬盘坏了导致osd进程挂了，这也都通过监控去发现。 组件构成 监控节点：nginx、php、mysql、influxdb、grafana、zabbix-server、zabbix-agent控制节点：telegraf、zabbix-agent融合节点：zabbix-agent 组件作用nginx+php+mysql提供基础的lnmp平台让zabbix可以运行，mysql存储zabbix监控数据zabbix-server：用于接收各个机器agent端发过来的数据zabbix-agent：采集数据发给zabbix-servertelegraf：用于采集ceph的监控数据influxdb：存储telegraf的数据grafana：用于对采集数据图形化展示 其中需要注意的是因为教程所以监控节点组件都放一起了，并且还是单节点，在生产环境中监控节点要做高可用，根据监控的机器数决定是否将数据库能独立出来。 以下为最终效果。 效果展示ceph监控界面 openstack节点性能状态 控制节点服务状态展示 计算节点服务状态展示 存储节点osd监控 zabbix-server监控]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python socket模块]]></title>
      <url>%2F2017%2F10%2F01%2Fpython_socket%2F</url>
      <content type="text"><![CDATA[socket俗称套接字，应用程序的进程和进程的之间的沟通是通过套接字来进行的。在python中，socket模块来创建套节字。 在同一台机器上，不同进程之间，通过进程号进行区分，但在不同的机器上，会存在相同的进程号，比如cenos7的PID为1的进程为systemd，在另外一个机器的centos7也是同样的，所以在网络环境下PID并不能唯一标识一个进程，比如主机A也有PID为1的进程，主机B也有PID为1的进程，但此问题，tcp/ip协议族已经帮我们解决了，网络层的ip地址可以唯一的确定一台主机，传输层的协议和端口可以唯一确定这台主机上的进程。这样利用三元组(ip+协议+端口）可以与其进程进行交互。 python socket编程思路tcp服务端12345678910111213141516创建socket [socket.socket(socket.AF_INET,socket.SOCK_STREAM)] | |将ip和端口与socket绑定 [socket.bind((host,port))] | |监听端口 [socket.listen()] | |接收连接，建立连接请求[socket.accept()] | |接收数据，并发送数据[socket.recv(),socket.send()] | |连接结束，关闭连接。[socket.close()] tcp客户端 12345678910创建socket | |建立连接[socket.socket(socket.AF_INET,socket.SOCK_STREAM)] | |接收数据，同时也发送数据。[socket.recv(),socket.send()] | |关闭连接[socket.close()] socket功能列表socket(family,type[,protocal]) 使用给定的地址族、套接字类型、协议编号（默认为0）来创建套接字。 socket类型 描述 socket.AF_UNIX 只能够用于单一的Unix系统进程间通信 socket.AF_INET 服务器之间网络通信 socket.AF_INET6 IPv6 socket.SOCK_STREAM 流式socket , for TCP socket.SOCK_DGRAM 数据报式socket , for UDP socket.SOCK_RAW 原始套接字，普通的套接字无法处理ICMP、IGMP等网络报文，而SOCK_RAW可以；其次，SOCK_RAW也可以处理特殊的IPv4报文；此外，利用原始套接字，可以通过IP_HDRINCL套接字选项由用户构造IP头 socket.SOCK_SEQPACKET 可靠的连续数据包服务 创建TCP Socket： s=socket.socket(socket.AF_INET,socket.SOCK_STREAM) 创建UDP Socket： s=socket.socket(socket.AF_INET,socket.SOCK_DGRAM) socket函数服务端函数 socket函数 描述 s.bind() 将套接字绑定到地址, 在AF_INET下,以元组（host,port）的形式表示地址 s.listen(backlog) 开始监听TCP传入连接。backlog指定在拒绝连接之前，操作系统可以挂起的最大连接数量。该值至少为1，大部分应用程序设为5就可以了。 s.accept() 接受TCP连接并返回（conn,address）,中conn是新的套接字对象，可以用来接收和发送数据。address是连接客户端的地址。 accept默认是阻塞，当有connect过来时才会打开 客户端socket函数 socket函数 描述 s.connect(address) 连接到address处的套接字。一般address的格式为元组（hostname,port），如果连接出错，返回socket.error错误。 s.connect_ex(adddress) 功能与connect(address)相同，但是成功返回0，失败返回errno的值。 公共socket函数 socket函数 描述 s.recv(bufsize[,flag]) 接受TCP套接字的数据。数据以字符串形式返回，bufsize指定要接收的最大数据量。flag提供有关消息的其他信息，通常可以忽略。 s.send(string[,flag]) 发送TCP数据。将string中的数据发送到连接的套接字。返回值是要发送的字节数量，该数量可能小于string的字节大小。 s.sendall(string[,flag]) 完整发送TCP数据。将string中的数据发送到连接的套接字，但在返回之前会尝试发送所有数据。成 功返回None，失败则抛出异常。 s.recvfrom(bufsize[.flag]) 接受UDP套接字的数据。与recv()类似，但返回值是（data,address）。其中data是包含接收数据的字符串，address是发送数据的套接字地址。 s.sendto(string[,flag],address) 发送UDP数据。将数据发送到套接字，address是形式为（ipaddr，port）的元组，指定远程地址。返回值是发送的字节数。 s.close() 关闭套接字。 s.getpeername() 返回连接套接字的远程地址。返回值通常是元组（ipaddr,port）。 s.getsockname() 返回套接字自己的地址。通常是一个元组(ipaddr,port) s.setsockopt(level,optname,value) 设置给定套接字选项的值。 s.getsockopt(level,optname[.buflen]) 返回套接字选项的值。 s.settimeout(timeout) 设置套接字操作的超时期，timeout是一个浮点数，单位是秒值为None表示没有超时期。一般，超时期应该在刚创建套接字时设置，因为它们可能用于连接的操作（如connect()） s.fileno() 返回套接字的文件描述符。 s.setblocking(flag) 如果flag为0，则将套接字设为非阻塞模式，否则将套接字设为阻塞模式（默认值）。非阻塞模式下，如果调用recv()没有发现任何数据，或send()调用无法立即发送数据，那么将引起socket.error异常。 s.makefile() 创建一个与该套接字相关连的文件 简单例子server端 1、使用一个死循环while True：将会使server端一直处于监听状态。2、children,addr=s.accept 将创建一个新的socket，这样原先的socket继续侦听，而新的socket将接收client端的数据,addr返回的是客户端的ip。那么问题来了，当客户端发送数据关联时是与哪个socket进行连接呢，首先我们需要知道的是客户端发送的数据有两种，一种是请求建立连接的，一种是已经建立好连接后的数据传输，就如上所说tcp/ip有接收缓存和发送缓存，当收到建立连接的请求时，则传给正在监听端口的socket调用accept，当收到连接好连接后的数据传输时，将输据放入接收缓冲区，这样当服务器需要读取数据时调用accept建立的新socket，的recv函数从接收缓冲区读取。3、将socket.accept写在循环里面client新的连接一次将重新创建一个新的socket。 client端 1，s.recv(1024)一次最大接收1024字节2, 当收到连接好连接后的数据传输时，将输据放入接收缓冲区，这样当服务器需要读取数据时调用accept建立的新socket，的recv函数从接收缓冲区读取。输出server端 client端socket.recv()tcp/ip socket在内核中都有一个接收缓冲区和发送缓冲区，当socket接收到数据时，并不是马上调用socket.recv(),而是将数据拷贝到socket中的接收缓冲区中，调用socket.recv()后就是将接收缓冲区的数据，移动到应用层的buff中，并返回。当接收窗口满了后发生的操作是，收端通知发端，停止发送。socket.send()socket.send()是将应用层的buff拷贝到tcp-socket的发送缓冲区中 UDPserver端 client 输出server端 client端 socket实现tcp简单聊天server端 需要注意的是这里的socket.accept()是写在while循怀外的，因为写在死循怀里，脚本一直在执行，client调用一次accept后就进入阻塞状态了，而client端用的还是旧的connect所以，如果写在循怀里，就是执行client后建立一个连接后只能进行一次对话，因为server端的accept又重新回到阻塞状态了,重新执行client生成一个新的connec又可以一次对话。 client端 结果server端 client端 优化点：1，目前程序没有多线程，IO复用，还是半双工状态，一次只能一个用户说话，一个发时，另一个只能收，server在说话，client就不能说。 参考链接123456http://www.oschina.net/question/12_76126?sort=default&amp;p=1https://segmentfault.com/q/1010000000591930http://www.cnblogs.com/aylin/p/5572104.htmlhttp://blog.csdn.net/rebelqsp/article/details/22109925http://blog.csdn.net/hguisu/article/details/7445768/http://blog.csdn.net/rebelqsp/article/details/22191409]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python logging模块]]></title>
      <url>%2F2017%2F09%2F27%2Fpython_logging%2F</url>
      <content type="text"><![CDATA[默认logging函数输出的warning级别的日志。 日志级别日志级别大小关系为：CRITICAL &gt; ERROR &gt; WARNING &gt; INFO &gt; DEBUG &gt; NOTSET logging库提供了多个组件：Logger、Handler、Filter、Formatter：1234Logger 对象提供应用程序可直接使用的接口，供应用代码使用；Handler 发送日志到适当的目的地，比如socket和文件等Filter 提供了过滤日志信息的方法，控制输出；Formatter 指定日志输出和显示的具体格式。 通过logging.basicConfig对日志的输出格式配置 cat test.log 需要注意的是只有等级大于等于basicConfig定义的level的log才会被输出，比如这里定义的等级为DEBUG、debug、info、warning、error日志等级都大于等于debug logging.basicConfig各参数level：日志等级format格式123456789101112131415161718192021222324252627282930313233343536373839format: 指定输出的格式和内容，format可以输出很多有用信息，如上例所示: %(levelno)s: 打印日志级别的数值 %(levelname)s: 打印日志级别名称 %(pathname)s: 打印当前执行程序的路径，其实就是sys.argv[0] %(filename)s: 打印当前执行程序名 %(funcName)s: 打印日志的当前函数 %(lineno)d: 打印日志的当前行号 %(asctime)s: 打印日志的时间 %(thread)d: 打印线程ID %(threadName)s: 打印线程名称 %(process)d: 打印进程ID %(message)s: 打印日志信息 filename：输出文件名filemode：写入模式w为直接写入，a为追加datafmt：输出的时间格式 这里用%Y-python中时间日期格式化符号：%y 两位数的年份表示（00-99）%Y 四位数的年份表示（000-9999）%m 月份（01-12）%d 月内中的一天（0-31）%H 24小时制小时数（0-23）%I 12小时制小时数（01-12）%M 分钟数（00=59）%S 秒（00-59）%a 本地简化星期名称%A 本地完整星期名称%b 本地简化的月份名称%B 本地完整的月份名称%c 本地相应的日期表示和时间表示%j 年内的一天（001-366）%p 本地A.M.或P.M.的等价符%U 一年中的星期数（00-53）星期天为星期的开始%w 星期（0-6），星期天为星期的开始%W 一年中的星期数（00-53）星期一为星期的开始%x 本地相应的日期表示%X 本地相应的时间表示%Z 当前时区的名称 将日志同时输出到屏幕和文件 输出 logging 日志轮询使用TimedRotatingFileHandler设置日志轮转，轮转的方式有两种一种是基于时间的轮转，一种是基于日志文件大小的轮转TimedRotatingFileHandler函数参数说明1logging.handlers.TimedRotatingFileHandler(file_name,when=时间单位, interval=&apos;时间间隔&apos;,backupCount=&quot;保留的文件个数&quot;) interval:表示等待多长时间文件重建，重建的文件名等于file_name+suffix 以下面例子说明 myadd.addHandler(filehandler)的意思是给myapp这个logger添加filehandler这个handler。 执行脚本可以看见每隔一秒会自动生成一个新的日志文件，到满3个时会自动进行一次新的轮转 TimedRotatingFileHandler设置的重建时间间隔后，suffix就需要按装下面的进行配置不然删除不了，比如设置的为S则suffix为%Y-%m-%d_%H-%M-%S RotatingFileHandler安文件大小切分 logger实例的父子关系通过前面几个例子对比你应该发现了前面我用logging.basicConfig()去设置format，后面我是通过getlogger创建一个logger后，通过setformat方法去给他对应的handler设置format。 root logger是处于最顶层的logger，同时也是默认的logger，如果不创建logger实例默认调用logger.info(),logger.debug(),logger.error()使用 如何得到root logger通过logging.getLogger()和logging.getLogger(“”)得到root logger实例。logger的父子关系logger以name命名方式来表达父子关系比如父logging.getLogger(foo)子logging.getLogger(foo.tar) effective level一个looger如果没有指定level，就继承父level，如果父logger也没有就直接继承root的level。handler同样，子没有就继承父的，父也没有的话就继承root的例 root logger这里没设置logger的setLevel默认是warning，但父logger设置了，所以父logger会将自己的logger setlevel传递给root logger 调用配置好的logging正常写程序中只要配置好一个logging，其他程序只要调用他就可以了一种是通过logging.config，一种是通过模块导入 介绍方法二：比如我将配置好的logging写在test.py里面在另外一个程序中调用它12import testtest.myapp.info(&quot;test&quot;) 这样输出的就是按test.py里面myapp这个logger配置好的log了 12345http://blog.csdn.net/lizhe_dashuju/article/details/72579705http://blog.csdn.net/z_johnny/article/details/50812878http://kenby.iteye.com/blog/1162698http://blog.csdn.net/chosen0ne/article/details/7319306]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[kdump]]></title>
      <url>%2F2017%2F09%2F21%2Fkdump%2F</url>
      <content type="text"><![CDATA[什么是kdump?kdump 是一种的基于 kexec 的内核崩溃转储机制，类似飞机的黑匣子，系统一但崩溃，内核无法正常记录信息了，这时kdump将转入带第二个捕获内核，将第二个内核加载的内存中，对第一个内核的信息进行捕获。由于 kdump 利用 kexec 启动捕获内核，绕过了 BIOS，所以第一个内核的内存得以保留。这是内核崩溃转储的本质。kexec是一个快速启动机制，可以通过已经运行的内核，启动另外一个内核不需要经过bios kdump原理？kdump 需要两个不同目的的内核，生产内核和捕获内核。生产内核是捕获内核服务的对像。捕获内核会在生产内核崩溃时启动起来，与相应的 ramdisk 一起组建一个微环境，用以对生产内核下的内存进行收集和转存。 kdump配置和使用操作系统:centos7.2 安装配置分析工具crashyum install crash 安装kernel-debuginfo(需要根内核版本一一对应)1wget http://debuginfo.centos.org/7/x86_64/kernel-debuginfo-common-x86_64-3.10.0-327.el7.x86_64.rpm 12wget http://debuginfo.centos.org/7/x86_64/kernel-debuginfo-3.10.0-327.el7.x86_64.rpm` 下载完成后安装1rpm -ivh kernel-debuginfo-common-x86_64-3.10.0-327.el7.x86_64.rpm kernel-debuginfo-3.10.0-327.el7.x86_64.rpm 安装完后确认是否有1/usr/lib/debug/lib/modules/3.10.0-327.el7.x86_64/vmlinux此文件 配置kdump可以配置内核崩溃后崩溃日志存到何地 默认是放在本地/var/crash下图为配置scp，表示将log文件放到192.168.2.100/log 目录，同时key文件目录需要指定 测试kdump配置捕获占用的内存使用grubby修改grub2.cfg文件1grubby --update-kernel=DEFAULT --args=crashkernel=128M 重启服务器reboot 启动kdump1systemctl start kdump &amp;&amp; systemctl enable kdump ls /boot 会发现生成了一个kdump结尾的文件 执行以下命令让内核crash12echo 1 &gt; /proc/sys/kernel/sysrqecho c &gt; /proc/sysrq-trigger 此时系统会立刻失去连接进入捕获内核 开机后 以下参考IBM文档链接见文尾 crash解析崩溃日志vmcore-dmesg.txt可以查看错误信息(1) 错误类型首先可以在vmcore-dmesg.txt中先查看错误类型，如：divide error: 0000 [#1] SMP，除数为0造成内核崩溃，由1号CPU触发。 BUG: unable to handle kernel NULL pointer dereference at 000000000000012c，引用空指针。这样一来就能知道引发内核崩溃的错误类型。 (2) 错误地点RIP为造成内核崩溃的指令，Call Trace为函数调用栈，通过RIP和Call Trace可以确定函数的调用路径，以及在哪个函数中的哪条指令引发了错误。 例如RIP为：[&lt;ffffffff812cdb54&gt;] ? tcp_enter_loss+0x1d3/0x23b [&lt;ffffffff812cdb54&gt;]是指令在内存中的虚拟地址。tcp_enter_loss是函数名(symbol)。0x1d3是这条指令相对于tcp_enter_loss入口的偏移，0x23b是函数编译成机器码后的长度。这样一来就能确定在哪个函数中引发了错误，以及错误的大概位置。 Call Trace为函数的调用栈，是从下往上看的。可以用来分析函数的调用关系。 vmcore信息1crash /usr/lib/debug/lib/modules/3.10.0-327.el7.x86_64/vmlinux /var/crash/127.0.0.1-2017-09-20-13\:09\:30/vmcore 12345678910111213KERNEL: 系统崩溃时运行的 kernel 文件DUMPFILE: 内核转储文件CPUS: 所在机器的 CPU 数量DATE: 系统崩溃的时间TASKS: 系统崩溃时内存中的任务数NODENAME: 崩溃的系统主机名RELEASE: 和 VERSION: 内核版本号MACHINE: CPU 架构MEMORY: 崩溃主机的物理内存PID:3507表示当崩溃时3507这个bash的进程在操作PANIC: 崩溃类型，常见的崩溃类型包括：SysRq (System Request)：通常是测试使用。通过 echo c &gt; /proc/sysrq-trigger，就可以触发系统崩溃。oops：可以看成是内核级的 Segmentation Fault。应用程序如果进行了非法内存访问或执行了非法指令，会得到 Segfault 信号，一般行为是 coredump，应用程序也可以自己截获 Segfault 信号，自行处理。如果内核自己犯了这样的错误，则会弹出 oops 信息。 bt(backtrace)显示内核堆栈信息 如上输出中，以“# 数字”开头的行为调用堆栈，即系统崩溃前内核依次调用的一系列函数，通过这个可以迅速推断内核在何处崩溃。 log - dump system message bufferlog 命令可以打印系统消息缓冲区，从而可能找到系统崩溃的线索。log 命令的截图如下（为节省篇幅，已将部分行省略）： ps 命令用于显示进程的状态，（如图）带 &gt; 标识代表是活跃的进程。ps 命令的截图如下（省略部分行）： dis - disassembling instructiondis 命令用于对给定地址的内容进行反汇编。dis 命令的截图如下： 1234参考链接http://www.361way.com/centos-kdump/3751.htmlhttp://blog.csdn.net/zhangskd/article/details/38084337https://www.ibm.com/developerworks/cn/linux/l-cn-kdump4/index.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[OpenStack对接vmware]]></title>
      <url>%2F2017%2F09%2F08%2Fvmware%26openstack%2F</url>
      <content type="text"><![CDATA[openstack版本：libertyvsphere版本：6.0 拓扑图 架构图 vmware对接openstack有两种驱动1，VCDriver，以nova driver的方式通过vcenter来控制计算和网络2，VMDKDriver，以cinder driver的方式通过vcenter来控制datastore。 需要注意的是本文档的对接都是对接vmware的标准交换机进行对接的，如果需要对接vmware的dvs的话需要安装另外的包并配置neutron。 这里采用vcdriver的方式这种方式目前有几个问题1，就是已经存在vmware集群的vm是不能被openstack管理了，只有新创建的机器可以被openstack管理。 2，vlan模式下，VCDriver要求vCenter里被使用的VLAN port group的名字必须和OpenStack里integration bridge（默认是br-int）的名字一样，而且vlan 号也得一样，这里我配置port group为0，表示不做限制。 3，首次创建虚机会花费很长时间，因为需要将镜像从glance通过vcenter 拷贝到cluster后端的datasotre，后面再创建速度就会很快。 要点：一台计算节点对接vsphere，需要这台计算节点能根vcenter通信。 Vsphere上的配置配置好对应的openstack计算节点对应的cluster如这个vsphere的cluster_name为UAT_Lenovo_6.0 cluster对应的storage名字 创建对应的port_group nova配置需要注意对应的物理适配器，待会需要写到配置项内。12345678910openstack平台对接vcenter配置参考compute_driver #对应的驱动host_ip #连接vsphere的ip地址host_username #管理员帐户host_password #管理员密码datastore_rege #cluster对应的后端存储cluster_name #对应的cluster 名字vlan_interface #port group对应的物理网卡vmware.integration_bridge #对应的port_groupinsecure=True #关闭ssl连接不然报下面错 修改/etc/nova/nova.conf配置文件：123456789101112[default]compute_driver = vmwareapi.VMwareVCDriver[vmware]host_ip=10.3.1.11host_username=administrator@UAT.comhost_password=1qaz@WSXdatastore_regex=Huawei_S5500_01cluster_name=UAT_Lenovo_6.0api_retry_count=10vlan_interface=vmnic0vmware.integration_bridge=br-intinsecure=True 需要注意这些参数仔细核对。 启动openstack-nova-compute服务：systemctl restart openstack-nova-compute 此时应该可以看见vmware 对应的hypervisor 转换镜像将一个qcow2的镜像转换成vmware用的vmdk格式 glance配置转换镜像格式为vmdk：qemu-img convert -f raw -O vmdk centos_6.5.raw centos_6.5.vmdk 上传镜像：1glance image-create --name=centos-6.5.vmdk --disk-format=vmdk --property hypervisor_type=&quot;vmware&quot; --property vmware_adaptertype=&quot;ide&quot; --property vmware_disktype=&quot;sparse&quot; --is-public=True --property vmware_ostype=&quot;otherLinuxGuest&quot; --container-format=bare &lt; centos_6.5.vmdk 创建虚拟机进行测试 第一次创建时间比较久因为要将镜像从glance 拷贝到vmware 的storage内 可以看见vsphere内已经有以uuid命名的vm了，表示创建成功 后面为可选配置，配置cinder对接vmware存储 cinder配置编辑/etc/cinder/cinder.conf配置文件，修改如下参数：12345volume_driver=cinder.volume.drivers.vmware.vmdk.VMwareVcVmdkDrivervolume_backend_name=vmwarevmware_host_ip=10.68.35.24vmware_host_username=administratorvmware_host_password=admin123 重启cinder-volume服务：/etc/init.d/openstack-cinder-volume restart创建多类型存储：cinder type-create vmwarecinder type-key vmware set volume_backend_name=vmwarecinder type-create cephcinder type-key ceph set volume_backend_name=DEFAULT 创建云硬盘，挂在云主机进行测试 mitaka版本对接vsphere6.5mitaka对接vsphere有个bug就是在https的连接情况下会连接失败https://review.openstack.org/#/c/341816/替换/usr/lib/python2.7/site-packages/nova/virt/vmwareapi/vmops.py 重启nova-computesystemctl restart openstack-nova-compute编辑nova.conf1234567891011[vmware]host_port=443host_ip=vcenter.xjrccu.comhost_username=administrator@xjrccu.comhost_password=Rayking,135datastore_regex=Datastore2cluster_name=LenovoServer-Clusterapi_retry_count=10vlan_interface=vmnic4vmware.integration_bridge=br-intinsecure=True 重启nova-computesystemctl restart openstack-nova输入nova hypervisor-list可以看见vmware这个hypervisor cinder对接vmware datastore1cp /usr/lib/python2.7/site-packages/oslo_vmware/wsdl/6.0 /usr/lib/python2.7/site-packages/oslo_vmware/wsdl/6.5 2、修改 enabled_backends = ceph为 enabled_backends = ceph,vmware 3、创建type-key，其中ceph的也需要创建（web界面也可以做，在全局管理里——云硬盘类型）12345cinder type-create vmwarecinder type-key vmware set volume_backend_name=vmwarecinder type-create cephcinder type-key ceph set volume_backend_name=ceph` 4、给vmware type-key添加扩展属性，vmware:storage_profile（可以在web页面里添加），但前提条件需要在vmware那边先创建好“虚机存储规则策略” 5、登录vsphere web client，先创建一个标签，在主页里，名字和类别随便起。 6、给要使用datastore分配新建标签 7、回到主页，点击策略和配置文件，点击虚拟机存储策略，点击创建一个，按照向导做。名字很重要！！ 8、对到云平台1cinder type-key vmware set vmware:storage_profile=cinder-storage-policy 即可。=号后是之前创建的策略名字！！另外，M版对接6.5，创建云硬盘，不会出现volume开头的虚机，存储里也看不到volume-id文件夹，只有挂载后才会出现，注意！！ 参考链接12345https://docs.openstack.org/mitaka/config-reference/compute/hypervisor-vmware.htmlhttp://blog.csdn.net/jmilk/article/details/52102020http://blog.csdn.net/halcyonbaby/article/details/37818789http://www.cnblogs.com/zhoumingang/p/5514556.htmlhttp://www.iyunv.com/forum.php?mod=viewthread&amp;tid=86702]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ceph crushmap]]></title>
      <url>%2F2017%2F08%2F05%2Fcrush%2F</url>
      <content type="text"><![CDATA[前言通过crushmap划分性能型池和容量型池。在实际生产环境中，考虑到成本因素，很少土豪公司会将osd全部用ssd，但私有云上有部分业务需要高性能存储，部分业务只需要普通的sas盘做容量性存储，在公有云中也经常有，不同性能的存储卖不同的价格。在ceph中的解决方法就是通过修改crushmap，创建多种host，将osd加入到host中，在创建多个pool，每个pool对应不同的rule 本例中两个存储节点的前两个osd为ssd，后面两个osd为sas。需要划分ssd pool和sas pool，其中云主机和性能型存储用sas pool， 性能型存储为ssd pool。环境两台存储节点node-4、node-5，每个存储节点4个osd，将每个存储节点的前两个osd是ssd盘，后两个osd是sas盘 层级结构:中host bucket高于osd bucket，root bucket高于host bucket，划分步骤为：1、创建对应的host bucket 如node-4-sas、node-4-ssd、node-5-sas、node-5-ssd。2、将对应的osd划到对应的host中。3、创建root，如ssd、sas，将对应的host加到对应的root中。4、创建rules将root加入到对应的rule中。5、pool调用ruleset。 权重：修改crushmap时需要特别注意osd的权重问题,1TB OSD为1.00，500G为0.50，3TB位3.00 rules：pool所使用的规则，在crushmap中有一个对应的id，pool直接使用这个id表示这个pool的pg按这个规则进行分布。修改方法有两个 先修改ceph.conf禁止osd启动时自动修改crushmapecho ‘osd_crush_update_on_start = false’ &gt;&gt; ceph.conf 第一直接直接使用ceph命令创建bucket，move bucket，在修改rules。第二通过将crushmap导出，修改crushmap的方式。 方法1(直接通过ceph命令)：12345678910111213141516171819202122231、创建对应的root ceph osd crush add-bucket ssd root ceph osd crush add-bucket sas root2、创建对应的host ceph osd crush add-bucket node-4-sata host ceph osd crush add-bucket node-5-sata host ceph osd crush add-bucket node-4-ssd host ceph osd crush add-bucket node-5-ssd host3、移动host到对应的root下 ceph osd crush move node-4-sas root=sas ceph osd crush move node-5-sas root=sas ceph osd crush move node-4-ssd root=ssd ceph osd crush move node-5-ssd root=ssd4、将osd移到host下 ceph osd crush move osd.3 0.88 host=node-4-sas ceph osd crush move osd.4 0.88 host=node-4-sas ceph osd crush move osd.6 0.88 host=node-5-sas ceph osd crush move osd.7 0.88 host=node-5-sas ceph osd crush move osd.1 0.97 host=node-4-ssd ceph osd crush move osd.2 0.88 host=node-4-ssd ceph osd crush move osd.0 0.97 host=node-5-ssd ceph osd crush move osd.5 0.88 host=node-5-ssd 导出crushceph osd getcrushmap -o crushmap.txt反编译crushtool -d crushmap.txt -o crushmap-decompile 打开反编译后的文件修改rule（修改ruleset、和step take) 123456789101112131415161718rule ssd &#123; ruleset 1 type replicated min_size 1 max_size 10 step take ssd step chooseleaf firstn 0 type host step emit&#125;rule sas &#123; ruleset 0 type replicated min_size 1 max_size 10 step take ssd step chooseleaf firstn 0 type host step emit&#125; 重新编译crushtool -c crushmap-decompile -o crushmap-compiled 应用到集群ceph osd setcrushmap -i crushmap-compiled 创建一个新的poolceph osd pool create ssd 1024 设置ssd pool使用rules 1ceph osd pool set ssd crush_ruleset 1 校验object的pg的散落方法参考方法2 方法2(直接修改crushmap)提取现集群中使用的crushmap保存到一个文件ceph osd getcrushmap -o crushmap.txt默认导出来的crushmap打开是乱码的，需要进行反编译才能修改crushtool -d crushmap.txt -o crushmap-decompile重新编译这个crushmapcrushtool -c crushmap-decompile -o crushmap-compiled 将新的CRUSH map 应用到ceph 集群中ceph osd setcrushmap -i crushmap-compiled修改crushmap，需要注意的是bucket ID不要重复了，还有osd的weigh，我这一个osd是90G所以为0.088 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768host node-5-sas &#123; id -2 # do not change unnecessarily # weight 0.176 alg straw hash 0 # rjenkins1 item osd.6 weight 0.088 item osd.7 weight 0.088&#125;host node-4-sas &#123; id -3 # do not change unnecessarily # weight 0.176 alg straw hash 0 # rjenkins1 item osd.3 weight 0.088 item osd.4 weight 0.088&#125;root sas &#123; id -1 # do not change unnecessarily # weight 0.720 alg straw hash 0 # rjenkins1 item node-5-sas weight 0.360 item node-4-sas weight 0.360&#125;host node-5-ssd &#123; id -6 # do not change unnecessarily # weight 0.185 alg straw hash 0 # rjenkins1 item osd.0 weight 0.097 item osd.5 weight 0.088&#125;host node-4-ssd &#123; id -5 # do not change unnecessarily # weight 0.185 alg straw hash 0 # rjenkins1 item osd.1 weight 0.097 item osd.2 weight 0.088&#125;root ssd &#123; id -4 # do not change unnecessarily # weight 0.720 alg straw hash 0 # rjenkins1 item node-5-ssd weight 0.360 item node-4-ssd weight 0.360&#125;# rulesrule ssd &#123; ruleset 1 type replicated min_size 1 max_size 10 step take ssd step chooseleaf firstn 0 type host step emit&#125;rule sas &#123; ruleset 0 type replicated min_size 1 max_size 10 step take sas step chooseleaf firstn 0 type host step emit&#125; 重新编译这个crushmapcrushtool -c crushmap-decompile -o crushmap-compiled将新的CRUSH map 应用到ceph 集群中ceph osd setcrushmap -i crushmap-compiled 创建一个新的poolceph osd pool create ssd 1024 设置ssd pool使用rules 1ceph osd pool set ssd crush_ruleset 1 检查下ceph osd pool get ssd crush_ruleset 测试在ssd中写入个数据是否都落到osd.0、osd.5、osd.1、osd.2rbd create ssd/testimg -s 10240 #在ssd pool中创建块 查看pg的散落情况因为我这是两副本所以只会落到两个osd上，分别落在osd.0和osd.1上。 cinder多后端修改cinder.conf1234567891011enabled_backends=sata,ssd[ssd]volume_backend_name=ssdvolume_driver=cinder.volume.drivers.rbd.RBDDriverrbd_pool=ssdrbd_user=volumesrbd_ceph_conf=/etc/ceph/ceph.confsrbd_secret_uuid=a5d0dd94-57c4-ae55-ffe0-7e3732a24455rbd_max_clone_depth=5 secret_uuid就是对接ceph时导入的secret 创建typecinder type-create ssdcinder type-key ssd set volume_backend_name=ssd 重启cinder服务systemctl restart openstack-cinder-apisystemctl restart openstack-cinder-schedulersystemctl restart openstack-cinder-volume 通过crushmap隔离故障域，让pg分布在不同机柜上主机上但这样故域还是host，pg的分布还是比较散乱的，但集群规模大时，如果按照默认的host为故障域的话副本pg很有可能分布同一机架相邻的host的osd上，这样如果你一但此机架断电很有可能导致集群出现ERROR 但我们可以通过修改crushmap 让副本pg分布到不同机架的服务器上去，来达到隔离故障域的目的。 rack 底层的分支 下面方案如下有4个rack，每个rack有一个host ,通过修改crushmap将pg分布到不同列的rack的host上，比如可以指定compute pool的第一个副本放rackB01的node4上，第二个副本放rackC01的node-5上， 123456789node-5上，rackB01 node-4rackC01 node-5rackB02 node-6rackC02 node-7 这样做的好处就是将副本放在不同列的不同机柜上，来提高可靠性12345rack方案node-4 ---B01柜node-5 ---B02柜node-6 ---C01柜node-7 ---C02柜 将compute pool的副本都放到B、C 01柜里面讲test_pool的副本都放到B、C 02柜里面 修改后 方法一(直接通过ceph命令)：123456789101112131415161718192021222324251、添加racks: ceph osd crush add-bucket rackB01 rack ceph osd crush add-bucket rackB02 rack ceph osd crush add-bucket rackC01 rack ceph osd crush add-bucket rackC02 rack3、把每一个host移动到相应的rack下面： ceph osd crush move node-4 rack=rackB01 ceph osd crush move node-5 rack=rackB02 ceph osd crush move node-6 rack=rackC01 ceph osd crush move node-7 rack=rackC024、添加root ceph osd crush add-bucket rackB_C01 root ceph osd crush add-bucket rackB_C02 root4、把所有rack移动到对应 root 下面： ceph osd crush move rackB01 root=rackB_C01 ceph osd crush move rackB02 root=rackB_C02 ceph osd crush move rackC01 root=rackB_C01 ceph osd crush move rackC02 root=rackB_C02 导出crushmap 添加rulesceph osd getcrushmap -o crushmap.txt 反编译crushtool -d crushmap.txt -o crushmap-decompile 修改 添加如下(注意ruleset、和step take) 123456789101112131415161718rule rackB_C01 &#123; ruleset 0 type replicated min_size 1 max_size 10 step take rackB_C01 step chooseleaf firstn 0 type rack step emit&#125;rule rackB_C02 &#123; ruleset 1 type replicated min_size 1 max_size 10 step take rackB_C02 step chooseleaf firstn 0 type rack step emit&#125; 编译crushtool -c crushmap-decompile -o crushmap-compiled 将新的CRUSH map 应用到ceph 集群中ceph osd setcrushmap -i crushmap-compiled 设置test_pool套用rule rackB_C02 ceph osd pool set test_pool crush_ruleset 1 查看是否应用成功 测试在test_pool里面创建个对象应该分到B02和C02柜的host上面的osd上,rbd create test_pool/testimg -s 1024在test_pool里面创建个对象应该分到B01和C01柜的host上面的osd上,rbd create compute/test_2.img -s 1024 查看 这样副本pg就会分布在不同机柜的不同host上的osd。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack秒级创建云主级秒级创建快照原理]]></title>
      <url>%2F2017%2F08%2F05%2Fceph_clone%2F</url>
      <content type="text"><![CDATA[概念cow （copy on write）：写时复制，也就是做快照时，先圈好个位置但这里面是空的，只有当父镜像有数据有变化时，这时会先将变化前的数据cp到快照的空间，然后继续修改父镜象，这样做的好处是可以节省大量做快照的时间和减少存储空间，因为snapshot存储的都是发生改变前的区域，其它区域都是与父镜像共享的。 ceph快照是基于cow（copy on write）ceph 使用 COW （copy on write）方式实现 snapshot：在写入object 之前，将其拷贝出来，作为 snapshot 的 data object，然后继续修改 拷贝出来的object 中的数据。rbd在创建快照时，并不会向pool中创建对象，也就是说并不会占用实际的存储空间,只是增加了一些对象信息 Ceph 使用 COW 方式实现 snapshot：在写入object 之前，将其拷贝出来，作为 snapshot 的 data object，然后继续修改 object 中的数据。 ceph快照ceph快照是基于cow（copy on write）ceph 使用 COW （copy on write）方式实现 snapshot：在写入object 之前，将其拷贝出来，作为 snapshot 的 data object，然后继续修改 拷贝出来的object 中的数据。rbd在创建快照时，并不会向pool中创建对象，也就是说并不会占用实际的存储空间,只是增加了一些对象信息 Ceph 使用 COW 方式实现 snapshot：在写入object 之前，将其拷贝出来，作为 snapshot 的 data object，然后继续修改 object 中的数据。 ceph cloneclone：clone是将一个snapshot变成一个image，它是基于snapshot 创建 Clone 是将 image 的某一个 Snapshot 的状态复制变成一个 image。如 imageA 有一个 Snapshot-1，clone 是根据 ImageA 的 Snapshot-1 克隆得到 imageB。imageB 此时的状态与Snapshot-1完全一致，并且拥有 image 的相应能力，其区别在于 ImageB 此时可写。 从用户角度来看，一个 clone 和别的 RBD image 完全一样。你可以对它做 snapshot、读/写、改变大小 等等，总之从用户角度来说没什么限制。同时，创建速度很快，这是因为 Ceph 只允许从 snapshot 创建 clone，而 snapshot 需要是只读（protect）的。 向 clone 的 instance 的object 写数据ceph的克隆也是采用cow技术， 从本质上是 clone 的 RBD image 中读数据，对于不是它自己的 data objects，ceph 会从它的 parent snapshot 上读，如果它也没有，继续找它的parent image，直到一个 data object 存在。从这个过程也看得出来，该过程是缺乏效率的。 向 clone 的 instance object 写数据 Ceph 会首先检查该 clone image 上的 data object 是否存在。如果不存在，则从 parent snapshot 或者 image 上拷贝该 data object，然后执行数据写入操作。这时候，clone 就有自己的 data object 了。 flatten克隆操作本质上复制了一个 metadata object，而 data objects 是不存在的。因此在每次读操作时会先向本卷可能的 data object 访问。在返回对象不存在错误后会向父卷访问对应的对象最终决定这块数据是否存在。因此当存在多个层级的克隆链后，读操作需要更多的损耗去读上级卷的 data objects。只有当本卷的 data object 存在后(也就是写操作后)，才不需要访问上级卷。 为了防止父子层数过多，Ceph 提供了 flattern 函数将 clone 与 parent snapshot 共享的 data objects 复制到 clone，并删除父子关系。 flatten就是将 与父镜像共享的镜像都copy到clone中一层层递归，然后clone 与原来的父 snapshot 之间也不再有关系了，真正成为一个独立的 image，断绝父子关系，然后将那个snapshot删除但flatten是极其消耗网络IO的，非常耗时间 flatten有什么好处呢1，flatten后与父镜像和snapshot已经脱离关系了，可以任意删除。不然不能删除。2，clone的instance 当访问某个不存在data object，需要向上级一级查找，这是非常影响效率，而flatten后改instance这些data object都从上级copy过来，速率会更快。 使用ceph做后端存储，创建虚机如果没有使用ceph做后端存储，openstack创建虚机的流程是，先探测本地是否已经有镜像存在了，如果没有，则需要从glance仓库拷贝到对应的计算节点启动，如果有则直接启动，因此网络IO开销是非常大的如果使用Qcow2镜像格式，创建快照时需要commit当前镜像与base镜像合并并且上传到Glance中，这个过程也通常需要花费数分钟的时间。 当使用ceph做后端存储时，由于ceph是分布式存储，虚拟机镜像和根磁盘都是ceph的rbd image，所以就不需要copy到对应的计算节点，直接从原来的镜像中clone一个新镜像RBD image clone使用了COW技术，即写时拷贝，克隆操作并不会立即复制所有的对象，而只有当需要写入对象时才从parent image中拷贝对象到当前image中。因此，创建虚拟机几乎能够在秒级完成。 注意Glance使用Ceph RBD做存储后端时，镜像必须为raw格式，否则启动虚拟机时需要先在计算节点下载镜像到本地，并转为为raw格式，这开销非常大。 步骤如下1，先基于镜像 做个snapshot、并添加protect、因为clone操作只能针对snapshot、 2， 创建虚拟机根磁盘创建虚拟机时，直接从glance镜像的快照中clone一个新的RBD image作为虚拟机根磁盘: rbd clone 1b364055-e323-4785-8e94-ebef1553a33b@snap fe4c108a-7ba0-4238-9953-15a7b389e43a_disk 3，启动虚拟机启动虚拟机时指定刚刚创建的根磁盘，由于libvirt支持直接读写rbd镜像，因此不需要任何下载、导出工作。 openstack给云主机做快照openstack给云主机做快照，因为还原是基于快照重新创建个云主机，所以本质上是做一个clone操作 具体流程1基于云主机创建个snapshot---&gt;给snapshot设置只读权限（protect）-----&gt;基于该snapshot clone一个image出来----&gt;flatten操作----&gt;删除snapshot 为了秒级快照，这导致的后果就是，1，做了快照的云主机在控制台删除后在ceph存储的pool中仍然还会存在，因为你的快照根主机的images还是存在父子关系，数据还是共享的，2，云主机 xxxx_disk 存在snapshot，因为做clone是基于protect的snapshot，没flatten的话，snapshot自然没删除。3，残留云主机和snapshot没清理的话会导致上传在glance的镜像也无法删除，因为你的云主机的xxx_disk是基于你glance的镜像clone的存在父子关系不能删除。。 解决办法，后台定期flatten然后rm那些客户已经删了云主机的残留数据。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[neutron-高可用(1)--DVR]]></title>
      <url>%2F2017%2F07%2F23%2Fdvr%2F</url>
      <content type="text"><![CDATA[环境 fuel8.0 介绍DVR简称分布式路由，M版前的neutron网络结构。 无论是南北流量中的1:N还是1:1还是东西流量也好都经过网络节点，这就带来一个问题了，网络节点负载很大，往往成为整个架构的性能短板，同时还很容易出现单点故障。在Juno版本后可以配置DVR模式，每个compute节点上都配置了一个L3 agent，集群内的流量（东西流量）这里有个前提条件，同网段内主机在不同compute节点，在同一个compute节点就直接通过本机就能直接转发了。直接就通过tunnel到达另外一个compute节点，不在需要通过网络节点去做转发了，而配置了floatting ip的主机，直接通过本compute节点的L3 agent 通过br-ex连接外网。 进行深入前的一些基本概念的讲解：路由策略引用百科里的一句话路由策略是一种基于目标网络进行路由更加灵活的数据包路由转发机制。应用了路由策略，路由器将通过路由图决定如何对需要路由的数据包进行处理，路由图决定了一个数据包的下一跳转发路由器。 路由策略的种类 大体上分为两种：一种是根据路由的目的地址来进行的策略称为：目的地址路由； 另一种是根据路由源地址来进行策略实施的称为：源地址路由！ 随着路由策略的发展现在有了第三种路由方式：智能均衡的策略方式！ 使用 ip rule 操作策略路由 基于策略的路由比传统路由在功能上更强大，使用更灵活，它使网络管理员不仅能够根据目的地址而且能够根据报文大小、应用或IP源地址等属性来选择转发路径。ip rule 查看策略路由表 数字越小，优先级越高 规则0，它是优先级别最高的规则，规则规定，所有的包，都必须首先使用local表（254）进行路由。本规则不能被更改和删除。 规则32766，规定所有的包，使用表main进行路由。本规则可以被更改和删除。 规则32767，规定所有的包，使用表default进行路由。本规则可以被更改和删除。 在默认情况下进行路由时，首先会根据规则0在本地路由表里寻找路由，如果目的地址是本网络，或是广播地址的话，在这里就可以找到合适的路由；如果路由失败，就会匹配下一个不空的规则，在这里只有32766规则，在这里将会在主路由表里寻找路由;如果失败，就会匹配32767规则，即寻找默认路由表。如果失败，路由将失败。重这里可以看出，策略性路由是往前兼容的。 3232243201: from 192.168.30.1/24 lookup 3232243201 #这条规则的含义是规则3232243201 源ip为192.168.30.1/24的包，使用local表3232243201 进行路由 linux二层网络中一些基本名词 TAP设备：如kvm创建虚拟机后，宿主机上的vnet0、vnet1等，是操作系统内核中的虚拟网络设备。veth配对设备（veth pair）或ovs配对设备，它其实是一对网卡把一块叫veth，另外一块叫peer，当一个数据真被发送到其中一端中，veth的另外一端也会收到此帧。在虚拟化中常用veth pair来连接两个bridge，比如qbr网桥跟br-int网桥上的qvb-xxx和qvo-xxx就是一对veth pair。 启用DVR以下配置的前提是开启了l2_populationl2population原理介绍如下http://blog.csdn.net/cloudman6/article/details/53167522 1，安装compute节点(1)安装l3-agent(2)修改的内核参数123456vi /etc/sysctl.confnet.ipv4.ip_forward = 1net.ipv4.conf.all.rp_filter = 0net.ipv4.conf.default.rp_filter = 0sysctl -p （3）增加一块能上外网的网卡，并创建个br-ex网桥把这块网卡桥接到br-ex上12ovs-vsctl add-br br-exovs-vsctl add-port br-ex ethx (4)配置l3agent.ini123456vim /etc/neutron/l3_agent.iniinterface_driver =neutron.agent.linux.interface.OVSInterfaceDriveruse_namespaces = Trueagent_mode = dvr` vim /etc/neutron/plugins/ml2/ml2_conf.ini12[agent]enable_distributed_routing=True 重新启动 neutron-openvswitch-agent, netron-l3-agent ,openvswitch-switch 服务。 网络节点12vim /etc/neutron/l3_agent.iniagent_mode = dvr_snat 123vim /etc/neutron/plugins/ml2/ml2_conf.ini[agent]enable_distributed_routing=True 1重启neutron-server、netron-l3-agent ,openvswitch-switch 服务。 配置成功后可以neutron agent-list 看见compute的l3-agent。 原理解释1台control节点（server-4），2台compute（server-5、server-6）、外网为192.168.122.0/24在control节点上在控制台创建个网络，创建个路由器，设置网关，在路由器上将网络端口添加进去。在基于此网络创建个主机。可以发现在control节点上创建路由器时，会同时在namespace里面创建qroute-xxx和snat-xxx两个命名空间，snat-xxx命名空间会根据创建的子网来添加接口，qg-xxx和sg-xxx，其中qg-xxx上面配置了外网ip，sg-xxx配置的是本子网可用的第一个ip（前两个为网关和dhcp） 当该网络内其他主机需要上外网时，源ip会已postrouting形式，snat成路由器的网关，这步操作会在snat-xx命名空间内做 在传统网络架构下，snat操作本来是应该在qroute下进行，而在DVR模式下qroute-xxx完全是连接内网的网关。 当创建一个实例时，会自动在实例所在的compute节点创建一个qroute-xxx 此qrouter-xxx的ip和mac都与control节点的一样。 当给实例分配一个floating_ip时，在自动在实例所在的计算节点namespace里面生成fip-xxx命名空间 需要注意的是fip-xxx这个id是与你的public网络id是一致 也就是说一个public网络对应一个fip-xxx namespace,在下面会解释为什么这样做节点fg-xxx对应的是外网接口，fpr接口与qrouter接口一对veth pair DVR模式下流量的走向南北流量（1：N）所谓南北流量（1：N）实际上就是指SNAT就是子网内的主机通过路由器的floatingIP去上外网1，首先需要确认router已经发到compute节点，我这router叫test2可以看见已经在server-6上有了 我在vm（192.168.50.4）上面ping百度 因为是跨网段的，所以发送给默认路由，就是网关。在网关上路由策略可以看见发现，发给默认路由192.168.50.1的数据，最终会发给192.168.50.3 那192.168.50.3在哪呢，通过前面分析，我们知道，192.168.50.3在control节点snat-xx命名空间里面 然后在iptables里面做一次snat 就出外网了，因为我这是KVM虚拟机里面所以还要nat一层。 ping public网络 192.168.122.1 然后抓包分析 所以SNAT流程12 （compute节点） vxlan （network节点）vm---&gt;qr-xxx---&gt;br-int----&gt;br-tun---&gt; br-tun-----&gt;br-int-----&gt;snat-xxx----br-ex 南北流量（1：1）给主机192.168.50.4绑定一个floating_ip 观察control节点和compute节点的namespace发现control节点没有发生变化，而vm所在的compute的节点的namespace发现多了一个fip命名空间 注意看这个fip的ip是我们创建的外网网络id，这个在上面已经说过了。查看一个这个fip-xxx的内容 vm绑定floating_ip后如何跟外网进行通信 1，vm ping外网，首先数据包丢给默认路由，qroute，2，查找路由表 命中table 16 走rfp端口 3，然后进行snat 4，然后发送给fip-xxx命名空间 因为已经进行snat了地址变成外网的，所以这里都default route 直接给外网网关，完成 如下图来源http://www.cnblogs.com/sammyliu/p/4713562.html vm绑定vip后外网DNAT进入内网主机1，arp-proxy外网ip要ping通内网，首先需要知道内网ip的mac，fip，并没有直接绑定在一个特定的端口是怎么知道mac的呢 进行路由追踪发现 arping发现mac地址为 尾数为C7:C3的 那这个mac地址为谁的 竟然是fip-xxx的fg端口 原来是fip-xxx在fg这个端口上做了arp-proxy，这样，fg既可以响应发给它自己的arp请求，也可以响应发给经过它路由的端的arp请求 可以看见内核参数里面已经开启了 2，将包丢给路由器 我们从fip-xxx ip route 可以看见包被丢给169.254.93.254这个接口了，这个接口是qroute-xxx的一个接口 3，qroute–xxx做DNAT 1234流程如下 arp欺骗 通过fpr DNATextra-----&gt;fip--xxx----&gt;qroute-----&gt; VM 为什么要独立出一个snat namespace来做SNAT，而不继续之前的qrouter多SNAT方案？ 因为 当创建子网的第一台主机时，会同时在compute节点上也创建个qrouter-xxx的namespace，但computer节点上的qrouter并不做snat功能，只做当有绑定float_ip时 连接FIPnamespace中转，所以需要将SNAT功能独立出来，于是就有了SNATnamespace当然这个也只会在网络节点的namespace里面创建。 当虚拟机绑定fip时，为什么compute要多出一个fip namespace，不直接在qrouter上的qg直接连接br-ex？是为了减少暴露在外部网络上的mac和ip地址，所以才需要有fip的出现。由于传统router只在网络节点上，数量很少，直接在传统router的qg上配floating ip和用其mac地址应答arp请求，所以没这个问题。但是dvr router由于分布到大量的compute node（每个私有网络都可能有一个router），如果再这么做，会导致有大量router的qg接口暴露在外部网络上，因此分离出fip，而fip是按照（compute node，external network）分配的，数量少很多。一个external_network就一个另外fip也可以做集中的arp proxy，不需要把floating ip真正绑到fip的接口上。同时也可以减少交换机MAC地址表的占用，减轻交换机的负担。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ironic-简介]]></title>
      <url>%2F2017%2F05%2F08%2Fironic%2F</url>
      <content type="text"><![CDATA[ironic简介介绍openstack中对物理机管理的组件，通过ironic可以给物理机，上电、下电、重启，自动安装操作系统，根据设定的规则模板，进行自动化配置。ironic是I版开始进入孵化，j版与nova进行集成、K版正式release,ironic原本是mirantis开发的，后面贡献到社区。部署虚机和部署物理机对底层nova调用来创建虚机，但对nova-compute的底层的hypevisor是不一样的调用虚机底层调用的驱动可能是libvirtd、vmwareapi.VMwareVCDriver等，而物理机底层驱动是ironic。 用途目前有些服务在虚机上运行达不到理想的性能，仅管openstack拥有trove、sahara、magnum等组件用于支持数据库平台、大数据平台、容器平台、但毕竟是在虚拟机里面部署服务，性能远远不够。但直接部署在物理机上又不好管理，此时ironic应运而生了。ironic可以解决物理机的添加、删除、电源管理、安装部署、未来可能能支持自动部署数据库、大数据平台、容器平台。 项目构成ironic： 包含ironic-api 和ironic-conductor进程python-ironicclinet: python clinet and CLIironic-python-agent: 一个运行在deployment ramdisk中的Python程序，用于执行一系列部署动作pyghmi: 一个python的IPMI库，可以代替IPMItoolironic-inspector: 硬件自检工具ironic-lib： ironic的通用库函数ironic-webclinet ：web consoleironic-ui：ironic的horizon插件bifrost：一套只运行Ironic的Ansible脚本 ironic组件ironic-api：负责接收请求，并且将请求传递给ironic-conductorironic-conductor：ironic中唯一一个能根数据库进行交互的组件，负责接收ironic-api的请求，然后根据请求执行相应的，创建、开机、关机、删除操作ironic-python-agent：部署裸机时，pxe启动进入的一个bootstrap镜像，此镜像是用社区diskimage-builder工具制做，镜像内安装了ironic-python-agent，进入系统后与ironic-conductor进行交互， 将本地磁盘以iscsi方式挂载到控制节点，控制节点将系统dd到裸机磁盘上 ironic用到的技术PXE+tftp+dhcp IPMI iscsi 裸机部署原理1，nova boot启动一个实例，nova-api将请求通过rabbitmq，传给nova-conductor，在传给nova-scheduler。2，nova-scheduler根据传递进来的flavor和image筛选出合适的计算节点，然后将请求传递到对应的计算节点。3，nova-compute与ironic通信，调用ironic-api。4，ironic-api将请求下发给ironic-conductor，ironic-ductor调用glance下载bootstrap镜像。在tftp-serer目录生成pxelinux配置文件，并将bootstrap镜像放到tftp目录。5，ironic-conductor通过调用ipmi driver将物理机开机，并设置以pxe的方式启动。6，物理服务器启动后，通过pxe加载了bootstrap镜像启动后，镜像内ironic-python-agent 启动，与ironic-ductor进行交互，将本地磁盘通过iscsi方式挂载到控制节点，控制节点通过dd的方式将用户选择的系统写入到磁盘中。7，写入完毕后ironic将调用ipmi driver对物理服务器进行重启操作。8，重新启动后主机，初次进入系统时，运行镜像内的cloud-init进行初始化，配置hostname和密码。这里需要注意，使用的镜像，需要提前在 /etc/sysconfig/network-scripts/里面将网卡的配置文件提交准备好不然开机，网卡会up不起来。centos7需要修改grub参数，将网卡名以ethx显示，不然以设备名显示，配置文件不好写。 流程图如下 需要注意的是镜像ironic有两种镜像一种是部署时的bootstrap镜像，这种镜像是内含了，ironic-python-agent，启动后会主动与ironic-conductor进行交互。 另外一种是用户需要安装到裸机的镜像，这种镜像是通过bootstrap，通过iscsi将磁盘挂载到控制节点dd写入的。 网络在Newton版前，ironic都是不支持多租户，都是在一个flat网络上，互相之间是没有隔离关系的。 https://www.ibm.com/developerworks/cn/cloud/library/cl-cn-virtualboxironic/index.htmlhttp://www.cnblogs.com/menkeyi/p/6063551.htmlhttps://docs.openstack.org/developer/ironic/liberty/deploy/user-guide.htmlhttps://wiki.openstack.org/wiki/Ironic]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[boot from volume的两种方式]]></title>
      <url>%2F2017%2F05%2F08%2Fboot_from_volume%2F</url>
      <content type="text"><![CDATA[boot for volume的两种方式简介： openstack对接商业存储一般直接用cinder直接对接商业存储，但想将nova创建的云主机放商业存储上，此时需要使用boot from volume 方式一直接选择镜像，dest选择云硬盘，需要注意的是云硬盘的大小必须大于等于镜像大小。 方式二创建空云硬盘，创建主机时将选择的镜像，加载到这个空硬盘。（需要注意的是，创建的空盘大小必续要大于你选择的镜像的大小）。 方式一1nova boot --flavor 1 --nic net-id=48baa60d-b785-45c1-8f90-8467f56abb5b --block-device source=image,id=d3a0512b-8cfc-4ad4-9fd4-7b38c9a44a32,dest=volume,size=10,shutdown=preserve,bootindex=0 test 流程：先是基于image创建block volume，然后从这个volume中boot instance 。shutdown选项选为preserve, 而不要选为remove， 这样在instance关闭时， volume会被save下来；其中的size选项要求大于等于flavor中的disk大小，同时要求我们的后端存储要有大于此size大小的空间。 方法二openstack volume create –image IMAGE_ID –size SIZE_IN_GB –name CINDER_NAME 获取云硬盘id 基于云硬盘创建云主机 1nova boot --flavor 1 --nic net-id=48baa60d-b785-45c1-8f90-8467f56abb5b --boot-volume f9af8677-28cf-4dfb-8dbc-b722025f9fa0 --security-group default --admin-pass test test]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[vxlan]]></title>
      <url>%2F2017%2F02%2F24%2Fvxlan%2F</url>
      <content type="text"><![CDATA[什么是vxlan？VXLAN（Virtual Extensible LAN）虚拟可扩展局域网是目前NVO3(Network Virtualization Over Layer 3 基于三层IP overlay网络构建虚拟网络技术统称NVO3)，它是目前NVO3中影响力最为广泛的一种，它通过L2 over L4 (MAC in UDP)的报文封装方式，实现基于IP overlay的虚拟局域网。 传统网络存在的一些问题1，传统的二层网络，交换机通过mac地址进行数据转发，mac地址一多会造成交换机的转发速率变慢，同时交换机的mac地址表的大小是有限制的，在云计算中，mac表容量限制了虚拟机的数量。2，VLAN的vlan id是一个12bit，最大只支持4096个vlan，这在云计算中是远远不够的。 vxlan的提出很好的解决了上述问题1，vxlan采用MAC in UDP的方式将vm主机的数据报文封装在UDP中，并使用物理网络VTEP的IP和MAC地址在外层包头封装进行数据传输，对外表现为VTEP之间的mac地址，极大的降低了交换机mac地址使用。 2，VXLAN报文中拥有一个24bit vni段，vxlan隔离不同租户就是通过vni来进行，一个vni表示一个租户，不同vni之间二层不能直接通信，但可以通过vxlan三层网关进行通信。即使多个终端用户属于同一个vni也属于同一个租户。 VXLAN的优点1，基于ip的overlay网络，仅需要边界VTEP设备间可通信2，ip overlay TTL避免环路。3，overlay+vni构建虚拟网络支持多达16M的虚拟网络，充分满足多租户的需求4，接入交换机只需要学习物理服务器的mac地址，不需要学习每台VM的mac地址减轻了交换机的负担 概念NVE(Network virtrualization Edge网络虚拟边缘节点）是实现网络虚拟化功能的实体。VTEP(vxlan tunnel end point vxlan 隧道端点）vxlan网络中的NVE以VTEP为标识；每个NVE至少得有一个VTEP；VTEP使用NVE的IP地址表示；两个VTEP之间可以确立一条VXLAN隧道，VTEP间的这条VXLAN隧道将两个NVE之间的所有的VNI所公用。VTEP可以由专有硬件来实现，也可以使用纯软件实现，硬件的实现是通过一些SDN交换机，软件的实现主要有 1，带vxlan内核模块的Linux2，openvswitch vxlan报文格式 端口默认使用4789端口报文中源IP为报文的虚拟机所属的VTEP的IP地址，目的IP为目的虚拟机所属的VTEP的IP，目标ip地址可以为单播地址也可以为组播地址 vxlan网络架构图 vxlan使用 MAC IN UDP的方式来延伸二层网络，是一种扩展大二层网络的隧道封装技术。 NVE负责将vm的报文封装，建立隧道网络。如服务器上的openvswitch就是一个NVE。 VXLAN报文的转发一种是BUM(broadcast&amp;unknown-unicast&amp;multicast)报文转发，一种是已知的单播报文转发 BUM报文转发：采用头端复制的方式(vm上层VTEP接口收到BUM报文后本地VTEP通过控制平面获取同一VNI的VTEP列表，将收到的BUM报文根据VTEP列表进行复制并发送给属于同一VNI的所有VTEP)并进行报文封装。 1，switch_1收到来终端A发出的报文后，提取报文信息，并判断报文的目的MAC地址是否为BUM MAC. 是，在对应的二层广播域内广播，并跳转2。 不是，走已知单播报文转发流程。2，switch_1根据报文中的VNI信息获取同一VNI的VTEP列表获取对应的二层广播域，进行vxlan封装，基于出端口和vxlan封装信息封装vxlan头和外层IP信息，进行二层广播。3，switch_2和switch_3上VTEP收到VXLAN报文后，根据UDP目的端口号、源和目的IP地址VNI判断VXLAN报文的合法有效性，依据VNI获取对应的二层广播域，然后进行VXLAN解封装，获取内层二层报文，判断报文的目的MAC是否为BUM MAC 是，在对应的二层广播域内非VXLAN侧进行广播处理。 不是，再判断是否是本机的MAC地址。 是，本机MAC，上送主机处理。 不是，在对应的二层广播域内查找出接口和封装信息并跳转4.4，switch_2和switch_3根据查找到的出接口和封装信息，为报文添加VLAN tag，转发给对应的终端B/C. 已知单播报文转发 1，switch_1收到来自终端A的报文，根据报文信息接入端口和VLAN信息获取对应的二层广播域，并判断报文的目的MAC是否为已知的单播MAC 是，在判断是否为本机MAC. 是，上送主机处理。 不是，在对应的二层广播域内查找出接口和封装信息，并跳装到2. 2， switch_1上VTEP根据查找到的出接口和封装信息进行VXLAN封装和报文转发。3，switch_2上VTEP收到VXLAN报文后，根据UDP目的端口号，源/目的IP地址，VNI判断VXLAN报文的合法有效性，依据VNI获取对应的二层广播域，然后进行VXLAN解封装、获取内层二层报文，判断报文的目的MAC是否为已知单播报文MAC。 是，在对应的二层广播域内查找出接口和封装信息，并跳转到4. 不是，在判断是否是本机的mac。 是，上送主机处理。 不是，走BUM报文转发流程。4，switch_2根据查找的出接口和封装信息，为报文添加VLAN tag，转发给对应的终端B。 为了防止不同vm之间通信发送arp广播引发广播风暴，vtep有一个arp proxy的功能也就是说在请求这个VTEP节点上的虚拟机的mac地址都是由VTEP的mac地址应答。也就是l2population。 vxlan网关vxlan下vm之间的通信方式有3种，同VNI下不同VM，不同VNI下跨网访问，vxlan和非vxlan之间访问。 vxlan网关分为二层网关、三层网关。二层网关：主要用于解决同VNI下不同VM之间通信，一般为vetp IP三层网关：用于解决不同VNI下跨网访问，和vxlan和非vxlan之间访问。 通俗理解就是在一台实体服务器上可以虚拟出一个交换机来，这个交换机就是VSwitch，而这个VSwitch下挂的不再是实体服务器，而是一个个VM，一个VM其实就是一个租户租用的服务器，不同租户之间肯定是不能互访的，要不然租户数据的安全性如何保障，这个隔离就是靠的VNI这个ID，其实这个你可以向下VLAN是如何隔离的，目的就是为了隔离租户。我一个租户有2个VM的话，那么我这2个之间应该可以互访吧。所以说基于VNI定义的租户，而非基于VM。内部的结构说清楚了再来说上行如何访问，在一个L2交换机你要跨网访问必然要经过网关，这个网关的IP地址就是VTEP IP，在网络上有个概念叫arp-proxy，一般用途是为了保护内部私有网络，外界的所有应答都有网关来代替回答（可以理解为门卫）。在这里外界只需要你的VTEP IP即可，对端报文到达VTEP这个网关后自己在内部走L2进行转发。因此VXLAN报文中的目的IP就是对端的网关（VTEP IP），而源地址自然也是自己的网关（VTEP IP）。而对于不同leaf上的同一VNI的VM来说，他们的VTEP IP肯定要配置相同，想下同一vlan下的服务器的网关是如何配置的就明白了 http://www.cnblogs.com/hbgzy/p/5279269.htmlhttp://blog.csdn.net/zztflyer/article/details/51883523 vxlan实验环境操作系统：centos7.2内核版本：3.10openvswitch版本：2.5 在两台虚拟机上安装好ovs，并启动这个实验目的，就是HOST 1的 br1，没有根物理网卡绑定，但可以通过建利vxlan隧道与HOST2的br1通信。 配置host1创建两个网桥br0、br1ovs-vsctl add-br br0ovs-vsctl add-br br1将eth0挂载到br0上ovs-vsctl add-port br0 eth0 将eth0的ip分配到br0上ifconfig eth0 0 &amp;&amp; ifconfig br0 192.168.1.2/24 给br1分配一个ipifconfig br1 192.168.2.2/24 配置host2创建两个网桥br0、br1ovs-vsctl add-br br0ovs-vsctl add-br br1将eth0挂载到br0上 ovs-vsctl add-port br0 eth0 将eth0的ip分配到br0上ifconfig eth0 0 &amp;&amp; ifconfig br0 192.168.1.3/24 给br1分配一个ipifconfig br1 192.168.2.3/24 此时在hots1上ping host2上的br0的ip是可以通的 ping br1的ip是pint不通的 通过建立vxlan隧道来实现br1之间通信在host1上执行给br1上增加一个vxlan0接口类型为vxlan，远程ip为 host2上的br0 ip， vni为100在host2上执行远端ip为host1上br0的ip 此时在host1上ping host2的br1的ip 就可以ping通了 在host2上的 eth0上抓包 有两层报文封装第一层源ip为192.168.2.2 目标ip为192.168.2.3，然后被udp封装，走vxlan隧道，第二层源ip为192.168.1.2 目标ip为192.168.1.3 网络包经过 vxlan interface 到达 eth1 的过程中，Linux vxlan 内核模块会将网络包二层帧封装成 UDP 包，因此，vxlan interface 必须设置适当的 MTU 来限制通过它的网络包的大小（vxlan interface 的 MTU 需要比它所绑定的物理网卡的 MTU 小 50），否则，封装后的包会被 eth1 丢弃。 VTEP (vxlan 随道端点）vswitch生成br-tun连接各个节点 像这些 为什么vlan只支持4096个因为vid只有12bit 2的12次方=4096 为什么用vxlan不用vlan？因为vlan最大只有4096个虚拟化技术的话用vlan运行vm一多，交换机上的mac地址会很多，影响交换机性能。stp算法会产生大量多路路径冗余 vxlan：建立在物理网络上的虚拟以太网vlxan是一种将二层报文用三层协议进行封装的技术，它进行传输的标识是通过VNI vni包含24bit所以vxlan最大支持2的24次方约16m个，会将二层数据包封装成udp报文通过隧道组播传输，一般配置的组播地址224.0.0.1发送arp，udp端口是4789，共50字节封装报头 UDP校验和：一般为0，非0则此包将会被丢弃。 数据包都是通过vtep进行封装传输在 OVS 中, 有几个非常重要的概念： Bridge: Bridge 代表一个以太网交换机（Switch），一个主机中可以创建一个或者多个 Bridge 设备。 Port: 端口与物理交换机的端口概念类似，每个 Port 都隶属于一个 Bridge。 Interface: 连接到 Port 的网络接口设备。在通常情况下，Port 和 Interface 是一对一的关系, 只有在配置 Port 为 bond 模式后，Port 和 Interface 是一对多的关系。 Controller: OpenFlow 控制器。OVS 可以同时接受一个或者多个 OpenFlow 控制器的管理。 datapath: 在 OVS 中，datapath 负责执行数据交换，也就是把从接收端口收到的数据包在流表中进行匹配，并执行匹配到的动作。 Flow table: 每个 datapath 都和一个“flow table”关联，当 datapath 接收到数据之后， OVS 会在flow table 中查找可以匹配的 flow，执行对应的操作, 例如转发数据到另外的端口。 veth-pair：一对接口，一个接口收另外一个接口同时也能收到，用于连接两个Brigge 设置网络接口设备的类型为“internal”。对于 internal 类型的的网络接口，OVS 会同时在 Linux 系统中创建一个可以用来收发数据的模拟网络设备。我们可以为这个网络设备配置 IP 地址、进行数据监听等等。 参考链接http://www.360doc.com/content/16/0227/12/3038654_537760576.shtmlhttp://blog.csdn.net/xjtuse2014/article/details/51376123?locationNum=7http://www.cnblogs.com/hbgzy/p/5279269.htmlhttp://blog.csdn.net/zztflyer/article/details/51883523]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[kolla 3.0.3部署openstack newton]]></title>
      <url>%2F2017%2F02%2F09%2Fkillo_newton%2F</url>
      <content type="text"><![CDATA[kolla是什么？容器化部署openstack，简而言之就是openstack里面每个组件都是用docker封装好了，对应docker的一个image。 容器化好处是什么?通过docker，降低openstack升级的难度，使升级对openstack影响最小，一旦升级失败，直接回滚旧的docker image，升级只需要三步：Pull新版本的容器镜像，停止老版本的容器服务，然后启动新版本容器。回滚也不需要重新安装包了，直接启动老版本容器服务就行，非常方便。对比过目前生产环境主流的部署方式，fuel、puppet、ROD， 我个人认为容器化部署将是未来的趋势。kolla底层通过ansoble去启动配置 已经封装好了的docker image。 缺点：要熟悉kolla，不仅仅是要熟悉openstack那么简单了，还要熟悉docker、和ansiable。 环境cpu：8核内存：16G根分区大小：100G至少需要2个网络接口（一个管理口，一个外网口）管理地址 192.168.122.77haproxy vip：192.168.122.76 安装时注意审查软件版本，我这里是安装newton版对应的是koll 3 ，2月24日发布的ocata版对应的是kolla 4 搭建docker本地镜像库 安装epel源和python-pip 安装dockercurl -sSL https://get.docker.io | bash 查看docker版本，确认docker是否安装成功。 修改docker参数，如果没有修改的话，会造成部署 neutron-dhcp-agent container 和访问APIError/HTTPError mkdir -p /etc/systemd/system/docker.service.d vi /etc/systemd/system/docker.service.d/koll.conf12345[Service]MountFlags=sharedEnvironmentFile=/etc/sysconfig/dockerExecStart=ExecStart=/usr/bin/docker daemon $other_args 保存退出修改/etc/sysconfig/docker参数并添加下面一行，目的是为了配置本地镜像仓库。other_args=&quot;--insecure-registry 192.168.122.77:4000&quot; 重启docker进程systemctl daemon-reloadsystemctl enable dockersystemctl restart docker 制做docker本地镜像仓库链接：http://pan.baidu.com/s/1gf5LTV9 密码：aysu下载解压将已经build好的openstack镜像解压到本地tar -xvf kolla-image-newton-latest.tgz 加载下载好的docker registry，docker搭建私有镜像仓库使用registry这个软件docker load &lt; ./registry-server.tar将镜像文件放到/home/下docker run -d -p 4000:5000 --restart=always -e REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY=/tmp/registry -v /home/tmp/registry/:/tmp/registry --name registry registry:2 测试是否搭建本地镜像仓库成功curl -XGET http://127.0.0.1:4000/v2/_catalog #正常情况会输出很多image name。仓库里面存在的镜像查看该镜像的tagcurl -XGET http://127.0.0.1:4000/v2/kolla/centos-binary-nova-compute/tags/list 3.0.3是版本号查看仓库数据 安装ansibleyum install ansibleclone newton分支git clone -b stable/newton https://github.com/openstack/kollacd kollacp -r etc/kolla /etc/ 安装kollapip install kolla vim /etc/kolla/globals.yml12345678openstack_release: &quot;3.0.3&quot; #上面搭建的本地仓库里面curl出来的tag号，写错了的话会导致找不到image。network_interface: &quot;ens3&quot;kolla_internal_vip_address: &quot;192.168.123.76&quot;#配置给高可用的vipneutron_external_interface: &quot;eth0&quot;docker_registry: &quot;192.168.122.77:4000&quot; #本地仓库地址docker_namespace: &quot;kolla&quot; 生成密码文件 kolla-genpwd路径/etc/kolla/password.yml 执行部署cd /root/kolla/tools/./kolla-ansible deploy -i /root/kolla/ansible/inventory/all-in-one这里需要注意的是，我是部署all-in-one，如果需要部署多节点的用multinode 修改一下里面的hostname部署完成查看容器pip install python-openstackclientpip install python-neutronclient 输入以下命令生成一个环境变量文件kolla-ansible post-deploy 文件路径为/etc/kolla/admin-openrc.sh cp /etc/kolla/admin-openrc.sh /root/ source /root/admin-openrc.sh 查看nova service查看neutron agent 打开控制台访问 帐户admin，密码通过刚刚生成的admin-openrc.sh获取 执行cd /usr/share/kolla ./init-runonce #一个测试脚本，自动下载镜像，上传，创建网络，创建路由器…… 最后创建虚拟机 需要注意的是如果是在虚拟机中测试kolla需要在宿主机上修改nova-compute的配置文件 为virt_type=qemu不然默认用的是kvm，会造成创建云主机失败。vim /etc/kolla/nova-compute/nova.conf 重启这个容器。docker restart nova_compute创建云主机测试 最后 Docker使用heka来展现收集到的日志信息。这些openstack容器的log都在heka 容器内展现默认是没有安装cinder和其他一些软件的，如果需要安装在部署时可以修改/etc/kolla/globals.yml 参考链接http://blog.csdn.net/u011211976/article/details/52085891http://docs.openstack.org/developer/kolla-ansible/quickstart.htmlhttp://geek.csdn.net/news/detail/60805?utm_source=tuicool&amp;utm_medium=referral]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack N版对接ceph jewel版]]></title>
      <url>%2F2017%2F02%2F06%2Fopenstac%26ceph%2F</url>
      <content type="text"><![CDATA[环境网络配置： public_network cluster_network 控制节点/ceph-mon 192.168.4.6 192.168.3.5 ceph-osd1 192.168.4.12 192.168.3.9 ceph-osd2 192.168.4.8 192.168.3.7 硬件配置：ceph-moncpu：4核内存：4G ceph-osd1cpu：4核内存：4G硬盘：3块100G磁盘 ceph-osd2cpu：4核内存：4G硬盘：3块100G磁盘将selinux和firewalld关闭，或配置防火墙规则 配置软件源：163 yum源wget -O /etc/yum.repo/ CentOS7-Base-163.repo http://mirrors.163.com/.help/CentOS7-Base-163.repo 配置epel源wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo 增加ceph源vim /etc/yum.repos.d/ceph.repo12345678[ceph]name=cephbaseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/x86_64/gpgcheck=0[ceph-noarch]name=cephnoarchbaseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/noarch/gpgcheck=0 清理yum缓存yum clean all创建缓存yum makecache所有节点安装cephyum install ceph –y 开始部署在部署节点安装我这里的是openstack的controller节点安装ceph-deploy，手动部署参考上一篇文章 http://www.bladewan.com/2017/01/01/manual_ceph/#more yum install ceph-deploy –y 在部署节点创建部署目录mkdir /etc/cephcd /etc/ceph/ceph-deploy new control-node1没有erro继续向下 此时目录下有ceph.conf、ceph-deploy-ceph.log、ceph.mon.keyring 修改ceph.conf添加public_network和cluster_network，同时增加允许时钟偏移vim /etc/ceph/ceph.conf 开始monitor在controller上执行ceph-deploy mon create-initial …… 部署目录多了以下文件 查看ceph状态ceph -s此时ceph状态应该是ERROR的health HEALTH_ERRno osdsMonitor clock skew detected 部署osd ceph-deploy --overwrite-conf osd prepare ceph-osd1:/dev/vdb /dev/vdc /dev/vdd ceph-osd2:/dev/vdb /dev/vdbc /dev/vdd --zap-disk部署完后查看ceph状态 查看osd tree 推送配置ceph-deploy --overwrite-conf config push ceph-osd1 ceph-osd2 重启ceph进程mon节点systemctl restart ceph-mon@control-node1.service osd节点重启systemctl restart ceph-osd@x 查看public_network和cluster_network配置是否生效 根openstack对接Ceph创建pool根据公式计算出每个pool的合适pg数 PG NumberPG和PGP数量一定要根据OSD的数量进行调整，计算公式如下，但是最后算出的结果一定要接近或者等于一个2的指数。Total PGs = (Total_number_of_OSD * 100) / max_replication_count 例：有6个osd，2副本，3个pool Total PGs =6*100/2=300每个pool 的PG=300/3=100，那么创建pool的时候就指定pg为128ceph osd pool create pool_name 128ceph osd pool create pool_name 128 创建3个poolceph osd pool create volumes 128ceph osd pool create images 128ceph osd pool create vms 128 创建nova、cinder、glance、backup用户并授权12345ceph auth get-or-create client.cinder mon &apos;allow r&apos; osd &apos;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rwx pool=images&apos;ceph auth get-or-create client.glance mon &apos;allow r&apos; osd &apos;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rwx pool=images&apos;ceph auth get-or-create client.nova mon &apos;allow r&apos; osd &apos;allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rwx pool=images&apos; 生成keyring文件控制节点 ceph auth get-or-create client.cinder | tee /etc/ceph/ceph.client.cinder.keyringceph auth get-or-create client.glance | tee /etc/ceph/ceph.client.glance.keyring 修改文件属组chown cinder:cinder /etc/ceph/ceph.client.cinder.keyringchown glance:glance /etc/ceph/ceph.client.glance.keyring 计算节点ceph auth get-or-create client.cinder |tee /etc/ceph/ceph.client.cinder.keyringceph auth get-or-create client.nova |tee /etc/ceph/ceph.client.nova.keyringceph auth get-or-create client.glance |tee /etc/ceph/ceph.client.glance.keyring 修改文件属组chown cinder:cinder /etc/ceph/ceph.client.cinder.keyringchown nova:nova /etc/ceph/ceph.client.nova.keyring 在计算节点上生成uuidgen（所有计算节点用一个就可以）uuidgenf77169a0-7d56-4fc3-a436-35298081f9f9 创建secret.xmlvim secret.xml123456&lt;secret ephemeral=&apos;no&apos; private=&apos;no&apos;&gt; &lt;uuid&gt;f77169a0-7d56-4fc3-a436-35298081f9f9&lt;/uuid&gt; &lt;usage type=&apos;ceph&apos;&gt; &lt;name&gt;client.nova secret&lt;/name&gt; &lt;/usage&gt;&lt;/secret&gt; 导出nova的keyring ceph auth get-key client.nova | tee client.nova.key virsh secret-define –file secret.xmlvirsh secret-set-value –secret f77169a0-7d56-4fc3-a436-35298081f9f9 –base64 $(cat client.nova.key ) 查看secret-value 另外一台计算节点一样 修改openstack组件配置 glancecp /etc/glance/glance-api.conf /etc/glance/glance-api.conf.bakvim /etc/glance/glance-api.conf1234567891011[DEFAULT]...show_image_direct_url = True...[glance_store]stores=glance.store.rbd.Storedefault_store = rbdrbd_store_pool = imagesrbd_store_user = glancerbd_store_ceph_conf = /etc/ceph/ceph.confrbd_store_chunk_size = 8 重启glance-api和glance-registrysystemctl restart openstack-glance-apisystemctl restart openstack-glance-registry cindercp /etc/cinder/cinder.conf /etc/cinder/cinder.conf.bakvim /etc/cinder/cinder.conf12345678910111213enabled_backends = rbd[rbd]volume_driver = cinder.volume.drivers.rbd.RBDDriverrbd_pool = volumesrbd_ceph_conf = /etc/ceph/ceph.confrbd_flatten_volume_from_snapshot = falserbd_max_clone_depth = 5rbd_store_chunk_size = 4rados_connect_timeout = -1glance_api_version = 1rbd_user = cinderrbd_secret_uuid = f77169a0-7d56-4fc3-a436-35298081f9f9volume_backend_name = rbd 重启cinder-api、cinder-schedule、cinder-volume systemctl restart openstack-cinder-apisystemctl restart openstack-cinder-volumesystemctl restart openstack-cinder-scheduler nova修改nova-computecp /etc/nova/nova.conf /etc/nova/nova.conf.bak修改nova.conf添加如下配置12345678[libvirt]virt_type = qemuimages_type = rbdimages_rbd_pool = vmsimages_rbd_ceph_conf = /etc/ceph/ceph.confrbd_user = novarbd_secret_uuid = f77169a0-7d56-4fc3-a436-35298081f9f9live_migration_flag=&quot;VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE,VIR_MIGRATE_PERSIST_DEST,VIR_MIGRATE_TUNNELLED 重启nova-computesystemctl restart openstack-nova-compute 测试glance上传镜像，在ceph pool中查看是否存在openstack image create &quot;cirros&quot; --file cirros-0.3.4-x86_64-disk.img --disk-format qcow2 --container-format bare –public 存在说明对接正常 cinder在控制台创建个云硬盘，创建成功后在ceph的volumes pool池中可以看见刚刚创建的云硬盘说明创建成功 nova在控制台创建个云主记，创建成功后在ceph的vm pool池中可以看见刚刚创建的云主机说明创建成功]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[手动安装ceph]]></title>
      <url>%2F2017%2F01%2F01%2Fmanual_ceph%2F</url>
      <content type="text"><![CDATA[环镜操作系统centos6.5网络规划Cluster_net：192.168.20.0/24Public_net：192.168.2.0/24 角色 Public_net Cluster_net Ceph-mon 192.168.2.4 192.168.20.2 Ceph-mon2 192.168.2.5 192.168.20.5 Ceph-mon3 192.168.2.6 192.168.20.6 Ceph-osd1 192.168.2.7 192.168.20.7 Ceph-osd2 192.168.2.8 192.168.20.8 Ceph-osd3 192.168.2.9 192.168.20.9 绑定hosts 123456192.168.2.4 ceph-mon.novalocal mon192.168.2.5 ceph-mon2.novalocal mon2192.168.2.6 ceph-mon3.novalocal mon3192.168.2.7 ceph-osd1.novalocal osd1192.168.2.8 ceph-osd2.novalocal osd2192.168.2.9 ceph-osd3.novalocal osd3 osd数规划每台osd节点挂载3块SSD硬盘 配置yum源根据操作系统版本任意调整(所有节点)vim /etc/yum.repos.d/ceph.repo12345678910[ceph]name=cephbaseurl=http://mirrors.aliyun.com/ceph/rpm-hammer/el6/x86_64/gpgcheck=0enablde=1[ceph-noarch]name=cephnoarchbaseurl=http://mirrors.aliyun.com/ceph/rpm-hammer/el6/x86_64/gpgcheck=0enablde=1 yum cleanyum makecache 在第一台mon节点安装pssh批量执行操作yum install pssh创建hosts（会被批量执行的主机）vim /root/hosts.txt12345192.168.2.5192.168.2.6192.168.2.7192.168.2.8192.168.2.9 先进行测试 安装软件（所有节点）pssh -P -h hosts.txt yum install ceph –y同步hostspscp.pssh -h /root/hosts.txt /etc/hosts /etc/ 都success进行下一步 配置ceph-mon创建集群fsiduuidgenfdc3d06b-7e05-44a8-b982-e8e04e4156db创建/etc/ceph/ceph.conf将fsid写入配置文件[global]fsid = fdc3d06b-7e05-44a8-b982-e8e04e4156db 将ceph-mon写入配置文件(有多个mon，用逗号隔开)mon_initial_members = mon,mon2,mon3将mon节点ip写入ceph配置文件mon_host = 192.168.2.4,192.168.2.5,192.168.2.6为集群创建mon密钥ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon &#39;allow *&#39; 生成管理员密钥ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --set-uid=0 --cap mon &#39;allow *&#39; --cap osd &#39;allow *&#39; --cap mds &#39;allow&#39; 将client.admin密钥加入到ceph.mon.keyringceph-authtool/tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring 生成mon mapmonmaptool --create --add mon 192.168.2.4 --fsid fdc3d06b-7e05-44a8-b982-e8e04e4156db /tmp/monmap在每个mon节点数据目录分别在mon、mon2、mon3上执行格式为(默认cluster-name为ceph)mkdir /var/lib/ceph/mon/{cluster-name}-{hostname}如mon为mkdir /var/lib/ceph/mon/ceph-monmon初始化(-i后接hostname)ceph-mon --mkfs -i mon --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring ceph.conf增加如下配置1234public network = 192.168.2.0/24auth cluster required = cephxauth service required = cephxauth client required = cephx 创建两个空文件touch /var/lib/ceph/mon/ceph-mon/donetouch /var/lib/ceph/mon/ceph-mon/sysvinit启动第一个ceph-mon/etc/init.d/ceph status mon.mon部署第二个mon将keyring复制到mon2scp /tmp/ceph.mon.keyring mon2:/tmp/在mon2节点上建立一个/var/lib/ceph/mon/ceph-mon2目录mkdir –p /var/lib/ceph/mon/ceph-mon2/在mon2节点上初始化mon节点ceph-mon --mkfs -i mon2 --keyring /tmp/ceph.mon.keyring为了防止重新被安装，初始化一个空的done文件touch /var/lib/ceph/mon/ceph-mon2/donetouch /var/lib/ceph/mon/ceph-mon2/sysvinit/etc/init.d/ceph start mon.mon2检查进程 第三个mon同上完后检查 发现有时种偏移问题，默认ceph是0.05s，为了方便同步直接把时钟偏移设置成0.5s修改ceph配置文件增加两条配置1234[global]mon osd down out interval = 900 #设置osd节点down后900s，把此osd节点逐出ceph集群，把之前映射到此节点的数据映射到其他节点。 [mon] mon clock drift allowed = .50 同步配置 pscp.pssh -h hosts.txt /etc/ceph/ceph.conf /etc/ceph/重启进程pssh -h /root/hosts.txt /etc/init.d/ceph restart 配置osd节点将keyring同步到osd节点pscp.pssh -h /root/hosts.txt /etc/ceph/ceph.client.admin.keyring /etc/ceph/ 为osd分配uuid(我每台osd节点有3个osd所以创建3个uuid)uuidgen19ebc47d-9b29-4cf3-9720-b62896ce6f33uuidgen1721ce0b-7f65-43ef-9dfc-c49e6210d375uuidgenf5e0f54b-2ee3-41df-bf25-ad37371ab6ce 创建3个osd123ceph osd create 19ebc47d-9b29-4cf3-9720-b62896ce6f33ceph osd create 1721ce0b-7f65-43ef-9dfc-c49e6210d375ceph osd create f5e0f54b-2ee3-41df-bf25-ad37371ab6ce 创建数据存储目录1234567mkdir /var/lib/ceph/osd/&#123;cluster-name&#125;-&#123;osd-number&#125;mkdir /var/lib/ceph/osd/ceph-0mkdir /var/lib/ceph/osd/ceph-1mkdir /var/lib/ceph/osd/ceph-2mkfs.xfs /dev/vdbmkfs.xfs /dev/vdcmkfs.xfs /dev/vdd 挂载123mount -o defaults,_netdev /dev/vdb /var/lib/ceph/osd/ceph-0mount -o defaults,_netdev /dev/vdc /var/lib/ceph/osd/ceph-1mount -o defaults,_netdev /dev/vdd /var/lib/ceph/osd/ceph-2 修改fstable 初始化osd目录 123ceph-osd -i 0 --mkfs --mkkey --osd-uuid 19ebc47d-9b29-4cf3-9720-b62896ce6f33ceph-osd -i 1 --mkfs --mkkey --osd-uuid 1721ce0b-7f65-43ef-9dfc-c49e6210d375ceph-osd -i 2 --mkfs --mkkey --osd-uuid f5e0f54b-2ee3-41df-bf25-ad37371ab6ce 注册此osd密钥123ceph auth add osd.0 osd &apos;allow *&apos; mon &apos;allow profile osd&apos; -i /var/lib/ceph/osd/ceph-0/keyringceph auth add osd.1 osd &apos;allow *&apos; mon &apos;allow profile osd&apos; -i /var/lib/ceph/osd/ceph-1/keyringceph auth add osd.2 osd &apos;allow *&apos; mon &apos;allow profile osd&apos; -i /var/lib/ceph/osd/ceph-2/keyring 加入crush map ceph osd crush add-bucket ceph-osd1 hostceph osd crush move ceph-osd1 root=default 设置权重123ceph osd crush add osd.0 1.0 host=ceph-osd1 ceph osd crush add osd.1 1.0 host=ceph-osd1ceph osd crush add osd.2 1.0 host=ceph-osd1 要已守护进程开机启动，须创建一个空文件123touch /var/lib/ceph/osd/ceph-0/sysvinittouch /var/lib/ceph/osd/ceph-1/sysvinittouch /var/lib/ceph/osd/ceph-2/sysvinit 启动osd进程123/etc/init.d/ceph start osd.0/etc/init.d/ceph start osd.1/etc/init.d/ceph start osd.2 查看osd 树按上述方法配置osd2、osd33台节点添加完毕 这里有个warn，是pool的pg问题，我们重新计算，修改。查看我们现在拥有的poolceph osd lspools0 rbd, 查看默认的rbd pool的pg1234567ceph osd pool get rbd pg_numpg_num: 64ceph osd pool get rbd pg_numpg_num: 64修改为128ceph osd pool set rbd pg_num 128ceph osd pool set rbd pgp_num 128 再查看 修改ceph.conf默认数据存两份，默认pg为128同步配置重启ceph 配置ceph-radosgw直接将第一个mon节点当radosgw，ceph在0.80及以上版本可以直接使用civeweb来构建对象网关，可以不需要使用apache或nginx+fastcgi了，所以我这里用civetweb。 安装软件yum install ceph-radosgw没有www-data用户创建useradd –r –s /sbin/nologin www-data创建gateway keyring并授权 ceph auth get-or-create client.radosgw.gateway osd &#39;allow rwx&#39; mon &#39;allow rwx&#39; -o /etc/ceph/keyring.radosgw.gateway 编辑ceph.conf文件增加下面内容1234567891011[client.radosgw.gateway]keyring = /etc/ceph/keyring.radosgw.gatewayrgw_socket_path = /tmp/radosgw.sockrgw_frontends= &quot;civetweb port=7480&quot;host = ceph-monrgw_dns_name = *.domain.tldrgw_print_continue = Truergw_data = /var/lib/ceph/radosgwuser = www-datargw s3 auth use keystone = truelog file =/var/log/radosgw/client.radosgw.gateway.log 重启进程/etc/init.d/ceph-radosgw restart查看是否启动成功 新建用户radosgw-admin user create –uid=”testuser” –display-name=”First User”得到如下结果]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mariadb galera]]></title>
      <url>%2F2016%2F12%2F25%2Fmariadb-base%2F</url>
      <content type="text"><![CDATA[传统的数据同步方式传统的mysql数据库同步是使用二进制日志进行主从同步也就是semi-sync，这种同步也只是一种半同步，并不是完全的实时同步，在mysql 5.7.17中推出了MySQL Group Replication，这种实现方式与Galera cluster基本完全一样，不过MGR比Galera的优势在于 Mysql官方出品，能得到后续技术支持，Galera是芬兰的一家叫codership的公司开发的。 MGR使用Paxos协议，性能比Galera好，并且性能稳定，Galera实际只能使用三个节点，有性能和稳定性问题。 Galera目前只支持linux，MGR支持多个平台。 mysql 5.6下semi-sync数据同步方式master的dump进程需要发送binlog日志到slave，master要等待至少一个slave通知，slave将已经接收到传过来的events并写入relay log，salve发送ack信息到master，这个事务才能提交。 5.6版semi-sync的缺陷，dump thread要承但两份任务，传送binlog给slave还要等slave的返回，并且这两个任务是串行的，也就是说，dump thread要先传送binlog给slave，并还要slave返回才能传送下一个events事务，这样的dump thread成为性能瓶径。 mysql 5.7下semi-sync数据同步方式5.7.4为解决上述的问题，在master端增加了ack进程。这样事务写发送binlog与接收ack可以并行进行，提高semi-sync的效率。 GaleraGalera cluster是可以实现mariadb多主集群的软件，它目前只能用于linux和只支持XtraDB和Innodb存储引擎 ，mariadb和perconna提供了原生的Galera cluster的支持，所以可以直接使用Galera cluster，mysql要使用Galera cluster需要使用Galera cluster提供的插件。传统主从只能有一个数据库进行写服务，Galera集群，每个节点都可读可写，在Galera上层部署负载均衡软件如lvs和haproxy进行流量分担是常用做法。 原理各个节点的数据同步由wsrep接口实现。client发起一个commit命令时，所有本事务内对数据库的操作和primary_key都会打包写入到write-set，write-set随后会复制到其他节点，各个节点接收后，会根据write-set传送的primary key进行检验，检查是否与本地事务种read-write或write-write锁冲突，冲突则回滚，没有冲突就执行，完成后返回success。如果其他节点没有执行成功则存放在队列种，稍后会重新尝式。 Galera集群的优点 支持多个节点的数据写入，能保证数据的强一致性。 同步复制，各个节点无延迟，不会因为一台宕机导致数据丢失。 故障节点将自动从集群中移除 基于行级的并行复制 缺点 只支持Innodb存储引擎 只支持linux平台， 集群写入的tps由最弱节点限制，如果一个节点变慢，整的集群就是缓慢的，所以一般情况下部署，要求统一的硬件配置。 会因为网络抖动造成性能和稳定性问题。 参考链接:http://www.oschina.net/news/79983/galera-will-die-mysql-group-replication-realeasehttp://www.itbaofeng.com/?p=236]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[win10启用ubuntu子shell]]></title>
      <url>%2F2016%2F12%2F22%2Fwin10_shell%2F</url>
      <content type="text"><![CDATA[以前想用linux的shell都得装第三方软件实现，比如git bash等，在win10周年更新版14393直接集成了bash on ubuntu了集成新的 Windows Subsystem for Linux 子系统，这样能直接在 Bash on Ubuntu 环境里编译运行 Linux 程序非常爽。 开启win10开发者模式win10设置 打开控制面板，启用或关闭windows功能 点确定，然后重启然后打开powershell或cmd直接在里面输入bash命令 它会询问你是否安装canonical 分发的ubuntu，输入y，然后等待。这里推荐使用vpn，然后安装完后，设置用户名，输入root用root权限吧。安装完后，然后你打开powershell和cmd输入bash就直接进入ubuntu子系统了 可以看出子bash还是非常给力的是ubuntu 14.04 3.4的内核。以后可以直接在上面跑python脚本了。不需要专门虚拟机。 默认字体蓝色不好看清楚，修改powershell或cmd的背景色即可。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mariadb galera集群搭建]]></title>
      <url>%2F2016%2F12%2F22%2Fmariadb-galera-2%2F</url>
      <content type="text"><![CDATA[环境配置准备3台服务器 192.168.1.16 mariadb-1.novalocal mariadb-1192.168.1.19 mariadb-2.novalocal mariadb-2192.168.1.18 mariadb-3.novalocal mariadb-3 配置repo文件 vim /etc/yum.repos.d/MariaDB.repo12345[mariadb]name = MariaDBbaseurl = http://yum.mariadb.org/10.1/centos7-amd64gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDBgpgcheck=1 yum makecache 禁用防火墙和selinux 如果要使用防火墙添加允许3306、和4567 端口规则。 安装Mariadb和galera cluster（三个节点都执行） yum install MariaDB-server MariaDB-client galera 启动mariadb systemctl start mariadb 一些初始化安全配置 /usr/bin/mysql_secure_installation 关闭数据库systemctl stop mariadb 修改mariadb-1上的/etc/my.cnf.d/server.cnf文件如下1234567891011121314[galera]wsrep_provider = /usr/lib64/galera/libgalera_smm.sowsrep_cluster_address = &quot;gcomm://192.168.1.16,192.168.1.18,192.168.1.19&quot;wsrep_node_name = mariadb-1wsrep_node_address=192.168.1.16wsrep_on=ONbinlog_format=ROWdefault_storage_engine=InnoDBinnodb_autoinc_lock_mode=2#bind-address=0.0.0.0wsrep_slave_threads=1innodb_flush_log_at_trx_commit=0innodb_buffer_pool_size=122Mwsrep_sst_method=rsync 将此文件复制到mariadb-2、mariadb-3，注意要把 wsrep_node_name 和 wsrep_node_address 改成相应节点的 hostname 和 ip。 启动 MariaDB Galera Cluster 服务 /usr/sbin/mysqld –wsrep-new-cluster –user=root &amp;–wsrep-new-cluster 这个参数只能在初始化集群使用，且只能在一个节点使用。 观察日志： [root@node4 ~]# tail -f /var/log/message123150701 19:54:17 [Note] WSREP: wsrep_load(): loading provider library &apos;none&apos;150701 19:54:17 [Note] /usr/libexec/mysqld: ready for connections.Version: &apos;5.5.40-MariaDB-wsrep&apos; socket: &apos;/var/lib/mysql/mysql.sock&apos; port: 3306 MariaDB Server, wsrep_25.11.r4026 出现 ready for connections ,证明我们启动成功 查看是否启用galera插件连接mariadb,查看是否启用galera插件 目前集群机器数 查看集群状态show status like ‘wsrep%’; 查看连接的主机 另外两个节点mariadb会自动加入集群systemctl start mariadb这时查看galera集群机器数量 已经连接机器的ip 测试在mariadb-1上创建数据库，创建表，插入数据123456789101112131415161718192021MariaDB [(none)]&gt; create database test1；MariaDB [test1]&gt; insert into test values(1);Query OK, 1 row affected (0.01 sec)MariaDB [test1]&gt; insert into test values(2);Query OK, 1 row affected (0.01 sec)MariaDB [test1]&gt; insert into test values(3);Query OK, 1 row affected (0.00 sec)MariaDB [test1]&gt; insert into test values(4);Query OK, 1 row affected (0.00 sec)MariaDB [test1]&gt; insert into test values(5);Query OK, 1 row affected (0.03 sec)MariaDB [test1]&gt; insert into test values(6);Query OK, 1 row affected (0.01 sec)MariaDB [test1]&gt; insert into test values(7);Query OK, 1 row affected (0.01 sec) 在另外两台mariadb-2、mariadb-3上可以看见刚刚插入的数据，说明数据同步了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack 本地yum源搭建]]></title>
      <url>%2F2016%2F12%2F12%2Fopenstack-localyumbuild%2F</url>
      <content type="text"><![CDATA[我们在部署openstack时用国外yum源的快，经常会很慢导致等待时间太久，所以建议使用本地yum源安装 这里以newton版centos7平台为例 首先下载官方repoyum install https://rdoproject.org/repos/rdo-release.rpm这时侯/etc/yum.repos.d里面会产生3个文件12[root@test yum.repos.d]# ls rdo-qemu-ev.repo rdo-release.repo rdo-testing.repo 我这里打算用http搭建我的本地yum服务器 先安装httpdyum install httpd mkdir /var/www/html/newton 待会将同步下来的包放这个目录cd /vaw/www/html/newton yum repolist –列出你所有的仓库 前面是repo id不包含x86_64 这里我只需要openstack-newton、和rdo-qemu-ev这两个软件库 先同步openstack-newtonreposync --repoid=openstack-newton 指定要下载的仓库id，会通过网络全部下载到当前目录下载下来。 同步完第一个继续同步第二个reposync --repoid=rdo-qemu-ev 同步完后这时查看 /vaw/www/html/newton里面已经有很多包了，只有软件包，没有repodate清单，所以需要自己重新createrepo来创建清单没有createrepo自己安装，创建软件清单createrepo /var/www/html/newton/ 然后启动httpd服务，其他机器通过httpd服务来访问yum源 例如控制节点yum源配置vim /etc/yum.repos.d/openstack.repo123456[openstack]name=openstackbaseurl=http://192.168.4.3/newtonenabled=1gpgcheck=0~ yum makecache 其他节点一样。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[openstack newton 安装]]></title>
      <url>%2F2016%2F12%2F12%2Fopenstack-newton-install%2F</url>
      <content type="text"><![CDATA[环境配置机器配置：3台8v8G的虚拟机，1台做控制节点2台做融合节点。 网络划分：192.168.122.0/24 public网络192.168.3.0/24 存储网络192.168.4.0/24管理网络、sdn隧道网络 我这里配置了本地源，就不用在手动配置官网源本地源的搭建和配置会在另外一个文档说明。节点网络信息 ： 管理网络和随道网络 存储网络 公网 控制节点 192.168.4.6 192.168.3.5 192.168.122.2 计算节点 192.168.4.7 192.168.3.6 192.168.125.5 计算节点 192.168.4.8 192.168.3.7 192.168.122.6 网络拓扑 安装chrony控制节点向外同步时间，其他节点如计算节点都直接同步控制节点yum install chrony 修改配置文件vim /etc/chrony.conf添加下面这两条server cn.ntp.org.cn iburstallow 192.168.4.0/24 设置开机启动systemctl enable chronysystemctl start chrony 其他节点：yum install chrony 修改配置文件vim /etc/chrony.conf 添加下面这两条server 192.168.4.6 iburst 设置开机启动systemctl enable chrony 启动进程systemctl start chrony 安装openstack客户端yum install python-openstackclient 安装Mariadb（数据库服务）vim /etc/my.cnf.d/openstack.cnf1234567[mysqld] bind-address = 192.168.4.6 #填写管理网段ip default-storage-engine = innodb innodb_file_per_table max_connections = 4096collation-server = utf8_general_cicharacter-set-server = utf8 设置开机启动 systemctl enable mariadb 启动Mariadb systemctl start mariadb 安装rabbitmq（用于消息队列）yum install rabbitmq-server 设置开机启动 systemctl enable rabbitmq-server 开启rabbitmq systemctl start rabbitmq-server 创建openstack用户和配置密码 rabbitmqctl add_user openstack 123456 给openstack用户配置读和写权限 rabbitmqctl set_permissions openstack &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; 安装memcache（缓存token）yum install memcached phython-memcached systemctl enable memcached systemctl start memcached 安装keystone（认证服务）连接数据库 [root@control-node1 yum.repos.d]# mysql 创建keystone数据库 create database keystone; 数据库授权 密码自己设置，这里为了方便设置123456grant all privileges on keystone.* to &#39;keystone&#39;@&#39;localhost&#39; identified by &#39;123456&#39;; grant all privileges on keystone.* to &#39;keystone&#39;@&#39;%&#39; identified by &#39;123456&#39;; keystone使用httpd的mod_wsgi运行在端口5000和35357处理认证服务请求。默认情况下，keystone服务依然监听在5000和35357端口。 安装 keystone和wsgi yum install openstack-keystone httpd mod_wsgi 修改keystone配置文件 vim /etc/keystone/keystone.conf connection = mysql+pymysql://keystone:123456@192.168.4.6/keystone #加入连接数据库配置 配置使用哪种产生token方式目前keystone支持4种(UUID、PKI、PKIZ、Fernet)这里我们配置fernethttp://www.tuicool.com/articles/jQJNFrn 这篇文章有几种模式的详细介绍。 [token]provider = fernet 同步数据库 su -s /bin/sh -c “keystone-manage db_sync” keystone #发现同步数据库就是错了也没有反应，需要检查keystone的日志文件查看是否正确 初始化fernet keykeystone-manage fernet_setup –keystone-user keystone –keystone-group keystone keystone-manage credential_setup –keystone-user keystone –keystone-group keystoen 创建Bootstrap the Identity service(就是创建admin用户的帐号信息) 12345keystone-manage bootstrap --bootstrap-password 123456 \ --bootstrap-admin-url http://192.168.4.6:35357/v3/ \ --bootstrap-internal-url http://192.168.4.6:35357/v3/ \ --bootstrap-public-url http://192.168.122.2:5000/v3/ \ --bootstrap-region-id RegionOne 配置apache服务器vim /etc/httpd/conf/httpd.conf 配置成管理网段的ipServerName 192.168.4.6 将keystone的配置文件软链接到apache的配置文件ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/ 设置开机启动systemctl enable httpd 启动httpdsystemctl start httpd检查端口 12345678lsof -i:5000COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEhttpd 18883 root 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18894 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18895 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18896 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18897 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN)httpd 18898 apache 6u IPv6 57978 0t0 TCP *:commplex-main (LISTEN) 到root下创建环境变量文件 123456789vim /root/openrc#!/bin/bashexport OS_USERNAME=adminexport OS_PASSWORD=123456 #这个密码是上面Bootstrap the Identity service填的密码export OS_PROJECT_NAME=adminexport OS_USER_DOMAIN_NAME=defaultexport OS_PROJECT_DOMAIN_NAME=defaultexport OS_AUTH_URL=http://192.168.4.6:35357/v3export OS_IDENTITY_API_VERSION=3 创建域、项目、用户创建service projectopenstack project create --domain default --description &quot;Service Project&quot; service 创建user角色openstack role create user这里不创建普通用户了测试admin用户获取tokenopenstack --os-auth-url http://192.168.4.6:35357/v3 token issue 安装glance镜像服务连接Mariadb创建数据库 create database glance; 授权123grant all privileges on glance.* to &apos;glance&apos;@&apos;localhost&apos; identified by &apos;123456&apos;; grant all privileges on glance.* to &apos;glance&apos;@&apos;%&apos; identified by &apos;123456&apos;; grant all privileges on glance.* to &apos;glance&apos;@&apos;control-node1.novalocal&apos; identified by &apos;123456&apos;; control-xxx换成主机名，我这里就算api.conf里面配置的ip默认还是去连接host是主机名，所以只能在加个主机名授权。 创建glance用户并设置密码openstack user create –domain default –password-prompt glance 给glance用户添加admin角色权限openstack role add –project service –user glance admin 创建glance serviceopenstack service create –name glance –description “OpenStack Image” image 创建glance endpoint123openstack endpoint create --region RegionOne image public http://192.168.122.2:9292openstack endpoint create --region RegionOne image internal http://192.168.4.6:9292openstack endpoint create --region RegionOne image admin http://192.168.4.6:9292 安装软件包yum install openstack-glance 配置glancevim /etc/glance/glance-api.conf 配置数据库12[database]connection = mysql+pymysql://glance:123456@192.168.4.6/glance 配置glancevim /etc/glance/glance-api.conf配置数据库12345678910111213141516[database]connection = mysql+pymysql://glance:123456@192.168.4.6/glance配置keystone[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = glancepassword = 123456[paste_deploy]flavor = keystone 配置镜像存储1234[glance_store]stores = file,httpdefault_store = filefilesystem_store_datadir = /var/lib/glance/images/ vim /etc/glance/glance-registry.conf12345678910111213141516[database]connection = mysql+pymysql://glance:123456@192.168.4.6/glance配置keystone[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = glancepassword = 123456[paste_deploy]flavor = keystone 同步数据库su -s /bin/sh -c “glance-manage db_sync” glance 设置开机启动systemctl enable openstack-glance-api 启动服务systemctl start openstack-glance-api 这些做完后最好在查看下日志，看看是否有错误，每部署完一个组件都这样，这样出错的可以很快定位。 下载cirros测试一下 wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.imgopenstack image create &quot;cirros&quot; --file cirros-0.3.4-x86_64-disk.img --disk-format qcow2 --container-format bare --public glance image-list 检查一下镜像是否上传成功 安装nova组件控制节点安装创建数据库create database nova_api;create database nova; 授权12345678GRANT ALL PRIVILEGES ON nova_api.* TO &apos;nova&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON nova_api.* TO &apos;nova&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON nova_api.* TO &apos;nova&apos;@&apos;control-node1.novalocal&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON nova.* TO &apos;nova&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON nova.* TO &apos;nova&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON nova.* TO &apos;nova&apos;@&apos;control-node1.novalocal&apos; IDENTIFIED BY &apos;123456&apos;; 创建为nova组件创建用户、service、endpointopenstack user create --domain default --password-prompt nova 给nova用户添加admin角色权限openstack role add --project service --user nova admin 创建serviceopenstack service create --name nova --description &quot;OpenStack Compute&quot; compute 创建endpoint12345openstack endpoint create --region RegionOne compute public http://192.168.122.2:8774/v2.1/%\(tenant_id\)s openstack endpoint create --region RegionOne compute internal http://192.168.4.6:8774/v2.1/%\(tenant_id\)s openstack endpoint create --region RegionOne compute admin http://192.168.4.6:8774/v2.1/%\(tenant_id\)s 安装nova-api组件yum install openstack-nova-api openstack-nova-conductor openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler 配置novavim /etc/nova/nova.conf 123456789101112131415161718192021222324252627282930313233[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6 #配置rabbitmq帐户和密码my_ip = 192.168.4.6use_neutron = Truefirewall_drive = nova.virt.firewall.NoopFirewallDriverenabled_apis=osapi_compute,metadataauth_strategy=keystone[api_database]connection = mysql+pymysql://nova:123456@192.168.4.6/nova_api #配置nova连接数据库[database]connection = mysql+pymysql://nova:123456@192.168.4.6/nova[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = novapassword = 123456配置novncnovncproxy_port=6080novncproxy_base_url=http://211.156.182.144:6080/vnc_auto.htmlvncserver_listen=192.168.4.6[glance]api_servers = http://192.168.4.6:9292[oslo_concurrency]lock_path = /var/lib/nova/tmp 同步数据库su -s /bin/sh -c “nova-manage api_db sync” novasu -s /bin/sh -c “nova-manage db sync” nova 1234567891011systemctl enable openstack-nova-api.service \openstack-nova-consoleauth.service \openstack-nova-scheduler.service \openstack-nova-conductor.service \openstack-nova-novncproxy.servicesystemctl start openstack-nova-api.service \openstack-nova-consoleauth.service \openstack-nova-scheduler.service \openstack-nova-conductor.service \openstack-nova-novncproxy.service 计算机节点安装安装nova-computeyum install openstack-nova-compute 配置nova-computevim /etc/nova/nova.conf123456789101112131415161718192021222324252627282930313233[DEFAULT]enabled_apis = osapi_compute,metadatatransport_url = rabbit://openstack:123456@192.168.4.6 #配置rabbitmq帐号和密码auth_strategy = keystonemy_ip = 192.168.4.7use_neutron = Truefirewall_driver = nova.virt.firewall.NoopFirewallDriver[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = novapassword = 123456[vnc]enabled=truevncserver_listen=0.0.0.0vncserver_proxyclient_address=192.168.4.7 #填写本机ipnovncproxy_base_url=http://211.156.182.144:6080/vnc_auto.html #这个填你要用控制节点的public ip[glance]api_servers = http://192.168.4.6:9292配置锁路径[oslo_concurrency]lock_path = /var/lib/nova/tmp[libvirt]virt_type = qemu #物理服务器就配置kvm虚拟机就配置qemu 设置开机启动systemctl enable libvirtd.service openstack-nova-compute.service 启动nova-computesystemctl start libvirtd.service openstack-nova-compute.service 在控制节点查看检查一下compute进程根控制节点连接 配置neutron控制节点安装我这里使用openvswitch不使用linux bridge，因为openvswitch功能比linux Brige功能强太多了。但配置稍微复杂点。 创建数据库create database neutron; 授权123GRANT ALL PRIVILEGES ON neutron.* TO &apos;neutron&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON neutron.* TO &apos;neutron&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;; GRANT ALL PRIVILEGES ON neutron.* TO &apos;neutron&apos;@&apos;control-node1.novalocal&apos; IDENTIFIED BY &apos;123456&apos;; 创建neutron用户并设置密码openstack user create --domain default --password-prompt neutron 给neutron用户添加admin角色权限openstack role add --project service --user neutron admin 创建neutron serviceopenstack service create --name neutron --description &quot;OpenStack Networking&quot; network 创建neutron endpoint123openstack endpoint create --region RegionOne network public http://192.168.122.2:9696 openstack endpoint create --region RegionOne network admin http://192.168.4.6:9696 openstack endpoint create --region RegionOne network internal http://192.168.4.6:9696 安装neutron组件yum -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch vim /etc/neutron/neutron.conf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[DEFAULT]service_plugins = routertransport_url = rabbit://openstack:123456@192.168.4.6auth_strategy = keystonenotify_nova_on_port_status_changes = Truenotify_nova_on_port_data_changes = Truestate_path = /var/lib/neutronuse_syslog = Truesyslog_log_facility = LOG_LOCAL4log_dir =/var/log/neutroncore_plugin = neutron.plugins.ml2.plugin.Ml2Pluginbase_mac = fa:16:3e:00:00:00mac_generation_retries = 32dhcp_lease_duration = 600dhcp_agent_notification = Trueallow_bulk = Trueallow_pagination = Falseallow_sorting = Falseallow_overlapping_ips = Trueadvertise_mtu = Trueagent_down_time = 30router_scheduler_driver = neutron.scheduler.l3_agent_scheduler.ChanceSchedulerallow_automatic_l3agent_failover = Truedhcp_agents_per_network = 2api_workers = 9rpc_workers = 9network_device_mtu=1450[database]connection = mysql+pymysql://neutron:123456@192.168.4.6/neutron[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = neutronpassword = 123456[nova]auth_url = http://192.168.4.6:35357auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = novapassword = 123456[oslo_concurrency]lock_path = /var/lib/neutron/tmp 配置modular layer 2（ml2）插件 vim /etc/neutron/plugins/ml2/ ml2_conf.ini1234567891011121314151617181920212223242526272829[DEFAULT]type_drivers = flat,vxlantenant_network_types = vxlanmechanism_drivers = openvswitch,l2populationextension_drivers = port_security[ml2]path_mtu = 1450type_drivers = flat,vxlantenant_network_types = vxlanphysical_network_mtus =physnet1:1500[ml2_type_flat]flat_networks =*[ml2_type_vxlan]vni_ranges =2:65535vxlan_group =224.0.0.1[securitygroup]enable_security_group = Truefirewall_driver=neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver[ovs]local_ip=192.168.4.6tunnel_bridge=br-tunenable_tunneling=Trueintegration_bridge=br-intbridge_mappings=physnet1:br-ex 配置l3-agentvim /etc/neutron/l3_agent.ini12345678910[DEFAULT]debug = Falseinterface_driver =neutron.agent.linux.interface.OVSInterfaceDriverhandle_internal_only_routers = Truemetadata_port = 8775send_arp_for_ha = 3periodic_interval = 40periodic_fuzzy_delay = 5enable_metadata_proxy = Truerouter_delete_namespaces = True 配置dhcp_agentvim /etc/neutron/dhcp_agent.ini12345678910[DEFAULT]resync_interval = 30interface_driver =neutron.agent.linux.interface.OVSInterfaceDriverenable_isolated_metadata = Trueenable_metadata_network = Falsedhcp_domain = openstacklocaldhcp_broadcast_reply = Falsedhcp_delete_namespaces = Trueroot_helper=sudo neutron-rootwrap /etc/neutron/rootwrap.confstate_path=/var/lib/neutron vim /etc/neutron/plugins/ml2/openvswitch_agent.ini1234567891011121314151617[agent]polling_interval = 2tunnel_types = vxlanvxlan_udp_port = 4789l2_population = Trueprevent_arp_spoofing = Falseextensions =[ovs]local_ip=192.168.4.6tunnel_bridge=br-tunenable_tunneling=Trueintegration_bridge=br-intbridge_mappings=physnet1:br-ex[securitygroup]firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriverenable_security_group = true 设置openvswitch开机启动systemctl enable openvswitch.service 启动openvswitchsystemctl start openvswitch 创建br-ex br-tun br-intovs-vsctl add-br br-intovs-vsctl add-br br-exovs-vsctl add-br br-tun 将上外网网卡挂载到br-ex上ovs-vsctl add-port br-ex eth2 设置开机启动项systemctl enable neutron-openvswitch-agent.service启动进程systemctl start neutron-openvswitch-agent.service 配置计算节点neutron配置一下内核参数修改配置文件 /etc/sysctl.conf12net.ipv4.conf.all.rp_filter=0net.ipv4.conf.default.rp_filter=0 sysctl –pyum -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch vim /etc/neutron/neutron.conf 1234567891011121314151617[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6auth_strategy = keystone[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = neutronpassword = 123456[oslo_concurrency]lock_path = /var/lib/neutron/tmp vim /etc/neutron/plugins/ml2/ml2_conf.ini1234567891011[ml2]path_mtu = 1450type_drivers = flat,vxlantenant_network_types = vxlanphysical_network_mtus =physnet1:1500mechanism_drivers = openvswitch,l2populationextension_drivers = port_security[securitygroup]enable_ipset = truefirewall_driver=neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver vim /etc/neutron/plugins/ml2/openvswitch_agent.ini1234567891011121314151617181920[ovs]local_ip=192.168.4.7tunnel_bridge=br-tunenable_tunneling=Trueintegration_bridge=br-intbridge_mappings=physnet1:br-ex[agent]enable_distributed_routing=Trueprevent_arp_spoofing=Truearp_responder=Truepolling_interval=2drop_flows_on_start=Falsevxlan_udp_port=4789l2_population=Truetunnel_types=vxlan[securitygroup]firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriverenable_security_group = true systemctl enable openvswitch.servicesystemctl start openvswitch.service 创建br-ex、br-int、br-tunovs-vsctl add-br br-intovs-vsctl add-br br-exovs-vsctl add-br br-tun vim /etc/nova/nova.conf123456789101112131415[DEFAULT]network_api_class = nova.network.neutronv2.api.APIsecurity_group_api = neutronlinuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver[neutron]url = http://192.168.4.6:9696auth_url = http://192.168.4.6:35357auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = neutronpassword = 123456 systemctl restart neutron-openvswitch-agent.servicesystemctl restart openstack-nova-compute 安装完后可以在在控制节点检查是否安装成功 安装控制台yum install openstack-dashboardvim /etc/openstack-dashboard/local_settings12345678910111213141516171819202122232425262728这里配置控制节点ipOPENSTACK_HOST = &quot;192.168.4.6&quot;配置允许所有节点访问ALLOWED_HOSTS = [&apos;*&apos;, ]配置memcacheSESSION_ENGINE = &apos;django.contrib.sessions.backends.cache&apos;CACHES = &#123; &apos;default&apos;: &#123; &apos;BACKEND&apos;: &apos;django.core.cache.backends.memcached.MemcachedCache&apos;, &apos;LOCATION&apos;: &apos;192.168.4.6:11211&apos;, &#125;&#125;配置keystone v3验证OPENSTACK_KEYSTONE_URL = &quot;http://%s:5000/v3&quot; % OPENSTACK_HOST配置域OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = &apos;default&apos;配置api版本OPENSTACK_API_VERSIONS = &#123; &quot;identity&quot;: 3, &quot;image&quot;: 2, &quot;volume&quot;: 2,&#125;设置通过控制台默认创建用户的角色是userOPENSTACK_KEYSTONE_DEFAULT_ROLE = &quot;user&quot; 重启服务systemctl restart httpd.service memcached.service 通过http://control_ip/dashboard可以访问 Admin 登录，密码是你通过keystone创建的，如果不记得查看openrc 创建flat网络做float_ip池管理员—&gt;网络——&gt;创建网络 Phynet1是在ml2.ini里面bridge_mappings定义的br-ex对应的名字，创建完后增加子网，然后在创建个普通网络，创建个路由器，路由器绑定普通子网，创建个主机配置，然后创建vm加入到你创建的普通网络 这时在vm所在的计算节点或控制节点 ovs-vsctl show 可以看见计算节点根网络节道隧道已经建立。 Cinder配置配置控制节点创建数据库create database cinder;用户授权123GRANT ALL PRIVILEGES ON cinder.* TO &apos;cinder&apos;@&apos;localhost&apos; identified by &apos;123456&apos;; GRANT ALL PRIVILEGES ON cinder.* TO &apos;cinder&apos;@&apos;%&apos; identified by &apos;123456&apos;; GRANT ALL PRIVILEGES ON cinder.* TO &apos;cinder&apos;@&apos;control-node1.novalocal&apos; identified by &apos;123456&apos;; 创建用户openstack user create --domain default --password-prompt cinder 给cinder用户赋予admin权限openstack role add --project service --user cinder admin openstack service create --name cinder --description &quot;OpenStack Block Storage&quot; volume openstack service create --name cinderv2 --description &quot;OpenStack Block Storage&quot; volumev2 创建endpoint1234567891011121314151617openstack endpoint create --region RegionOne \volume public http://192.168.122.2:8776/v1/%\(tenant_id\)sopenstack endpoint create --region RegionOne \volume internal http://192.168.4.6:8776/v1/%\(tenant_id\)sopenstack endpoint create --region RegionOne \volume admin http://192.168.4.6:8776/v1/%\(tenant_id\)sopenstack endpoint create --region RegionOne \volumev2 public http://192.168.122.2:8776/v2/%\(tenant_id\)sopenstack endpoint create --region RegionOne \volumev2 internal http://192.168.4.6:8776/v2/%\(tenant_id\)sopenstack endpoint create --region RegionOne \volumev2 admin http://192.168.4.6:8776/v2/%\(tenant_id\)s 安装cinderyum install openstack-cinder vim /etc/cinder/cinder.conf1234567891011121314151617181920[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6auth_strategy = keystone[database]connection = mysql+pymysql://cinder:123456@192.168.4.6/cinder[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = cinderpassword = 123456[oslo_concurrency]lock_path = /var/lib/cinder/tmp 同步数据库su -s /bin/sh -c “cinder-manage db sync” cinder 配置计算机节点使用cindervim /etc/nova/nova.conf [cinder]os_region_name = RegionOne 重启服务systemctl restart openstack-nova-api.service 设置开机自启cindersystemctl enable openstack-cinder-api.service openstack-cinder-scheduler.service systemctl start openstack-cinder-api.service openstack-cinder-scheduler.service 配置一个存储节点安装lvmyum install lvm2 systemctl enable lvm2-lvmetad.servicesystemctl start lvm2-lvmetad.service 创建个lvm卷pvcreate /dev/vdb 创建vgvgcreate cinder-volumes /dev/vdb vim /etc/lvm/lvm.conf123devices &#123;filter = [ &quot;a/vdb/&quot;, &quot;r/.*/&quot;]&#125; 安装软件yum install openstack-cinder targetcli python-keystone 修改cinder配置文件vim /etc/cinder/cinder.conf1234567891011121314151617181920212223242526272829[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6verbose = Trueauth_strategy = keystoneenabled_backends = lvmglance_api_servers = http://192.168.4.6:9292[database]connection = mysql+pymysql://cinder:123456@192.168.4.6/cinder[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = cinderpassword = 123456[lvm]volume_driver = cinder.volume.drivers.lvm.LVMVolumeDrivervolume_group = cinder-volumesiscsi_protocol = iscsiiscsi_helper = lioadm[oslo_concurrency]lock_path = /var/lib/cinder/tmpsystemctl enable openstack-cinder-volume.service target.service systemctl start openstack-cinder-volume.service target.service 到控制台创建个卷，并挂载到云主机。 Ceilometer配置Ceilometer使用Mongdb存储meter数据，所以需要先在控制节点安装Mongdb 在控制节点安装Mongdbyum install mongodb-server mongodb 配置Mongdbvim /etc/mongod.confsmallfiles = true #限制日志大小创建Mongdb数据库和帐户授权替换123456为你自己设置的密码创建用户openstack user create –domain default –password-prompt ceilometer 给ceilometer用户添加admin角色权限openstack role add –project service –user ceilometer admin创建ceilometer serviceopenstack service create –name ceilometer –description “Telemetry” metering 创建ceilometer endpoint123openstack endpoint create --region RegionOne metering public http://192.168.122.2:8777openstack endpoint create --region RegionOne metering admin http://192.168.4.6:8777openstack endpoint create --region RegionOne metering internal http://192.168.4.6:8777 安装包yum install openstack-ceilometer-api \openstack-ceilometer-collector \openstack-ceilometer-notification \openstack-ceilometer-central \python-ceilometerclient 配置ceilometervim /etc/ceilometer/ceilometer.conf123456789101112131415161718192021222324252627282930[DEFAULT]rpc_backend = rabbitauth_strategy = keystone[oslo_messaging_rabbit]rabbit_host = 192.168.4.6rabbit_userid = openstackrabbit_password = 123456[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = ceilometerpassword = 123456[service_credentials]auth_type = passwordauth_url = http://192.168.4.6:5000/v3project_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = ceilometerpassword = 123456 ##密码替换成在keystone创建ceilometer时设置的密码interface = internalURLregion_name = RegionOne 创建ceilometer的vhost vim /etc/httpd/conf.d/wsgi-ceilometer.conf 123456789101112Listen 8777&lt;VirtualHost *:8777&gt; WSGIDaemonProcess ceilometer-api processes=2 threads=10 user=ceilometer group=ceilometer display-name=%&#123;GROUP&#125; WSGIProcessGroup ceilometer-api WSGIScriptAlias / /usr/lib/python2.7/site-packages/ceilometer/api/app.wsgi WSGIApplicationGroup %&#123;GLOBAL&#125; ErrorLog /var/log/httpd/ceilometer_error.log CustomLog /var/log/httpd/ceilometer_access.log combined&lt;/VirtualHost&gt;WSGISocketPrefix /var/run/httpd 重启httpdsystemctl reload httpd.service设置服务开机启动systemctl enable openstack-ceilometer-notification.service \openstack-ceilometer-central.service \openstack-ceilometer-collector.service 启动进程systemctl start openstack-ceilometer-notification.service \openstack-ceilometer-central.service \openstack-ceilometer-collector.service 配置glance的ceilometer统计vim /etc/glance/glance-api.conf 12345678910[DEFAULT]rpc_backend = rabbit[oslo_messaging_amqp]driver = messagingv2[oslo_messaging_rabbit]rabbit_host = 192.168.4.6rabbit_userid = openstackrabbit_password = 123456 重启进程systemctl restart openstack-glance-api.service openstack-glance-registry.service 配置nova的ceilometer统计安装软件yum install openstack-ceilometer-compute python-ceilometerclient python-pecan 123456789101112131415161718192021222324252627282930[DEFAULT]rpc_backend = rabbitauth_strategy = keystone[oslo_messaging_rabbit]rabbit_host = 192.168.4.6rabbit_userid = openstackrabbit_password = 123456[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = ceilometerpassword = 123456 #将密码替换成keystone创建ceilometer用户时设置的密码[service_credentials]auth_url = http://192.168.4.6:5000project_domain_id = defaultuser_domain_id = defaultauth_type = passwordusername = ceilometerproject_name = servicepassword = 123456 #将密码替换成keystone创建ceilometer用户时设置的密码interface = internalURLregion_name = RegionOne 修改nova-compute配置文件 vim /etc/nova/nova.conf 1234567[DEFAULT]instance_usage_audit = Trueinstance_usage_audit_period = hournotify_on_state_change = vm_and_task_state[oslo_messaging_amqp]driver = messagingv2 设置开机启动systemctl enable openstack-ceilometer-compute.service启动ceilometer-compute进程systemctl start openstack-ceilometer-compute.service重启nova-computesystemctl restart openstack-nova-compute.service 配置块设备使用ceilometer计量服务 验证ceilometer meter-list 正常情况下是会出现如上图一些资源的数据的，但我这里默认报 打debug 访问被拒绝解决办法：修改httpd.conf systemctl restart httpd在测试应该没问题了。 Aodh报警服务：创件aodh数据库create database aodh; 授权123GRANT ALL PRIVILEGES ON aodh.* TO &apos;aodh&apos;@&apos;localhost&apos; identified by &apos;123456&apos;; GRANT ALL PRIVILEGES ON aodh.* TO &apos;aodh&apos;@&apos;%&apos; identified by &apos;123456&apos;; GRANT ALL PRIVILEGES ON aodh.* TO &apos;aodh&apos;@&apos;control-node1.novalocal&apos; identified by &apos;123456&apos;; 创建用户openstack user create --domain default --password-prompt aodh 给adoh帐户添加admin权限openstack role add --project service --user aodh admin 添加服务openstack service create --name aodh --description &quot;Telemetry&quot; alarming 创建endpoint123openstack endpoint create --region RegionOne alarming public http://192.168.122.2:8042 openstack endpoint create --region RegionOne alarming internal http://192.168.4.6:8042 openstack endpoint create --region RegionOne alarming admin http://192.168.4.6:8042 安装软件yum install openstack-aodh-api \openstack-aodh-evaluator \openstack-aodh-notifier \openstack-aodh-listener \openstack-aodh-expirer \python-aodhclient 修改配置文件 vim /etc/aodh/aodh.conf12345678910111213141516171819202122232425262728[DEFAULT]transport_url = rabbit://openstack:123456@192.168.4.6auth_strategy = keystone[database]connection = mysql+pymysql://aodh:123456@192.168.4.6/aodh[keystone_authtoken]auth_uri = http://192.168.4.6:5000auth_url = http://192.168.4.6:35357memcached_servers = 192.168.4.6:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = aodhpassword = 123456 #填写通过keystone创建帐户时设置的帐号和密码[service_credentials]auth_type = passwordauth_url = http://192.168.4.6:5000/v3project_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = aodhpassword = 123456 #填写通过keystone创建帐户时设置的帐号和密码interface = internalURLregion_name = RegionOne systemctl enable openstack-aodh-api.service \openstack-aodh-evaluator.service \openstack-aodh-notifier.service \openstack-aodh-listener.service systemctl start openstack-aodh-api.service \openstack-aodh-evaluator.service \openstack-aodh-notifier.service \openstack-aodh-listener.service]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Fastnetmon配置与使用]]></title>
      <url>%2F2016%2F12%2F09%2Ffastnetmon%2F</url>
      <content type="text"><![CDATA[Fastnetmon介绍FastNetmon是一个基于多种抓包引擎来对数据包进行统计分析的DOS/DDOS工具，可以探测和分析网络中的异常流量情况，同时，也可以对捕获的异常调用外部脚本进行处理警报啊或者进行阻断处理，全靠外部脚本是如何定义。 项目首页http://www.open-open.com/lib/view/home/143193107973 部署架构 安装和配置方法安装 下载自动安装脚本wget https://raw.githubusercontent.com/FastVPSEestiOu/fastnetmon/master/src/fastnetmon_install.pl -Ofastnetmon_install.pl 将整个项目克隆下来https://github.com/FastVPSEestiOu/fastnetmon 安装注意若安装报连接错误，请将DNS改为8.8.8.8或223.5.5.5，因为下载地址都是在国外，有些DNS可能解析不够好。perl fastnetmon_install.pl 启动安装脚本。 配置 配置文件，cd到你刚刚克隆的项目里面。cp src/Fastnetmon.conf 到/etc/cp src/fastnetmon_init_script_centos6 /etc/init.d/fastnetmon #cp启动脚本到/etc/init.d/chmod 755 /etc/init.d/fastnetmon #修改启动脚本权限cp src/notify_about_attack.sh /usr/local/bin/ #cp 通知脚本 /etc/fastentmon配置项ban_time = 1900 #对检测到攻击的ip进行多久封锁。enable_subnet_counters = on #检测每个子网进出流量enable_connection_tracking = on #开启攻击追踪检测，通过这个选项在日志文件里可以详细看见攻击者ip和其他一些详细情况ban_for_pps = on，ban_for_bandwidth = on，ban_for_flows = on #检测的选项pps(每秒包)，bandwidth(带宽)，flows(流量)。threshold_pps = 20000，threshold_mbps = 1000，threshold_flows = 3500 #监控的限定值抓包引擎选择mirrof=off没有安装PF_RING就不要开启，不然会启动报错。pfring_sampling_ratio = 1 #端口镜像采样率mirror_netmap = off没有安装Netmap就不要开启，不然会会启动报错。mirror_snabbswitch=on #开启snabbswitch流量捕获。mirror_afpacket =on #AF_PACKET捕获引擎开启netmap_sampling_ratio = 1 #端口镜像抽样比pcap=on #pcap引擎开启netflopw=on #使用Netflow捕获方法interfaces=enp5s0f1 #监控的端口，我这里使用的是镜像端口,不然监控不到整个网端流量notify_script_path=/usr/local/bin/notify_about_attack.sh #触发脚本位置monitor_local_ip_addresses = on #监控本地地址sort_parameter = packets #在控制台排序单位，包max_ips_in_list =400 #在控制台显示多少地址 编辑要监控的网段vim /etc/networks_list #编辑networks_list加入要监控的网段这里我加入211.156.182.0/24 220.242.2.0/24/etc/init.d/fastnetmon start #启动fastentmon，启动失败查看/var/log/fastnetmon.log/opt/fastnetmon/fastnetmon #打开监控控制台 修改监控脚本1,要先外发邮件必须配置mailx 安装mailx，配置SMTP vim /etc/mail.rc 我这里配置的是我163的邮箱set bsdcompatset from=xxxxx@163.com smtp=smtp.163.comset smpt-auth-user=xxxx@163.com set smtp-auth-user=xxxxxx@163.com smtp-auth-password=xxxxxx smtp-auth=login vim /usr/local/bin/notify_about_attack.sh #这里填要触发监控的脚本修改邮件地址为接受人。cat | mail -s “FastNetMon Guard: IP $1 blocked because $2 attack with power $3 pps” $email_notify; 测试使用低轨道等离子炮（Loic）进行测试这里我对我们211.156.182.143进行Tcp DDOS攻击我们先把这些值调低threshold_pps = 2000，threshold_mbps = 100，threshold_flows = 350 #监控的限定值然后重启Fastnetmon 测试完修改回值]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hello]]></title>
      <url>%2F2016%2F12%2F06%2Fhello%2F</url>
      <content type="text"><![CDATA[你好,欢迎来到我的个人技术博客.]]></content>
    </entry>

    
  
  
</search>
